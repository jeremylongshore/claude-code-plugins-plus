{
  "skills": [
    {
      "slug": "000-jeremy-content-consistency-validator",
      "name": "000-jeremy-content-consistency-validator",
      "description": "Validate messaging consistency across website, GitHub repos, and local documentation generating read-only discrepancy reports. Use when checking content alignment or finding mixed messaging. Trigger with phrases like \"check consistency\", \"validate documentation\", or \"audit messaging\".",
      "allowedTools": [
        "Read",
        "WebFetch",
        "WebSearch",
        "Grep",
        "Bash(diff:*)",
        "Bash(grep:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "**CRITICAL OPERATING PARAMETERS:**\n- **Temperature: 0.0** - ZERO creativity. Pure factual analysis only.\n- **Read-only** - Report discrepancies, never suggest creative fixes\n- **Exact matching** - Report differences precisely as found\n- **No interpretation** - Facts only, no opinions\n\n**WORKFLOW MANDATE:**\n- Website = OFFICIAL source of truth\n- Local docs (SOPs, standards, principles, beliefs) MUST match website\n- Report what internal docs are missing compared to published website\n\n## What This Skill Does\n\nThis skill performs comprehensive **read-only validation** of messaging consistency across three critical content sources:\n\n1. **Website Content** (ANY HTML site: WordPress, Hugo, Astro, Next.js, static HTML, etc.) - **OFFICIAL SOURCE OF TRUTH**\n2. **GitHub Repositories** (README files, technical documentation)\n3. **Local Documentation** (SOPs, standards, principles, beliefs, training materials, internal docs, procedures)\n\n**CRITICAL: This skill NEVER makes changes.** It only generates detailed discrepancy reports for human review.\n\n## When This Skill Activates\n\nTrigger this skill when you mention:\n- \"Check consistency between website and GitHub\"\n- \"Validate documentation consistency\"\n- \"Audit messaging across platforms\"\n- \"Find mixed messaging\"\n- \"Before I update internal docs, check website first\"\n- \"Ensure website matches GitHub\"\n- \"Generate consistency report\"\n\n## How It Works\n\n### Phase 1: Source Discovery\n\n1. **Identify Website Sources**\n   - Detect and analyze ANY HTML-based website:\n     - Static HTML sites (index.html, about.html)\n     - Hugo/Astro static site generators\n     - Jekyll/GitHub Pages sites\n     - WordPress sites (wp-content/)\n     - Next.js/React sites (build/, out/, .next/)\n     - Vue/Nuxt sites (dist/, .nuxt/)\n     - Gatsby sites (public/)\n     - 11ty/Eleventy sites (_site/)\n     - Docusaurus sites (build/)\n     - Any other HTML-based website structure\n   - Find marketing pages, landing pages, product descriptions\n   - Extract key messaging: taglines, value propositions, feature lists\n\n2. **Identify GitHub Sources**\n   - Locate relevant repositories\n   - Find README.md, CONTRIBUTING.md, documentation folders\n   - Extract: project descriptions, feature claims, installation instructions\n\n3. **Identify Local Documentation**\n   - Find internal docs, training materials, SOPs\n   - Locate claudes-docs/, docs/, internal/ directories\n   - Extract: procedures, guidelines, technical specifications\n\n### Phase 2: Content Extraction\n\nFor each source, extract:\n- **Core messaging** (mission statements, value propositions)\n- **Feature descriptions** (what the product/service does)\n- **Version numbers** (software versions, release dates)\n- **URLs and links** (external references, documentation links)\n- **Contact information** (emails, support channels)\n- **Technical specifications** (requirements, dependencies)\n- **Terminology** (consistent use of product names, technical terms)\n\n### Phase 3: Consistency Analysis\n\nCompare content across sources and identify:\n\n**üî¥ Critical Discrepancies:**\n- Conflicting version numbers\n- Different feature lists\n- Contradictory technical requirements\n- Mismatched contact information\n- Broken cross-references\n\n**üü° Warning-Level Issues:**\n- Inconsistent terminology (e.g., \"plugin\" vs \"extension\")\n- Different phrasing of same concept\n- Missing information in one source\n- Outdated timestamps or dates\n\n**üü¢ Informational Notes:**\n- Stylistic differences (acceptable)\n- Platform-specific variations (expected)\n- Different levels of detail (appropriate)\n\n### Phase 4: Generate Discrepancy Report\n\nCreate a comprehensive Markdown report with:\n\n```markdown\n# Content Consistency Validation Report\nGenerated: [timestamp]\n\n## Executive Summary\n- Total sources analyzed: X\n- Critical discrepancies: X\n- Warnings: X\n- Informational notes: X\n\n## 1. Website vs GitHub Discrepancies\n\n### üî¥ CRITICAL: Version Mismatch\n**Website says:** v1.2.0\n**GitHub says:** v1.2.1\n**Location:**\n- Website: /about/index.html:45\n- GitHub: README.md:12\n**Recommendation:** Update website to reflect v1.2.1\n\n### üü° WARNING: Feature Description Inconsistency\n**Website says:** \"Supports 236 plugins\"\n**GitHub says:** \"Over 230 plugins available\"\n**Impact:** Potential customer confusion\n**Recommendation:** Standardize on exact number\n\n## 2. Website vs Local Docs Discrepancies\n\n### üî¥ CRITICAL: Contact Email Mismatch\n**Website says:** support@example.com\n**Local docs say:** help@example.com\n**Training materials:** Support email is support@example.com\n**Recommendation:** Update local docs to support@example.com\n\n## 3. GitHub vs Local Docs Discrepancies\n\n### üü° WARNING: Installation Instructions Differ\n**GitHub:** \"Run npm install\"\n**Local docs:** \"Use pnpm install\"\n**Impact:** Training may teach wrong commands\n**Recommendation:** Synchronize to pnpm install\n\n## 4. Terminology Consistency Issues\n\n| Term Used | Website | GitHub | Local Docs | Recommendation |\n|-----------|---------|--------|------------|----------------|\n| Plugin/Extension | Plugin | Extension | Plugin | Standardize on \"Plugin\" |\n| Marketplace/Repository | Marketplace | Repository | Marketplace | Standardize on \"Marketplace\" |\n\n## 5. Action Items (Priority Order)\n\n1. üî¥ Update website version to v1.2.1\n2. üî¥ Fix contact email in local docs\n3. üü° Standardize plugin count messaging\n4. üü° Align installation instructions\n5. üü¢ Standardize terminology usage\n```\n\n## Validation Workflow Example\n\n**User:** \"Before I update my internal training materials, check if my website matches GitHub\"\n\n**Skill Actions:**\n1. Scans website for core messaging, features, version\n2. Scans GitHub README, docs for same information\n3. Extracts current training materials content\n4. Compares all three sources\n5. Generates detailed discrepancy report\n6. Highlights critical issues that must be fixed first\n7. Provides specific file locations and line numbers\n\n**Output:** Comprehensive report showing exactly what's inconsistent and where to fix it\n\n## Best Practices\n\n### Source Priority (Use This When Conflicts Exist)\n\n**Trust Priority Order:**\n1. **Website** - Public-facing, most authoritative\n2. **GitHub** - Developer-facing, technical accuracy\n3. **Local Docs** - Internal-use, lowest priority for public messaging\n\n**Update Flow:**\nWebsite ‚Üí GitHub ‚Üí Local Docs\n\n### When to Run Validation\n\n‚úÖ **Run validation BEFORE:**\n- Updating internal documentation\n- Creating training materials\n- Writing new marketing content\n- Publishing blog posts\n- Releasing new versions\n\n‚úÖ **Run validation AFTER:**\n- Website updates\n- GitHub README changes\n- Major feature releases\n- Rebranding efforts\n\n### What This Skill Does NOT Do\n\n‚ùå Does NOT automatically fix issues\n‚ùå Does NOT modify any files\n‚ùå Does NOT make content decisions\n‚ùå Does NOT prioritize which version is \"correct\"\n‚úÖ ONLY generates read-only reports for human review\n\n## Integration with Your Workflow\n\n### Scenario: Pre-Update Validation\n\n**You:** \"I need to update our internal SOPs. First, validate consistency with the website.\"\n\n**Skill Response:**\n1. Reads current website content\n2. Reads current GitHub documentation\n3. Reads existing internal SOPs\n4. Generates comparison report\n5. Shows you exactly what needs updating in SOPs\n6. Identifies messaging that website uses but SOPs don't\n\n**Result:** You update SOPs with confidence, knowing they match public messaging\n\n### Scenario: Post-Website Update\n\n**You:** \"I just updated the website pricing page. Check if GitHub and docs are now inconsistent.\"\n\n**Skill Response:**\n1. Reads NEW website pricing information\n2. Compares to GitHub repository pricing docs\n3. Compares to internal sales training materials\n4. Flags any discrepancies created by website update\n5. Provides checklist of what to update next\n\n**Result:** Prevents mixed messaging cascade\n\n## Technical Implementation\n\n### Read-Only Tools Used\n\n- `Read` - Reads local files (website, docs, SOPs)\n- `Glob` - Finds relevant files by pattern\n- `Grep` - Searches for specific terms across files\n- `WebFetch` - Reads deployed website pages (if needed)\n- `Bash` (read-only) - Uses `cat`, `grep`, `find` for analysis\n\n### NO Write Operations\n\nThis skill NEVER uses:\n- ‚ùå `Write` tool\n- ‚ùå `Edit` tool\n- ‚ùå `git commit` commands\n- ‚ùå File modification operations\n\n### Output Format\n\n- Markdown report saved to `consistency-reports/YYYY-MM-DD-HH-MM-SS.md`\n- Terminal-friendly summary\n- Export to JSON for automation (optional)\n\n## Example Use Cases\n\n### Use Case 1: Version Consistency Check\n\n**Trigger:** \"Check if all docs mention the same version number\"\n\n**Result:**\n```\nVersion Analysis Report\nWebsite: v1.2.1 (5 mentions)\nGitHub: v1.2.1 (3 mentions), v1.2.0 (2 mentions) ‚ö†Ô∏è\nLocal Docs: v1.2.0 (8 mentions) üî¥\n\nAction: Update Local Docs to v1.2.1\n```\n\n### Use Case 2: Feature Claim Validation\n\n**Trigger:** \"Validate that all platforms claim the same features\"\n\n**Result:**\n```\nFeature Consistency Analysis\n\"236 plugins\": Website ‚úÖ, GitHub ‚úÖ, Docs ‚ùå (says \"230+\")\n\"Agent Skills\": Website ‚úÖ, GitHub ‚úÖ, Docs ‚úÖ\n\"MCP Support\": Website ‚úÖ, GitHub ‚úÖ, Docs ‚ö†Ô∏è (unclear mention)\n\nAction: Update Docs to specify \"236 plugins\" and clarify MCP support\n```\n\n### Use Case 3: Pre-Training Update\n\n**Trigger:** \"Before I update training materials, what's changed on the website?\"\n\n**Result:**\n```\nWebsite Changes Since Last Training Update (Oct 15)\n- New feature added: \"Skill Enhancers\" (not in training)\n- Pricing updated: $39/mo ‚Üí $49/mo (not in training)\n- Contact form URL changed (broken link in training)\n\nSuggested Training Updates:\n1. Add Skill Enhancers section\n2. Update pricing screenshots\n3. Fix contact form URL\n```\n\n## Integration Points\n\nWorks seamlessly with:\n- **All HTML-based websites**: Static HTML, Hugo, Astro, Jekyll, WordPress, Next.js, React, Vue, Nuxt, Gatsby, 11ty, Docusaurus, and more\n- **GitHub repositories**: README files, documentation, code comments\n- **Local markdown documentation**: Internal docs, training materials\n- **Internal wikis and knowledge bases**: Confluence, Notion exports, custom wikis\n- **Content management systems**: WordPress, Drupal, custom CMS\n- **Static site generators**: Hugo, Jekyll, 11ty, Gatsby, Astro, Docusaurus\n- **Modern web frameworks**: Next.js, Nuxt, SvelteKit build outputs\n\n## Report Storage\n\nReports saved to:\n```\nconsistency-reports/\n‚îú‚îÄ‚îÄ 2025-10-23-10-30-45-full-audit.md\n‚îú‚îÄ‚îÄ 2025-10-22-15-20-12-website-github.md\n‚îî‚îÄ‚îÄ 2025-10-20-09-15-33-docs-sync.md\n```\n\n## Expected Activation Patterns\n\n**Natural Language Triggers:**\n- \"Check consistency\"\n- \"Validate documentation\"\n- \"Audit messaging\"\n- \"Find discrepancies\"\n- \"Compare website to GitHub\"\n- \"Before I update X, check Y\"\n- \"What's out of sync?\"\n\n**Context-Aware Activation:**\n- When user is about to update documentation\n- When user asks about version consistency\n- When user mentions \"mixed messaging\"\n- When user is preparing training materials\n\n## Prerequisites\n\n- Access to website content (local build or deployed site)\n- Access to GitHub repositories\n- Local documentation in {baseDir}/docs/ or claudes-docs/\n- WebFetch permissions for remote content\n\n## Instructions\n\n1. Identify and discover all content sources (website, GitHub, local docs)\n2. Extract key messaging, features, versions from each source\n3. Compare content systematically across sources\n4. Identify critical discrepancies, warnings, and informational notes\n5. Generate comprehensive Markdown report\n6. Provide prioritized action items for consistency fixes\n\n## Output\n\n- Comprehensive consistency validation report in Markdown format\n- Executive summary with discrepancy counts by severity\n- Detailed comparison by source pairs (website vs GitHub, etc.)\n- Terminology consistency matrix\n- Prioritized action items with file locations and line numbers\n- Reports saved to consistency-reports/YYYY-MM-DD-HH-MM-SS.md\n\n## Error Handling\n\nIf validation fails:\n- Verify website accessibility (local or deployed)\n- Check GitHub repository permissions\n- Validate local documentation paths\n- Ensure WebFetch permissions configured\n- Review content extraction patterns\n\n## Resources\n\n- Content consistency best practices\n- Documentation style guides\n- Version control strategies for content\n- Multi-platform content management approaches",
      "parentPlugin": {
        "name": "000-jeremy-content-consistency-validator",
        "category": "productivity",
        "path": "plugins/productivity/000-jeremy-content-consistency-validator",
        "version": "1.0.0",
        "description": "Read-only validator that generates comprehensive discrepancy reports comparing messaging consistency across ANY HTML-based website (WordPress, Hugo, Next.js, React, Vue, static HTML, etc.), GitHub repositories, and local documentation. Detects mixed messaging without making changes."
      },
      "filePath": "plugins/productivity/000-jeremy-content-consistency-validator/skills/000-jeremy-content-consistency-validator/SKILL.md"
    },
    {
      "slug": "adapting-transfer-learning-models",
      "name": "adapting-transfer-learning-models",
      "description": "This skill automates the adaptation of pre-trained machine learning models",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill streamlines the process of adapting pre-trained machine learning models via transfer learning. It enables you to quickly fine-tune models for specific tasks, saving time and resources compared to training from scratch. It handles the complexities of model adaptation, data validation, and performance optimization.\n\n## How It Works\n\n1. **Analyze Requirements**: Examines the user's request to understand the target task, dataset characteristics, and desired performance metrics.\n2. **Generate Adaptation Code**: Creates Python code using appropriate ML frameworks (e.g., TensorFlow, PyTorch) to fine-tune the pre-trained model on the new dataset. This includes data preprocessing steps and model architecture modifications if needed.\n3. **Implement Validation and Error Handling**: Adds code to validate the data, monitor the training process, and handle potential errors gracefully.\n4. **Provide Performance Metrics**: Calculates and reports key performance indicators (KPIs) such as accuracy, precision, recall, and F1-score to assess the model's effectiveness.\n5. **Save Artifacts and Documentation**: Saves the adapted model, training logs, performance metrics, and automatically generates documentation outlining the adaptation process and results.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Fine-tune a pre-trained model for a specific task.\n- Adapt a pre-trained model to a new dataset.\n- Perform transfer learning to improve model performance.\n- Optimize an existing model for a particular application.\n\n## Examples\n\n### Example 1: Adapting a Vision Model for Image Classification\n\nUser request: \"Fine-tune a ResNet50 model to classify images of different types of flowers.\"\n\nThe skill will:\n1. Download the ResNet50 model and load a flower image dataset.\n2. Generate code to fine-tune the model on the flower dataset, including data augmentation and optimization techniques.\n\n### Example 2: Adapting a Language Model for Sentiment Analysis\n\nUser request: \"Adapt a BERT model to perform sentiment analysis on customer reviews.\"\n\nThe skill will:\n1. Download the BERT model and load a dataset of customer reviews with sentiment labels.\n2. Generate code to fine-tune the model on the review dataset, including tokenization, padding, and attention mechanisms.\n\n## Best Practices\n\n- **Data Preprocessing**: Ensure data is properly preprocessed and formatted to match the input requirements of the pre-trained model.\n- **Hyperparameter Tuning**: Experiment with different hyperparameters (e.g., learning rate, batch size) to optimize model performance.\n- **Regularization**: Apply regularization techniques (e.g., dropout, weight decay) to prevent overfitting.\n\n## Integration\n\nThis skill can be integrated with other plugins for data loading, model evaluation, and deployment. For example, it can work with a data loading plugin to fetch datasets and a model deployment plugin to deploy the adapted model to a serving infrastructure.",
      "parentPlugin": {
        "name": "transfer-learning-adapter",
        "category": "ai-ml",
        "path": "plugins/ai-ml/transfer-learning-adapter",
        "version": "1.0.0",
        "description": "Transfer learning adaptation"
      },
      "filePath": "plugins/ai-ml/transfer-learning-adapter/skills/transfer-learning-adapter/SKILL.md"
    },
    {
      "slug": "adk-deployment-specialist",
      "name": "adk-deployment-specialist",
      "description": "Deploy and orchestrate Vertex AI ADK agents using A2A protocol. Manages",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## What This Skill Does\n\nExpert in building and deploying production multi-agent systems using Google's Agent Development Kit (ADK). Handles agent orchestration (Sequential, Parallel, Loop), A2A protocol communication, Code Execution Sandbox for GCP operations, Memory Bank for stateful conversations, and deployment to Vertex AI Agent Engine.\n\n### Core Capabilities\n\n1. **ADK Agent Creation**: Build agents in Python (stable), Java (0.3.0), or Go (Nov 2025)\n2. **Multi-Agent Orchestration**: Sequential/Parallel/Loop agent patterns\n3. **A2A Protocol Management**: Agent-to-Agent communication and task delegation\n4. **Code Execution**: Secure sandbox for running gcloud commands and Python/Go code\n5. **Memory Bank**: Persistent conversation memory across sessions (14-day TTL)\n6. **Production Deployment**: One-command deployment with `adk deploy`\n7. **Observability**: Agent Engine UI dashboards, token tracking, error monitoring\n\n## When This Skill Activates\n\n### Trigger Phrases\n- \"Deploy ADK agent to Agent Engine\"\n- \"Create multi-agent system with ADK\"\n- \"Implement A2A protocol\"\n- \"Use Code Execution Sandbox\"\n- \"Set up Memory Bank for agent\"\n- \"Orchestrate multiple agents\"\n- \"Build ADK agent in Python/Java/Go\"\n- \"Deploy to Vertex AI Agent Engine\"\n\n### Use Case Patterns\n- Building GCP deployment automation agents\n- Creating RAG agents with LangChain integration\n- Orchestrating Genkit flows with ADK supervisors\n- Implementing stateful conversational agents\n- Deploying secure code execution environments\n\n## How It Works\n\n### Phase 1: Agent Architecture Design\n\n```\nUser Request ‚Üí Analyze:\n- Single agent vs multi-agent system?\n- Tools needed (Code Exec, Memory Bank, custom tools)?\n- Orchestration pattern (Sequential, Parallel, Loop)?\n- Integration with LangChain/Genkit?\n- Deployment target (local, Agent Engine, Cloud Run)?\n```\n\n### Phase 2: ADK Agent Implementation\n\n**Simple Agent (Python)**:\n```python\nfrom google import adk\n\n# Define agent with tools\nagent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[\n        adk.tools.CodeExecution(),  # Secure sandbox\n        adk.tools.MemoryBank(),     # Persistent memory\n    ],\n    system_instruction=\"\"\"\nYou are a GCP deployment specialist.\nHelp users deploy resources securely using gcloud commands.\n    \"\"\"\n)\n\n# Run agent\nresponse = agent.run(\"Deploy a GKE cluster named prod in us-central1\")\nprint(response)\n```\n\n**Multi-Agent Orchestrator (Python)**:\n```python\nfrom google import adk\n\n# Define specialized sub-agents\nvalidator_agent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    system_instruction=\"Validate GCP configurations\"\n)\n\ndeployer_agent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[adk.tools.CodeExecution()],\n    system_instruction=\"Deploy validated GCP resources\"\n)\n\nmonitor_agent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    system_instruction=\"Monitor deployment status\"\n)\n\n# Orchestrate with Sequential pattern\norchestrator = adk.SequentialAgent(\n    agents=[validator_agent, deployer_agent, monitor_agent],\n    system_instruction=\"Coordinate validation ‚Üí deployment ‚Üí monitoring\"\n)\n\nresult = orchestrator.run(\"Deploy a production GKE cluster\")\n```\n\n### Phase 3: Code Execution Integration\n\nThe Code Execution Sandbox provides:\n- **Security**: Isolated environment, no access to your system\n- **State Persistence**: 14-day memory, configurable TTL\n- **Stateful Sessions**: Builds on previous executions\n\n```python\n# Agent with Code Execution\nagent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[adk.tools.CodeExecution()],\n    system_instruction=\"\"\"\nExecute gcloud commands in the secure sandbox.\nRemember previous operations in this session.\n    \"\"\"\n)\n\n# Turn 1: Create cluster\nagent.run(\"Create GKE cluster named dev-cluster with 3 nodes\")\n# Sandbox executes: gcloud container clusters create dev-cluster --num-nodes=3\n\n# Turn 2: Deploy to cluster (remembers cluster from Turn 1)\nagent.run(\"Deploy my-app:latest to that cluster\")\n# Sandbox remembers dev-cluster, executes kubectl commands\n```\n\n### Phase 4: Memory Bank Integration\n\nPersistent conversation memory across sessions:\n\n```python\nagent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[adk.tools.MemoryBank()],\n    system_instruction=\"Remember user preferences and project context\"\n)\n\n# Session 1 (Monday)\nagent.run(\"I prefer deploying to us-central1 region\", session_id=\"user-123\")\n\n# Session 2 (Wednesday) - same session_id\nagent.run(\"Deploy a Cloud Run service\", session_id=\"user-123\")\n# Agent remembers: uses us-central1 automatically\n```\n\n### Phase 5: A2A Protocol Deployment\n\nDeploy agent to Agent Engine with A2A endpoint:\n\n```bash\n# Install ADK\npip install google-adk\n\n# Deploy with one command\nadk deploy \\\n  --agent-file agent.py \\\n  --project-id my-project \\\n  --region us-central1 \\\n  --service-name gcp-deployer-agent\n```\n\nAgent Engine creates:\n- **A2A Endpoint**: `https://gcp-deployer-agent-{hash}.run.app`\n- **AgentCard**: `/.well-known/agent-card` metadata\n- **Task API**: `/v1/tasks:send` for task submission\n- **Status API**: `/v1/tasks/{task_id}` for polling\n\n### Phase 6: Calling from Claude\n\nOnce deployed, Claude can invoke via A2A protocol:\n\n```python\n# In Claude Code plugin / external script\nimport requests\n\ndef invoke_adk_agent(message, session_id=None):\n    \"\"\"\n    Call deployed ADK agent via A2A protocol.\n    \"\"\"\n    response = requests.post(\n        \"https://gcp-deployer-agent-xyz.run.app/v1/tasks:send\",\n        json={\n            \"message\": message,\n            \"session_id\": session_id or \"claude-session-123\",\n            \"config\": {\n                \"enable_code_execution\": True,\n                \"enable_memory_bank\": True,\n            }\n        },\n        headers={\"Authorization\": f\"Bearer {get_token()}\"}\n    )\n\n    return response.json()\n\n# Use from Claude\nresult = invoke_adk_agent(\"Deploy GKE cluster named prod-api\")\n```\n\n## Workflow Examples\n\n### Example 1: GCP Deployment Agent\n\n**User**: \"Create an ADK agent that deploys GCP resources\"\n\n**Implementation**:\n```python\nfrom google import adk\n\ndeployment_agent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[\n        adk.tools.CodeExecution(),\n        adk.tools.MemoryBank(),\n    ],\n    system_instruction=\"\"\"\nYou are a GCP deployment specialist.\n\nCAPABILITIES:\n- Deploy GKE clusters\n- Deploy Cloud Run services\n- Deploy Vertex AI Pipelines\n- Manage IAM permissions\n- Monitor deployments\n\nSECURITY:\n- Validate all configurations before deployment\n- Use least-privilege IAM\n- Log all operations\n- Never expose credentials\n    \"\"\"\n)\n\n# Deploy to Agent Engine\n# $ adk deploy --agent-file deployment_agent.py --service-name gcp-deployer\n```\n\n### Example 2: Multi-Agent RAG System\n\n**User**: \"Build a RAG system with ADK orchestrating a LangChain retriever\"\n\n**Implementation**:\n```python\nfrom google import adk\nfrom langchain.retrievers import VertexAISearchRetriever\n\n# Sub-Agent 1: LangChain RAG\nclass RAGAgent(adk.Agent):\n    def __init__(self):\n        self.retriever = VertexAISearchRetriever(...)\n        super().__init__(model=\"gemini-2.5-flash\")\n\n    def retrieve_docs(self, query):\n        return self.retriever.get_relevant_documents(query)\n\n# Sub-Agent 2: ADK Answer Generator\nanswer_agent = adk.Agent(\n    model=\"gemini-2.5-pro\",  # More powerful for final answer\n    system_instruction=\"Generate comprehensive answers from retrieved docs\"\n)\n\n# Orchestrator\norchestrator = adk.SequentialAgent(\n    agents=[RAGAgent(), answer_agent],\n    system_instruction=\"First retrieve docs, then generate answer\"\n)\n```\n\n### Example 3: Async Deployment with Status Polling\n\n**User**: \"Deploy a GKE cluster and monitor progress\"\n\n**Implementation**:\n```python\n# Submit async task\ntask_response = invoke_adk_agent(\n    \"Deploy GKE cluster named prod-api with 5 nodes in us-central1\"\n)\n\ntask_id = task_response[\"task_id\"]\nprint(f\"‚úÖ Task submitted: {task_id}\")\n\n# Poll for status\nimport time\nwhile True:\n    status = requests.get(\n        f\"https://gcp-deployer-agent-xyz.run.app/v1/tasks/{task_id}\",\n        headers={\"Authorization\": f\"Bearer {get_token()}\"}\n    ).json()\n\n    if status[\"status\"] == \"SUCCESS\":\n        print(f\"‚úÖ Cluster deployed!\")\n        break\n    elif status[\"status\"] == \"FAILURE\":\n        print(f\"‚ùå Deployment failed: {status['error']}\")\n        break\n    else:\n        print(f\"‚è≥ Status: {status['status']} ({status.get('progress', 0)*100}%)\")\n        time.sleep(10)\n```\n\n## Production Best Practices\n\n1. **Agent Identities**: ADK agents get Native Agent Identities (IAM principals)\n2. **Least Privilege**: Grant minimum required permissions\n3. **VPC Service Controls**: Enable for enterprise security\n4. **Model Armor**: Protects against prompt injection\n5. **Session Management**: Use consistent session_ids for Memory Bank\n6. **Error Handling**: Implement retries with exponential backoff\n7. **Observability**: Monitor via Agent Engine UI dashboard\n\n## Tool Permissions\n\n- **Read**: Analyze existing agent code\n- **Write**: Create new agent files\n- **Edit**: Modify agent configurations\n- **Grep**: Find integration points\n- **Glob**: Locate related files\n- **Bash**: Install ADK, deploy agents, run tests\n\n## Integration Patterns\n\n### ADK + Genkit\n```python\n# Use Genkit for flows, ADK for orchestration\ngenkit_flow_agent = create_genkit_flow()\norchestrator = adk.SequentialAgent(\n    agents=[validator, genkit_flow_agent, monitor]\n)\n```\n\n### ADK + LangChain\n```python\n# LangChain for RAG, ADK for multi-agent coordination\nlangchain_rag = create_langchain_retriever()\norchestrator = adk.ParallelAgent(\n    agents=[langchain_rag, fact_checker, answer_generator]\n)\n```\n\n## Deployment Commands\n\n```bash\n# Install ADK\npip install google-adk  # Python\ngo get google.golang.org/adk  # Go\n\n# Deploy to Agent Engine\nadk deploy \\\n  --agent-file my_agent.py \\\n  --project-id my-project \\\n  --region us-central1 \\\n  --service-name my-agent\n\n# Deploy to Cloud Run (custom)\ngcloud run deploy my-agent \\\n  --source . \\\n  --region us-central1\n\n# Deploy locally for testing\nadk run --agent-file my_agent.py\n```\n\n## Version History\n\n- **1.0.0** (2025): ADK Preview with Python/Java/Go support, Agent Engine GA, Code Execution Sandbox, Memory Bank\n\n## References\n\n- ADK Docs: https://google.github.io/adk-docs/\n- A2A Protocol: https://google.github.io/adk-docs/a2a/\n- Agent Engine: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\n- Code Execution: https://cloud.google.com/agent-builder/agent-engine/code-execution/overview\n- Memory Bank: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/overview",
      "parentPlugin": {
        "name": "jeremy-adk-orchestrator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-adk-orchestrator",
        "version": "1.0.0",
        "description": "Production ADK orchestrator for A2A protocol and multi-agent coordination on Vertex AI"
      },
      "filePath": "plugins/ai-ml/jeremy-adk-orchestrator/skills/adk-deployment-specialist/SKILL.md"
    },
    {
      "slug": "adk-infra-expert",
      "name": "adk-infra-expert",
      "description": "Use when provisioning Vertex AI ADK infrastructure with Terraform. Trigger with phrases like \"deploy ADK terraform\", \"agent engine infrastructure\", \"provision ADK agent\", \"vertex AI agent terraform\", or \"code execution sandbox terraform\". Provisions Agent Engine runtime, 14-day code execution sandbox, Memory Bank, VPC Service Controls, IAM roles, and secure multi-agent infrastructure.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(terraform:*)",
        "Bash(gcloud:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Google Cloud project with billing enabled\n- Terraform 1.0+ installed\n- gcloud CLI authenticated with appropriate permissions\n- Vertex AI API enabled in target project\n- VPC Service Controls access policy created (for enterprise)\n- Understanding of Agent Engine architecture and requirements\n\n## Instructions\n\n1. **Initialize Terraform**: Set up backend for remote state storage\n2. **Configure Variables**: Define project_id, region, agent configuration\n3. **Provision VPC**: Create network infrastructure with Private Service Connect\n4. **Set Up IAM**: Create service accounts with least privilege roles\n5. **Deploy Agent Engine**: Configure runtime with code execution and memory bank\n6. **Enable VPC-SC**: Apply service perimeter for data exfiltration protection\n7. **Configure Monitoring**: Set up Cloud Monitoring dashboards and alerts\n8. **Validate Deployment**: Test agent endpoint and verify all components\n\n## Output\n\n**Agent Engine Deployment:**\n```hcl\n# {baseDir}/terraform/main.tf\nresource \"google_vertex_ai_agent_runtime\" \"adk_agent\" {\n  project  = var.project_id\n  location = var.region\n  display_name = \"adk-production-agent\"\n\n  agent_config {\n    model = \"gemini-2.5-flash\"\n    code_execution {\n      enabled = true\n      state_ttl_days = 14\n      sandbox_type = \"SECURE_ISOLATED\"\n    }\n    memory_bank {\n      enabled = true\n    }\n  }\n\n  vpc_config {\n    vpc_network = google_compute_network.agent_vpc.id\n    private_service_connect {\n      enabled = true\n    }\n  }\n}\n```\n\n**VPC Service Controls:**\n```hcl\nresource \"google_access_context_manager_service_perimeter\" \"adk_perimeter\" {\n  parent = \"accessPolicies/${var.access_policy_id}\"\n  title  = \"ADK Agent Engine Perimeter\"\n\n  status {\n    restricted_services = [\n      \"aiplatform.googleapis.com\",\n      \"run.googleapis.com\"\n    ]\n  }\n}\n```\n\n**IAM Configuration:**\n```hcl\nresource \"google_service_account\" \"adk_agent\" {\n  account_id   = \"adk-agent-sa\"\n  display_name = \"ADK Agent Service Account\"\n}\n\nresource \"google_project_iam_member\" \"agent_identity\" {\n  project = var.project_id\n  role    = \"roles/aiplatform.agentUser\"\n  member  = \"serviceAccount:${google_service_account.adk_agent.email}\"\n}\n```\n\n## Error Handling\n\n**Terraform State Lock**\n- Error: \"Error acquiring the state lock\"\n- Solution: Use `terraform force-unlock <lock-id>` or wait for lock expiry\n\n**API Not Enabled**\n- Error: \"Vertex AI API has not been used\"\n- Solution: Enable with `gcloud services enable aiplatform.googleapis.com`\n\n**VPC-SC Configuration**\n- Error: \"Access denied by VPC Service Controls\"\n- Solution: Add project to service perimeter or adjust ingress/egress policies\n\n**IAM Permission Denied**\n- Error: \"does not have required permission\"\n- Solution: Grant roles/owner temporarily to service account running Terraform\n\n**Resource Already Exists**\n- Error: \"Resource already exists\"\n- Solution: Import existing resource or use data source instead\n\n## Resources\n\n- Agent Engine: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\n- VPC-SC: https://cloud.google.com/vpc-service-controls/docs\n- Terraform Google Provider: https://registry.terraform.io/providers/hashicorp/google/latest\n- ADK Terraform examples in {baseDir}/examples/",
      "parentPlugin": {
        "name": "jeremy-adk-terraform",
        "category": "devops",
        "path": "plugins/devops/jeremy-adk-terraform",
        "version": "1.0.0",
        "description": "Terraform infrastructure as code for ADK and Vertex AI Agent Engine deployments"
      },
      "filePath": "plugins/devops/jeremy-adk-terraform/skills/adk-infra-expert/SKILL.md"
    },
    {
      "slug": "agent-context-loader",
      "name": "agent-context-loader",
      "description": "Proactive auto-loading: automatically detects and loads agents.md files. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. version: 1.0.0 allowed-tools: license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore",
      "license": "MIT",
      "content": "# Agent Context Auto-Loader\n\n**‚ö° This skill activates AUTOMATICALLY - no user action required!**\n\n## Purpose\n\nThis skill makes Claude Code recognize and load `AGENTS.md` files with the same priority as `CLAUDE.md` files, enabling specialized agent-specific instructions for your projects.\n\n## How It Works\n\n### Automatic Trigger Conditions\n\nThis skill automatically activates when:\n1. **Starting a new Claude Code session** in any directory\n2. **Changing directories** during a session (via `cd` or file operations)\n3. **Any other agent skill is invoked** (ensures agent context is loaded first)\n4. **User explicitly requests**: \"load agent context\", \"check for AGENTS.md\", or \"read agent rules\"\n\n### Execution Flow\n\nWhen triggered, Claude Code will:\n\n1. **Check for AGENTS.md**: Look for `./AGENTS.md` in the current working directory\n2. **Read the file** (if it exists): Use the Read tool to load full content\n3. **Incorporate into context**: Treat AGENTS.md rules as session-level instructions\n4. **Announce loading**: Confirm with user: \"üìã Loaded agent-specific context from AGENTS.md\"\n5. **Apply for session**: Follow these rules for all subsequent operations\n\n### Priority and Conflict Resolution\n\n- **AGENTS.md supplements CLAUDE.md**: Both are active simultaneously\n- **In case of conflicts**: AGENTS.md takes precedence for agent-specific behaviors\n- **Scope**: AGENTS.md applies to agent workflows; CLAUDE.md applies to general project context\n\n## Expected Behavior\n\n### If AGENTS.md exists:\n```\nüìã Loaded agent-specific context from AGENTS.md\n\nFollowing specialized agent rules for this session:\n- [rule 1 from AGENTS.md]\n- [rule 2 from AGENTS.md]\n...\n```\n\n### If AGENTS.md doesn't exist:\n```\nNo AGENTS.md found - using standard CLAUDE.md context only\n```\n\n## User Experience\n\n**Fully Automatic** (preferred):\n- Install plugin ‚Üí AGENTS.md loads automatically ‚Üí Agent rules active ‚Üí No user action needed\n\n**Manual Invocation** (fallback):\n```bash\n# If auto-loading doesn't trigger, user can say:\n\"load agent context\"\n\"check for AGENTS.md\"\n\"read agent rules from AGENTS.md\"\n```\n\n## Implementation Details\n\n### Step 1: Check for File\n```bash\n# Claude executes internally:\nif [ -f \"./AGENTS.md\" ]; then\n    echo \"üìã AGENTS.md detected\"\nfi\n```\n\n### Step 2: Read Content\n```markdown\nUse Read tool:\nfile_path: ./AGENTS.md\n\nLoad full content into session context\n```\n\n### Step 3: Apply Rules\n```\nTreat AGENTS.md content as:\n- Session-level instructions (like CLAUDE.md)\n- Agent-specific behavioral rules\n- Overrides for agent workflows\n```\n\n## Example AGENTS.md Structure\n\n```markdown\n# AGENTS.md - Agent-Specific Instructions\n\n## Agent Behavior Rules\n\nWhen working with Agent Skills in this project:\n\n1. **Always use TypeScript strict mode** for all generated code\n2. **Never create files** without explicit user permission\n3. **Follow naming convention**: use kebab-case for all file names\n4. **Auto-commit after changes**: Create git commits automatically when tasks complete\n\n## Specialized Workflows\n\n### Code Generation\n- Use templates from `./templates/` directory\n- Run ESLint after generating any .ts/.js files\n- Add comprehensive JSDoc comments\n\n### Testing\n- Generate tests alongside implementation files\n- Use Jest for all test files\n- Achieve 80%+ code coverage\n\n## Priority Overrides\n\nThese rules override CLAUDE.md when agent skills are active:\n- AGENTS.md ‚Üí agent-specific strict rules\n- CLAUDE.md ‚Üí general project context\n```\n\n## Integration with Other Skills\n\nThis skill runs **before** other agent skills to ensure agent context is loaded first. When any other skill is invoked, this skill checks if AGENTS.md has been loaded for the current directory and loads it if not already present.\n\n## Troubleshooting\n\n**If AGENTS.md isn't loading automatically:**\n\n1. **Manual invoke**: Say \"load agent context\"\n2. **Check file location**: Ensure `AGENTS.md` is in current working directory (`pwd`)\n3. **Check file permissions**: Ensure `AGENTS.md` is readable\n4. **Use slash command**: Run `/sync-agent-context` to merge AGENTS.md into CLAUDE.md permanently\n\n## Related Features\n\n- **Slash Command**: `/sync-agent-context` - Permanently merges AGENTS.md into CLAUDE.md\n- **Hook Script**: Runs on directory change to remind Claude to load context\n- **Manual Loading**: Can always explicitly request \"load AGENTS.md\"\n\n## Benefits\n\n- **Zero configuration**: Just create `AGENTS.md` and it works\n- **Project-specific rules**: Different agent behaviors per project\n- **No CLAUDE.md pollution**: Keep agent-specific rules separate\n- **Automatic synchronization**: Always up-to-date with current directory\n\n---\n\n**Status**: Proactive Auto-Loading Enabled\n**Requires User Action**: No (automatic)\n**Fallback**: Manual invocation if auto-loading fails\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nIf execution fails:\n- Verify prerequisites are met\n- Check input parameters and formats\n- Validate file paths and permissions\n- Review error messages for root cause\n- Consult documentation for troubleshooting\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "agent-context-manager",
        "category": "productivity",
        "path": "plugins/productivity/agent-context-manager",
        "version": "1.0.0",
        "description": "Automatically detects and loads AGENTS.md files to provide agent-specific instructions alongside CLAUDE.md. Enables specialized agent behaviors without manual intervention."
      },
      "filePath": "plugins/productivity/agent-context-manager/skills/agent-context-loader/SKILL.md"
    },
    {
      "slug": "agent-patterns",
      "name": "agent-patterns",
      "description": "This skill should be used when the user asks about \"SPAWN REQUEST format\", \"agent reports\", \"agent coordination\", \"parallel agents\", \"report format\", \"agent communication\", or needs to understand how agents coordinate within the sprint system.",
      "allowedTools": [
        "Read"
      ],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Agent Patterns\n\nSprint coordinates multiple specialized agents through structured communication patterns. This skill covers the SPAWN REQUEST format, report structure, parallel execution, and inter-agent coordination.\n\n## Agent Hierarchy\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         Sprint Orchestrator         ‚îÇ\n‚îÇ    (parses requests, spawns agents) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                  ‚îÇ\n                  ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ        Project Architect            ‚îÇ\n‚îÇ  (plans, creates specs, coordinates)‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                  ‚îÇ SPAWN REQUEST\n                  ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ      Implementation & Test Agents   ‚îÇ\n‚îÇ   (execute, report, never spawn)    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Key rule**: Only the orchestrator spawns agents. Agents communicate by returning structured messages.\n\n## SPAWN REQUEST Format\n\nThe architect requests agent spawning using a specific format:\n\n```markdown\n## SPAWN REQUEST\n\n- python-dev\n- nextjs-dev\n- cicd-agent\n```\n\n### Parsing Rules\n\nThe orchestrator parses SPAWN REQUEST blocks:\n- Section must start with `## SPAWN REQUEST` on its own line\n- Each agent listed as a bullet point\n- Multiple agents spawn in parallel\n\n### Implementation Agents\n\nRequest during Phase 2:\n```markdown\n## SPAWN REQUEST\n\n- python-dev\n- nextjs-dev\n```\n\nNever include test agents in implementation requests.\n\n### Testing Agents\n\nRequest during Phase 3:\n```markdown\n## SPAWN REQUEST\n\n- qa-test-agent\n- ui-test-agent\n```\n\nThe ui-test-agent runs in AUTOMATED or MANUAL mode based on `specs.md`:\n- `UI Testing Mode: automated` (default) ‚Üí runs test scenarios automatically\n- `UI Testing Mode: manual` ‚Üí opens browser for user to test, waits for tab close\n\n## Agent Report Format\n\nAll agents return structured reports. The orchestrator saves these as files.\n\n### Standard Report Sections\n\nEvery report includes:\n\n| Section | Purpose |\n|---------|---------|\n| CONFORMITY STATUS | YES/NO - did agent follow specs? |\n| SUMMARY | Brief outcome description |\n| DEVIATIONS | Justified departures from specs |\n| FILES CHANGED | List of modified files |\n| ISSUES | Problems encountered |\n| NOTES FOR ARCHITECT | Suggestions, observations |\n\n### Example: Backend Report\n\n```markdown\n## BACKEND REPORT\n\n### CONFORMITY STATUS: YES\n\n### SUMMARY\nImplemented user authentication endpoints per api-contract.md.\n\n### FILES CHANGED\n- backend/app/routers/auth.py (new)\n- backend/app/models/user.py (modified)\n- backend/app/schemas/auth.py (new)\n- backend/alembic/versions/001_add_users.py (new)\n\n### DEVIATIONS\nNone.\n\n### ISSUES\nNone.\n\n### NOTES FOR ARCHITECT\n- Consider adding rate limiting middleware in next sprint\n- Password hashing uses bcrypt (industry standard)\n```\n\n### Example: QA Report\n\n```markdown\n## QA REPORT\n\n### SUITE STATUS\n- New test files: 2\n- Updated test files: 1\n- Test framework(s): pytest\n- Test command(s): pytest tests/api/\n\n### API CONFORMITY STATUS: YES\n\n### SUMMARY\n- Total endpoints in contract: 5\n- Endpoints covered by automated tests: 5\n- Endpoints with failing tests: 0\n\n### FAILURES AND DEVIATIONS\nNone.\n\n### TEST EXECUTION\n- Tests run: YES\n- Result: ALL PASS\n- Notes: 23 tests passed in 1.4s\n\n### NOTES FOR ARCHITECT\n- Added edge case tests for password validation\n```\n\n### Example: UI Test Report\n\n```markdown\n## UI TEST REPORT\n\n### MODE\nAUTOMATED\n\n### SUMMARY\n- Total tests run: 8\n- Passed: 7\n- Failed: 1\n- Session duration: 45s\n\n### COVERAGE\n- Scenarios covered:\n  - Login with valid credentials\n  - Login with invalid password\n  - Registration flow\n  - Password reset request\n- Not covered (yet):\n  - Email verification flow (requires email testing setup)\n\n### FAILURES\n- Scenario: Registration validation\n  - Path/URL: /register\n  - Symptom: Error message not displayed\n  - Expected: \"Email already exists\" message\n  - Actual: Form submits without feedback\n\n### CONSOLE ERRORS\nNone.\n\n### NOTES FOR ARCHITECT\n- Registration error handling needs frontend fix\n```\n\n## Parallel Execution\n\n### Implementation Agents\n\nSpawn all implementation agents simultaneously:\n\n```\nTask: python-dev     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Backend Report\nTask: nextjs-dev     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Frontend Report\nTask: cicd-agent     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ CICD Report\n```\n\nUse a single message with multiple Task tool calls.\n\n### Testing Agents\n\nQA runs first, then UI tests:\n\n```\nTask: qa-test-agent  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ QA Report\n                                    ‚îÇ\n                                    ‚ñº\nTask: ui-test-agent  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ UI Test Report\nTask: diagnostics    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Diagnostics Report\n     (parallel with ui-test)\n```\n\nUI test and diagnostics agents run in parallel with each other.\n\n## Agent Constraints\n\n### What Agents Can Do\n\n- Read specification files\n- Read project-map.md (read-only)\n- Implement code in their domain\n- Run tests in their domain\n- Return structured reports\n\n### What Agents Cannot Do\n\n- Spawn other agents\n- Modify `status.md` (architect only)\n- Modify `project-map.md` (architect only)\n- Create methodology documentation\n- Push to git repositories\n\n## File-Based Coordination\n\nWhen agents need to coordinate beyond reports:\n\n### Report Files\n\nOrchestrator saves reports as:\n```\n.claude/sprint/[N]/[slug]-report-[iteration].md\n```\n\nSlugs:\n- `python-dev` ‚Üí `backend`\n- `nextjs-dev` ‚Üí `frontend`\n- `qa-test-agent` ‚Üí `qa`\n- `ui-test-agent` ‚Üí `ui-test`\n\n## Specialized Agent Roles\n\n### Project Architect\n\nThe decision-maker:\n- Creates specification files\n- Maintains project-map.md and status.md\n- Analyzes agent reports\n- Decides next steps (implement, test, finalize)\n\n### Implementation Agents\n\nTechnology-specific builders:\n- `python-dev`: Python/FastAPI backend\n- `nextjs-dev`: Next.js frontend\n- `cicd-agent`: CI/CD pipelines\n- `allpurpose-agent`: Any other technology\n\n### Testing Agents\n\nQuality validators:\n- `qa-test-agent`: API and unit tests\n- `ui-test-agent`: Browser-based E2E tests\n- Framework-specific diagnostics agents\n\n## FINALIZE Signal\n\nThe architect signals sprint completion:\n\n```markdown\nFINALIZE\nPhase 5 complete. Sprint [N] is finalized.\n```\n\nThe orchestrator detects this and exits the iteration loop.\n\n## Best Practices\n\n### Report Quality\n\n- Keep reports concise\n- Focus on actionable information\n- No verbose logs or stack traces\n- Summarize, don't dump\n\n### Parallel Efficiency\n\n- Spawn independent agents together\n- Don't wait unnecessarily\n\n### Spec Adherence\n\n- Always read specs before implementing\n- Report deviations with justification\n- Follow API contracts exactly\n\n## Additional Resources\n\nFor detailed agent definitions and report formats, see the agent files in the plugin's `agents/` directory.",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/agent-patterns/SKILL.md"
    },
    {
      "slug": "aggregating-crypto-news",
      "name": "aggregating-crypto-news",
      "description": "Aggregate breaking crypto news, announcements, and market-moving events in real-time. Use when staying updated on crypto market events. Trigger with phrases like \"get crypto news\", \"check latest announcements\", or \"scan for updates\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:news-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:news-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "crypto-news-aggregator",
        "category": "crypto",
        "path": "plugins/crypto/crypto-news-aggregator",
        "version": "1.0.0",
        "description": "Aggregate and analyze crypto news from multiple sources with sentiment analysis"
      },
      "filePath": "plugins/crypto/crypto-news-aggregator/skills/crypto-news-aggregator/SKILL.md"
    },
    {
      "slug": "aggregating-performance-metrics",
      "name": "aggregating-performance-metrics",
      "description": "Aggregate and centralize performance metrics from applications, systems, databases, caches, and services. Use when consolidating monitoring data from multiple sources. Trigger with phrases like \"aggregate metrics\", \"centralize monitoring\", or \"collect performance data\".",
      "allowedTools": [
        "Read",
        "Write",
        "Bash(prometheus:*)",
        "Bash(metrics:*)",
        "Bash(monitoring:*)",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to streamline performance monitoring by aggregating metrics from diverse systems into a unified view. It simplifies the process of collecting, centralizing, and analyzing performance data, leading to improved insights and faster issue resolution.\n\n## How It Works\n\n1. **Metrics Taxonomy Design**: Claude assists in defining a clear and consistent naming convention for metrics across all systems.\n2. **Aggregation Tool Selection**: Claude helps select the appropriate metrics aggregation tool (e.g., Prometheus, StatsD, CloudWatch) based on the user's environment and requirements.\n3. **Configuration and Integration**: Claude guides the configuration of the chosen aggregation tool and its integration with various data sources.\n4. **Dashboard and Alert Setup**: Claude helps set up dashboards for visualizing metrics and defining alerts for critical performance indicators.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Centralize performance metrics from multiple applications and systems.\n- Design a consistent metrics naming convention.\n- Choose the right metrics aggregation tool for your needs.\n- Set up dashboards and alerts for performance monitoring.\n\n## Examples\n\n### Example 1: Centralizing Application and System Metrics\n\nUser request: \"Aggregate application and system metrics into Prometheus.\"\n\nThe skill will:\n1. Guide the user in defining metrics for applications (e.g., request latency, error rates) and systems (e.g., CPU usage, memory utilization).\n2. Help configure Prometheus to scrape metrics from the application and system endpoints.\n\n### Example 2: Setting Up Alerts for Database Performance\n\nUser request: \"Centralize database metrics and set up alerts for slow queries.\"\n\nThe skill will:\n1. Help the user define metrics for database performance (e.g., query execution time, connection pool usage).\n2. Guide the user in configuring the aggregation tool to collect these metrics from the database.\n3. Assist in setting up alerts in the aggregation tool to notify the user when query execution time exceeds a defined threshold.\n\n## Best Practices\n\n- **Naming Conventions**: Use a consistent and well-defined naming convention for all metrics to ensure clarity and ease of analysis.\n- **Granularity**: Choose an appropriate level of granularity for metrics to balance detail and storage requirements.\n- **Retention Policies**: Define retention policies for metrics to manage storage space and ensure data is available for historical analysis.\n\n## Integration\n\nThis skill integrates with other plugins that manage infrastructure, deploy applications, and monitor system health. For example, it can be used in conjunction with a deployment plugin to automatically configure metrics collection after a new application deployment.\n\n## Prerequisites\n\n- Access to metrics collection tools (Prometheus, StatsD, CloudWatch)\n- Network connectivity to metric sources\n- Metrics storage configuration in {baseDir}/metrics/\n- Understanding of metrics taxonomy\n\n## Instructions\n\n1. Design consistent metrics naming convention\n2. Select appropriate aggregation tool for environment\n3. Configure metric collection from all sources\n4. Set up centralized storage and retention policies\n5. Create dashboards for visualization\n6. Define alerts for critical metrics\n\n## Output\n\n- Metrics aggregation configuration files\n- Unified naming convention documentation\n- Dashboard definitions for key metrics\n- Alert rules for performance thresholds\n- Integration guides for metric sources\n\n## Error Handling\n\nIf metrics aggregation fails:\n- Verify network connectivity to sources\n- Check authentication credentials\n- Validate metrics format compatibility\n- Review storage capacity and retention\n- Ensure aggregation tool configuration\n\n## Resources\n\n- Prometheus aggregation documentation\n- StatsD protocol specifications\n- CloudWatch metrics API reference\n- Metrics naming best practices",
      "parentPlugin": {
        "name": "metrics-aggregator",
        "category": "performance",
        "path": "plugins/performance/metrics-aggregator",
        "version": "1.0.0",
        "description": "Aggregate and centralize performance metrics"
      },
      "filePath": "plugins/performance/metrics-aggregator/skills/metrics-aggregator/SKILL.md"
    },
    {
      "slug": "analyzing-capacity-planning",
      "name": "analyzing-capacity-planning",
      "description": "This skill enables claude to analyze capacity requirements and plan for",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to analyze current resource utilization, predict future capacity needs, and provide actionable recommendations for scaling infrastructure. It generates insights into growth trends, identifies potential bottlenecks, and estimates costs associated with capacity expansion.\n\n## How It Works\n\n1. **Analyze Utilization**: The plugin analyzes current CPU, memory, database storage, network bandwidth, and request rate utilization.\n2. **Forecast Growth**: Based on historical data, the plugin forecasts future growth trends for key capacity metrics.\n3. **Generate Recommendations**: The plugin recommends scaling strategies, including vertical and horizontal scaling options, and estimates associated costs.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze current infrastructure capacity and identify potential bottlenecks.\n- Forecast future resource requirements based on projected growth.\n- Develop a capacity roadmap to ensure optimal performance and availability.\n\n## Examples\n\n### Example 1: Planning for Database Growth\n\nUser request: \"Analyze database capacity and plan for future growth.\"\n\nThe skill will:\n1. Analyze current database storage utilization and growth rate.\n2. Forecast future storage requirements based on historical trends.\n3. Recommend scaling options, such as adding storage or migrating to a larger instance.\n\n### Example 2: Identifying CPU Bottlenecks\n\nUser request: \"Analyze CPU utilization and identify potential bottlenecks.\"\n\nThe skill will:\n1. Analyze CPU utilization trends across different servers and applications.\n2. Identify periods of high CPU usage and potential bottlenecks.\n3. Recommend scaling options, such as adding more CPU cores or optimizing application code.\n\n## Best Practices\n\n- **Data Accuracy**: Ensure that the data used for analysis is accurate and up-to-date.\n- **Metric Selection**: Choose the right capacity metrics to monitor based on your specific application requirements.\n- **Regular Monitoring**: Regularly monitor capacity metrics to identify potential issues before they impact performance.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools to provide proactive capacity management. It can also be used in conjunction with infrastructure-as-code tools to automate scaling operations.",
      "parentPlugin": {
        "name": "capacity-planning-analyzer",
        "category": "performance",
        "path": "plugins/performance/capacity-planning-analyzer",
        "version": "1.0.0",
        "description": "Analyze and plan for capacity requirements"
      },
      "filePath": "plugins/performance/capacity-planning-analyzer/skills/capacity-planning-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-database-indexes",
      "name": "analyzing-database-indexes",
      "description": "Use when you need to work with database indexing. This skill provides index design and optimization with comprehensive guidance and automation. Trigger with phrases like \"create indexes\", \"optimize indexes\", or \"improve query performance\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-index-advisor/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-index-advisor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-index-advisor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-index-advisor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-index-advisor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-index-advisor-dashboard.json`",
      "parentPlugin": {
        "name": "database-index-advisor",
        "category": "database",
        "path": "plugins/database/database-index-advisor",
        "version": "1.0.0",
        "description": "Analyze query patterns and recommend optimal database indexes with impact analysis"
      },
      "filePath": "plugins/database/database-index-advisor/skills/database-index-advisor/SKILL.md"
    },
    {
      "slug": "analyzing-dependencies",
      "name": "analyzing-dependencies",
      "description": "Check dependencies for known security vulnerabilities and outdated versions. Use when auditing third-party libraries. Trigger with 'check dependencies', 'scan for vulnerabilities', or 'audit packages'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to automatically analyze your project's dependencies for security vulnerabilities, outdated packages, and license compliance issues. It uses the dependency-checker plugin to identify potential risks and provides insights for remediation.\n\n## How It Works\n\n1. **Detecting Package Manager**: The skill identifies the relevant package manager (npm, pip, composer, gem, go modules) based on the presence of manifest files (e.g., package.json, requirements.txt, composer.json).\n2. **Scanning Dependencies**: The skill utilizes the dependency-checker plugin to scan the identified dependencies against known vulnerability databases (CVEs), outdated package lists, and license information.\n3. **Generating Report**: The skill presents a comprehensive report summarizing the findings, including vulnerability summaries, detailed vulnerability information, outdated packages with recommended updates, and license compliance issues.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Check a project for known security vulnerabilities in its dependencies.\n- Identify outdated packages that may contain security flaws or performance issues.\n- Ensure that the project's dependencies comply with licensing requirements.\n\n## Examples\n\n### Example 1: Identifying Vulnerabilities Before Deployment\n\nUser request: \"Check dependencies for vulnerabilities before deploying to production.\"\n\nThe skill will:\n1. Detect the relevant package manager (e.g., npm).\n2. Scan the project's dependencies for known vulnerabilities using the dependency-checker plugin.\n3. Generate a report highlighting any identified vulnerabilities, their severity, and recommended fixes.\n\n### Example 2: Updating Outdated Packages\n\nUser request: \"Scan for outdated packages and suggest updates.\"\n\nThe skill will:\n1. Detect the relevant package manager (e.g., pip).\n2. Scan the project's dependencies for outdated packages.\n3. Generate a report listing the outdated packages and their available updates, including major, minor, and patch releases.\n\n## Best Practices\n\n- **Regular Scanning**: Schedule dependency checks regularly (e.g., weekly or monthly) to stay informed about new vulnerabilities and updates.\n- **Pre-Deployment Checks**: Always run a dependency check before deploying any code to production to prevent introducing vulnerable dependencies.\n- **Review and Remediation**: Carefully review the generated reports and take appropriate action to remediate identified vulnerabilities and update outdated packages.\n\n## Integration\n\nThis skill seamlessly integrates with other Claude Code tools, allowing you to use the identified vulnerabilities to guide further actions, such as automatically creating pull requests to update dependencies or generating security reports for compliance purposes.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "dependency-checker",
        "category": "security",
        "path": "plugins/security/dependency-checker",
        "version": "1.0.0",
        "description": "Check dependencies for known vulnerabilities, outdated packages, and license compliance"
      },
      "filePath": "plugins/security/dependency-checker/skills/dependency-checker/SKILL.md"
    },
    {
      "slug": "analyzing-liquidity-pools",
      "name": "analyzing-liquidity-pools",
      "description": "Analyze liquidity pool metrics including TVL, volume, fees, and impermanent loss. Use when analyzing DEX liquidity pools. Trigger with phrases like \"analyze pool\", \"check TVL\", or \"calculate impermanent loss\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:liquidity-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:liquidity-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "liquidity-pool-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/liquidity-pool-analyzer",
        "version": "1.0.0",
        "description": "Analyze DeFi liquidity pools for impermanent loss, APY, and optimization opportunities"
      },
      "filePath": "plugins/crypto/liquidity-pool-analyzer/skills/liquidity-pool-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-logs",
      "name": "analyzing-logs",
      "description": "Analyze application logs for performance insights and issue detection including slow requests, error patterns, and resource usage. Use when troubleshooting performance issues or debugging errors. Trigger with phrases like \"analyze logs\", \"find slow requests\", or \"detect error patterns\".",
      "allowedTools": [
        "Read",
        "Write",
        "Bash(logs:*)",
        "Bash(grep:*)",
        "Bash(awk:*)",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to automatically analyze application logs, pinpoint performance bottlenecks, and identify recurring errors. It streamlines the debugging process and helps optimize application performance by extracting key insights from log data.\n\n## How It Works\n\n1. **Initiate Analysis**: Claude activates the log analysis tool upon detecting relevant trigger phrases.\n2. **Log Data Extraction**: The tool extracts relevant data, including timestamps, request durations, error messages, and resource usage metrics.\n3. **Pattern Identification**: The tool identifies patterns such as slow requests, frequent errors, and resource exhaustion warnings.\n4. **Report Generation**: Claude presents a summary of findings, highlighting potential performance issues and optimization opportunities.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in an application.\n- Debug recurring errors and exceptions.\n- Analyze log data for trends and anomalies.\n- Set up structured logging or log aggregation.\n\n## Examples\n\n### Example 1: Identifying Slow Requests\n\nUser request: \"Analyze logs for slow requests.\"\n\nThe skill will:\n1. Activate the log analysis tool.\n2. Identify requests exceeding predefined latency thresholds.\n3. Present a list of slow requests with corresponding timestamps and durations.\n\n### Example 2: Detecting Error Patterns\n\nUser request: \"Find error patterns in the application logs.\"\n\nThe skill will:\n1. Activate the log analysis tool.\n2. Scan logs for recurring error messages and exceptions.\n3. Group similar errors and present a summary of error frequencies.\n\n## Best Practices\n\n- **Log Level**: Ensure appropriate log levels (e.g., INFO, WARN, ERROR) are used to capture relevant information.\n- **Structured Logging**: Implement structured logging (e.g., JSON format) to facilitate efficient analysis.\n- **Log Rotation**: Configure log rotation policies to prevent log files from growing excessively.\n\n## Integration\n\nThis skill can be integrated with other tools for monitoring and alerting. For example, it can be used in conjunction with a monitoring plugin to automatically trigger alerts based on log analysis results. It can also work with deployment tools to rollback deployments when critical errors are detected in the logs.\n\n## Prerequisites\n\n- Access to application log files in {baseDir}/logs/\n- Log parsing tools (grep, awk, sed)\n- Understanding of application log format and structure\n- Read permissions for log directories\n\n## Instructions\n\n1. Identify log files to analyze based on timeframe and application\n2. Extract relevant data (timestamps, durations, error messages)\n3. Apply pattern matching to identify slow requests and errors\n4. Aggregate and group similar issues\n5. Generate analysis report with findings and recommendations\n6. Suggest optimization opportunities based on patterns\n\n## Output\n\n- Summary of slow requests with response times\n- Error frequency reports grouped by type\n- Resource usage patterns and anomalies\n- Performance bottleneck identification\n- Recommendations for log improvements and optimizations\n\n## Error Handling\n\nIf log analysis fails:\n- Verify log file paths and permissions\n- Check log format compatibility\n- Validate timestamp parsing\n- Ensure sufficient disk space for analysis\n- Review log rotation configuration\n\n## Resources\n\n- Application logging best practices\n- Structured logging format guides\n- Log aggregation tools documentation\n- Performance analysis methodologies",
      "parentPlugin": {
        "name": "log-analysis-tool",
        "category": "performance",
        "path": "plugins/performance/log-analysis-tool",
        "version": "1.0.0",
        "description": "Analyze logs for performance insights and issues"
      },
      "filePath": "plugins/performance/log-analysis-tool/skills/log-analysis-tool/SKILL.md"
    },
    {
      "slug": "analyzing-market-sentiment",
      "name": "analyzing-market-sentiment",
      "description": "Analyze crypto market sentiment from social media, news, and on-chain metrics. Use when gauging market sentiment and social trends. Trigger with phrases like \"analyze sentiment\", \"check market mood\", or \"gauge social trends\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:sentiment-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:sentiment-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "market-sentiment-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/market-sentiment-analyzer",
        "version": "1.0.0",
        "description": "Analyze market sentiment from social media, news, and on-chain data"
      },
      "filePath": "plugins/crypto/market-sentiment-analyzer/skills/market-sentiment-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-mempool",
      "name": "analyzing-mempool",
      "description": "Monitor blockchain mempools for pending transactions, front-running, and MEV opportunities. Use when monitoring pending blockchain transactions. Trigger with phrases like \"check mempool\", \"scan pending txs\", or \"find MEV\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:mempool-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:mempool-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "mempool-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/mempool-analyzer",
        "version": "1.0.0",
        "description": "Advanced mempool analysis for MEV opportunities, pending transaction monitoring, and gas price optimization"
      },
      "filePath": "plugins/crypto/mempool-analyzer/skills/mempool-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-network-latency",
      "name": "analyzing-network-latency",
      "description": "Analyze network latency and optimize request patterns for faster communication. Use when diagnosing slow network performance or optimizing API calls. Trigger with phrases like \"analyze network latency\", \"optimize API calls\", or \"reduce network delays\".",
      "allowedTools": [
        "Read",
        "Write",
        "Bash(curl:*)",
        "Bash(ping:*)",
        "Bash(traceroute:*)",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to diagnose network latency issues and propose optimizations to improve application performance. It analyzes request patterns, identifies potential bottlenecks, and recommends solutions for faster and more efficient network communication.\n\n## How It Works\n\n1. **Request Pattern Identification**: Claude identifies all network requests made by the application.\n2. **Latency Analysis**: Claude analyzes the latency associated with each request, looking for patterns and anomalies.\n3. **Optimization Recommendations**: Claude suggests optimizations such as parallelization, request batching, connection pooling, and timeout adjustments.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze network latency in an application.\n- Optimize network request patterns for improved performance.\n- Identify bottlenecks in network communication.\n\n## Examples\n\n### Example 1: Optimizing API Calls\n\nUser request: \"Analyze network latency and suggest improvements for our API calls.\"\n\nThe skill will:\n1. Identify all API calls made by the application.\n2. Analyze the latency of each API call.\n3. Suggest parallelizing certain API calls and implementing connection pooling.\n\n### Example 2: Reducing Page Load Time\n\nUser request: \"Optimize network request patterns to reduce page load time.\"\n\nThe skill will:\n1. Identify all network requests made during page load.\n2. Analyze the latency of each request.\n3. Suggest batching multiple requests into a single request and optimizing timeout configurations.\n\n## Best Practices\n\n- **Parallelization**: Identify serial requests that can be executed in parallel to reduce overall latency.\n- **Request Batching**: Batch multiple small requests into a single larger request to reduce overhead.\n- **Connection Pooling**: Reuse existing HTTP connections to avoid the overhead of establishing new connections for each request.\n\n## Integration\n\nThis skill can be used in conjunction with other plugins that manage infrastructure or application code, allowing for automated implementation of the suggested optimizations. For instance, it can work with a code modification plugin to automatically apply connection pooling or adjust timeout values.\n\n## Prerequisites\n\n- Access to application network configuration\n- Network monitoring tools (curl, ping, traceroute)\n- Request pattern documentation\n- Performance baseline metrics\n\n## Instructions\n\n1. Identify all network requests in the application\n2. Measure latency for each request type\n3. Analyze patterns for serial vs parallel execution\n4. Identify opportunities for batching and pooling\n5. Recommend timeout and retry configurations\n6. Provide optimization implementation plan\n\n## Output\n\n- Network latency analysis report\n- Request pattern visualizations\n- Optimization recommendations with priorities\n- Implementation examples for suggested changes\n- Expected performance improvements\n\n## Error Handling\n\nIf latency analysis fails:\n- Verify network connectivity to endpoints\n- Check DNS resolution and routing\n- Validate request authentication\n- Review firewall and security rules\n- Ensure monitoring tools are installed\n\n## Resources\n\n- HTTP connection pooling guides\n- Request batching best practices\n- Network performance optimization references\n- API design patterns for latency reduction",
      "parentPlugin": {
        "name": "network-latency-analyzer",
        "category": "performance",
        "path": "plugins/performance/network-latency-analyzer",
        "version": "1.0.0",
        "description": "Analyze network latency and optimize request patterns"
      },
      "filePath": "plugins/performance/network-latency-analyzer/skills/network-latency-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-nft-rarity",
      "name": "analyzing-nft-rarity",
      "description": "Calculate NFT rarity scores and floor prices across collections and marketplaces. Use when analyzing NFT collections and rarity. Trigger with phrases like \"check NFT rarity\", \"analyze collection\", or \"calculate floor price\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:nft-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:nft-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "nft-rarity-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/nft-rarity-analyzer",
        "version": "1.0.0",
        "description": "Analyze NFT rarity scores and valuations across collections"
      },
      "filePath": "plugins/crypto/nft-rarity-analyzer/skills/nft-rarity-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-on-chain-data",
      "name": "analyzing-on-chain-data",
      "description": "Perform on-chain analysis including whale tracking, token flows, and network activity. Use when performing crypto analysis. Trigger with phrases like \"analyze crypto\", \"check blockchain\", or \"monitor market\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:onchain-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:onchain-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "on-chain-analytics",
        "category": "crypto",
        "path": "plugins/crypto/on-chain-analytics",
        "version": "1.0.0",
        "description": "Analyze on-chain metrics including whale movements, network activity, and holder distribution"
      },
      "filePath": "plugins/crypto/on-chain-analytics/skills/on-chain-analytics/SKILL.md"
    },
    {
      "slug": "analyzing-options-flow",
      "name": "analyzing-options-flow",
      "description": "Track crypto options flow to identify institutional positioning and market sentiment. Use when tracking institutional options flow. Trigger with phrases like \"track options flow\", \"analyze derivatives\", or \"check institutional\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:options-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:options-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "options-flow-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/options-flow-analyzer",
        "version": "1.0.0",
        "description": "Track institutional options flow, unusual activity, and smart money movements"
      },
      "filePath": "plugins/crypto/options-flow-analyzer/skills/options-flow-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-query-performance",
      "name": "analyzing-query-performance",
      "description": "Use when you need to work with query optimization. This skill provides query performance analysis with comprehensive guidance and automation. Trigger with phrases like \"optimize queries\", \"analyze performance\", or \"improve query speed\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/query-performance-analyzer/`\n\n**Documentation and Guides**: `{baseDir}/docs/query-performance-analyzer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/query-performance-analyzer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/query-performance-analyzer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/query-performance-analyzer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/query-performance-analyzer-dashboard.json`",
      "parentPlugin": {
        "name": "query-performance-analyzer",
        "category": "database",
        "path": "plugins/database/query-performance-analyzer",
        "version": "1.0.0",
        "description": "Analyze query performance with EXPLAIN plan interpretation, bottleneck identification, and optimization recommendations"
      },
      "filePath": "plugins/database/query-performance-analyzer/skills/query-performance-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-security-headers",
      "name": "analyzing-security-headers",
      "description": "Analyze HTTP security headers of web domains to identify vulnerabilities and misconfigurations. Use when you need to audit website security headers, assess header compliance, or get security recommendations for web applications. Trigger with phrases like \"analyze security headers\", \"check HTTP headers\", \"audit website security headers\", or \"evaluate CSP and HSTS configuration\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Target URL or domain name is accessible\n- Network connectivity for HTTP requests\n- Permission to scan the target domain\n- Optional: Save results to {baseDir}/security-reports/\n\n## Instructions\n\n### 1. Domain Input Phase\n\nAccept domain specification:\n- Full URL with protocol (https://example.com)\n- Domain name only (example.com - will test HTTPS first)\n- Multiple domains for batch analysis\n- Specific paths for header variation testing\n\n### 2. Header Fetching Phase\n\nRetrieve HTTP response headers:\n- Make HEAD or GET request to target\n- Capture all security-relevant headers\n- Test both HTTP and HTTPS responses\n- Record redirect chains and final destination\n\n### 3. Analysis Phase\n\nEvaluate each security header against best practices:\n\n**Critical Headers**:\n- Strict-Transport-Security (HSTS)\n- Content-Security-Policy (CSP)\n- X-Frame-Options\n- X-Content-Type-Options\n- Permissions-Policy\n\n**Important Headers**:\n- Referrer-Policy\n- Cross-Origin-Embedder-Policy (COEP)\n- Cross-Origin-Opener-Policy (COOP)\n- Cross-Origin-Resource-Policy (CORP)\n\n**Additional Checks**:\n- Server header information disclosure\n- X-Powered-By header exposure\n- Cookie security attributes (Secure, HttpOnly, SameSite)\n\n### 4. Grading Phase\n\nCalculate security score:\n- A+ (95-100): All critical headers properly configured\n- A (85-94): Critical headers present, minor issues\n- B (75-84): Most headers present, some weaknesses\n- C (65-74): Missing critical headers\n- D (50-64): Significant security gaps\n- F (<50): Multiple critical vulnerabilities\n\n### 5. Report Generation Phase\n\nCreate comprehensive report with:\n- Overall security grade and numeric score\n- Missing headers with impact assessment\n- Misconfigured headers with specific issues\n- Remediation recommendations with examples\n- Priority ranking for fixes\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Security headers analysis report\n\n**Report Structure**:\n```\n# Security Headers Analysis - example.com\n## Overall Grade: B (82/100)\n\n## Critical Headers Status\n‚úÖ Strict-Transport-Security: Present (max-age=31536000; includeSubDomains)\n‚ùå Content-Security-Policy: Missing\n‚úÖ X-Frame-Options: Present (DENY)\n‚úÖ X-Content-Type-Options: Present (nosniff)\n‚ö†Ô∏è  Permissions-Policy: Misconfigured\n\n## Detailed Findings\n\n### Missing Headers (High Priority)\n1. Content-Security-Policy\n   - Risk: XSS vulnerability exposure\n   - Recommendation: Implement strict CSP\n   - Example: Content-Security-Policy: default-src 'self'; script-src 'self' 'unsafe-inline'\n\n### Misconfigured Headers\n1. Permissions-Policy\n   - Current: geolocation=*\n   - Issue: Allows all origins\n   - Fix: geolocation=(self)\n\n## Priority Actions\n1. Add Content-Security-Policy (Critical)\n2. Fix Permissions-Policy wildcard (High)\n3. Add Referrer-Policy (Medium)\n```\n\n**Optional Outputs**:\n- JSON format for automation: {baseDir}/security-reports/headers-DOMAIN-YYYYMMDD.json\n- CSV for spreadsheet analysis\n- Comparison report for multiple domains\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Domain Unreachable**\n   - Error: \"Failed to connect to example.com\"\n   - Resolution: Check domain spelling, network connectivity, firewall rules\n   - Fallback: Test alternate protocols (HTTP vs HTTPS)\n\n2. **SSL/TLS Errors**\n   - Error: \"SSL certificate verification failed\"\n   - Resolution: Note in report, test with certificate validation disabled\n   - Impact: Indicates HSTS not properly enforced\n\n3. **Redirect Loops**\n   - Error: \"Too many redirects\"\n   - Resolution: Report redirect chain, analyze headers at each hop\n   - Note: Headers may differ across redirect chain\n\n4. **Rate Limiting**\n   - Error: \"HTTP 429 Too Many Requests\"\n   - Resolution: Implement exponential backoff, reduce request frequency\n   - Fallback: Queue domain for later analysis\n\n5. **Mixed Content Issues**\n   - Error: \"Headers differ between HTTP and HTTPS\"\n   - Resolution: Report both sets, highlight critical differences\n   - Recommendation: Ensure HSTS enforces HTTPS-only\n\n## Resources\n\n**Security Header References**:\n- OWASP Secure Headers Project: https://owasp.org/www-project-secure-headers/\n- MDN Security Headers Guide: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers#security\n- Security Headers Scanner: https://securityheaders.com/\n\n**Header-Specific Documentation**:\n- CSP Reference: https://content-security-policy.com/\n- HSTS Preload: https://hstspreload.org/\n- Permissions Policy: https://www.w3.org/TR/permissions-policy/\n\n**Best Practice Guides**:\n- NIST Web Security Guidelines: https://pages.nist.gov/800-63-3/\n- Mozilla Observatory: https://observatory.mozilla.org/\n\n**Testing Tools**:\n- Online header checker: https://securityheaders.com/\n- Browser DevTools Network tab for manual inspection\n- curl command for command-line testing: `curl -I https://example.com`\n\n**Integration Examples**:\n- Automated header checks in CI/CD pipelines\n- Periodic scanning with alerting on grade degradation\n- Compliance reporting for security audits",
      "parentPlugin": {
        "name": "security-headers-analyzer",
        "category": "security",
        "path": "plugins/security/security-headers-analyzer",
        "version": "1.0.0",
        "description": "Analyze HTTP security headers"
      },
      "filePath": "plugins/security/security-headers-analyzer/skills/security-headers-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-system-throughput",
      "name": "analyzing-system-throughput",
      "description": "Analyze and optimize system throughput including request handling, data processing, and resource utilization. Use when identifying capacity limits or evaluating scaling strategies. Trigger with phrases like \"analyze throughput\", \"optimize capacity\", or \"identify bottlenecks\".",
      "allowedTools": [
        "Read",
        "Write",
        "Bash(performance:*)",
        "Bash(monitoring:*)",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill allows Claude to analyze system performance and identify areas for throughput optimization. It uses the `throughput-analyzer` plugin to provide insights into request handling, data processing, and resource utilization.\n\n## How It Works\n\n1. **Identify Critical Components**: Determines which system components are most relevant to throughput.\n2. **Analyze Throughput Metrics**: Gathers and analyzes current throughput metrics for the identified components.\n3. **Identify Limiting Factors**: Pinpoints the bottlenecks and constraints that are hindering optimal throughput.\n4. **Evaluate Scaling Strategies**: Explores potential scaling strategies and their impact on overall throughput.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze system throughput to identify performance bottlenecks.\n- Optimize system performance for increased capacity.\n- Evaluate scaling strategies to improve throughput.\n\n## Examples\n\n### Example 1: Analyzing Web Server Throughput\n\nUser request: \"Analyze the throughput of my web server and identify any bottlenecks.\"\n\nThe skill will:\n1. Activate the `throughput-analyzer` plugin.\n2. Analyze request throughput, data throughput, and resource saturation of the web server.\n3. Provide a report identifying potential bottlenecks and optimization opportunities.\n\n### Example 2: Optimizing Data Processing Pipeline\n\nUser request: \"Optimize the throughput of my data processing pipeline.\"\n\nThe skill will:\n1. Activate the `throughput-analyzer` plugin.\n2. Analyze data throughput, queue processing, and concurrency limits of the data processing pipeline.\n3. Suggest improvements to increase data processing rates and overall throughput.\n\n## Best Practices\n\n- **Component Selection**: Focus the analysis on the most throughput-critical components to avoid unnecessary overhead.\n- **Metric Interpretation**: Carefully interpret throughput metrics to accurately identify limiting factors.\n- **Scaling Evaluation**: Thoroughly evaluate the potential impact of scaling strategies before implementation.\n\n## Integration\n\nThis skill can be used in conjunction with other monitoring and performance analysis tools to gain a more comprehensive understanding of system behavior. It provides a starting point for further investigation and optimization efforts.\n\n## Prerequisites\n\n- Access to throughput metrics in {baseDir}/metrics/throughput/\n- System performance monitoring tools\n- Historical throughput baselines\n- Current capacity and scaling limits\n\n## Instructions\n\n1. Identify critical system components for throughput analysis\n2. Collect request and data throughput metrics\n3. Analyze resource saturation and queue depths\n4. Identify bottlenecks and limiting factors\n5. Evaluate horizontal and vertical scaling strategies\n6. Generate capacity planning recommendations\n\n## Output\n\n- Throughput analysis reports with current capacity\n- Bottleneck identification and root cause analysis\n- Resource saturation metrics\n- Scaling strategy recommendations\n- Capacity planning projections\n\n## Error Handling\n\nIf throughput analysis fails:\n- Verify metrics collection infrastructure\n- Check system monitoring tool access\n- Validate historical baseline data\n- Ensure performance testing environment\n- Review component identification logic\n\n## Resources\n\n- Throughput optimization best practices\n- Capacity planning methodologies\n- Scaling strategy comparison guides\n- Performance bottleneck detection techniques",
      "parentPlugin": {
        "name": "throughput-analyzer",
        "category": "performance",
        "path": "plugins/performance/throughput-analyzer",
        "version": "1.0.0",
        "description": "Analyze and optimize system throughput"
      },
      "filePath": "plugins/performance/throughput-analyzer/skills/throughput-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-test-coverage",
      "name": "analyzing-test-coverage",
      "description": "Analyze code coverage metrics and identify untested code paths. Use when analyzing untested code or coverage gaps. Trigger with phrases like \"analyze coverage\", \"check test coverage\", or \"find untested code\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:coverage-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:coverage-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "test-coverage-analyzer",
        "category": "testing",
        "path": "plugins/testing/test-coverage-analyzer",
        "version": "1.0.0",
        "description": "Analyze code coverage metrics, identify untested code, and generate comprehensive coverage reports"
      },
      "filePath": "plugins/testing/test-coverage-analyzer/skills/test-coverage-analyzer/SKILL.md"
    },
    {
      "slug": "analyzing-text-sentiment",
      "name": "analyzing-text-sentiment",
      "description": "This skill enables claude to analyze the sentiment of text data. it identifies",
      "allowedTools": [
        "Read",
        "Write",
        "Bash",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to perform sentiment analysis on text, providing insights into the emotional content and polarity of the provided data. By leveraging AI/ML techniques, it helps understand public opinion, customer feedback, and overall emotional tone in written communication.\n\n## How It Works\n\n1. **Text Input**: The skill receives text data as input from the user.\n2. **Sentiment Analysis**: The skill processes the text using a pre-trained sentiment analysis model to determine the sentiment polarity (positive, negative, or neutral).\n3. **Result Output**: The skill provides a sentiment score and classification, indicating the overall sentiment expressed in the text.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Determine the overall sentiment of customer reviews.\n- Analyze the emotional tone of social media posts.\n- Gauge public opinion on a particular topic.\n- Identify positive and negative feedback in survey responses.\n\n## Examples\n\n### Example 1: Analyzing Customer Reviews\n\nUser request: \"Analyze the sentiment of these customer reviews: 'The product is amazing!', 'The service was terrible.', 'It was okay.'\"\n\nThe skill will:\n1. Process the provided customer reviews.\n2. Classify each review as positive, negative, or neutral and provide sentiment scores.\n\n### Example 2: Monitoring Social Media Sentiment\n\nUser request: \"Perform sentiment analysis on the following tweet: 'I love this new feature!'\"\n\nThe skill will:\n1. Analyze the provided tweet.\n2. Identify the sentiment as positive and provide a corresponding sentiment score.\n\n## Best Practices\n\n- **Data Quality**: Ensure the input text is clear and free from ambiguous language for accurate sentiment analysis.\n- **Context Awareness**: Consider the context of the text when interpreting sentiment scores, as sarcasm or irony can affect results.\n- **Model Selection**: Use appropriate sentiment analysis models based on the type of text being analyzed (e.g., social media, customer reviews).\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins to automate workflows, such as summarizing feedback alongside sentiment scores or triggering actions based on sentiment polarity (e.g., escalating negative feedback).",
      "parentPlugin": {
        "name": "sentiment-analysis-tool",
        "category": "ai-ml",
        "path": "plugins/ai-ml/sentiment-analysis-tool",
        "version": "1.0.0",
        "description": "Sentiment analysis on text data"
      },
      "filePath": "plugins/ai-ml/sentiment-analysis-tool/skills/sentiment-analysis-tool/SKILL.md"
    },
    {
      "slug": "analyzing-text-with-nlp",
      "name": "analyzing-text-with-nlp",
      "description": "This skill enables claude to perform natural language processing and",
      "allowedTools": [
        "Read",
        "Bash",
        "Grep",
        "Glob"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to analyze text using the nlp-text-analyzer plugin, extracting meaningful information and insights. It facilitates tasks such as sentiment analysis, keyword extraction, and topic modeling, enabling a deeper understanding of textual data.\n\n## How It Works\n\n1. **Request Analysis**: Claude receives a user request to analyze text.\n2. **Text Processing**: The nlp-text-analyzer plugin processes the text using NLP techniques.\n3. **Insight Extraction**: The plugin extracts insights such as sentiment, keywords, and topics.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform sentiment analysis on a piece of text.\n- Extract keywords from a document.\n- Identify the main topics discussed in a text.\n\n## Examples\n\n### Example 1: Sentiment Analysis\n\nUser request: \"Analyze the sentiment of this product review: 'I loved the product! It exceeded my expectations.'\"\n\nThe skill will:\n1. Process the review text using the nlp-text-analyzer plugin.\n2. Determine the sentiment as positive and provide a confidence score.\n\n### Example 2: Keyword Extraction\n\nUser request: \"Extract the keywords from this news article about the latest AI advancements.\"\n\nThe skill will:\n1. Process the article text using the nlp-text-analyzer plugin.\n2. Identify and return a list of relevant keywords, such as \"AI\", \"advancements\", \"machine learning\", and \"neural networks\".\n\n## Best Practices\n\n- **Clarity**: Be specific in your requests to ensure accurate and relevant analysis.\n- **Context**: Provide sufficient context to improve the quality of the analysis.\n- **Iteration**: Refine your requests based on the initial results to achieve the desired outcome.\n\n## Integration\n\nThis skill can be integrated with other tools to provide a comprehensive workflow, such as using the extracted keywords to perform further research or using sentiment analysis to categorize customer feedback.",
      "parentPlugin": {
        "name": "nlp-text-analyzer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/nlp-text-analyzer",
        "version": "1.0.0",
        "description": "Natural language processing and text analysis"
      },
      "filePath": "plugins/ai-ml/nlp-text-analyzer/skills/nlp-text-analyzer/SKILL.md"
    },
    {
      "slug": "api-contract",
      "name": "api-contract",
      "description": "This skill should be used when the user asks about \"API contract\", \"api-contract.md\", \"shared interface\", \"TypeScript interfaces\", \"request response schemas\", \"endpoint design\", or needs guidance on designing contracts that coordinate backend and frontend agents.",
      "allowedTools": [
        "Read"
      ],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# API Contract Design\n\nThe API contract (`api-contract.md`) is the shared interface between implementation agents. It defines what the backend provides and what the frontend consumes, ensuring both sides build compatible systems without direct communication.\n\n## Purpose\n\nThe contract serves as:\n- **Single source of truth** for API shape\n- **Coordination mechanism** between backend and frontend agents\n- **Validation reference** for QA testing\n- **Documentation** for the implemented API\n\n## Contract Structure\n\nA complete api-contract.md contains:\n\n```markdown\n# API Contract: [Feature Name]\n\n## Base URL\n[API base path, e.g., /api/v1]\n\n## Authentication\n[Auth requirements for endpoints]\n\n## Endpoints\n\n### [Endpoint Group]\n\n#### [METHOD] [Route]\n[Description]\n\n**Request:**\n[Request schema]\n\n**Response:**\n[Response schema]\n\n**Errors:**\n[Error codes and meanings]\n\n## TypeScript Interfaces\n[Shared type definitions]\n\n## Validation Rules\n[Input validation requirements]\n```\n\n## Writing Endpoints\n\n### Endpoint Definition\n\n```markdown\n#### POST /auth/register\n\nCreate a new user account.\n\n**Request:**\n```json\n{\n  \"email\": \"string (required, valid email)\",\n  \"password\": \"string (required, min 8 chars)\",\n  \"name\": \"string (optional)\"\n}\n```\n\n**Response (201):**\n```json\n{\n  \"id\": \"uuid\",\n  \"email\": \"string\",\n  \"name\": \"string | null\",\n  \"createdAt\": \"ISO 8601 datetime\"\n}\n```\n\n**Errors:**\n- 400: Invalid request body\n- 409: Email already exists\n- 422: Validation failed\n```\n\n### Key Elements\n\n| Element | Purpose |\n|---------|---------|\n| Method + Route | HTTP verb and path |\n| Description | What the endpoint does |\n| Request | Input schema with types and constraints |\n| Response | Output schema with status code |\n| Errors | Possible error responses |\n\n## TypeScript Interfaces\n\nDefine shared types for type safety across agents:\n\n```markdown\n## TypeScript Interfaces\n\n```typescript\n// User types\ninterface User {\n  id: string;\n  email: string;\n  name: string | null;\n  createdAt: string;\n}\n\ninterface CreateUserRequest {\n  email: string;\n  password: string;\n  name?: string;\n}\n\ninterface LoginRequest {\n  email: string;\n  password: string;\n}\n\ninterface AuthResponse {\n  user: User;\n  token: string;\n  expiresAt: string;\n}\n\n// Error response\ninterface ApiError {\n  code: string;\n  message: string;\n  details?: Record<string, string[]>;\n}\n```\n```\n\n### Type Guidelines\n\n- Use explicit types, not `any`\n- Mark optional fields with `?`\n- Use union types for nullable: `string | null`\n- Include all possible response shapes\n- Match types to JSON serialization\n\n## Validation Rules\n\nDocument validation requirements explicitly:\n\n```markdown\n## Validation Rules\n\n### User Registration\n| Field | Rules |\n|-------|-------|\n| email | Required, valid email format, unique |\n| password | Required, min 8 chars, 1 uppercase, 1 number |\n| name | Optional, max 100 chars |\n\n### Product Creation\n| Field | Rules |\n|-------|-------|\n| title | Required, 3-200 chars |\n| price | Required, positive number, max 2 decimals |\n| category | Required, must exist in categories table |\n```\n\n### Why Document Validation\n\n- Backend implements the rules\n- Frontend can pre-validate before submit\n- QA tests edge cases\n- All agents have same understanding\n\n## Error Handling\n\n### Standard Error Format\n\n```markdown\n## Error Response Format\n\nAll errors return:\n```json\n{\n  \"code\": \"ERROR_CODE\",\n  \"message\": \"Human-readable message\",\n  \"details\": {\n    \"field\": [\"error1\", \"error2\"]\n  }\n}\n```\n\n### Error Codes\n| Code | HTTP Status | Meaning |\n|------|-------------|---------|\n| VALIDATION_ERROR | 422 | Input validation failed |\n| NOT_FOUND | 404 | Resource doesn't exist |\n| UNAUTHORIZED | 401 | Authentication required |\n| FORBIDDEN | 403 | Permission denied |\n| CONFLICT | 409 | Resource conflict |\n```\n\n### Error Consistency\n\nUse consistent error codes across all endpoints. This helps:\n- Frontend implement unified error handling\n- QA test all error scenarios\n- Users get predictable feedback\n\n## Pagination\n\nFor list endpoints:\n\n```markdown\n#### GET /products\n\nList products with pagination.\n\n**Query Parameters:**\n| Param | Type | Default | Description |\n|-------|------|---------|-------------|\n| page | integer | 1 | Page number |\n| limit | integer | 20 | Items per page (max 100) |\n| sort | string | createdAt | Sort field |\n| order | string | desc | Sort order (asc/desc) |\n\n**Response (200):**\n```json\n{\n  \"data\": [Product],\n  \"pagination\": {\n    \"page\": 1,\n    \"limit\": 20,\n    \"total\": 150,\n    \"totalPages\": 8\n  }\n}\n```\n```\n\n### Pagination Interface\n\n```typescript\ninterface PaginatedResponse<T> {\n  data: T[];\n  pagination: {\n    page: number;\n    limit: number;\n    total: number;\n    totalPages: number;\n  };\n}\n```\n\n## Contract Evolution\n\n### During Sprint\n\n1. **Initial**: Architect creates contract based on specs\n2. **Implementation**: Agents follow contract exactly\n3. **Deviations**: If agent must deviate, document in report\n4. **Update**: Architect updates contract if deviation justified\n5. **Final**: Contract reflects implemented reality\n\n### Contract Changes\n\nWhen changing an existing contract:\n- Note breaking changes\n- Consider versioning for major changes\n- Update both request and response if affected\n- Notify dependent agents via specs\n\n## Best Practices\n\n### Be Specific\n\n```markdown\n// Good\n\"email\": \"string (required, valid email format)\"\n\n// Bad\n\"email\": \"string\"\n```\n\n### Include Examples\n\n```markdown\n**Request Example:**\n```json\n{\n  \"email\": \"user@example.com\",\n  \"password\": \"SecurePass123\"\n}\n```\n```\n\n### Document All States\n\nInclude responses for:\n- Success (200, 201, 204)\n- Client errors (400, 401, 403, 404, 422)\n- Empty states (empty arrays, null values)\n\n### Keep It DRY\n\nReference shared types instead of duplicating:\n\n```markdown\n**Response:** `User` (see TypeScript Interfaces)\n```\n\n### No Implementation Details\n\nThe contract defines WHAT, not HOW:\n- Don't mention database columns\n- Don't specify frameworks\n- Don't include file paths\n\n## Common Patterns\n\n### CRUD Endpoints\n\n```markdown\n### Products\n\n| Method | Route | Description |\n|--------|-------|-------------|\n| GET | /products | List all products |\n| GET | /products/:id | Get single product |\n| POST | /products | Create product |\n| PUT | /products/:id | Update product |\n| DELETE | /products/:id | Delete product |\n```\n\n### Authentication Endpoints\n\n```markdown\n### Authentication\n\n| Method | Route | Auth | Description |\n|--------|-------|------|-------------|\n| POST | /auth/register | No | Create account |\n| POST | /auth/login | No | Get token |\n| POST | /auth/logout | Yes | Invalidate token |\n| GET | /auth/me | Yes | Current user |\n| POST | /auth/refresh | Yes | Refresh token |\n```\n\n## Additional Resources\n\nFor complete examples, examine the `api-contract.md` files generated in sprint directories after running `/sprint`.",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/api-contract/SKILL.md"
    },
    {
      "slug": "archiving-databases",
      "name": "archiving-databases",
      "description": "Use when you need to archive historical database records to reduce primary database size. This skill automates moving old data to archive tables or cold storage (S3, Azure Blob, GCS). Trigger with phrases like \"archive old database records\", \"implement data retention policy\", \"move historical data to cold storage\", or \"reduce database size with archival\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Database credentials with SELECT and DELETE permissions on source tables\n- Access to destination storage (archive table or cloud storage credentials)\n- Network connectivity to cloud storage services if using S3/Azure/GCS\n- Backup of database before first archival run\n- Understanding of data retention requirements and compliance policies\n- Monitoring tools configured to track archival job success\n\n## Instructions\n\n### Step 1: Define Archival Criteria\n1. Identify tables containing historical data for archival\n2. Define age threshold for archival (e.g., records older than 1 year)\n3. Determine additional criteria (status flags, record size, access frequency)\n4. Calculate expected data volume to be archived\n5. Document business requirements and compliance policies\n\n### Step 2: Choose Archival Destination\n1. Evaluate options: archive table in same database, separate archive database, or cold storage\n2. For cloud storage: select S3, Azure Blob, or GCS based on infrastructure\n3. Configure destination storage with appropriate security and access controls\n4. Set up compression settings for storage efficiency\n5. Define data format for archived records (CSV, Parquet, JSON)\n\n### Step 3: Create Archive Schema\n1. Design archive table schema matching source table structure\n2. Add metadata columns (archived_at, source_table, archive_reason)\n3. Create indexes on commonly queried archive columns\n4. For cloud storage: define bucket structure and naming conventions\n5. Test archive schema with sample data\n\n### Step 4: Implement Archival Logic\n1. Write SQL query to identify records meeting archival criteria\n2. Create extraction script to export records from source tables\n3. Implement transformation logic if archive format differs from source\n4. Build verification queries to confirm data integrity after archival\n5. Add transaction handling to ensure atomicity (delete only if archive succeeds)\n\n### Step 5: Execute Archival Process\n1. Run archival in staging environment first with subset of data\n2. Verify archived data integrity and completeness\n3. Execute archival in production during low-traffic window\n4. Monitor database performance during archival operation\n5. Generate archival report with record counts and storage savings\n\n### Step 6: Automate Retention Policy\n1. Schedule periodic archival jobs (weekly, monthly)\n2. Configure automated monitoring and alerting for job failures\n3. Implement cleanup of successfully archived records from source tables\n4. Set up expiration policies on archived data per compliance requirements\n5. Document archival schedule and retention periods\n\n## Output\n\nThis skill produces:\n\n**Archival Scripts**: SQL and shell scripts to extract, transform, and load data to archive destination\n\n**Archive Tables/Files**: Structured storage containing historical records with metadata and timestamps\n\n**Verification Reports**: Row counts, data checksums, and integrity checks confirming successful archival\n\n**Storage Metrics**: Database size reduction, archive storage utilization, and cost savings estimates\n\n**Archival Logs**: Detailed logs of each archival run with timestamps, record counts, and any errors\n\n## Error Handling\n\n**Insufficient Storage Space**:\n- Check available disk space on archive destination before execution\n- Implement storage monitoring and alerting\n- Use compression to reduce archive size\n- Clean up old archives per retention policy before new archival\n\n**Data Integrity Issues**:\n- Run checksums on source data before and after archival\n- Implement row count verification between source and archive\n- Keep source data until archive verification completes\n- Rollback archive transaction if verification fails\n\n**Permission Denied Errors**:\n- Verify database user has SELECT on source tables and INSERT on archive tables\n- Confirm cloud storage credentials have write permissions\n- Check network security groups allow connections to cloud storage\n- Document required permissions for archival automation\n\n**Timeout During Large Archival**:\n- Split archival into smaller batches by date ranges\n- Run archival incrementally over multiple days\n- Increase database timeout settings for archival sessions\n- Schedule archival during maintenance windows with extended timeouts\n\n## Resources\n\n**Archival Configuration Templates**:\n- PostgreSQL archival: `{baseDir}/templates/postgresql-archive-config.yaml`\n- MySQL archival: `{baseDir}/templates/mysql-archive-config.yaml`\n- S3 cold storage: `{baseDir}/templates/s3-archive-config.yaml`\n- Azure Blob storage: `{baseDir}/templates/azure-archive-config.yaml`\n\n**Retention Policy Definitions**: `{baseDir}/policies/retention-policies.yaml`\n\n**Archival Scripts Library**: `{baseDir}/scripts/archival/`\n- Extract to CSV script\n- Extract to Parquet script\n- S3 upload with compression\n- Archive verification queries\n\n**Monitoring Dashboards**: `{baseDir}/monitoring/archival-dashboard.json`\n**Cost Analysis Tools**: `{baseDir}/tools/storage-cost-calculator.py`",
      "parentPlugin": {
        "name": "database-archival-system",
        "category": "database",
        "path": "plugins/database/database-archival-system",
        "version": "1.0.0",
        "description": "Database plugin for database-archival-system"
      },
      "filePath": "plugins/database/database-archival-system/skills/database-archival-system/SKILL.md"
    },
    {
      "slug": "assisting-with-soc2-audit-preparation",
      "name": "assisting-with-soc2-audit-preparation",
      "description": "Automate SOC 2 audit preparation including evidence gathering, control assessment, and compliance gap identification. Use when you need to prepare for SOC 2 audits, assess Trust Service Criteria compliance, document security controls, or generate readiness reports. Trigger with phrases like \"SOC 2 audit preparation\", \"SOC 2 readiness assessment\", \"collect SOC 2 evidence\", or \"Trust Service Criteria compliance\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Documentation directory accessible in {baseDir}/docs/\n- Infrastructure-as-code and configuration files available\n- Access to cloud provider logs (AWS CloudTrail, Azure Activity Log, GCP Audit Logs)\n- Security policies and procedures documented\n- Employee training records available\n- Incident response documentation accessible\n- Write permissions for audit reports in {baseDir}/soc2-audit/\n\n## Instructions\n\n### 1. Trust Service Criteria Assessment\n\nEvaluate controls across five categories:\n\n**Security (Common Criteria)** - Required for all SOC 2 audits:\n- CC1: Control Environment\n- CC2: Communication and Information\n- CC3: Risk Assessment\n- CC4: Monitoring Activities\n- CC5: Control Activities\n- CC6: Logical and Physical Access Controls\n- CC7: System Operations\n- CC8: Change Management\n- CC9: Risk Mitigation\n\n**Additional Criteria** (Optional):\n- Availability\n- Processing Integrity\n- Confidentiality\n- Privacy\n\n### 2. Evidence Collection Phase\n\n**Security Controls Evidence**:\n- Access control policies and configurations\n- Multi-factor authentication implementation\n- Password policy documentation\n- Firewall rules and network segmentation\n- Encryption at rest and in transit\n- Security monitoring and alerting configs\n\n**Operational Evidence**:\n- Change management logs\n- Backup and recovery procedures\n- Disaster recovery testing results\n- System monitoring dashboards\n- Capacity planning documentation\n- Performance metrics\n\n**Policy and Procedure Evidence**:\n- Information security policy\n- Incident response plan\n- Business continuity plan\n- Vendor management procedures\n- Employee onboarding/offboarding\n- Security awareness training records\n\n**System Evidence**:\n- System architecture diagrams\n- Data flow diagrams\n- Asset inventory\n- Software bill of materials (SBOM)\n- Configuration management database\n\n### 3. Control Effectiveness Testing\n\nFor each control point:\n- Verify control design (is it properly designed?)\n- Test operating effectiveness (is it working as intended?)\n- Document test results with screenshots/logs\n- Identify gaps or weaknesses\n- Recommend remediation actions\n\n### 4. Compliance Gap Analysis\n\nCompare current state against SOC 2 requirements:\n- Missing controls (critical gaps)\n- Partially implemented controls (needs improvement)\n- Improperly documented controls (evidence gaps)\n- Ineffective controls (design or operating failures)\n\n### 5. Evidence Documentation\n\nOrganize evidence by Trust Service Criteria:\n```\n{baseDir}/soc2-audit/\n‚îú‚îÄ‚îÄ CC1-control-environment/\n‚îÇ   ‚îú‚îÄ‚îÄ org-chart.pdf\n‚îÇ   ‚îú‚îÄ‚îÄ security-policy.md\n‚îÇ   ‚îî‚îÄ‚îÄ training-records.xlsx\n‚îú‚îÄ‚îÄ CC6-access-controls/\n‚îÇ   ‚îú‚îÄ‚îÄ iam-policies.json\n‚îÇ   ‚îú‚îÄ‚îÄ mfa-config.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ access-review-logs.csv\n‚îú‚îÄ‚îÄ CC7-system-operations/\n‚îÇ   ‚îú‚îÄ‚îÄ monitoring-configs/\n‚îÇ   ‚îú‚îÄ‚îÄ backup-procedures.md\n‚îÇ   ‚îî‚îÄ‚îÄ incident-logs/\n‚îî‚îÄ‚îÄ readiness-report.md\n```\n\n### 6. Generate Readiness Report\n\nCreate comprehensive SOC 2 readiness assessment with:\n- Executive summary with readiness score\n- Control-by-control assessment\n- Gap analysis with severity ratings\n- Remediation roadmap with timelines\n- Evidence collection checklist\n- Auditor interview preparation guide\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: SOC 2 readiness report saved to {baseDir}/soc2-audit/readiness-report-YYYYMMDD.md\n\n**Report Structure**:\n```\n# SOC 2 Readiness Assessment\nAssessment Date: 2024-01-15\nOrganization: TechCorp Inc.\nAudit Type: SOC 2 Type II (Security + Availability)\n\n## Executive Summary\n- Overall Readiness: 75% (Needs Improvement)\n- Controls Implemented: 28/40 (70%)\n- Critical Gaps: 3\n- High Priority Items: 8\n- Estimated Remediation Time: 8-12 weeks\n\n## Readiness by Trust Service Category\n\n### CC1: Control Environment (80%)\n‚úÖ Implemented (4):\n- Organizational structure documented\n- Security policy established\n- Risk assessment framework in place\n- Board oversight of security\n\n‚ö†Ô∏è Gaps (1):\n- Security role and responsibility matrix incomplete\n\n### CC6: Logical and Physical Access Controls (60%)\n‚úÖ Implemented (5):\n- Multi-factor authentication enabled\n- Role-based access control (RBAC) implemented\n- Password policy enforced\n- Access review process established\n- Visitor access controls in place\n\n‚ùå Critical Gaps (2):\n- No automated user deprovisioning\n- Privileged access not logged/monitored\n\n‚ö†Ô∏è High Priority (3):\n- Access logs retention < 1 year\n- No formal access request workflow\n- Physical security cameras not monitored 24/7\n\n### CC7: System Operations (70%)\n[Similar breakdown...]\n\n## Critical Gaps Requiring Immediate Action\n\n### 1. Automated User Deprovisioning (CC6.2)\n**Current State**: Manual offboarding process\n**Risk**: Terminated employees retain system access\n**Evidence**: {baseDir}/hr/offboarding-checklist.pdf\n**Remediation**:\n- Implement automated deprovisioning tied to HR system\n- Set up alerts for access not removed within 24 hours\n- Estimated effort: 2-3 weeks\n**Priority**: CRITICAL\n\n### 2. Privileged Access Monitoring (CC6.7)\n**Current State**: No logging of admin actions\n**Risk**: Insider threats undetected\n**Remediation**:\n- Enable audit logging for all admin accounts\n- Set up SIEM alerts for privileged actions\n- Implement session recording for production access\n**Priority**: CRITICAL\n\n### 3. Disaster Recovery Testing (CC7.4)\n**Current State**: DR plan exists but never tested\n**Risk**: Recovery time objectives may not be achievable\n**Remediation**:\n- Schedule quarterly DR tests\n- Document test results\n- Update plan based on test findings\n**Priority**: CRITICAL\n\n## Evidence Collection Status\n\n| Control | Evidence Type | Status | Location |\n|---------|--------------|--------|----------|\n| CC1.1 | Org Chart | ‚úÖ Complete | {baseDir}/soc2-audit/CC1/ |\n| CC6.1 | MFA Config | ‚úÖ Complete | {baseDir}/soc2-audit/CC6/ |\n| CC6.2 | Offboarding Logs | ‚ùå Missing | N/A |\n| CC7.1 | Monitoring Dashboards | ‚ö†Ô∏è Partial | Need 90-day history |\n\n## Remediation Roadmap\n\n**Weeks 1-2 (Critical Fixes)**:\n- Implement automated deprovisioning\n- Enable privileged access monitoring\n- Begin DR test planning\n\n**Weeks 3-6 (High Priority)**:\n- Extend log retention to 1 year\n- Implement access request workflow\n- Complete initial DR test\n\n**Weeks 7-12 (Medium Priority)**:\n- Enhance physical security monitoring\n- Improve change management documentation\n- Complete second DR test cycle\n\n## Auditor Preparation\n\n**Key Interview Topics**:\n- Control environment and tone from the top\n- Incident response capabilities\n- Change management process\n- Access control procedures\n- Monitoring and alerting effectiveness\n\n**Suggested Interviewees**:\n- CTO/CISO (control environment)\n- Security Engineer (technical controls)\n- HR Manager (employee lifecycle)\n- Operations Lead (monitoring, DR)\n\n## Next Steps\n\n1. Review and approve remediation roadmap\n2. Assign owners to each gap remediation\n3. Begin evidence collection for completed controls\n4. Schedule monthly progress reviews\n5. Engage SOC 2 auditor for scoping discussion\n```\n\n**Secondary Outputs**:\n- Evidence collection checklist (Excel/CSV)\n- Control testing templates\n- Auditor questionnaire responses\n- Gap tracking dashboard (JSON for dashboarding tools)\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Missing Evidence Files**\n   - Error: \"Cannot locate security policy in {baseDir}/docs/\"\n   - Resolution: Request document locations from user\n   - Fallback: Mark as evidence gap in report\n\n2. **Incomplete Access Logs**\n   - Error: \"Log retention < SOC 2 requirement (1 year)\"\n   - Resolution: Note current retention period, flag as gap\n   - Remediation: Extend retention, backfill if possible\n\n3. **Undocumented Procedures**\n   - Error: \"No incident response playbook found\"\n   - Resolution: Flag as critical gap requiring documentation\n   - Assistance: Provide template for creating procedure\n\n4. **Cloud Provider Access Required**\n   - Error: \"Cannot assess AWS controls without API access\"\n   - Resolution: Request CloudTrail exports or console screenshots\n   - Alternative: Provide manual checklist for cloud controls\n\n5. **Multiple Environments Not Distinguished**\n   - Error: \"Production and dev configs mixed in {baseDir}/\"\n   - Resolution: Request environment separation or clear labeling\n   - Risk: May audit wrong environment\n\n## Resources\n\n**SOC 2 Framework References**:\n- AICPA Trust Service Criteria: https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/trustdataintegritytaskforce.html\n- SOC 2 Compliance Checklist: https://secureframe.com/hub/soc-2/checklist\n\n**Control Implementation Guides**:\n- CIS Controls: https://www.cisecurity.org/controls/\n- NIST Cybersecurity Framework: https://www.nist.gov/cyberframework\n\n**Compliance Automation Tools**:\n- Drata: SOC 2 compliance automation\n- Vanta: Continuous compliance monitoring\n- Secureframe: Evidence collection platform\n\n**Template Documents**:\n- Information Security Policy: {baseDir}/templates/security-policy-template.md\n- Incident Response Plan: {baseDir}/templates/incident-response-template.md\n- Risk Assessment Template: {baseDir}/templates/risk-assessment-template.xlsx\n\n**Auditor Resources**:\n- Find SOC 2 auditors: https://www.aicpa.org/\n- SOC 2 vs ISO 27001 comparison\n- Type I vs Type II audit differences\n\n**Evidence Examples**:\n- Sample SOC 2 control matrix\n- Evidence collection best practices\n- Auditor interview preparation guide",
      "parentPlugin": {
        "name": "soc2-audit-helper",
        "category": "security",
        "path": "plugins/security/soc2-audit-helper",
        "version": "1.0.0",
        "description": "Assist with SOC2 audit preparation"
      },
      "filePath": "plugins/security/soc2-audit-helper/skills/soc2-audit-helper/SKILL.md"
    },
    {
      "slug": "auditing-access-control",
      "name": "auditing-access-control",
      "description": "Audit access control implementations for security vulnerabilities and misconfigurations. Use when reviewing authentication and authorization. Trigger with 'audit access control', 'check permissions', or 'validate authorization'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill leverages the access-control-auditor plugin to perform comprehensive audits of access control configurations. It helps identify potential security risks associated with overly permissive access, misconfigured permissions, and non-compliance with security policies.\n\n## How It Works\n\n1. **Analyze Request**: Claude identifies the user's intent to audit access control.\n2. **Invoke Plugin**: The access-control-auditor plugin is activated.\n3. **Execute Audit**: The plugin analyzes the specified access control configuration (e.g., IAM policies, ACLs).\n4. **Report Findings**: The plugin generates a report highlighting potential vulnerabilities and misconfigurations.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Audit IAM policies in a cloud environment.\n- Review access control lists (ACLs) for network resources.\n- Assess user permissions in an application.\n- Identify potential privilege escalation paths.\n- Ensure compliance with access control security policies.\n\n## Examples\n\n### Example 1: Auditing AWS IAM Policies\n\nUser request: \"Audit the AWS IAM policies in my account for overly permissive access.\"\n\nThe skill will:\n1. Invoke the access-control-auditor plugin, specifying the AWS account and IAM policies as the target.\n2. Generate a report identifying IAM policies that grant overly broad permissions or violate security best practices.\n\n### Example 2: Reviewing Network ACLs\n\nUser request: \"Review the network ACLs for my VPC to identify any potential security vulnerabilities.\"\n\nThe skill will:\n1. Activate the access-control-auditor plugin, specifying the VPC and network ACLs as the target.\n2. Produce a report highlighting ACL rules that allow unauthorized access or expose the VPC to unnecessary risks.\n\n## Best Practices\n\n- **Scope Definition**: Clearly define the scope of the audit (e.g., specific IAM roles, network segments, applications).\n- **Contextual Information**: Provide contextual information about the environment being audited (e.g., security policies, compliance requirements).\n- **Remediation Guidance**: Use the audit findings to develop and implement remediation strategies to address identified vulnerabilities.\n\n## Integration\n\nThis skill can be integrated with other security plugins to provide a more comprehensive security assessment. For example, it can be combined with a vulnerability scanner to identify vulnerabilities that could be exploited due to access control misconfigurations. It can also be integrated with compliance tools to ensure adherence to regulatory requirements.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "access-control-auditor",
        "category": "security",
        "path": "plugins/security/access-control-auditor",
        "version": "1.0.0",
        "description": "Audit access control implementations"
      },
      "filePath": "plugins/security/access-control-auditor/skills/access-control-auditor/SKILL.md"
    },
    {
      "slug": "automating-api-testing",
      "name": "automating-api-testing",
      "description": "Automate API endpoint testing including request generation, validation, and comprehensive test coverage for REST and GraphQL APIs. Use when testing API contracts, validating OpenAPI specifications, or ensuring endpoint reliability. Trigger with phrases like \"test the API\", \"generate API tests\", or \"validate API contracts\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:api-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API definition files (OpenAPI/Swagger, GraphQL schema, or endpoint documentation)\n- Base URL for the API service (development, staging, or test environment)\n- Authentication credentials or API keys if endpoints require authorization\n- Testing framework installed (Jest, Mocha, Supertest, or equivalent)\n- Network connectivity to the target API service\n\n## Instructions\n\n### Step 1: Analyze API Definition\nExamine the API structure and endpoints:\n1. Use Read tool to load OpenAPI/Swagger specifications from {baseDir}/api-specs/\n2. Identify all available endpoints, HTTP methods, and request/response schemas\n3. Document authentication requirements and rate limiting constraints\n4. Note any deprecated endpoints or breaking changes\n\n### Step 2: Generate Test Cases\nCreate comprehensive test coverage:\n1. Generate CRUD operation tests (Create, Read, Update, Delete)\n2. Add authentication flow tests (login, token refresh, logout)\n3. Include edge case tests (invalid inputs, boundary conditions, malformed requests)\n4. Create contract validation tests against OpenAPI schemas\n5. Add performance tests for critical endpoints\n\n### Step 3: Execute Test Suite\nRun automated API tests:\n1. Use Bash(test:api-*) to execute test framework with generated test files\n2. Validate HTTP status codes match expected responses (200, 201, 400, 401, 404, 500)\n3. Verify response headers (Content-Type, Cache-Control, CORS headers)\n4. Validate response body structure against schemas using JSON Schema validation\n5. Test authentication token expiration and renewal flows\n\n### Step 4: Generate Test Report\nDocument results in {baseDir}/test-reports/api/:\n- Test execution summary with pass/fail counts\n- Coverage metrics by endpoint and HTTP method\n- Failed test details with request/response payloads\n- Performance benchmarks (response times, throughput)\n- Contract violation details if schema mismatches detected\n\n## Output\n\nThe skill generates structured API test artifacts:\n\n### Test Suite Files\nGenerated test files organized by resource:\n- `{baseDir}/tests/api/users.test.js` - User endpoint tests\n- `{baseDir}/tests/api/products.test.js` - Product endpoint tests\n- `{baseDir}/tests/api/auth.test.js` - Authentication flow tests\n\n### Test Coverage Report\n- Endpoint coverage percentage (target: 100% for critical paths)\n- HTTP method coverage per endpoint (GET, POST, PUT, PATCH, DELETE)\n- Authentication scenario coverage (authenticated vs. unauthenticated)\n- Error condition coverage (4xx and 5xx responses)\n\n### Contract Validation Results\n- OpenAPI schema compliance status for each endpoint\n- Breaking changes detected between specification versions\n- Undocumented endpoints or parameters found in implementation\n- Response schema violations with diff details\n\n### Performance Metrics\n- Average response time per endpoint\n- 95th and 99th percentile latencies\n- Requests per second throughput measurements\n- Timeout occurrences and slow endpoint identification\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Connection Refused**\n- Error: Cannot connect to API service at specified base URL\n- Solution: Verify service is running using Bash(test:api-healthcheck); check network connectivity and firewall rules\n\n**Authentication Failures**\n- Error: 401 Unauthorized or 403 Forbidden on protected endpoints\n- Solution: Verify API keys are valid and not expired; ensure bearer token format is correct; check scope permissions\n\n**Schema Validation Errors**\n- Error: Response does not match OpenAPI schema definition\n- Solution: Update OpenAPI specification to match actual API behavior; file bug if API implementation is incorrect\n\n**Timeout Errors**\n- Error: Request exceeded configured timeout threshold\n- Solution: Increase timeout for slow endpoints; investigate performance issues on API server; add retry logic for transient failures\n\n## Resources\n\n### API Testing Frameworks\n- Supertest for Node.js HTTP assertion testing\n- REST-assured for Java API testing\n- Postman/Newman for collection-based API testing\n- Pact for contract testing and consumer-driven contracts\n\n### Validation Libraries\n- Ajv for JSON Schema validation\n- OpenAPI Schema Validator for spec compliance\n- Joi for Node.js schema validation\n- GraphQL Schema validation tools\n\n### Best Practices\n- Test against non-production environments to avoid data corruption\n- Use test data factories to create consistent test fixtures\n- Implement proper test isolation with database cleanup between tests\n- Version control test suites alongside API specifications\n- Run tests in CI/CD pipeline for continuous validation",
      "parentPlugin": {
        "name": "api-test-automation",
        "category": "testing",
        "path": "plugins/testing/api-test-automation",
        "version": "1.0.0",
        "description": "Automated API endpoint testing with request generation, validation, and comprehensive test coverage"
      },
      "filePath": "plugins/testing/api-test-automation/skills/api-test-automation/SKILL.md"
    },
    {
      "slug": "automating-database-backups",
      "name": "automating-database-backups",
      "description": "Use when you need to automate database backup processes with scheduling and encryption. This skill creates backup scripts for PostgreSQL, MySQL, MongoDB, and SQLite with compression. Trigger with phrases like \"automate database backups\", \"schedule database dumps\", \"create backup scripts\", or \"implement disaster recovery for database\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Database credentials with backup permissions (SELECT on all tables)\n- Sufficient disk space for backup files (estimate 2-3x database size with compression)\n- Cron or task scheduler access for automated scheduling\n- Backup destination storage (local disk, NFS, S3, GCS, Azure Blob)\n- Encryption tools installed (gpg, openssl) for secure backups\n- Test database available for restore validation\n\n## Instructions\n\n### Step 1: Assess Backup Requirements\n1. Identify database type (PostgreSQL, MySQL, MongoDB, SQLite)\n2. Determine backup frequency (hourly, daily, weekly, monthly)\n3. Define retention policy (how long to keep backups)\n4. Calculate expected backup size and storage needs\n5. Document RTO (Recovery Time Objective) and RPO (Recovery Point Objective)\n\n### Step 2: Design Backup Strategy\n1. Choose backup type: full, incremental, or differential\n2. Select backup destination (local, network storage, cloud)\n3. Plan backup scheduling to avoid peak usage times\n4. Define backup naming convention with timestamps\n5. Determine compression and encryption requirements\n\n### Step 3: Generate Backup Scripts\n1. Create database-specific backup command (pg_dump, mysqldump, mongodump)\n2. Add compression using gzip or zstd for storage efficiency\n3. Implement encryption using gpg or openssl for security\n4. Add error handling and logging to backup script\n5. Include backup verification (checksum, test restore)\n\n### Step 4: Configure Backup Schedule\n1. Create cron job entry for automated execution\n2. Set appropriate schedule based on backup frequency\n3. Configure environment variables for credentials\n4. Set up log rotation for backup logs\n5. Test manual execution before enabling automation\n\n### Step 5: Implement Retention Policy\n1. Create cleanup script to remove old backups\n2. Implement tiered retention (daily 7 days, weekly 4 weeks, monthly 12 months)\n3. Schedule retention cleanup after backup completion\n4. Add safeguards to prevent accidental deletion of recent backups\n5. Log all backup deletions for audit trail\n\n### Step 6: Create Restore Procedures\n1. Document step-by-step restore process\n2. Create restore scripts for each database type\n3. Include procedures for point-in-time recovery\n4. Test restore process on non-production environment\n5. Document restore time estimates and validation steps\n\n## Output\n\nThis skill produces:\n\n**Backup Scripts**: Shell scripts for database dumps with compression and encryption\n\n**Cron Configurations**: Crontab entries for automated backup scheduling\n\n**Retention Scripts**: Automated cleanup scripts implementing retention policies\n\n**Restore Procedures**: Step-by-step documentation and scripts for database restoration\n\n**Monitoring Configuration**: Log file locations and success/failure notification setup\n\n## Error Handling\n\n**Backup Failures**:\n- Check database connectivity and credentials\n- Verify sufficient disk space for backup files\n- Review database logs for lock or permission issues\n- Implement retry logic with exponential backoff\n- Send alerts on backup failures\n\n**Insufficient Disk Space**:\n- Monitor disk usage before backup execution\n- Implement pre-backup cleanup of old backups\n- Use incremental backups to reduce space requirements\n- Compress backups more aggressively\n- Move backups to remote storage immediately after creation\n\n**Encryption Errors**:\n- Verify encryption tools (gpg, openssl) are installed\n- Check encryption key availability and permissions\n- Test encryption/decryption process manually\n- Document key management procedures\n- Store encryption keys securely separate from backups\n\n**Schedule Conflicts**:\n- Ensure only one backup runs at a time (use lock files)\n- Adjust backup schedule to avoid peak database usage\n- Implement backup queuing for multiple databases\n- Monitor backup duration and adjust schedule if needed\n- Alert if backup duration exceeds acceptable window\n\n## Resources\n\n**Backup Script Templates**:\n- PostgreSQL: `{baseDir}/templates/backup-scripts/postgresql-backup.sh`\n- MySQL: `{baseDir}/templates/backup-scripts/mysql-backup.sh`\n- MongoDB: `{baseDir}/templates/backup-scripts/mongodb-backup.sh`\n- SQLite: `{baseDir}/templates/backup-scripts/sqlite-backup.sh`\n\n**Restore Procedures**: `{baseDir}/docs/restore-procedures/`\n- Point-in-time recovery\n- Full database restore\n- Selective table restore\n- Cross-server migration\n\n**Retention Policy Templates**: `{baseDir}/templates/retention-policies.yaml`\n**Cron Job Examples**: `{baseDir}/examples/crontab-entries.txt`\n**Monitoring Scripts**: `{baseDir}/scripts/backup-monitoring.sh`",
      "parentPlugin": {
        "name": "database-backup-automator",
        "category": "database",
        "path": "plugins/database/database-backup-automator",
        "version": "1.0.0",
        "description": "Automate database backups with scheduling, compression, encryption, and restore procedures"
      },
      "filePath": "plugins/database/database-backup-automator/skills/database-backup-automator/SKILL.md"
    },
    {
      "slug": "backtesting-trading-strategies",
      "name": "backtesting-trading-strategies",
      "description": "Backtest crypto trading strategies against historical data with performance metrics. Use when validating trading strategies with historical data. Trigger with phrases like \"backtest strategy\", \"test trading signals\", or \"validate approach\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:backtest-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:backtest-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "trading-strategy-backtester",
        "category": "crypto",
        "path": "plugins/crypto/trading-strategy-backtester",
        "version": "1.0.0",
        "description": "Backtest trading strategies with historical data, performance metrics, and risk analysis"
      },
      "filePath": "plugins/crypto/trading-strategy-backtester/skills/trading-strategy-backtester/SKILL.md"
    },
    {
      "slug": "building-api-authentication",
      "name": "building-api-authentication",
      "description": "Build secure API authentication systems with OAuth2, JWT, API keys, and session management. Use when implementing secure authentication flows. Trigger with phrases like \"build authentication\", \"add API auth\", or \"secure the API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:auth-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:auth-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-authentication-builder",
        "category": "api-development",
        "path": "plugins/api-development/api-authentication-builder",
        "version": "1.0.0",
        "description": "Build authentication systems with JWT, OAuth2, and API keys"
      },
      "filePath": "plugins/api-development/api-authentication-builder/skills/api-authentication-builder/SKILL.md"
    },
    {
      "slug": "building-api-gateway",
      "name": "building-api-gateway",
      "description": "Create API gateways with routing, load balancing, rate limiting, and authentication. Use when routing and managing multiple API services. Trigger with phrases like \"build API gateway\", \"create API router\", or \"setup API gateway\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:gateway-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:gateway-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-gateway-builder",
        "category": "api-development",
        "path": "plugins/api-development/api-gateway-builder",
        "version": "1.0.0",
        "description": "Build API gateway with routing, authentication, and rate limiting"
      },
      "filePath": "plugins/api-development/api-gateway-builder/skills/api-gateway-builder/SKILL.md"
    },
    {
      "slug": "building-automl-pipelines",
      "name": "building-automl-pipelines",
      "description": "Build automated machine learning pipelines with feature engineering, model selection, and hyperparameter tuning. Use when automating ML workflows from data preparation through model deployment. Trigger with phrases like \"build automl pipeline\", \"automate ml workflow\", or \"create automated training pipeline\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(python:*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Python environment with AutoML libraries (Auto-sklearn, TPOT, H2O AutoML, or PyCaret)\n- Training dataset in accessible format (CSV, Parquet, or database)\n- Understanding of problem type (classification, regression, time-series)\n- Sufficient computational resources for automated search\n- Knowledge of evaluation metrics appropriate for task\n- Target variable and feature columns clearly defined\n\n## Instructions\n\n### Step 1: Define Pipeline Requirements\nSpecify the machine learning task and constraints:\n1. Identify problem type (binary/multi-class classification, regression, etc.)\n2. Define evaluation metrics (accuracy, F1, RMSE, etc.)\n3. Set time and resource budgets for AutoML search\n4. Specify feature types and preprocessing needs\n5. Determine model interpretability requirements\n\n### Step 2: Prepare Data Infrastructure\nSet up data access and preprocessing:\n1. Load training data using Read tool\n2. Perform initial data quality assessment\n3. Configure train/validation/test split strategy\n4. Define feature engineering transformations\n5. Set up data validation checks\n\n### Step 3: Configure AutoML Pipeline\nBuild the automated pipeline configuration:\n- Select AutoML framework based on requirements\n- Define search space for algorithms (random forest, XGBoost, neural networks, etc.)\n- Configure feature preprocessing steps (scaling, encoding, imputation)\n- Set hyperparameter tuning strategy (Bayesian optimization, random search, grid search)\n- Establish early stopping criteria and timeout limits\n\n### Step 4: Execute Pipeline Training\nRun the automated training process:\n1. Initialize AutoML pipeline with configuration\n2. Execute automated feature engineering\n3. Perform model selection across algorithm families\n4. Conduct hyperparameter optimization for top models\n5. Evaluate models using cross-validation\n\n### Step 5: Analyze and Export Results\nEvaluate pipeline performance and prepare for deployment:\n- Compare model performances across metrics\n- Extract best model and configuration\n- Generate feature importance analysis\n- Create model performance visualizations\n- Export trained pipeline for deployment\n\n## Output\n\nThe skill generates comprehensive AutoML pipeline artifacts:\n\n### Pipeline Configuration Files\n```python\n# {baseDir}/automl_config.py\n{\n  \"task_type\": \"classification\",\n  \"time_budget\": 3600,\n  \"algorithms\": [\"rf\", \"xgboost\", \"catboost\"],\n  \"preprocessing\": [\"scaling\", \"encoding\"],\n  \"tuning_strategy\": \"bayesian\",\n  \"cv_folds\": 5\n}\n```\n\n### Pipeline Code\n- Complete Python implementation of AutoML pipeline\n- Data loading and preprocessing functions\n- Feature engineering transformations\n- Model training and evaluation logic\n- Hyperparameter search configuration\n\n### Model Performance Report\n- Best model architecture and hyperparameters\n- Cross-validation scores with confidence intervals\n- Feature importance rankings\n- Confusion matrix or residual plots\n- ROC curves and precision-recall curves (for classification)\n\n### Training Artifacts\n- Serialized best model file (pickle, joblib, or ONNX)\n- Feature preprocessing pipeline\n- Training history and search logs\n- Model performance metrics on test set\n- Documentation for model deployment\n\n### Deployment Package\n- Prediction API code for serving model\n- Input validation and preprocessing scripts\n- Model loading and inference functions\n- Example usage documentation\n- Requirements file with dependencies\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Insufficient Training Time**\n- Error: AutoML search terminated before finding good model\n- Solution: Increase time budget, reduce search space, or use faster algorithms\n\n**Memory Exhaustion**\n- Error: Out of memory during pipeline training\n- Solution: Reduce dataset size through sampling, use incremental learning, or simplify feature engineering\n\n**Poor Model Performance**\n- Error: Best model accuracy below acceptable threshold\n- Solution: Collect more data, engineer better features, expand algorithm search space, or adjust evaluation metrics\n\n**Feature Engineering Failures**\n- Error: Automated feature transformations produce invalid values\n- Solution: Add data validation checks, handle missing values explicitly, restrict transformation types\n\n**Model Convergence Issues**\n- Error: Optimization fails to converge for certain algorithms\n- Solution: Adjust hyperparameter ranges, increase iteration limits, or exclude problematic algorithms\n\n## Resources\n\n### AutoML Frameworks\n- **Auto-sklearn**: Automated scikit-learn pipeline construction with metalearning\n- **TPOT**: Genetic programming for pipeline optimization\n- **H2O AutoML**: Scalable AutoML with ensemble methods\n- **PyCaret**: Low-code ML library with automated workflows\n\n### Feature Engineering\n- Automated feature selection techniques\n- Categorical encoding strategies (one-hot, target, ordinal)\n- Numerical transformation methods (scaling, binning, polynomial features)\n- Time-series feature extraction\n\n### Hyperparameter Optimization\n- Bayesian optimization with Gaussian processes\n- Random search and grid search strategies\n- Hyperband and successive halving algorithms\n- Multi-objective optimization for multiple metrics\n\n### Evaluation Strategies\n- Cross-validation techniques (k-fold, stratified, time-series)\n- Evaluation metrics selection guide\n- Model ensembling and stacking approaches\n- Bias-variance tradeoff analysis\n\n### Best Practices\n- Start with baseline models before AutoML\n- Balance automation with domain knowledge\n- Monitor resource consumption during search\n- Validate model performance on holdout data\n- Document pipeline decisions for reproducibility",
      "parentPlugin": {
        "name": "automl-pipeline-builder",
        "category": "ai-ml",
        "path": "plugins/ai-ml/automl-pipeline-builder",
        "version": "1.0.0",
        "description": "Build AutoML pipelines"
      },
      "filePath": "plugins/ai-ml/automl-pipeline-builder/skills/automl-pipeline-builder/SKILL.md"
    },
    {
      "slug": "building-cicd-pipelines",
      "name": "building-cicd-pipelines",
      "description": "Use when you need to work with deployment and CI/CD. This skill provides deployment automation and pipeline orchestration with comprehensive guidance and automation. Trigger with phrases like \"deploy application\", \"create pipeline\", or \"automate deployment\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/ci-cd-pipeline-builder/`\n\n**Documentation and Guides**: `{baseDir}/docs/ci-cd-pipeline-builder/`\n\n**Example Scripts and Code**: `{baseDir}/examples/ci-cd-pipeline-builder/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/ci-cd-pipeline-builder-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/ci-cd-pipeline-builder-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/ci-cd-pipeline-builder-dashboard.json`",
      "parentPlugin": {
        "name": "ci-cd-pipeline-builder",
        "category": "devops",
        "path": "plugins/devops/ci-cd-pipeline-builder",
        "version": "1.0.0",
        "description": "Build CI/CD pipelines for GitHub Actions, GitLab CI, Jenkins, and more"
      },
      "filePath": "plugins/devops/ci-cd-pipeline-builder/skills/ci-cd-pipeline-builder/SKILL.md"
    },
    {
      "slug": "building-classification-models",
      "name": "building-classification-models",
      "description": "Build and evaluate classification models for supervised learning tasks",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to efficiently build and deploy classification models. It automates the process of model selection, training, and evaluation, providing users with a robust and reliable classification solution. The skill also provides insights into model performance and suggests potential improvements.\n\n## How It Works\n\n1. **Context Analysis**: Claude analyzes the user's request, identifying the dataset, target variable, and any specific requirements for the classification model.\n2. **Model Generation**: The skill utilizes the classification-model-builder plugin to generate code for training a classification model based on the identified dataset and requirements. This includes data preprocessing, feature selection, model selection, and hyperparameter tuning.\n3. **Evaluation and Reporting**: The generated model is trained and evaluated using appropriate metrics (e.g., accuracy, precision, recall, F1-score). Performance metrics and insights are then provided to the user.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Build a classification model from a given dataset.\n- Train a classifier to predict categorical outcomes.\n- Evaluate the performance of a classification model.\n\n## Examples\n\n### Example 1: Building a Spam Classifier\n\nUser request: \"Build a classifier to detect spam emails using this dataset.\"\n\nThe skill will:\n1. Analyze the provided email dataset to identify features and the target variable (spam/not spam).\n2. Generate Python code using the classification-model-builder plugin to train a spam classification model, including data cleaning, feature extraction, and model selection.\n\n### Example 2: Predicting Customer Churn\n\nUser request: \"Create a classification model to predict customer churn using customer data.\"\n\nThe skill will:\n1. Analyze the customer data to identify relevant features and the churn status.\n2. Generate code to build a classification model for churn prediction, including data validation, model training, and performance reporting.\n\n## Best Practices\n\n- **Data Quality**: Ensure the input data is clean and preprocessed before training the model.\n- **Model Selection**: Choose the appropriate classification algorithm based on the characteristics of the data and the specific requirements of the task.\n- **Hyperparameter Tuning**: Optimize the model's hyperparameters to achieve the best possible performance.\n\n## Integration\n\nThis skill integrates with the classification-model-builder plugin to automate the model building process. It can also be used in conjunction with other plugins for data analysis and visualization.",
      "parentPlugin": {
        "name": "classification-model-builder",
        "category": "ai-ml",
        "path": "plugins/ai-ml/classification-model-builder",
        "version": "1.0.0",
        "description": "Build classification models"
      },
      "filePath": "plugins/ai-ml/classification-model-builder/skills/classification-model-builder/SKILL.md"
    },
    {
      "slug": "building-gitops-workflows",
      "name": "building-gitops-workflows",
      "description": "Use when constructing GitOps workflows using ArgoCD or Flux. Trigger with phrases like \"create GitOps workflow\", \"setup ArgoCD\", \"configure Flux\", or \"automate Kubernetes deployments\". Generates production-ready configurations, implements best practices, and ensures security-first approach for continuous deployment.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(kubectl:*)",
        "Bash(git:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Kubernetes cluster is accessible and kubectl is configured\n- Git repository is available for GitOps source\n- ArgoCD or Flux is installed on the cluster (or ready to install)\n- Appropriate RBAC permissions for GitOps operator\n- Network connectivity between cluster and Git repository\n\n## Instructions\n\n1. **Select GitOps Tool**: Determine whether to use ArgoCD or Flux based on requirements\n2. **Define Application Structure**: Establish repository layout with environment separation (dev/staging/prod)\n3. **Generate Manifests**: Create Application/Kustomization files pointing to Git sources\n4. **Configure Sync Policy**: Set automated or manual sync with self-heal and prune options\n5. **Implement RBAC**: Define service accounts and role bindings for GitOps operator\n6. **Set Up Monitoring**: Configure notifications and health checks for deployments\n7. **Validate Configuration**: Test sync behavior and verify reconciliation loops\n\n## Output\n\nGenerates GitOps workflow configurations including:\n\n**ArgoCD Application Manifest:**\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-name\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/repo\n    path: manifests/prod\n    targetRevision: main\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n```\n\n**Flux Kustomization:**\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: app-name\n  namespace: flux-system\nspec:\n  interval: 5m\n  path: ./manifests/prod\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: app-repo\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Sync Failures**\n- Error: \"ComparisonError: Failed to load target state\"\n- Solution: Verify Git repository URL, credentials, and target path exist\n\n**RBAC Permissions**\n- Error: \"User cannot create resource in API group\"\n- Solution: Grant GitOps service account appropriate cluster roles\n\n**Out of Sync State**\n- Warning: \"Application is OutOfSync\"\n- Solution: Enable automated sync or manually sync via UI/CLI\n\n**Git Authentication**\n- Error: \"Authentication failed for repository\"\n- Solution: Configure SSH keys or access tokens in {baseDir}/.git/config\n\n**Resource Conflicts**\n- Error: \"Resource already exists and is not managed by GitOps\"\n- Solution: Import existing resources or remove conflicting manual deployments\n\n## Resources\n\n- ArgoCD documentation: https://argo-cd.readthedocs.io/\n- Flux documentation: https://fluxcd.io/docs/\n- GitOps principles and patterns guide\n- Kubernetes manifest best practices\n- Repository structure templates in {baseDir}/gitops-examples/",
      "parentPlugin": {
        "name": "gitops-workflow-builder",
        "category": "devops",
        "path": "plugins/devops/gitops-workflow-builder",
        "version": "1.0.0",
        "description": "Build GitOps workflows with ArgoCD and Flux"
      },
      "filePath": "plugins/devops/gitops-workflow-builder/skills/gitops-workflow-builder/SKILL.md"
    },
    {
      "slug": "building-graphql-server",
      "name": "building-graphql-server",
      "description": "Build production-ready GraphQL servers with schema design, resolvers, and subscriptions. Use when building GraphQL APIs with schemas and resolvers. Trigger with phrases like \"build GraphQL API\", \"create GraphQL server\", or \"setup GraphQL\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:graphql-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:graphql-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "graphql-server-builder",
        "category": "api-development",
        "path": "plugins/api-development/graphql-server-builder",
        "version": "1.0.0",
        "description": "Build GraphQL servers with schema-first design, resolvers, and subscriptions"
      },
      "filePath": "plugins/api-development/graphql-server-builder/skills/graphql-server-builder/SKILL.md"
    },
    {
      "slug": "building-neural-networks",
      "name": "building-neural-networks",
      "description": "This skill allows claude to construct and configure neural network architectures",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to design and implement neural networks tailored to specific tasks. It leverages the neural-network-builder plugin to automate the process of defining network architectures, configuring layers, and setting training parameters. This ensures efficient and accurate creation of neural network models.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude analyzes the user's request to understand the desired neural network architecture, task, and performance goals.\n2. **Generating Configuration**: Based on the analysis, Claude generates the appropriate configuration for the neural-network-builder plugin, specifying the layers, activation functions, and other relevant parameters.\n3. **Executing Build**: Claude executes the `build-nn` command, triggering the neural-network-builder plugin to construct the neural network based on the generated configuration.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new neural network architecture for a specific machine learning task.\n- Modify an existing neural network's layers, parameters, or training process.\n- Design a neural network using specific layer types, such as convolutional, recurrent, or transformer layers.\n\n## Examples\n\n### Example 1: Image Classification\n\nUser request: \"Build a convolutional neural network for image classification with three convolutional layers and two fully connected layers.\"\n\nThe skill will:\n1. Analyze the request and determine the required CNN architecture.\n2. Generate the configuration for the `build-nn` command, specifying the layer types, filter sizes, and activation functions.\n\n### Example 2: Text Generation\n\nUser request: \"Define an RNN architecture for text generation with LSTM cells and an embedding layer.\"\n\nThe skill will:\n1. Analyze the request and determine the required RNN architecture.\n2. Generate the configuration for the `build-nn` command, specifying the LSTM cell parameters, embedding dimension, and output layer.\n\n## Best Practices\n\n- **Layer Selection**: Choose appropriate layer types (e.g., convolutional, recurrent, transformer) based on the task and data characteristics.\n- **Parameter Tuning**: Experiment with different parameter values (e.g., learning rate, batch size, number of layers) to optimize performance.\n- **Regularization**: Implement regularization techniques (e.g., dropout, L1/L2 regularization) to prevent overfitting.\n\n## Integration\n\nThis skill integrates with the core Claude Code environment by utilizing the `build-nn` command provided by the neural-network-builder plugin. It can be combined with other skills for data preprocessing, model evaluation, and deployment.",
      "parentPlugin": {
        "name": "neural-network-builder",
        "category": "ai-ml",
        "path": "plugins/ai-ml/neural-network-builder",
        "version": "1.0.0",
        "description": "Build and configure neural network architectures"
      },
      "filePath": "plugins/ai-ml/neural-network-builder/skills/neural-network-builder/SKILL.md"
    },
    {
      "slug": "building-recommendation-systems",
      "name": "building-recommendation-systems",
      "description": "This skill empowers claude to construct recommendation systems using",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill enables Claude to design and implement recommendation systems tailored to specific datasets and use cases. It automates the process of selecting appropriate algorithms, preprocessing data, training models, and evaluating performance, ultimately providing users with a functional recommendation engine.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude identifies the type of recommendation needed (collaborative, content-based, hybrid), data availability, and performance goals.\n2. **Generating Code**: Claude generates Python code using relevant libraries (e.g., scikit-learn, TensorFlow, PyTorch) to build the recommendation model. This includes data loading, preprocessing, model training, and evaluation.\n3. **Implementing Best Practices**: The code incorporates best practices for recommendation system development, such as handling cold starts, addressing scalability, and mitigating bias.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Build a personalized movie recommendation system.\n- Create a product recommendation engine for an e-commerce platform.\n- Implement a content recommendation system for a news website.\n\n## Examples\n\n### Example 1: Personalized Movie Recommendations\n\nUser request: \"Build a movie recommendation system using collaborative filtering.\"\n\nThe skill will:\n1. Generate code to load and preprocess movie rating data.\n2. Implement a collaborative filtering algorithm (e.g., matrix factorization) to predict user preferences.\n\n### Example 2: E-commerce Product Recommendations\n\nUser request: \"Create a product recommendation engine for an online store, using content-based filtering.\"\n\nThe skill will:\n1. Generate code to extract features from product descriptions and user purchase history.\n2. Implement a content-based filtering algorithm to recommend similar products.\n\n## Best Practices\n\n- **Data Preprocessing**: Ensure data is properly cleaned and formatted before training the recommendation model.\n- **Model Evaluation**: Use appropriate metrics (e.g., precision, recall, NDCG) to evaluate the performance of the recommendation system.\n- **Scalability**: Design the recommendation system to handle large datasets and user bases efficiently.\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins to access data sources, deploy models, and monitor performance. For example, it can use data analysis plugins to extract features from raw data and deployment plugins to deploy the recommendation system to a production environment.",
      "parentPlugin": {
        "name": "recommendation-engine",
        "category": "ai-ml",
        "path": "plugins/ai-ml/recommendation-engine",
        "version": "1.0.0",
        "description": "Build recommendation systems and engines"
      },
      "filePath": "plugins/ai-ml/recommendation-engine/skills/recommendation-engine/SKILL.md"
    },
    {
      "slug": "building-terraform-modules",
      "name": "building-terraform-modules",
      "description": "This skill empowers claude to build reusable terraform modules based",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill allows Claude to efficiently generate Terraform modules, streamlining infrastructure-as-code development. By utilizing the terraform-module-builder plugin, it ensures modules are production-ready, well-documented, and incorporate best practices.\n\n## How It Works\n\n1. **Receiving User Request**: Claude receives a request to create a Terraform module, including details about the module's purpose and desired features.\n2. **Generating Module Structure**: Claude invokes the terraform-module-builder plugin to create the basic file structure and configuration files for the module.\n3. **Customizing Module Content**: Claude uses the user's specifications to populate the module with variables, outputs, and resource definitions, ensuring best practices are followed.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new Terraform module from scratch.\n- Generate production-ready Terraform configuration files.\n- Implement infrastructure as code using Terraform modules.\n\n## Examples\n\n### Example 1: Creating a VPC Module\n\nUser request: \"Create a Terraform module for a VPC with public and private subnets, a NAT gateway, and appropriate security groups.\"\n\nThe skill will:\n1. Invoke the terraform-module-builder plugin to generate a basic VPC module structure.\n2. Populate the module with Terraform code to define the VPC, subnets, NAT gateway, and security groups based on best practices.\n\n### Example 2: Generating an S3 Bucket Module\n\nUser request: \"Generate a Terraform module for an S3 bucket with versioning enabled, encryption at rest, and a lifecycle policy for deleting objects after 30 days.\"\n\nThe skill will:\n1. Invoke the terraform-module-builder plugin to create a basic S3 bucket module structure.\n2. Populate the module with Terraform code to define the S3 bucket with the requested features (versioning, encryption, lifecycle policy).\n\n## Best Practices\n\n- **Documentation**: Ensure the generated Terraform module includes comprehensive documentation, explaining the module's purpose, inputs, and outputs.\n- **Security**: Implement security best practices, such as using least privilege principles and encrypting sensitive data.\n- **Modularity**: Design the Terraform module to be reusable and configurable, allowing it to be easily adapted to different environments.\n\n## Integration\n\nThis skill integrates seamlessly with other Claude Code plugins by providing a foundation for infrastructure provisioning. The generated Terraform modules can be used by other plugins to deploy and manage resources in various cloud environments.",
      "parentPlugin": {
        "name": "terraform-module-builder",
        "category": "devops",
        "path": "plugins/devops/terraform-module-builder",
        "version": "1.0.0",
        "description": "Build reusable Terraform modules"
      },
      "filePath": "plugins/devops/terraform-module-builder/skills/terraform-module-builder/SKILL.md"
    },
    {
      "slug": "building-websocket-server",
      "name": "building-websocket-server",
      "description": "Build scalable WebSocket servers for real-time bidirectional communication. Use when enabling real-time bidirectional communication. Trigger with phrases like \"build WebSocket server\", \"add real-time API\", or \"implement WebSocket\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:websocket-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:websocket-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "websocket-server-builder",
        "category": "api-development",
        "path": "plugins/api-development/websocket-server-builder",
        "version": "1.0.0",
        "description": "Build WebSocket servers for real-time bidirectional communication"
      },
      "filePath": "plugins/api-development/websocket-server-builder/skills/websocket-server-builder/SKILL.md"
    },
    {
      "slug": "calculating-crypto-taxes",
      "name": "calculating-crypto-taxes",
      "description": "Calculate cryptocurrency tax obligations with cost basis tracking and jurisdiction rules. Use when calculating crypto tax obligations. Trigger with phrases like \"calculate crypto taxes\", \"compute tax liability\", or \"generate tax report\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:tax-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:tax-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "crypto-tax-calculator",
        "category": "crypto",
        "path": "plugins/crypto/crypto-tax-calculator",
        "version": "1.0.0",
        "description": "Calculate crypto taxes with FIFO/LIFO methods and generate tax reports"
      },
      "filePath": "plugins/crypto/crypto-tax-calculator/skills/crypto-tax-calculator/SKILL.md"
    },
    {
      "slug": "checking-hipaa-compliance",
      "name": "checking-hipaa-compliance",
      "description": "Check HIPAA compliance for healthcare data security requirements. Use when auditing healthcare applications. Trigger with 'check HIPAA compliance', 'validate health data security', or 'audit PHI protection'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the process of identifying potential HIPAA compliance issues within a software project. By using the hipaa-compliance-checker plugin, it helps developers and security professionals proactively address vulnerabilities and ensure adherence to HIPAA guidelines.\n\n## How It Works\n\n1. **Analyze Request**: Claude identifies the user's intent to check for HIPAA compliance.\n2. **Initiate Plugin**: Claude activates the hipaa-compliance-checker plugin.\n3. **Execute Checks**: The plugin scans the specified codebase, configuration files, or documentation for potential HIPAA violations.\n4. **Generate Report**: The plugin generates a detailed report outlining identified issues and their potential impact on HIPAA compliance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Evaluate a codebase for HIPAA compliance before deployment.\n- Identify potential HIPAA violations in existing systems.\n- Assess the HIPAA readiness of infrastructure configurations.\n- Verify that documentation adheres to HIPAA guidelines.\n\n## Examples\n\n### Example 1: Checking a codebase for HIPAA compliance\n\nUser request: \"Check HIPAA compliance of the patient data API codebase.\"\n\nThe skill will:\n1. Activate the hipaa-compliance-checker plugin.\n2. Scan the specified API codebase for potential HIPAA violations.\n3. Generate a report listing any identified issues, such as insecure data storage or insufficient access controls.\n\n### Example 2: Assessing infrastructure configuration for HIPAA readiness\n\nUser request: \"Assess the HIPAA readiness of our AWS infrastructure configuration.\"\n\nThe skill will:\n1. Activate the hipaa-compliance-checker plugin.\n2. Analyze the AWS infrastructure configuration files for potential HIPAA violations, such as misconfigured security groups or inadequate encryption.\n3. Generate a report outlining any identified issues and recommendations for remediation.\n\n## Best Practices\n\n- **Specify Target**: Always clearly specify the target (e.g., codebase, configuration file, documentation) for the HIPAA compliance check.\n- **Review Reports**: Carefully review the generated reports to understand the identified issues and their potential impact.\n- **Prioritize Remediation**: Prioritize the remediation of identified issues based on their severity and potential impact on HIPAA compliance.\n\n## Integration\n\nThis skill can be integrated with other security and compliance tools to provide a comprehensive view of a system's security posture. The generated reports can be used as input for vulnerability management systems and security information and event management (SIEM) platforms.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "hipaa-compliance-checker",
        "category": "security",
        "path": "plugins/security/hipaa-compliance-checker",
        "version": "1.0.0",
        "description": "Check HIPAA compliance"
      },
      "filePath": "plugins/security/hipaa-compliance-checker/skills/hipaa-compliance-checker/SKILL.md"
    },
    {
      "slug": "checking-infrastructure-compliance",
      "name": "checking-infrastructure-compliance",
      "description": "Use when you need to work with compliance checking. This skill provides compliance monitoring and validation with comprehensive guidance and automation. Trigger with phrases like \"check compliance\", \"validate policies\", or \"audit compliance\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/compliance-checker/`\n\n**Documentation and Guides**: `{baseDir}/docs/compliance-checker/`\n\n**Example Scripts and Code**: `{baseDir}/examples/compliance-checker/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/compliance-checker-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/compliance-checker-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/compliance-checker-dashboard.json`",
      "parentPlugin": {
        "name": "compliance-checker",
        "category": "devops",
        "path": "plugins/devops/compliance-checker",
        "version": "1.0.0",
        "description": "Check infrastructure compliance (SOC2, HIPAA, PCI-DSS)"
      },
      "filePath": "plugins/devops/compliance-checker/skills/compliance-checker/SKILL.md"
    },
    {
      "slug": "checking-owasp-compliance",
      "name": "checking-owasp-compliance",
      "description": "Check compliance with OWASP Top 10 security risks and best practices. Use when performing comprehensive security audits. Trigger with 'check OWASP compliance', 'audit web security', or 'validate OWASP'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to assess your project's adherence to the OWASP Top 10 (2021) security guidelines. It automates the process of identifying potential vulnerabilities related to common web application security risks, providing actionable insights to improve your application's security posture.\n\n## How It Works\n\n1. **Initiate Scan**: The skill activates the owasp-compliance-checker plugin upon request.\n2. **Analyze Codebase**: The plugin scans the codebase for potential vulnerabilities related to each OWASP Top 10 category.\n3. **Generate Report**: A detailed report is generated, highlighting compliance gaps and providing specific remediation guidance for each identified issue.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Evaluate your application's security posture against the OWASP Top 10 (2021).\n- Identify potential vulnerabilities related to common web application security risks.\n- Obtain actionable remediation guidance to address identified vulnerabilities.\n- Generate a compliance report for auditing or reporting purposes.\n\n## Examples\n\n### Example 1: Identifying SQL Injection Vulnerabilities\n\nUser request: \"Check OWASP compliance for SQL injection vulnerabilities.\"\n\nThe skill will:\n1. Activate the owasp-compliance-checker plugin.\n2. Scan the codebase for potential SQL injection vulnerabilities.\n3. Generate a report highlighting any identified SQL injection vulnerabilities and providing remediation guidance.\n\n### Example 2: Assessing Overall OWASP Compliance\n\nUser request: \"/owasp\"\n\nThe skill will:\n1. Activate the owasp-compliance-checker plugin.\n2. Scan the entire codebase for vulnerabilities across all OWASP Top 10 categories.\n3. Generate a comprehensive report detailing compliance gaps and remediation steps for each category.\n\n## Best Practices\n\n- **Regular Scanning**: Integrate OWASP compliance checks into your development workflow for continuous security monitoring.\n- **Prioritize Remediation**: Address identified vulnerabilities based on their severity and potential impact.\n- **Stay Updated**: Keep your OWASP compliance checker plugin updated to benefit from the latest vulnerability detection rules and remediation guidance.\n\n## Integration\n\nThis skill can be integrated with other plugins to automate vulnerability remediation or generate comprehensive security reports. For example, it can be used in conjunction with a code modification plugin to automatically apply recommended fixes for identified vulnerabilities.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "owasp-compliance-checker",
        "category": "security",
        "path": "plugins/security/owasp-compliance-checker",
        "version": "1.0.0",
        "description": "Check OWASP Top 10 compliance"
      },
      "filePath": "plugins/security/owasp-compliance-checker/skills/owasp-compliance-checker/SKILL.md"
    },
    {
      "slug": "checking-session-security",
      "name": "checking-session-security",
      "description": "Analyze session management implementations to identify security vulnerabilities in web applications. Use when you need to audit session handling, check for session fixation risks, review session timeout configurations, or validate session ID generation security. Trigger with phrases like \"check session security\", \"audit session management\", \"review session handling\", or \"session fixation vulnerability\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Source code accessible in {baseDir}/\n- Session management code locations known (auth modules, middleware)\n- Framework information (Express, Django, Spring, etc.)\n- Configuration files for session settings\n- Write permissions for security report in {baseDir}/security-reports/\n\n## Instructions\n\n### 1. Code Discovery Phase\n\nLocate session management code:\n- Authentication/login handlers\n- Session middleware configuration\n- Cookie handling code\n- Session storage implementations\n- Logout/session termination code\n\n**Common file patterns**:\n- `**/auth/**`, `**/session/**`, `**/middleware/**`\n- `session.config.*`, `auth.config.*`\n- Framework-specific: `settings.py`, `application.yml`, `web.config`\n\n### 2. Session ID Security Analysis\n\n**Generation Strength**:\n- Check for cryptographically secure random generators\n- Verify sufficient entropy (at least 128 bits)\n- Ensure unpredictable session ID patterns\n- No sequential or timestamp-based IDs\n\n**Bad Patterns to Detect**:\n```javascript\n// INSECURE: Predictable\nsessionId = Date.now() + userId;\nsessionId = Math.random().toString();\n\n// SECURE: Cryptographically random\nsessionId = crypto.randomBytes(32).toString('hex');\n```\n\n### 3. Session Fixation Vulnerability Check\n\nVerify session ID regeneration:\n- New session ID generated after login\n- Session ID changes on privilege escalation\n- Old session ID invalidated after regeneration\n\n**Vulnerable Pattern**:\n```python\n# INSECURE: Reuses existing session ID\ndef login(username, password):\n    if authenticate(username, password):\n        session['authenticated'] = True  # Session ID not regenerated\n```\n\n**Secure Pattern**:\n```python\n# SECURE: Regenerates session ID\ndef login(username, password):\n    if authenticate(username, password):\n        session.regenerate()  # New session ID\n        session['authenticated'] = True\n```\n\n### 4. Session Timeout Analysis\n\nCheck timeout configurations:\n- Idle timeout (session inactivity limit)\n- Absolute timeout (maximum session lifetime)\n- Remember-me token expiration\n- Sensitive operation re-authentication\n\n**Configuration Review**:\n- Idle timeout: 15-30 minutes recommended\n- Absolute timeout: 8-12 hours maximum\n- Financial apps: Shorter timeouts (5-10 minutes)\n- Sensitive operations: Force re-authentication\n\n### 5. Cookie Security Attributes\n\nVerify secure cookie flags:\n- `HttpOnly`: Prevents JavaScript access (XSS protection)\n- `Secure`: Ensures HTTPS-only transmission\n- `SameSite`: Prevents CSRF attacks (Strict or Lax)\n- `Domain` and `Path`: Properly scoped\n\n**Insecure Cookie**:\n```javascript\nres.cookie('sessionId', sessionId);  // No security flags\n```\n\n**Secure Cookie**:\n```javascript\nres.cookie('sessionId', sessionId, {\n  httpOnly: true,\n  secure: true,\n  sameSite: 'strict',\n  maxAge: 3600000  // 1 hour\n});\n```\n\n### 6. Session Storage Security\n\nEvaluate storage mechanisms:\n- Server-side storage (recommended): Redis, Memcached, database\n- Encrypted storage for sensitive data\n- No sensitive data in client-side cookies\n- Proper session cleanup on logout\n\n**Check for**:\n- Sessions persisting after logout\n- Lack of session invalidation\n- Sensitive data stored in cookies\n- Unencrypted session stores\n\n### 7. Session Hijacking Protections\n\nVerify anti-hijacking measures:\n- IP address binding (with caution for mobile users)\n- User-Agent validation\n- Token-based CSRF protection\n- Automatic logout on suspicious activity\n\n### 8. Concurrent Session Management\n\nCheck concurrent session handling:\n- Limit concurrent sessions per user\n- Session conflict detection\n- Force logout previous sessions option\n- Session monitoring and alerting\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Session security report saved to {baseDir}/security-reports/session-security-YYYYMMDD.md\n\n**Report Structure**:\n```\n# Session Security Analysis Report\nAnalysis Date: 2024-01-15\nApplication: Web Portal\nFramework: Express.js\n\n## Executive Summary\n- Overall Security Rating: MEDIUM RISK\n- Critical Issues: 2\n- High Priority Issues: 4\n- Recommendations: 8\n\n## Critical Findings\n\n### 1. Session Fixation Vulnerability\n**File**: {baseDir}/src/auth/login.js\n**Line**: 45\n**Issue**: Session ID not regenerated after authentication\n**Risk**: Attacker can hijack authenticated session\n**Code**:\n```javascript\nfunction handleLogin(req, res) {\n  if (validateCredentials(req.body)) {\n    req.session.authenticated = true;  // VULNERABLE\n    res.redirect('/dashboard');\n  }\n}\n```\n**Remediation**:\n```javascript\nfunction handleLogin(req, res) {\n  if (validateCredentials(req.body)) {\n    req.session.regenerate((err) => {  // SECURE\n      req.session.authenticated = true;\n      res.redirect('/dashboard');\n    });\n  }\n}\n```\n\n### 2. Missing HttpOnly Flag\n**File**: {baseDir}/config/session.js\n**Line**: 12\n**Issue**: Session cookies accessible to JavaScript\n**Risk**: XSS attacks can steal session tokens\n**Remediation**: Set `httpOnly: true` in cookie configuration\n\n## High Priority Findings\n\n### 3. Weak Session Timeout\n**File**: {baseDir}/config/session.js\n**Line**: 15\n**Issue**: Session timeout set to 24 hours\n**Risk**: Extended exposure window for compromised sessions\n**Current**: `maxAge: 86400000  // 24 hours`\n**Recommendation**: `maxAge: 1800000  // 30 minutes`\n\n### 4. Insecure Session ID Generation\n**File**: {baseDir}/src/auth/session-manager.js\n**Line**: 28\n**Issue**: Using Math.random() for session IDs\n**Risk**: Predictable session IDs enable brute-force attacks\n**Remediation**: Use crypto.randomBytes()\n\n[Additional findings...]\n\n## Session Configuration Summary\n- Session Store: Redis (Good)\n- Cookie Secure Flag: Missing (Critical)\n- Cookie HttpOnly Flag: Missing (Critical)\n- Cookie SameSite: None (High Risk)\n- Idle Timeout: 24 hours (Too Long)\n- Session Regeneration: Not Implemented (Critical)\n\n## Compliance Check\n- OWASP Session Management: 4/10 controls implemented\n- PCI-DSS 8.1.8: Non-compliant (timeout too long)\n- NIST 800-63B: Partial compliance\n\n## Remediation Priority\n1. [CRITICAL] Implement session regeneration on login\n2. [CRITICAL] Enable HttpOnly and Secure cookie flags\n3. [HIGH] Reduce session timeout to 30 minutes\n4. [HIGH] Implement cryptographically secure session IDs\n5. [MEDIUM] Add SameSite=Strict to cookies\n6. [MEDIUM] Implement concurrent session limits\n```\n\n**Secondary Outputs**:\n- Vulnerable code snippets with line numbers\n- Remediation code examples\n- Framework-specific configuration guide\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Cannot Locate Session Management Code**\n   - Error: \"No session handling code found in {baseDir}/\"\n   - Resolution: Search for framework-specific patterns\n   - Fallback: Request explicit file paths from user\n\n2. **Framework Not Recognized**\n   - Error: \"Unknown session framework\"\n   - Resolution: Apply generic session security checks\n   - Note: Framework-specific recommendations unavailable\n\n3. **Encrypted or Obfuscated Code**\n   - Error: \"Cannot analyze minified/compiled code\"\n   - Resolution: Request source code or unminified version\n   - Limitation: Document inability to fully audit\n\n4. **Custom Session Implementation**\n   - Error: \"Non-standard session management detected\"\n   - Resolution: Apply fundamental security principles\n   - Extra Scrutiny: Custom implementations often have flaws\n\n5. **Configuration in Environment Variables**\n   - Error: \"Session config in environment, not code\"\n   - Resolution: Request .env.example or config documentation\n   - Fallback: Provide general configuration recommendations\n\n## Resources\n\n**OWASP Guidelines**:\n- Session Management Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html\n- OWASP Top 10 - Broken Authentication: https://owasp.org/www-project-top-ten/\n\n**Standards and Best Practices**:\n- NIST 800-63B Authentication: https://pages.nist.gov/800-63-3/sp800-63b.html\n- PCI-DSS Session Requirements: https://www.pcisecuritystandards.org/\n\n**Framework-Specific Guides**:\n- Express.js Session Security: https://expressjs.com/en/advanced/best-practice-security.html\n- Django Session Framework: https://docs.djangoproject.com/en/stable/topics/http/sessions/\n- Spring Session: https://spring.io/projects/spring-session\n\n**Security Tools**:\n- Burp Suite for session testing\n- OWASP ZAP session analysis\n- Browser DevTools for cookie inspection\n\n**Common Vulnerabilities**:\n- CWE-384: Session Fixation\n- CWE-613: Insufficient Session Expiration\n- CWE-539: Information Exposure Through Persistent Cookies\n- CWE-5 52: Insufficiently Protected Credentials",
      "parentPlugin": {
        "name": "session-security-checker",
        "category": "security",
        "path": "plugins/security/session-security-checker",
        "version": "1.0.0",
        "description": "Check session security implementation"
      },
      "filePath": "plugins/security/session-security-checker/skills/session-security-checker/SKILL.md"
    },
    {
      "slug": "code-formatter",
      "name": "code-formatter",
      "description": "Automatically formats and validates code files using Prettier and other formatting tools. Use when users mention \"format my code\", \"fix formatting\", \"apply code style\", \"check formatting\", \"make code consistent\", or \"clean up code formatting\". Handles JavaScript, TypeScript, JSON, CSS, Markdown, and many other file types. allowed-tools: Read, Write, Edit, Grep, Glob, Bash version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Code Formatter Skill\n\nThis skill provides comprehensive code formatting and validation capabilities using industry-standard tools like Prettier.\n\n## Prerequisites\n\n- Node.js and npm/npx installed\n- Prettier available globally or locally\n- Write permissions for target files\n- Supported file types in the project\n\n## Instructions\n\n### 1. Analyze Current Formatting\n\nFirst, check the current formatting state of files:\n\n```bash\n# Check if prettier is available\nnpx prettier --version || npm install -g prettier\n\n# Find all formattable files\nfind . -type f \\( -name \"*.js\" -o -name \"*.jsx\" -o -name \"*.ts\" -o -name \"*.tsx\" -o -name \"*.json\" -o -name \"*.css\" -o -name \"*.md\" \\) -not -path \"*/node_modules/*\" -not -path \"*/dist/*\"\n\n# Check which files need formatting\nnpx prettier --check \"**/*.{js,jsx,ts,tsx,json,css,md}\" --ignore-path .prettierignore\n```\n\n### 2. Configure Formatting Rules\n\nCreate or check for existing Prettier configuration:\n\n```javascript\n// .prettierrc\n{\n  \"semi\": true,\n  \"singleQuote\": true,\n  \"tabWidth\": 2,\n  \"trailingComma\": \"es5\",\n  \"printWidth\": 100,\n  \"bracketSpacing\": true,\n  \"arrowParens\": \"avoid\"\n}\n```\n\n### 3. Apply Formatting\n\nFormat individual files or entire directories:\n\n```bash\n# Format a single file\nnpx prettier --write src/app.js\n\n# Format all JavaScript files\nnpx prettier --write \"**/*.js\"\n\n# Format with specific config\nnpx prettier --write --config .prettierrc \"src/**/*.{js,jsx,ts,tsx}\"\n\n# Dry run to see what would change\nnpx prettier --check src/\n```\n\n### 4. Set Up Ignore Patterns\n\nCreate .prettierignore for files to skip:\n\n```\n# Dependencies\nnode_modules/\nvendor/\n\n# Build outputs\ndist/\nbuild/\n*.min.js\n*.min.css\n\n# Generated files\ncoverage/\n*.lock\n```\n\n### 5. Integrate with Git Hooks (Optional)\n\nSet up pre-commit formatting:\n\n```bash\n# Install husky and lint-staged\nnpm install --save-dev husky lint-staged\n\n# Configure in package.json\n{\n  \"lint-staged\": {\n    \"*.{js,jsx,ts,tsx,json,css,md}\": [\n      \"prettier --write\"\n    ]\n  }\n}\n```\n\n## Output\n\nThe formatter provides detailed feedback:\n\n### Success Output\n```\n‚úì Formatted: src/app.js\n‚úì Formatted: src/components/Button.jsx\n‚úì Formatted: package.json\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚úÖ Successfully formatted 3 files\n```\n\n### Validation Output\n```\nChecking formatting...\n[warn] src/app.js\n[warn] src/utils/helper.ts\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚ö† Found 2 files that need formatting\n```\n\n### Error Output\n```\n‚úó Error: Cannot format src/broken.js\n  SyntaxError: Unexpected token (line 15:8)\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚ùå Failed to format 1 file\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n### 1. Prettier Not Found\n```bash\n# Install globally\nnpm install -g prettier\n\n# Or use npx (no installation needed)\nnpx prettier --version\n```\n\n### 2. Syntax Errors\n```bash\n# Validate JavaScript syntax first\nnpx eslint src/app.js --fix-dry-run\n\n# Check for parsing errors\nnpx prettier --debug-check src/app.js\n```\n\n### 3. Configuration Conflicts\n```bash\n# Find all config files\nfind . -name \".prettier*\" -o -name \"prettier.config.js\"\n\n# Use specific config\nnpx prettier --config ./custom-prettier.json --write src/\n```\n\n### 4. Permission Issues\n```bash\n# Check file permissions\nls -la src/app.js\n\n# Fix permissions if needed\nchmod u+w src/app.js\n```\n\n## Resources\n\n### File Type Support\n\n| Extension | Language | Notes |\n|-----------|----------|-------|\n| .js, .jsx | JavaScript | ES6+ supported |\n| .ts, .tsx | TypeScript | Type annotations preserved |\n| .json | JSON | Sorts keys optionally |\n| .css, .scss | CSS/SASS | Vendor prefixes handled |\n| .md, .mdx | Markdown | Table formatting |\n| .yaml, .yml | YAML | Preserves comments |\n| .html | HTML | Attribute formatting |\n| .vue | Vue | Template + script + style |\n| .svelte | Svelte | Full component support |\n\n### Configuration Options\n\n```javascript\n{\n  // Semicolons\n  \"semi\": true,              // Add semicolons\n\n  // Quotes\n  \"singleQuote\": true,       // Use single quotes\n  \"jsxSingleQuote\": false,   // JSX double quotes\n\n  // Indentation\n  \"tabWidth\": 2,             // Spaces per tab\n  \"useTabs\": false,          // Use spaces\n\n  // Line Length\n  \"printWidth\": 100,         // Line wrap length\n  \"proseWrap\": \"preserve\",   // Markdown wrapping\n\n  // Trailing Commas\n  \"trailingComma\": \"es5\",    // Valid in ES5\n\n  // Brackets\n  \"bracketSpacing\": true,    // { foo: bar }\n  \"bracketSameLine\": false,  // JSX brackets\n\n  // Arrow Functions\n  \"arrowParens\": \"avoid\",    // x => x vs (x) => x\n\n  // Formatting\n  \"htmlWhitespaceSensitivity\": \"css\",\n  \"endOfLine\": \"lf\",\n  \"embeddedLanguageFormatting\": \"auto\"\n}\n```\n\n### Command Reference\n\n```bash\n# Basic formatting\nprettier --write <file>           # Format file\nprettier --check <file>           # Check if formatted\nprettier --debug-check <file>     # Debug parse errors\n\n# Multiple files\nprettier --write \"src/**/*.js\"    # Glob pattern\nprettier --write . --ignore-path .gitignore\n\n# Configuration\nprettier --config <path>          # Use specific config\nprettier --no-config              # Ignore config files\nprettier --find-config-path <file> # Show config used\n\n# Output options\nprettier --list-different         # List unformatted files\nprettier --log-level debug        # Verbose output\nprettier --no-color              # Disable colors\n\n# Special options\nprettier --require-pragma         # Only format with @format\nprettier --insert-pragma          # Add @format comment\nprettier --range-start 10 --range-end 50  # Partial format\n```\n\n### Integration Examples\n\n**VS Code Integration:**\n```json\n{\n  \"editor.defaultFormatter\": \"esbenp.prettier-vscode\",\n  \"editor.formatOnSave\": true,\n  \"[javascript]\": {\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n  }\n}\n```\n\n**CI/CD Pipeline:**\n```yaml\n# GitHub Actions\n- name: Check formatting\n  run: npx prettier --check .\n\n# Fail on unformatted code\n- name: Enforce formatting\n  run: |\n    npx prettier --check . || {\n      echo \"Code is not formatted!\"\n      exit 1\n    }\n```\n\n**Pre-commit Hook:**\n```bash\n#!/bin/sh\n# .git/hooks/pre-commit\n\nfiles=$(git diff --cached --name-only --diff-filter=ACMR | grep -E '\\.(js|jsx|ts|tsx|json|css|md)$')\n\nif [ -n \"$files\" ]; then\n  npx prettier --write $files\n  git add $files\nfi\n```\n\n### Performance Tips\n\n1. **Use .prettierignore** to skip large/generated files\n2. **Cache node_modules** in CI environments\n3. **Run in parallel** for multiple files:\n   ```bash\n   find . -name \"*.js\" | xargs -P 4 -I {} npx prettier --write {}\n   ```\n4. **Use --cache** flag for incremental formatting:\n   ```bash\n   npx prettier --write --cache src/\n   ```\n\n### Related Tools\n\n- **ESLint** - Linting and code quality\n- **Stylelint** - CSS/SCSS linting\n- **Markdownlint** - Markdown style checking\n- **Black** - Python formatting\n- **rustfmt** - Rust formatting\n- **gofmt** - Go formatting\n\n---\n\n**Version**: 1.0.0\n**Last Updated**: 2025-12-12\n**Compatibility**: Claude Code v1.5.0+",
      "parentPlugin": {
        "name": "formatter",
        "category": "examples",
        "path": "plugins/examples/formatter",
        "version": "2.0.1",
        "description": "Comprehensive code formatting plugin with Prettier integration. Use when you need to format code, validate formatting, or maintain consistent code style. Activates with phrases like 'format my code', 'check formatting', or 'apply code style'. Supports JavaScript, TypeScript, JSON, CSS, Markdown, and many other file types with automatic formatting on file operations."
      },
      "filePath": "plugins/examples/formatter/skills/code-formatter/SKILL.md"
    },
    {
      "slug": "collecting-infrastructure-metrics",
      "name": "collecting-infrastructure-metrics",
      "description": "Collect comprehensive infrastructure performance metrics across compute, storage, network, containers, load balancers, and databases. Use when monitoring system performance or troubleshooting infrastructure issues. Trigger with phrases like \"collect infrastructure metrics\", \"monitor server performance\", or \"track system resources\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(metrics:*)",
        "Bash(monitoring:*)",
        "Bash(system:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the process of setting up infrastructure metrics collection. It identifies key performance indicators (KPIs) across various infrastructure layers, configures agents to collect these metrics, and assists in setting up central aggregation and visualization.\n\n## How It Works\n\n1. **Identify Infrastructure Layers**: Determines the infrastructure layers to monitor (compute, storage, network, containers, load balancers, databases).\n2. **Configure Metrics Collection**: Sets up agents (Prometheus, Datadog, CloudWatch) to collect metrics from the identified layers.\n3. **Aggregate Metrics**: Configures central aggregation of the collected metrics for analysis and visualization.\n4. **Create Dashboards**: Generates infrastructure dashboards for health monitoring, performance analysis, and capacity tracking.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Monitor the performance of your infrastructure.\n- Identify bottlenecks in your system.\n- Set up dashboards for real-time monitoring.\n\n## Examples\n\n### Example 1: Setting up basic monitoring\n\nUser request: \"Collect infrastructure metrics for my web server.\"\n\nThe skill will:\n1. Identify compute, storage, and network layers relevant to the web server.\n2. Configure Prometheus to collect CPU, memory, disk I/O, and network bandwidth metrics.\n\n### Example 2: Troubleshooting database performance\n\nUser request: \"I'm seeing slow database queries. Can you help me monitor the database performance?\"\n\nThe skill will:\n1. Identify the database layer and relevant metrics such as connection pool usage, replication lag, and cache hit rates.\n2. Configure Datadog to collect these metrics and create a dashboard to visualize performance trends.\n\n## Best Practices\n\n- **Agent Selection**: Choose the appropriate agent (Prometheus, Datadog, CloudWatch) based on your existing infrastructure and monitoring tools.\n- **Metric Granularity**: Balance the granularity of metrics collection with the storage and processing overhead. Collect only the essential metrics for your use case.\n- **Alerting**: Configure alerts based on thresholds for key metrics to proactively identify and address performance issues.\n\n## Integration\n\nThis skill can be integrated with other plugins for deployment, configuration management, and alerting to provide a comprehensive infrastructure management solution. For example, it can be used with a deployment plugin to automatically configure metrics collection after deploying new infrastructure.\n\n## Prerequisites\n\n- Access to infrastructure monitoring systems (Prometheus, Datadog, CloudWatch)\n- System permissions for metrics agent installation\n- Network access to monitored infrastructure components\n- Storage for metrics data in {baseDir}/metrics/\n\n## Instructions\n\n1. Identify infrastructure layers to monitor (compute, storage, network, databases)\n2. Select appropriate metrics collection agent based on environment\n3. Configure agent with target endpoints and metric types\n4. Set up central aggregation for collected metrics\n5. Create dashboards for visualization\n6. Configure alerts for critical metrics thresholds\n\n## Output\n\n- Metrics collection configuration files\n- Agent installation and setup scripts\n- Dashboard definitions for infrastructure monitoring\n- Metric export configurations\n- Alert rules for critical thresholds\n\n## Error Handling\n\nIf metrics collection fails:\n- Verify agent installation and permissions\n- Check network connectivity to targets\n- Validate authentication credentials\n- Review firewall and security group rules\n- Confirm metric endpoint availability\n\n## Resources\n\n- Prometheus documentation for metric collection\n- Datadog agent configuration guides\n- AWS CloudWatch metrics reference\n- Infrastructure monitoring best practices",
      "parentPlugin": {
        "name": "infrastructure-metrics-collector",
        "category": "performance",
        "path": "plugins/performance/infrastructure-metrics-collector",
        "version": "1.0.0",
        "description": "Collect comprehensive infrastructure performance metrics"
      },
      "filePath": "plugins/performance/infrastructure-metrics-collector/skills/infrastructure-metrics-collector/SKILL.md"
    },
    {
      "slug": "comparing-database-schemas",
      "name": "comparing-database-schemas",
      "description": "Use when you need to work with schema comparison. This skill provides database schema diff and sync with comprehensive guidance and automation. Trigger with phrases like \"compare schemas\", \"diff databases\", or \"sync database schemas\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-diff-tool/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-diff-tool/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-diff-tool/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-diff-tool-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-diff-tool-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-diff-tool-dashboard.json`",
      "parentPlugin": {
        "name": "database-diff-tool",
        "category": "database",
        "path": "plugins/database/database-diff-tool",
        "version": "1.0.0",
        "description": "Database plugin for database-diff-tool"
      },
      "filePath": "plugins/database/database-diff-tool/skills/database-diff-tool/SKILL.md"
    },
    {
      "slug": "configuring-auto-scaling-policies",
      "name": "configuring-auto-scaling-policies",
      "description": "Use when you need to work with auto-scaling. This skill provides auto-scaling configuration with comprehensive guidance and automation. Trigger with phrases like \"configure auto-scaling\", \"set up elastic scaling\", or \"implement scaling\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/auto-scaling-configurator/`\n\n**Documentation and Guides**: `{baseDir}/docs/auto-scaling-configurator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/auto-scaling-configurator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/auto-scaling-configurator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/auto-scaling-configurator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/auto-scaling-configurator-dashboard.json`",
      "parentPlugin": {
        "name": "auto-scaling-configurator",
        "category": "devops",
        "path": "plugins/devops/auto-scaling-configurator",
        "version": "1.0.0",
        "description": "Configure auto-scaling policies for applications and infrastructure"
      },
      "filePath": "plugins/devops/auto-scaling-configurator/skills/auto-scaling-configurator/SKILL.md"
    },
    {
      "slug": "configuring-load-balancers",
      "name": "configuring-load-balancers",
      "description": "Use when configuring load balancers including ALB, NLB, Nginx, and HAProxy. Trigger with phrases like \"configure load balancer\", \"create ALB\", \"setup nginx load balancing\", or \"haproxy configuration\". Generates production-ready configurations with health checks, SSL termination, sticky sessions, and traffic distribution rules.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(aws:*)",
        "Bash(gcloud:*)",
        "Bash(nginx:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Backend servers are identified with IPs or DNS names\n- Load balancer type is determined (ALB, NLB, Nginx, HAProxy)\n- SSL certificates are available if using HTTPS\n- Health check endpoints are defined\n- Understanding of traffic distribution requirements (round-robin, least-connections)\n- Cloud provider CLI installed (if using cloud load balancers)\n\n## Instructions\n\n1. **Select Load Balancer Type**: Choose based on requirements (L4 vs L7, cloud vs on-prem)\n2. **Define Backend Pool**: List backend servers with ports and weights\n3. **Configure Health Checks**: Set check interval, timeout, and healthy threshold\n4. **Set Up SSL/TLS**: Configure certificates and cipher suites\n5. **Define Routing Rules**: Create path-based or host-based routing\n6. **Enable Session Persistence**: Configure sticky sessions if needed\n7. **Add Monitoring**: Set up logging and metrics collection\n8. **Test Configuration**: Validate syntax and test traffic distribution\n\n## Output\n\n**Nginx Configuration:**\n```nginx\n# {baseDir}/nginx/load-balancer.conf\nupstream backend_servers {\n    least_conn;\n    server 10.0.1.10:8080 weight=3;\n    server 10.0.1.11:8080 weight=2;\n    server 10.0.1.12:8080 backup;\n}\n\nserver {\n    listen 80;\n    server_name app.example.com;\n\n    location / {\n        proxy_pass http://backend_servers;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        proxy_connect_timeout 5s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n    }\n\n    location /health {\n        access_log off;\n        return 200 \"healthy\\n\";\n    }\n}\n```\n\n**AWS ALB (Terraform):**\n```hcl\n# {baseDir}/terraform/alb.tf\nresource \"aws_lb\" \"app_alb\" {\n  name               = \"app-load-balancer\"\n  internal           = false\n  load_balancer_type = \"application\"\n  security_groups    = [aws_security_group.alb_sg.id]\n  subnets            = aws_subnet.public[*].id\n}\n\nresource \"aws_lb_target_group\" \"app_tg\" {\n  name     = \"app-target-group\"\n  port     = 80\n  protocol = \"HTTP\"\n  vpc_id   = aws_vpc.main.id\n\n  health_check {\n    path                = \"/health\"\n    interval            = 30\n    timeout             = 5\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n  }\n}\n\nresource \"aws_lb_listener\" \"app_listener\" {\n  load_balancer_arn = aws_lb.app_alb.arn\n  port              = \"443\"\n  protocol          = \"HTTPS\"\n  ssl_policy        = \"ELBSecurityPolicy-TLS-1-2-2017-01\"\n  certificate_arn   = aws_acm_certificate.cert.arn\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.app_tg.arn\n  }\n}\n```\n\n## Error Handling\n\n**Backend Server Unreachable**\n- Error: \"502 Bad Gateway\" or connection refused\n- Solution: Verify backend server IPs, ports, and firewall rules\n\n**SSL Certificate Error**\n- Error: \"certificate verify failed\"\n- Solution: Check certificate validity, chain, and private key match\n\n**Health Check Failures**\n- Error: \"Target is unhealthy\"\n- Solution: Verify health check path returns 200 status and backends are running\n\n**Configuration Syntax Error**\n- Error: \"nginx: configuration file test failed\"\n- Solution: Run `nginx -t` to validate syntax and fix errors\n\n**Session Persistence Not Working**\n- Issue: Users losing session on subsequent requests\n- Solution: Enable sticky sessions using cookie-based or IP-based persistence\n\n## Resources\n\n- Nginx documentation: https://nginx.org/en/docs/\n- HAProxy configuration guide: https://www.haproxy.org/\n- AWS ALB documentation: https://docs.aws.amazon.com/elasticloadbalancing/\n- GCP Load Balancing: https://cloud.google.com/load-balancing/docs\n- Example configurations in {baseDir}/lb-examples/",
      "parentPlugin": {
        "name": "load-balancer-configurator",
        "category": "devops",
        "path": "plugins/devops/load-balancer-configurator",
        "version": "1.0.0",
        "description": "Configure load balancers (ALB, NLB, Nginx, HAProxy)"
      },
      "filePath": "plugins/devops/load-balancer-configurator/skills/load-balancer-configurator/SKILL.md"
    },
    {
      "slug": "configuring-service-meshes",
      "name": "configuring-service-meshes",
      "description": "This skill configures service meshes like istio and linkerd for microservices.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill enables Claude to generate configurations and setup code for service meshes like Istio and Linkerd. It simplifies the process of deploying and managing microservices by automating the configuration of essential service mesh components.\n\n## How It Works\n\n1. **Requirement Gathering**: Claude identifies the specific service mesh (Istio or Linkerd) and infrastructure requirements from the user's request.\n2. **Configuration Generation**: Based on the requirements, Claude generates the necessary configuration files, including YAML manifests and setup scripts.\n3. **Code Delivery**: Claude provides the generated configurations and setup code to the user, ready for deployment.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Configure Istio for a microservices application.\n- Configure Linkerd for a microservices application.\n- Generate service mesh configurations based on specific infrastructure requirements.\n\n## Examples\n\n### Example 1: Setting up Istio\n\nUser request: \"Configure Istio for my Kubernetes microservices deployment with mTLS enabled.\"\n\nThe skill will:\n1. Generate Istio configuration files with mTLS enabled.\n2. Provide the generated YAML manifests and setup instructions.\n\n### Example 2: Configuring Linkerd\n\nUser request: \"Setup Linkerd on my existing microservices cluster, focusing on traffic splitting and observability.\"\n\nThe skill will:\n1. Generate Linkerd configuration files for traffic splitting and observability.\n2. Provide the generated YAML manifests and setup instructions.\n\n## Best Practices\n\n- **Security**: Always prioritize security configurations, such as mTLS, when configuring service meshes.\n- **Observability**: Ensure that the service mesh is configured for comprehensive observability, including metrics, tracing, and logging.\n- **Traffic Management**: Use traffic management features like traffic splitting and canary deployments to manage application updates safely.\n\n## Integration\n\nThis skill can be integrated with other DevOps tools and plugins in the Claude Code ecosystem to automate the deployment and management of microservices applications. For example, it can work with a Kubernetes deployment plugin to automatically deploy the generated configurations.",
      "parentPlugin": {
        "name": "service-mesh-configurator",
        "category": "devops",
        "path": "plugins/devops/service-mesh-configurator",
        "version": "1.0.0",
        "description": "Configure service mesh (Istio, Linkerd) for microservices"
      },
      "filePath": "plugins/devops/service-mesh-configurator/skills/service-mesh-configurator/SKILL.md"
    },
    {
      "slug": "creating-alerting-rules",
      "name": "creating-alerting-rules",
      "description": "This skill enables claude to create intelligent alerting rules for proactive",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the creation of comprehensive alerting rules, reducing the manual effort required for performance monitoring. It guides you through defining alert categories, setting intelligent thresholds, and configuring routing and escalation policies. The skill also helps generate runbooks and establish alert testing procedures.\n\n## How It Works\n\n1. **Identify Alert Category**: Determines the type of alert to create (e.g., latency, error rate, resource utilization).\n2. **Define Thresholds**: Sets appropriate thresholds to avoid alert fatigue and ensure timely notification of performance issues.\n3. **Configure Routing and Escalation**: Establishes routing policies to direct alerts to the appropriate teams and escalation policies for timely response.\n4. **Generate Runbook**: Creates a basic runbook with steps to diagnose and resolve the alerted issue.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement performance monitoring for a new service.\n- Refine existing alerting rules to reduce false positives.\n- Create alerts for specific performance metrics, such as latency or error rate.\n\n## Examples\n\n### Example 1: Setting up Latency Alerts\n\nUser request: \"create latency alerts for the payment service\"\n\nThe skill will:\n1. Prompt for latency thresholds (e.g., warning and critical).\n2. Configure alerts to trigger when latency exceeds defined thresholds.\n\n### Example 2: Creating Error Rate Alerts\n\nUser request: \"set up alerting for error rate increases in the API gateway\"\n\nThe skill will:\n1. Request the baseline error rate and acceptable deviation.\n2. Configure alerts to trigger when the error rate exceeds the defined deviation from the baseline.\n\n## Best Practices\n\n- **Threshold Selection**: Use historical data and statistical analysis to determine appropriate thresholds that minimize false positives and negatives.\n- **Alert Routing**: Route alerts to the appropriate teams or individuals based on the alert category and severity.\n- **Runbook Creation**: Generate or link to detailed runbooks that provide clear instructions for diagnosing and resolving the alerted issue.\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins to automate incident response workflows. For example, it can trigger automated remediation actions or create tickets in an issue tracking system.",
      "parentPlugin": {
        "name": "alerting-rule-creator",
        "category": "performance",
        "path": "plugins/performance/alerting-rule-creator",
        "version": "1.0.0",
        "description": "Create intelligent alerting rules for performance monitoring"
      },
      "filePath": "plugins/performance/alerting-rule-creator/skills/alerting-rule-creator/SKILL.md"
    },
    {
      "slug": "creating-ansible-playbooks",
      "name": "creating-ansible-playbooks",
      "description": "Use when you need to work with Ansible automation. This skill provides Ansible playbook creation with comprehensive guidance and automation. Trigger with phrases like \"create Ansible playbook\", \"automate with Ansible\", or \"configure with Ansible\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/ansible-playbook-creator/`\n\n**Documentation and Guides**: `{baseDir}/docs/ansible-playbook-creator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/ansible-playbook-creator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/ansible-playbook-creator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/ansible-playbook-creator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/ansible-playbook-creator-dashboard.json`",
      "parentPlugin": {
        "name": "ansible-playbook-creator",
        "category": "devops",
        "path": "plugins/devops/ansible-playbook-creator",
        "version": "1.0.0",
        "description": "Create Ansible playbooks for configuration management"
      },
      "filePath": "plugins/devops/ansible-playbook-creator/skills/ansible-playbook-creator/SKILL.md"
    },
    {
      "slug": "creating-apm-dashboards",
      "name": "creating-apm-dashboards",
      "description": "This skill enables claude to create application performance monitoring",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the creation of Application Performance Monitoring (APM) dashboards, providing a structured approach to visualizing critical application metrics. By defining key performance indicators and generating dashboard configurations, this skill simplifies the process of monitoring application health and performance.\n\n## How It Works\n\n1. **Identify Requirements**: Determine the specific metrics and visualizations needed for the APM dashboard based on the user's request.\n2. **Define Dashboard Components**: Select relevant components such as golden signals (latency, traffic, errors, saturation), request metrics, resource utilization, database metrics, cache metrics, business metrics, and error tracking.\n3. **Generate Configuration**: Create the dashboard configuration file based on the selected components and user preferences.\n4. **Deploy Dashboard**: Deploy the generated configuration to the target monitoring platform (e.g., Grafana, Datadog).\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new APM dashboard for an application.\n- Define key metrics and visualizations for monitoring application performance.\n- Generate dashboard configurations for Grafana, Datadog, or other monitoring platforms.\n\n## Examples\n\n### Example 1: Creating a Grafana Dashboard\n\nUser request: \"Create a Grafana dashboard for monitoring my web application's performance.\"\n\nThe skill will:\n1. Identify the need for a Grafana dashboard focused on web application performance.\n2. Define dashboard components including request rate, response times, error rates, and resource utilization (CPU, memory).\n3. Generate a Grafana dashboard configuration file with pre-defined visualizations for these metrics.\n\n### Example 2: Setting up a Datadog Dashboard\n\nUser request: \"Set up a Datadog dashboard to track the golden signals for my microservice.\"\n\nThe skill will:\n1. Identify the need for a Datadog dashboard focused on golden signals.\n2. Define dashboard components including latency, traffic, errors, and saturation metrics.\n3. Generate a Datadog dashboard configuration file with pre-defined visualizations for these metrics.\n\n## Best Practices\n\n- **Specificity**: Provide detailed information about the application and metrics to be monitored.\n- **Platform Selection**: Clearly specify the target monitoring platform (Grafana, Datadog, etc.) to ensure compatibility.\n- **Iteration**: Review and refine the generated dashboard configuration to meet specific monitoring needs.\n\n## Integration\n\nThis skill can be integrated with other plugins that manage infrastructure or application deployment to automatically create APM dashboards as part of the deployment process. It can also work with alerting plugins to define alert rules based on the metrics displayed in the generated dashboards.",
      "parentPlugin": {
        "name": "apm-dashboard-creator",
        "category": "performance",
        "path": "plugins/performance/apm-dashboard-creator",
        "version": "1.0.0",
        "description": "Create Application Performance Monitoring dashboards"
      },
      "filePath": "plugins/performance/apm-dashboard-creator/skills/apm-dashboard-creator/SKILL.md"
    },
    {
      "slug": "creating-data-visualizations",
      "name": "creating-data-visualizations",
      "description": "Generate plots, charts, and graphs from data with automatic visualization",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to transform raw data into compelling visual representations. It leverages intelligent automation to select optimal visualization types and generate informative plots, charts, and graphs. This skill helps users understand complex data more easily.\n\n## How It Works\n\n1. **Data Analysis**: Claude analyzes the provided data to understand its structure, type, and distribution.\n2. **Visualization Selection**: Based on the data analysis, Claude selects the most appropriate visualization type (e.g., bar chart, scatter plot, line graph).\n3. **Visualization Generation**: Claude generates the visualization using appropriate libraries and best practices for visual clarity and accuracy.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a visual representation of data.\n- Generate a specific type of plot, chart, or graph (e.g., \"create a bar chart\").\n- Explore data patterns and relationships through visualization.\n\n## Examples\n\n### Example 1: Visualizing Sales Data\n\nUser request: \"Create a bar chart showing sales by region.\"\n\nThe skill will:\n1. Analyze the sales data, identifying regions and corresponding sales figures.\n2. Generate a bar chart with regions on the x-axis and sales on the y-axis.\n\n### Example 2: Plotting Stock Prices\n\nUser request: \"Plot the stock price of AAPL over the last year.\"\n\nThe skill will:\n1. Retrieve historical stock price data for AAPL.\n2. Generate a line graph showing the stock price over time.\n\n## Best Practices\n\n- **Data Clarity**: Ensure the data is clean and well-formatted before requesting a visualization.\n- **Specific Requests**: Be specific about the desired visualization type and any relevant data filters.\n- **Contextual Information**: Provide context about the data and the purpose of the visualization.\n\n## Integration\n\nThis skill can be integrated with other data processing and analysis tools within the Claude Code environment. It can receive data from other skills and provide visualizations for further analysis or reporting.",
      "parentPlugin": {
        "name": "data-visualization-creator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/data-visualization-creator",
        "version": "1.0.0",
        "description": "Create data visualizations and plots"
      },
      "filePath": "plugins/ai-ml/data-visualization-creator/skills/data-visualization-creator/SKILL.md"
    },
    {
      "slug": "creating-github-issues-from-web-research",
      "name": "creating-github-issues-from-web-research",
      "description": "This skill enhances claude's ability to conduct web research and translate",
      "allowedTools": [
        "Read",
        "WebFetch",
        "WebSearch",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to streamline the research-to-implementation workflow. By integrating web search with GitHub issue creation, Claude can efficiently convert research findings into trackable tasks for development teams.\n\n## How It Works\n\n1. **Web Search**: Claude utilizes its web search capabilities to gather information on the specified topic.\n2. **Information Extraction**: The plugin extracts relevant details, key findings, and supporting evidence from the search results.\n3. **GitHub Issue Creation**: A new GitHub issue is created with a clear title, a summary of the research, key recommendations, and links to the original sources.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Investigate a technical topic and create an implementation ticket.\n- Track security vulnerabilities and generate a security issue with remediation steps.\n- Research competitor features and create a feature request ticket.\n\n## Examples\n\n### Example 1: Researching Security Best Practices\n\nUser request: \"research Docker security best practices and create a ticket in myorg/backend\"\n\nThe skill will:\n1. Search the web for Docker security best practices.\n2. Extract key recommendations, security vulnerabilities, and mitigation strategies.\n3. Create a GitHub issue in the specified repository with a summary of the findings, a checklist of best practices, and links to relevant resources.\n\n### Example 2: Investigating API Rate Limiting\n\nUser request: \"find articles about API rate limiting, create issue with label performance\"\n\nThe skill will:\n1. Search the web for articles and documentation on API rate limiting.\n2. Extract different rate limiting techniques, their pros and cons, and implementation examples.\n3. Create a GitHub issue with the \"performance\" label, summarizing the findings and providing links to the source articles.\n\n## Best Practices\n\n- **Specify Repository**: When creating issues for a specific project, explicitly mention the repository name to ensure the issue is created in the correct location.\n- **Use Labels**: Add relevant labels to the issue to categorize it appropriately and facilitate issue tracking.\n- **Provide Context**: Include sufficient context in your request to guide the web search and ensure the generated issue contains the most relevant information.\n\n## Integration\n\nThis skill seamlessly integrates with Claude's web search Skill and requires authentication with a GitHub account. It can be used in conjunction with other skills to further automate development workflows.",
      "parentPlugin": {
        "name": "web-to-github-issue",
        "category": "skill-enhancers",
        "path": "plugins/skill-enhancers/web-to-github-issue",
        "version": "1.0.0",
        "description": "Enhances web_search Skill by automatically creating GitHub issues from research findings"
      },
      "filePath": "plugins/skill-enhancers/web-to-github-issue/skills/web-to-github-issue/SKILL.md"
    },
    {
      "slug": "creating-kubernetes-deployments",
      "name": "creating-kubernetes-deployments",
      "description": "Use when generating Kubernetes deployment manifests and services. Trigger with phrases like \"create kubernetes deployment\", \"generate k8s manifest\", \"deploy app to kubernetes\", or \"create service and ingress\". Produces production-ready YAML with health checks, auto-scaling, resource limits, ingress configuration, and TLS termination.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(kubectl:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Kubernetes cluster is accessible and kubectl is configured\n- Container image is built and pushed to registry\n- Understanding of application resource requirements\n- Namespace exists or will be created\n- Ingress controller is installed (if using ingress)\n- TLS certificates are available (if using HTTPS)\n\n## Instructions\n\n1. **Gather Requirements**: Application name, image, replicas, ports, environment\n2. **Create Deployment**: Generate YAML with container spec and resource limits\n3. **Add Health Checks**: Configure liveness and readiness probes\n4. **Define Service**: Create ClusterIP, NodePort, or LoadBalancer service\n5. **Configure Ingress**: Set up routing rules and TLS termination\n6. **Add ConfigMaps/Secrets**: Externalize configuration and sensitive data\n7. **Enable Auto-scaling**: Create HorizontalPodAutoscaler if needed\n8. **Apply Manifests**: Use kubectl apply to deploy resources\n\n## Output\n\n**Deployment Manifest:**\n```yaml\n# {baseDir}/k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-web-app\n  namespace: production\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-web-app\n  template:\n    metadata:\n      labels:\n        app: my-web-app\n    spec:\n      containers:\n      - name: app\n        image: registry/my-web-app:v1.0.0\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 5\n```\n\n**Service and Ingress:**\n```yaml\n# {baseDir}/k8s/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-web-app\n  namespace: production\nspec:\n  type: ClusterIP\n  selector:\n    app: my-web-app\n  ports:\n  - port: 80\n    targetPort: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-web-app\n  namespace: production\nspec:\n  tls:\n  - hosts:\n    - app.example.com\n    secretName: tls-cert\n  rules:\n  - host: app.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-web-app\n            port:\n              number: 80\n```\n\n## Error Handling\n\n**Image Pull Error**\n- Error: \"ErrImagePull\" or \"ImagePullBackOff\"\n- Solution: Verify image name, tag, and registry credentials in imagePullSecrets\n\n**CrashLoopBackOff**\n- Error: Pod repeatedly crashes and restarts\n- Solution: Check application logs with `kubectl logs` and review container health\n\n**Resource Quota Exceeded**\n- Error: \"forbidden: exceeded quota\"\n- Solution: Reduce resource requests or increase namespace quota\n\n**Ingress Not Working**\n- Error: 404 or 503 on ingress domain\n- Solution: Verify ingress controller is running and service endpoints are ready\n\n**TLS Certificate Error**\n- Error: \"certificate signed by unknown authority\"\n- Solution: Create or update TLS secret with valid certificate\n\n## Resources\n\n- Kubernetes documentation: https://kubernetes.io/docs/\n- kubectl reference: https://kubernetes.io/docs/reference/kubectl/\n- Deployment best practices: https://kubernetes.io/docs/concepts/workloads/\n- Example manifests in {baseDir}/k8s-examples/",
      "parentPlugin": {
        "name": "kubernetes-deployment-creator",
        "category": "devops",
        "path": "plugins/devops/kubernetes-deployment-creator",
        "version": "1.0.0",
        "description": "Create Kubernetes deployments, services, and configurations with best practices"
      },
      "filePath": "plugins/devops/kubernetes-deployment-creator/skills/kubernetes-deployment-creator/SKILL.md"
    },
    {
      "slug": "creating-webhook-handlers",
      "name": "creating-webhook-handlers",
      "description": "Create webhook endpoints with signature verification, retry logic, and payload validation. Use when receiving and processing webhook events. Trigger with phrases like \"create webhook\", \"handle webhook events\", or \"setup webhook handler\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:webhook-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:webhook-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "webhook-handler-creator",
        "category": "api-development",
        "path": "plugins/api-development/webhook-handler-creator",
        "version": "1.0.0",
        "description": "Create secure webhook endpoints with signature verification and retry logic"
      },
      "filePath": "plugins/api-development/webhook-handler-creator/skills/webhook-handler-creator/SKILL.md"
    },
    {
      "slug": "database-documentation-gen",
      "name": "database-documentation-gen",
      "description": "Use when you need to work with database documentation. This skill provides automated documentation generation with comprehensive guidance and automation. Trigger with phrases like \"generate docs\", \"document schema\", or \"create database documentation\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-documentation-gen/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-documentation-gen/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-documentation-gen/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-documentation-gen-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-documentation-gen-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-documentation-gen-dashboard.json`",
      "parentPlugin": {
        "name": "database-documentation-gen",
        "category": "database",
        "path": "plugins/database/database-documentation-gen",
        "version": "1.0.0",
        "description": "Database plugin for database-documentation-gen"
      },
      "filePath": "plugins/database/database-documentation-gen/skills/database-documentation-gen/SKILL.md"
    },
    {
      "slug": "deploying-machine-learning-models",
      "name": "deploying-machine-learning-models",
      "description": "This skill enables claude to deploy machine learning models to production",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill streamlines the process of deploying machine learning models to production, ensuring efficient and reliable model serving. It leverages automated workflows and best practices to simplify the deployment process and optimize performance.\n\n## How It Works\n\n1. **Analyze Requirements**: The skill analyzes the context and user requirements to determine the appropriate deployment strategy.\n2. **Generate Code**: It generates the necessary code for deploying the model, including API endpoints, data validation, and error handling.\n3. **Deploy Model**: The skill deploys the model to the specified production environment.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Deploy a trained machine learning model to a production environment.\n- Serve a model via an API endpoint for real-time predictions.\n- Automate the model deployment process.\n\n## Examples\n\n### Example 1: Deploying a Regression Model\n\nUser request: \"Deploy my regression model trained on the housing dataset.\"\n\nThe skill will:\n1. Analyze the model and data format.\n2. Generate code for a REST API endpoint to serve the model.\n3. Deploy the model to a cloud-based serving platform.\n\n### Example 2: Productionizing a Classification Model\n\nUser request: \"Productionize the classification model I just trained.\"\n\nThe skill will:\n1. Create a Docker container for the model.\n2. Implement data validation and error handling.\n3. Deploy the container to a Kubernetes cluster.\n\n## Best Practices\n\n- **Data Validation**: Implement thorough data validation to ensure the model receives correct inputs.\n- **Error Handling**: Include robust error handling to gracefully manage unexpected issues.\n- **Performance Monitoring**: Set up performance monitoring to track model latency and throughput.\n\n## Integration\n\nThis skill can be integrated with other tools for model training, data preprocessing, and monitoring.",
      "parentPlugin": {
        "name": "model-deployment-helper",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-deployment-helper",
        "version": "1.0.0",
        "description": "Deploy ML models to production"
      },
      "filePath": "plugins/ai-ml/model-deployment-helper/skills/model-deployment-helper/SKILL.md"
    },
    {
      "slug": "deploying-monitoring-stacks",
      "name": "deploying-monitoring-stacks",
      "description": "Use when deploying monitoring stacks including Prometheus, Grafana, and Datadog. Trigger with phrases like \"deploy monitoring stack\", \"setup prometheus\", \"configure grafana\", or \"install datadog agent\". Generates production-ready configurations with metric collection, visualization dashboards, and alerting rules.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(docker:*)",
        "Bash(kubectl:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Target infrastructure is identified (Kubernetes, Docker, bare metal)\n- Metric endpoints are accessible from monitoring platform\n- Storage backend is configured for time-series data\n- Alert notification channels are defined (email, Slack, PagerDuty)\n- Resource requirements are calculated based on scale\n\n## Instructions\n\n1. **Select Platform**: Choose Prometheus/Grafana, Datadog, or hybrid approach\n2. **Deploy Collectors**: Install exporters and agents on monitored systems\n3. **Configure Scraping**: Define metric collection endpoints and intervals\n4. **Set Up Storage**: Configure retention policies and data compaction\n5. **Create Dashboards**: Build visualization panels for key metrics\n6. **Define Alerts**: Create alerting rules with appropriate thresholds\n7. **Test Monitoring**: Verify metrics flow and alert triggering\n\n## Output\n\n**Prometheus + Grafana (Kubernetes):**\n```yaml\n# {baseDir}/monitoring/prometheus.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n      evaluation_interval: 15s\n    scrape_configs:\n      - job_name: 'kubernetes-pods'\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus:latest\n        args:\n          - '--config.file=/etc/prometheus/prometheus.yml'\n          - '--storage.tsdb.retention.time=30d'\n        ports:\n        - containerPort: 9090\n```\n\n**Grafana Dashboard Configuration:**\n```json\n{\n  \"dashboard\": {\n    \"title\": \"Application Metrics\",\n    \"panels\": [\n      {\n        \"title\": \"CPU Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(container_cpu_usage_seconds_total[5m])\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Error Handling\n\n**Metrics Not Appearing**\n- Error: \"No data points\"\n- Solution: Verify scrape targets are accessible and returning metrics\n\n**High Cardinality**\n- Error: \"Too many time series\"\n- Solution: Reduce label combinations or increase Prometheus resources\n\n**Alert Not Firing**\n- Error: \"Alert condition met but no notification\"\n- Solution: Check Alertmanager configuration and notification channels\n\n**Dashboard Load Failure**\n- Error: \"Failed to load dashboard\"\n- Solution: Verify Grafana datasource configuration and permissions\n\n## Resources\n\n- Prometheus documentation: https://prometheus.io/docs/\n- Grafana documentation: https://grafana.com/docs/\n- Example dashboards in {baseDir}/monitoring-examples/",
      "parentPlugin": {
        "name": "monitoring-stack-deployer",
        "category": "devops",
        "path": "plugins/devops/monitoring-stack-deployer",
        "version": "1.0.0",
        "description": "Deploy monitoring stacks (Prometheus, Grafana, Datadog)"
      },
      "filePath": "plugins/devops/monitoring-stack-deployer/skills/monitoring-stack-deployer/SKILL.md"
    },
    {
      "slug": "designing-database-schemas",
      "name": "designing-database-schemas",
      "description": "Use when you need to work with database schema design. This skill provides schema design and migrations with comprehensive guidance and automation. Trigger with phrases like \"design schema\", \"create migration\", or \"model database\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-schema-designer/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-schema-designer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-schema-designer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-schema-designer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-schema-designer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-schema-designer-dashboard.json`",
      "parentPlugin": {
        "name": "database-schema-designer",
        "category": "database",
        "path": "plugins/database/database-schema-designer",
        "version": "1.0.0",
        "description": "Design and visualize database schemas with normalization guidance, relationship mapping, and ERD generation"
      },
      "filePath": "plugins/database/database-schema-designer/skills/database-schema-designer/SKILL.md"
    },
    {
      "slug": "detecting-data-anomalies",
      "name": "detecting-data-anomalies",
      "description": "Identify anomalies and outliers in datasets using machine learning algorithms. Use when analyzing data for unusual patterns, outliers, or unexpected deviations from normal behavior. Trigger with phrases like \"detect anomalies\", \"find outliers\", or \"identify unusual patterns\". allowed-tools: Read, Bash(python:*), Grep, Glob license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Dataset in accessible format (CSV, JSON, or database)\n- Python environment with scikit-learn or similar ML libraries\n- Understanding of data distribution and expected patterns\n- Sufficient data volume for statistical significance\n- Knowledge of domain-specific normal behavior\n- Data preprocessing capabilities for cleaning and scaling\n\n## Instructions\n\n### Step 1: Prepare Data for Analysis\nSet up the dataset for anomaly detection:\n1. Load dataset using Read tool\n2. Inspect data structure and identify relevant features\n3. Clean data by handling missing values and inconsistencies\n4. Normalize or scale features as appropriate for algorithm\n5. Split temporal data if time-series analysis is needed\n\n### Step 2: Select Detection Algorithm\nChoose appropriate anomaly detection method based on data characteristics:\n- **Isolation Forest**: For high-dimensional data with complex anomalies\n- **One-Class SVM**: For clearly defined normal behavior patterns\n- **Local Outlier Factor (LOF)**: For density-based anomaly detection\n- **Statistical Methods**: For simple univariate or multivariate analysis\n- **Autoencoders**: For complex patterns in large datasets\n\n### Step 3: Configure Detection Parameters\nSet algorithm parameters to balance sensitivity:\n- Define contamination rate (expected proportion of anomalies)\n- Set distance metrics appropriate for feature types\n- Configure threshold values for anomaly scoring\n- Establish validation strategy for parameter tuning\n\n### Step 4: Execute Anomaly Detection\nRun the detection algorithm on prepared data:\n1. Apply selected algorithm using Bash tool\n2. Generate anomaly scores for each data point\n3. Classify points as normal or anomalous based on threshold\n4. Extract characteristics of identified anomalies\n\n### Step 5: Analyze and Report Results\nInterpret detection results and provide insights:\n- Summarize number and distribution of anomalies\n- Highlight most significant outliers with context\n- Identify patterns or clusters among anomalies\n- Generate visualizations showing anomaly distribution\n- Provide recommendations for further investigation\n\n## Output\n\nThe skill produces comprehensive anomaly detection results:\n\n### Anomaly Summary Report\n- Total data points analyzed\n- Number of anomalies detected\n- Contamination rate (percentage of anomalies)\n- Algorithm used and configuration parameters\n- Confidence scores for detected anomalies\n\n### Detailed Anomaly List\nFor each detected anomaly:\n- Record identifier and timestamp (if applicable)\n- Anomaly score and confidence level\n- Feature values showing deviation from normal\n- Contextual information about the outlier\n- Severity classification (low, medium, high, critical)\n\n### Statistical Analysis\n- Distribution of anomaly scores across dataset\n- Feature importance for anomaly classification\n- Comparison with normal data patterns\n- Temporal distribution of anomalies (if time-series)\n- Clustering analysis of anomaly types\n\n### Visualizations\n- Scatter plots highlighting anomalies in feature space\n- Time-series plots with anomaly markers\n- Distribution histograms comparing normal vs anomalous data\n- Heatmaps showing feature correlations for anomalies\n\n### Recommendations\n- Suggested follow-up investigations for critical anomalies\n- Data quality improvements to reduce false positives\n- Monitoring strategies for real-time detection\n- Algorithm refinements based on domain knowledge\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Insufficient Data Volume**\n- Error: Not enough data points for statistical significance\n- Solution: Collect more data, adjust contamination rate, or use simpler statistical methods\n\n**High False Positive Rate**\n- Error: Too many normal points classified as anomalies\n- Solution: Adjust detection threshold, refine feature selection, or use domain-specific constraints\n\n**Algorithm Performance Issues**\n- Error: Detection algorithm too slow for large datasets\n- Solution: Use sampling techniques, optimize parameters, or switch to faster algorithms like Isolation Forest\n\n**Feature Scaling Problems**\n- Error: Anomalies dominated by high-magnitude features\n- Solution: Apply appropriate normalization or standardization to all features before detection\n\n**Missing Ground Truth**\n- Error: Unable to validate detection accuracy without labels\n- Solution: Use domain expertise for manual validation, implement feedback loop for model improvement\n\n## Resources\n\n### Anomaly Detection Algorithms\n- Isolation Forest documentation and implementation examples\n- One-Class SVM for novelty detection\n- Local Outlier Factor (LOF) for density-based detection\n- Autoencoder-based anomaly detection for deep learning approaches\n\n### Python Libraries\n- scikit-learn anomaly detection module\n- PyOD (Python Outlier Detection) comprehensive library\n- TensorFlow/PyTorch for deep learning-based detection\n- statsmodels for statistical anomaly detection\n\n### Domain-Specific Applications\n- Fraud detection in financial transactions\n- Network intrusion detection and security monitoring\n- Manufacturing quality control and defect detection\n- Healthcare anomaly detection for patient monitoring\n- IoT sensor data anomaly identification\n\n### Best Practices\n- Balance sensitivity to avoid excessive false positives\n- Validate results with domain experts\n- Monitor detection performance over time\n- Update models as normal behavior evolves\n- Document anomaly investigation procedures",
      "parentPlugin": {
        "name": "anomaly-detection-system",
        "category": "ai-ml",
        "path": "plugins/ai-ml/anomaly-detection-system",
        "version": "1.0.0",
        "description": "Detect anomalies and outliers in data"
      },
      "filePath": "plugins/ai-ml/anomaly-detection-system/skills/anomaly-detection-system/SKILL.md"
    },
    {
      "slug": "detecting-database-deadlocks",
      "name": "detecting-database-deadlocks",
      "description": "Use when you need to work with deadlock detection. This skill provides deadlock detection and resolution with comprehensive guidance and automation. Trigger with phrases like \"detect deadlocks\", \"resolve deadlocks\", or \"prevent deadlocks\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-deadlock-detector/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-deadlock-detector/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-deadlock-detector/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-deadlock-detector-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-deadlock-detector-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-deadlock-detector-dashboard.json`",
      "parentPlugin": {
        "name": "database-deadlock-detector",
        "category": "database",
        "path": "plugins/database/database-deadlock-detector",
        "version": "1.0.0",
        "description": "Database plugin for database-deadlock-detector"
      },
      "filePath": "plugins/database/database-deadlock-detector/skills/database-deadlock-detector/SKILL.md"
    },
    {
      "slug": "detecting-infrastructure-drift",
      "name": "detecting-infrastructure-drift",
      "description": "Use when detecting infrastructure drift from desired state. Trigger with phrases like \"check for drift\", \"infrastructure drift detection\", \"compare actual vs desired state\", or \"detect configuration changes\". Identifies discrepancies between current infrastructure and IaC definitions using terraform plan, cloudformation drift detection, or manual comparison.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(terraform:*)",
        "Bash(aws:*)",
        "Bash(gcloud:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Infrastructure as Code (IaC) files are up to date in {baseDir}\n- Cloud provider CLI is installed and authenticated\n- IaC tool (Terraform/CloudFormation/Pulumi) is installed\n- Remote state storage is configured and accessible\n- Appropriate read permissions for infrastructure resources\n\n## Instructions\n\n1. **Identify IaC Tool**: Determine if using Terraform, CloudFormation, Pulumi, or ARM\n2. **Fetch Current State**: Retrieve actual infrastructure state from cloud provider\n3. **Load Desired State**: Read IaC configuration from {baseDir}/terraform or equivalent\n4. **Compare States**: Execute drift detection command for the IaC platform\n5. **Analyze Differences**: Identify added, modified, or removed resources\n6. **Generate Report**: Create detailed report of drift with affected resources\n7. **Suggest Remediation**: Provide commands to resolve drift (apply or import)\n8. **Document Findings**: Save drift report to {baseDir}/drift-reports/\n\n## Output\n\nGenerates drift detection reports:\n\n**Terraform Drift Report:**\n```\nDrift Detection Report - 2025-12-10 10:30:00\n==============================================\n\nResources with Drift: 3\n\n1. aws_instance.web_server\n   Status: Modified\n   Drift: instance_type changed from \"t3.micro\" to \"t3.small\"\n   Action: Update IaC to match or revert instance type\n\n2. aws_s3_bucket.assets\n   Status: Modified\n   Drift: versioning_enabled changed from true to false\n   Action: Re-enable versioning or update IaC\n\n3. aws_iam_role.lambda_exec\n   Status: Deleted\n   Drift: Role no longer exists in AWS\n   Action: terraform apply to recreate\n\nRemediation Command:\nterraform plan -out=drift-fix.tfplan\nterraform apply drift-fix.tfplan\n```\n\n**CloudFormation Drift:**\n```yaml\nStackName: production-vpc\nDriftStatus: DRIFTED\nResources:\n  - LogicalResourceId: VPC\n    ResourceType: AWS::EC2::VPC\n    DriftStatus: IN_SYNC\n  - LogicalResourceId: PublicSubnet\n    ResourceType: AWS::EC2::Subnet\n    DriftStatus: MODIFIED\n    PropertyDifferences:\n      - PropertyPath: /Tags\n        ExpectedValue: [{Key: Env, Value: prod}]\n        ActualValue: [{Key: Env, Value: production}]\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**State Lock Error**\n- Error: \"Error acquiring state lock\"\n- Solution: Ensure no other terraform process is running, or force-unlock if safe\n\n**Authentication Failure**\n- Error: \"Unable to authenticate to cloud provider\"\n- Solution: Refresh credentials with `aws configure` or `gcloud auth login`\n\n**Missing State File**\n- Error: \"No state file found\"\n- Solution: Initialize terraform with `terraform init` or specify remote backend\n\n**Permission Denied**\n- Error: \"Access denied reading resource\"\n- Solution: Grant read-only IAM permissions to service account\n\n**State Version Mismatch**\n- Error: \"State file version too new\"\n- Solution: Upgrade Terraform version or use compatible state version\n\n## Resources\n\n- Terraform drift documentation: https://www.terraform.io/docs/cli/state/\n- AWS CloudFormation drift detection: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detect-drift-stack.html\n- Drift remediation best practices in {baseDir}/docs/drift-remediation.md\n- Automated drift detection scripts in {baseDir}/scripts/drift-check.sh",
      "parentPlugin": {
        "name": "infrastructure-drift-detector",
        "category": "devops",
        "path": "plugins/devops/infrastructure-drift-detector",
        "version": "1.0.0",
        "description": "Detect infrastructure drift from desired state"
      },
      "filePath": "plugins/devops/infrastructure-drift-detector/skills/infrastructure-drift-detector/SKILL.md"
    },
    {
      "slug": "detecting-memory-leaks",
      "name": "detecting-memory-leaks",
      "description": "Detect potential memory leaks and analyze memory usage patterns in code. Use when troubleshooting performance issues related to memory growth or identifying leak sources. Trigger with phrases like \"detect memory leaks\", \"analyze memory usage\", or \"find memory issues\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(profiling:*)",
        "Bash(memory:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill helps you identify and resolve memory leaks in your code. By analyzing your code for common memory leak patterns, it can help you improve the performance and stability of your application.\n\n## How It Works\n\n1. **Initiate Analysis**: The user requests memory leak detection.\n2. **Code Analysis**: The plugin analyzes the codebase for potential memory leak patterns.\n3. **Report Generation**: The plugin generates a report detailing potential memory leaks and recommended fixes.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Detect potential memory leaks in your application.\n- Analyze memory usage patterns to identify performance bottlenecks.\n- Troubleshoot performance issues related to memory leaks.\n\n## Examples\n\n### Example 1: Identifying Event Listener Leaks\n\nUser request: \"detect memory leaks in my event handling code\"\n\nThe skill will:\n1. Analyze the code for unremoved event listeners.\n2. Generate a report highlighting potential event listener leaks and suggesting how to properly remove them.\n\n### Example 2: Analyzing Cache Growth\n\nUser request: \"analyze memory usage to find excessive cache growth\"\n\nThe skill will:\n1. Analyze cache implementations for unbounded growth.\n2. Identify caches that are not properly managed and recommend strategies for limiting their size.\n\n## Best Practices\n\n- **Code Review**: Always review the reported potential leaks to ensure they are genuine issues.\n- **Regular Analysis**: Incorporate memory leak detection into your regular development workflow.\n- **Targeted Analysis**: Focus your analysis on specific areas of your code that are known to be memory-intensive.\n\n## Integration\n\nThis skill can be used in conjunction with other performance analysis tools to provide a comprehensive view of application performance.\n\n## Prerequisites\n\n- Access to application source code in {baseDir}/\n- Memory profiling tools (valgrind, heapdump, etc.)\n- Understanding of application memory architecture\n- Runtime environment for testing\n\n## Instructions\n\n1. Analyze code for common memory leak patterns\n2. Identify unremoved event listeners and callbacks\n3. Check for unbounded cache growth\n4. Review closure usage and retained references\n5. Generate report with leak locations and severity\n6. Provide remediation recommendations\n\n## Output\n\n- Memory leak detection report with file locations\n- Pattern analysis for event listeners and caches\n- Memory usage trends and growth patterns\n- Code snippets highlighting potential leaks\n- Recommended fixes with code examples\n\n## Error Handling\n\nIf memory leak detection fails:\n- Verify code file access permissions\n- Check profiling tool installation\n- Validate code syntax and structure\n- Ensure sufficient memory for analysis\n- Review runtime environment configuration\n\n## Resources\n\n- Memory profiling tool documentation\n- Memory leak detection best practices\n- JavaScript/Node.js memory management guides\n- Performance optimization resources",
      "parentPlugin": {
        "name": "memory-leak-detector",
        "category": "performance",
        "path": "plugins/performance/memory-leak-detector",
        "version": "1.0.0",
        "description": "Detect memory leaks and analyze memory usage patterns"
      },
      "filePath": "plugins/performance/memory-leak-detector/skills/memory-leak-detector/SKILL.md"
    },
    {
      "slug": "detecting-performance-bottlenecks",
      "name": "detecting-performance-bottlenecks",
      "description": "This skill enables claude to detect and resolve performance bottlenecks",
      "allowedTools": [
        "Read",
        "Bash",
        "Grep",
        "Glob"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to identify and address performance bottlenecks across different layers of an application. By pinpointing performance issues in CPU, memory, I/O, and database operations, it assists in optimizing resource utilization and improving overall application speed and responsiveness.\n\n## How It Works\n\n1. **Architecture Analysis**: Claude analyzes the application's architecture and data flow to understand potential bottlenecks.\n2. **Bottleneck Identification**: The plugin identifies bottlenecks across CPU, memory, I/O, database, lock contention, and resource exhaustion.\n3. **Remediation Suggestions**: Claude provides remediation strategies with code examples to resolve the identified bottlenecks.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Diagnose slow application performance.\n- Optimize resource usage (CPU, memory, I/O, database).\n- Proactively prevent performance issues.\n\n## Examples\n\n### Example 1: Diagnosing Slow Database Queries\n\nUser request: \"detect bottlenecks in my database queries\"\n\nThe skill will:\n1. Analyze database query performance and identify slow-running queries.\n2. Suggest optimizations like indexing or query rewriting to improve database performance.\n\n### Example 2: Identifying Memory Leaks\n\nUser request: \"analyze performance and find memory leaks\"\n\nThe skill will:\n1. Profile memory usage patterns to identify potential memory leaks.\n2. Provide code examples and recommendations to fix the memory leaks.\n\n## Best Practices\n\n- **Comprehensive Analysis**: Always analyze all potential bottleneck areas (CPU, memory, I/O, database) for a complete picture.\n- **Prioritize by Severity**: Focus on addressing the most severe bottlenecks first for maximum impact.\n- **Test Thoroughly**: After implementing remediation strategies, thoroughly test the application to ensure the bottlenecks are resolved and no new issues are introduced.\n\n## Integration\n\nThis skill can be used in conjunction with code generation plugins to automatically implement the suggested remediation strategies. It also integrates with monitoring and logging tools to provide real-time performance data.",
      "parentPlugin": {
        "name": "bottleneck-detector",
        "category": "performance",
        "path": "plugins/performance/bottleneck-detector",
        "version": "1.0.0",
        "description": "Detect and resolve performance bottlenecks"
      },
      "filePath": "plugins/performance/bottleneck-detector/skills/bottleneck-detector/SKILL.md"
    },
    {
      "slug": "detecting-performance-regressions",
      "name": "detecting-performance-regressions",
      "description": "Automatically detect performance regressions in CI/CD pipelines by comparing metrics against baselines. Use when validating builds or analyzing performance trends. Trigger with phrases like \"detect performance regression\", \"compare performance metrics\", or \"analyze performance degradation\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(ci:*)",
        "Bash(metrics:*)",
        "Bash(testing:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the detection of performance regressions within a CI/CD pipeline. It utilizes various methods, including baseline comparison, statistical analysis, and threshold violation checks, to identify performance degradation. The skill provides insights into potential performance bottlenecks and helps maintain application performance.\n\n## How It Works\n\n1. **Analyze Performance Data**: The plugin gathers performance metrics from the CI/CD environment.\n2. **Detect Regressions**: It employs methods like baseline comparison, statistical analysis, and threshold checks to detect regressions.\n3. **Report Findings**: The plugin generates a report summarizing the detected performance regressions and their potential impact.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance regressions in a CI/CD pipeline.\n- Analyze performance metrics for potential degradation.\n- Compare current performance against historical baselines.\n\n## Examples\n\n### Example 1: Identifying a Response Time Regression\n\nUser request: \"Detect performance regressions in the latest build. Specifically, check for increases in response time.\"\n\nThe skill will:\n1. Analyze response time metrics from the latest build.\n2. Compare the response times against a historical baseline.\n3. Report any statistically significant increases in response time that exceed a defined threshold.\n\n### Example 2: Detecting Throughput Degradation\n\nUser request: \"Analyze throughput for performance regressions after the recent code merge.\"\n\nThe skill will:\n1. Gather throughput data (requests per second) from the post-merge CI/CD run.\n2. Compare the throughput to pre-merge values, looking for statistically significant drops.\n3. Generate a report highlighting any throughput degradation, indicating a potential performance regression.\n\n## Best Practices\n\n- **Define Baselines**: Establish clear and representative performance baselines for accurate comparison.\n- **Set Thresholds**: Configure appropriate thresholds for identifying significant performance regressions.\n- **Monitor Key Metrics**: Focus on monitoring critical performance metrics relevant to the application's behavior.\n\n## Integration\n\nThis skill can be integrated with other CI/CD tools to automatically trigger regression detection upon new builds or code merges. It can also be combined with reporting plugins to generate detailed performance reports.\n\n## Prerequisites\n\n- Historical performance baselines in {baseDir}/performance/baselines/\n- Access to CI/CD performance metrics\n- Statistical analysis tools\n- Defined regression thresholds\n\n## Instructions\n\n1. Collect performance metrics from current build\n2. Load historical baseline data\n3. Apply statistical analysis to detect significant changes\n4. Check for threshold violations\n5. Identify specific regressed metrics\n6. Generate regression report with root cause analysis\n\n## Output\n\n- Performance regression detection report\n- Statistical comparison with baselines\n- List of regressed metrics with severity\n- Visualization of performance trends\n- Recommendations for investigation\n\n## Error Handling\n\nIf regression detection fails:\n- Verify baseline data availability\n- Check metrics collection configuration\n- Validate statistical analysis parameters\n- Ensure threshold definitions are valid\n- Review CI/CD integration setup\n\n## Resources\n\n- Statistical process control for performance testing\n- CI/CD performance testing best practices\n- Regression detection algorithms\n- Performance monitoring strategies",
      "parentPlugin": {
        "name": "performance-regression-detector",
        "category": "performance",
        "path": "plugins/performance/performance-regression-detector",
        "version": "1.0.0",
        "description": "Detect performance regressions in CI/CD pipeline"
      },
      "filePath": "plugins/performance/performance-regression-detector/skills/performance-regression-detector/SKILL.md"
    },
    {
      "slug": "detecting-sql-injection-vulnerabilities",
      "name": "detecting-sql-injection-vulnerabilities",
      "description": "Detect and analyze SQL injection vulnerabilities in application code and database queries. Use when you need to scan code for SQL injection risks, review query construction, validate input sanitization, or implement secure query patterns. Trigger with phrases like \"detect SQL injection\", \"scan for SQLi vulnerabilities\", \"review database queries\", or \"check SQL security\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Application source code accessible in {baseDir}/\n- Database query files and ORM configurations available\n- Framework information (Django, Rails, Express, Spring, etc.)\n- Write permissions for security reports in {baseDir}/security-reports/\n\n## Instructions\n\n### 1. Code Discovery Phase\n\nLocate database interaction code:\n- SQL query construction\n- ORM usage (ActiveRecord, Hibernate, SQLAlchemy, etc.)\n- Stored procedure calls\n- Dynamic query builders\n- User input handling for database operations\n\n**Common patterns to search**:\n- Direct SQL: `SELECT`, `INSERT`, `UPDATE`, `DELETE` statements\n- String concatenation with user input\n- ORM raw query methods\n- Template-based query construction\n\n### 2. Vulnerability Pattern Detection\n\n**Critical SQL Injection Patterns**:\n\n**String Concatenation (Highly Vulnerable)**:\n```python\n# INSECURE: Direct concatenation\nquery = \"SELECT * FROM users WHERE username = '\" + user_input + \"'\"\ncursor.execute(query)\n\n# Attacker input: ' OR '1'='1' --\n# Results in: SELECT * FROM users WHERE username = '' OR '1'='1' --'\n```\n\n**Formatted Strings (Vulnerable)**:\n```javascript\n// INSECURE: Template literals\nconst query = `SELECT * FROM products WHERE id = ${productId}`;\ndb.query(query);\n```\n\n**Dynamic WHERE Clauses (Vulnerable)**:\n```php\n// INSECURE: Building conditions dynamically\n$sql = \"SELECT * FROM orders WHERE status = \" . $_GET['status'];\nmysqli_query($conn, $sql);\n```\n\n### 3. Secure Pattern Validation\n\n**Parameterized Queries (Secure)**:\n```python\n# SECURE: Parameterized query\nquery = \"SELECT * FROM users WHERE username = %s\"\ncursor.execute(query, (user_input,))\n```\n\n**Prepared Statements (Secure)**:\n```java\n// SECURE: Prepared statement\nString query = \"SELECT * FROM products WHERE category = ?\";\nPreparedStatement stmt = conn.prepareStatement(query);\nstmt.setString(1, userCategory);\n```\n\n**ORM Query Builders (Secure when used properly)**:\n```javascript\n// SECURE: ORM with parameter binding\nconst user = await User.findOne({\n  where: { username: userInput }\n});\n```\n\n### 4. Severity Classification\n\nRate SQL injection risks:\n- **Critical**: Direct user input in SQL without sanitization (authentication bypass, data exfiltration)\n- **High**: Partially sanitized input or weak escaping (potential bypass)\n- **Medium**: ORM misuse or raw queries with limited exposure\n- **Low**: SQL in administrative interfaces with access control\n\n### 5. Context Analysis\n\nFor each potential vulnerability:\n- Input source (GET/POST parameters, cookies, headers)\n- Query purpose (SELECT, INSERT, UPDATE, DELETE)\n- Data sensitivity (user data, financial records, PII)\n- Authentication requirements\n- Exploitability assessment\n\n### 6. Generate Security Report\n\nDocument findings with:\n- Vulnerability location (file, line number)\n- Vulnerable code snippet\n- Attack vector examples\n- Impact assessment\n- Secure code replacement\n- Framework-specific remediation\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: SQL injection vulnerability report saved to {baseDir}/security-reports/sqli-scan-YYYYMMDD.md\n\n**Report Structure**:\n```\n# SQL Injection Vulnerability Report\nScan Date: 2024-01-15\nApplication: E-commerce Platform\nFramework: Django 4.2\n\n## Executive Summary\n- Total Vulnerabilities: 12\n- Critical: 3\n- High: 5\n- Medium: 3\n- Low: 1\n\n## Critical Findings\n\n### 1. Authentication Bypass via SQL Injection\n**File**: {baseDir}/src/auth/login.py\n**Line**: 45\n**Severity**: CRITICAL (CVSS 9.8)\n\n**Vulnerable Code**:\n```python\ndef authenticate_user(username, password):\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    user = db.execute(query).fetchone()\n    return user is not None\n```\n\n**Attack Vector**:\n```\nUsername: admin' --\nPassword: anything\n\nResulting Query: SELECT * FROM users WHERE username='admin' --' AND password='anything'\nEffect: Password check bypassed, authentication as admin succeeds\n```\n\n**Impact**:\n- Complete authentication bypass\n- Unauthorized access to any account\n- Administrative privilege escalation\n- No audit trail of compromise\n\n**Remediation**:\n```python\ndef authenticate_user(username, password):\n    query = \"SELECT * FROM users WHERE username=%s AND password=%s\"\n    user = db.execute(query, (username, password)).fetchone()\n    return user is not None\n```\n\n**Additional Recommendations**:\n- Use password hashing (bcrypt, Argon2)\n- Implement account lockout after failed attempts\n- Add MFA for admin accounts\n\n### 2. Data Exfiltration via UNION-based SQLi\n**File**: {baseDir}/src/api/products.py\n**Line**: 78\n**Severity**: CRITICAL (CVSS 8.6)\n\n[Similar detailed structure...]\n\n## Summary by File\n- {baseDir}/src/auth/: 4 vulnerabilities (2 critical)\n- {baseDir}/src/api/: 6 vulnerabilities (1 critical, 5 high)\n- {baseDir}/src/reports/: 2 vulnerabilities (2 medium)\n\n## Remediation Checklist\n- [ ] Replace all string concatenation with parameterized queries\n- [ ] Audit ORM raw query usage\n- [ ] Implement input validation layer\n- [ ] Enable SQL query logging for monitoring\n- [ ] Deploy WAF rules for SQLi detection\n- [ ] Conduct penetration testing after fixes\n```\n\n**Secondary Outputs**:\n- SARIF format for GitHub Security scanning\n- JSON for vulnerability management systems\n- CWE mapping (CWE-89: SQL Injection)\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Framework Not Recognized**\n   - Error: \"Unknown ORM or database framework\"\n   - Resolution: Apply generic SQL injection pattern detection\n   - Note: Framework-specific recommendations unavailable\n\n2. **Obfuscated or Minified Code**\n   - Error: \"Cannot analyze compiled/minified code\"\n   - Resolution: Request source code or unminified version\n   - Limitation: Reduced detection accuracy\n\n3. **False Positives on Sanitized Input**\n   - Error: \"Flagged code that uses proper sanitization\"\n   - Resolution: Manual review required, check sanitization implementation\n   - Enhancement: Whitelist known-safe patterns\n\n4. **Dynamic Query Construction**\n   - Error: \"Complex query building logic difficult to analyze\"\n   - Resolution: Trace data flow manually, flag for manual review\n   - Recommendation: Refactor to simpler, auditable patterns\n\n5. **Stored Procedures**\n   - Error: \"Cannot analyze stored procedure definitions\"\n   - Resolution: Request SQL files or database exports\n   - Alternative: Focus on application-level code\n\n## Resources\n\n**OWASP Resources**:\n- SQL Injection Prevention Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html\n- OWASP Top 10 - Injection: https://owasp.org/www-project-top-ten/\n\n**Vulnerability Databases**:\n- CWE-89: SQL Injection: https://cwe.mitre.org/data/definitions/89.html\n- CAPEC-66: SQL Injection: https://capec.mitre.org/data/definitions/66.html\n\n**Framework-Specific Guides**:\n- Django Security: https://docs.djangoproject.com/en/stable/topics/security/\n- Rails Security: https://guides.rubyonrails.org/security.html\n- Node.js Best Practices: https://nodejs.org/en/docs/guides/security/\n\n**Testing Tools**:\n- SQLMap: Automated SQL injection testing\n- Burp Suite: Manual testing and exploitation\n- OWASP ZAP: Automated scanning\n\n**Secure Coding Examples**:\n- Parameterized queries by language/framework\n- Input validation patterns\n- Escaping techniques (when parameterization impossible)\n- Least privilege database user configuration",
      "parentPlugin": {
        "name": "sql-injection-detector",
        "category": "security",
        "path": "plugins/security/sql-injection-detector",
        "version": "1.0.0",
        "description": "Detect SQL injection vulnerabilities"
      },
      "filePath": "plugins/security/sql-injection-detector/skills/sql-injection-detector/SKILL.md"
    },
    {
      "slug": "emitting-api-events",
      "name": "emitting-api-events",
      "description": "Build event-driven APIs with webhooks, Server-Sent Events, and real-time notifications. Use when building event-driven API architectures. Trigger with phrases like \"add webhooks\", \"implement events\", or \"create event-driven API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:events-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:events-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-event-emitter",
        "category": "api-development",
        "path": "plugins/api-development/api-event-emitter",
        "version": "1.0.0",
        "description": "Implement event-driven APIs with message queues and event streaming"
      },
      "filePath": "plugins/api-development/api-event-emitter/skills/api-event-emitter/SKILL.md"
    },
    {
      "slug": "encrypting-and-decrypting-data",
      "name": "encrypting-and-decrypting-data",
      "description": "Validate encryption implementations and cryptographic practices. Use when reviewing data security measures. Trigger with 'check encryption', 'validate crypto', or 'review security keys'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to handle data encryption and decryption tasks seamlessly. It leverages the encryption-tool plugin to provide a secure way to protect sensitive information, ensuring confidentiality and integrity.\n\n## How It Works\n\n1. **Identify Encryption/Decryption Request**: Claude analyzes the user's request to determine whether encryption or decryption is required.\n2. **Select Encryption Method**: Claude prompts the user to specify the desired encryption algorithm (e.g., AES, RSA). If not specified, a default secure method is chosen.\n3. **Execute Encryption/Decryption**: Claude utilizes the encryption-tool plugin to perform the encryption or decryption operation on the provided data or file.\n4. **Return Encrypted/Decrypted Data**: Claude presents the encrypted or decrypted data to the user, or saves the result to a file as requested.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Encrypt sensitive data before storage or transmission.\n- Decrypt previously encrypted data for access or processing.\n- Generate encrypted files for secure archiving.\n\n## Examples\n\n### Example 1: Encrypting a Text File\n\nUser request: \"Encrypt the file 'sensitive_data.txt' using AES.\"\n\nThe skill will:\n1. Activate the encryption-tool plugin.\n2. Encrypt the contents of 'sensitive_data.txt' using AES encryption.\n3. Save the encrypted data to a new file (e.g., 'sensitive_data.txt.enc').\n\n### Example 2: Decrypting an Encrypted File\n\nUser request: \"Decrypt the file 'confidential.txt.enc'.\"\n\nThe skill will:\n1. Activate the encryption-tool plugin.\n2. Decrypt the contents of 'confidential.txt.enc' using the appropriate decryption key (assumed to be available or prompted for).\n3. Save the decrypted data to a new file (e.g., 'confidential.txt').\n\n## Best Practices\n\n- **Key Management**: Always store encryption keys securely and avoid hardcoding them in scripts.\n- **Algorithm Selection**: Choose encryption algorithms based on the sensitivity of the data and the required security level. Consider industry best practices and compliance requirements.\n- **Data Integrity**: Implement mechanisms to verify the integrity of encrypted data to detect tampering or corruption.\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins, such as file management tools, to automate the encryption and decryption of files during data processing workflows. It can also be combined with security auditing tools to ensure compliance with security policies.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "encryption-tool",
        "category": "security",
        "path": "plugins/security/encryption-tool",
        "version": "1.0.0",
        "description": "Encrypt and decrypt data with various algorithms"
      },
      "filePath": "plugins/security/encryption-tool/skills/encryption-tool/SKILL.md"
    },
    {
      "slug": "engineering-features-for-machine-learning",
      "name": "engineering-features-for-machine-learning",
      "description": "Create, select, and transform features to improve machine learning model",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill enables Claude to leverage the feature-engineering-toolkit plugin to enhance machine learning models. It automates the process of creating new features, selecting the most relevant ones, and transforming existing features to better suit the model's needs. By using this skill, you can improve the accuracy, efficiency, and interpretability of your machine learning models.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude analyzes the user's request and identifies the specific feature engineering task required.\n2. **Generating Code**: Claude generates Python code using the feature-engineering-toolkit plugin to perform the requested task. This includes data validation and error handling.\n3. **Executing Task**: The generated code is executed, creating, selecting, or transforming features as requested.\n4. **Providing Insights**: Claude provides performance metrics and insights related to the feature engineering process, such as the importance of newly created features or the impact of transformations on model performance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create new features from existing data to improve model accuracy.\n- Select the most relevant features from a dataset to reduce model complexity and improve efficiency.\n- Transform features to better suit the assumptions of a machine learning model (e.g., scaling, normalization, encoding).\n\n## Examples\n\n### Example 1: Improving Model Accuracy\n\nUser request: \"Create new features from the existing 'age' and 'income' columns to improve the accuracy of a customer churn prediction model.\"\n\nThe skill will:\n1. Generate code to create interaction terms between 'age' and 'income' (e.g., age * income, age / income).\n2. Execute the code and evaluate the impact of the new features on model performance.\n\n### Example 2: Reducing Model Complexity\n\nUser request: \"Select the top 10 most important features from the dataset to reduce the complexity of a fraud detection model.\"\n\nThe skill will:\n1. Generate code to calculate feature importance using a suitable method (e.g., Random Forest, SelectKBest).\n2. Execute the code and select the top 10 features based on their importance scores.\n\n## Best Practices\n\n- **Data Validation**: Always validate the input data to ensure it is clean and consistent before performing feature engineering.\n- **Feature Scaling**: Scale numerical features to prevent features with larger ranges from dominating the model.\n- **Encoding Categorical Features**: Encode categorical features appropriately (e.g., one-hot encoding, label encoding) to make them suitable for machine learning models.\n\n## Integration\n\nThis skill integrates with the feature-engineering-toolkit plugin, providing a seamless way to create, select, and transform features for machine learning models. It can be used in conjunction with other Claude Code skills to build complete machine learning pipelines.",
      "parentPlugin": {
        "name": "feature-engineering-toolkit",
        "category": "ai-ml",
        "path": "plugins/ai-ml/feature-engineering-toolkit",
        "version": "1.0.0",
        "description": "Feature creation, selection, and transformation tools"
      },
      "filePath": "plugins/ai-ml/feature-engineering-toolkit/skills/feature-engineering-toolkit/SKILL.md"
    },
    {
      "slug": "evaluating-machine-learning-models",
      "name": "evaluating-machine-learning-models",
      "description": "This skill allows claude to evaluate machine learning models using a",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to perform thorough evaluations of machine learning models, providing detailed performance insights. It leverages the `model-evaluation-suite` plugin to generate a range of metrics, enabling informed decisions about model selection and optimization.\n\n## How It Works\n\n1. **Analyzing Context**: Claude analyzes the user's request to identify the model to be evaluated and any specific metrics of interest.\n2. **Executing Evaluation**: Claude uses the `/eval-model` command to initiate the model evaluation process within the `model-evaluation-suite` plugin.\n3. **Presenting Results**: Claude presents the generated metrics and insights to the user, highlighting key performance indicators and potential areas for improvement.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess the performance of a machine learning model.\n- Compare the performance of multiple models.\n- Identify areas where a model can be improved.\n- Validate a model's performance before deployment.\n\n## Examples\n\n### Example 1: Evaluating Model Accuracy\n\nUser request: \"Evaluate the accuracy of my image classification model.\"\n\nThe skill will:\n1. Invoke the `/eval-model` command.\n2. Analyze the model's performance on a held-out dataset.\n3. Report the accuracy score and other relevant metrics.\n\n### Example 2: Comparing Model Performance\n\nUser request: \"Compare the F1-score of model A and model B.\"\n\nThe skill will:\n1. Invoke the `/eval-model` command for both models.\n2. Extract the F1-score from the evaluation results.\n3. Present a comparison of the F1-scores for model A and model B.\n\n## Best Practices\n\n- **Specify Metrics**: Clearly define the specific metrics of interest for the evaluation.\n- **Data Validation**: Ensure the data used for evaluation is representative of the real-world data the model will encounter.\n- **Interpret Results**: Provide context and interpretation of the evaluation results to facilitate informed decision-making.\n\n## Integration\n\nThis skill integrates seamlessly with the `model-evaluation-suite` plugin, providing a comprehensive solution for model evaluation within the Claude Code environment. It can be combined with other skills to build automated machine learning workflows.",
      "parentPlugin": {
        "name": "model-evaluation-suite",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-evaluation-suite",
        "version": "1.0.0",
        "description": "Comprehensive model evaluation with multiple metrics"
      },
      "filePath": "plugins/ai-ml/model-evaluation-suite/skills/model-evaluation-suite/SKILL.md"
    },
    {
      "slug": "excel-dcf-modeler",
      "name": "excel-dcf-modeler",
      "description": "Build discounted cash flow (DCF) valuation models in Excel with free",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Excel DCF Modeler\n\nCreates professional DCF valuation models following investment banking standards and best practices.\n\n## When to Invoke This Skill\n\nAutomatically load this Skill when the user asks to:\n- \"Create a DCF model\"\n- \"Build a valuation model\"\n- \"Calculate enterprise value\"\n- \"Value [company name]\"\n- \"DCF for [company]\"\n- \"Discounted cash flow analysis\"\n- \"What's the intrinsic value\"\n\n## Model Structure\n\nThis Skill creates a complete 4-sheet Excel DCF model:\n\n### Sheet 1: Assumptions\n- **Company Information**: Name, ticker, base year, fiscal year end\n- **Revenue Growth Rates**: Year 1-5 projections (%)\n- **Profitability Metrics**: EBITDA margin, D&A as % of revenue\n- **Working Capital**: NWC as % of revenue\n- **Capital Expenditures**: CapEx as % of revenue\n- **Tax Rate**: Corporate tax rate (%)\n- **Terminal Growth**: Long-term growth rate (typically 2-3%)\n- **Discount Rate (WACC)**: Weighted average cost of capital\n\n### Sheet 2: Free Cash Flow Projections\n```\nRevenue (Year 0 - Year 5)\n  Less: Operating Expenses\n= EBITDA\n  Less: Depreciation & Amortization\n= EBIT\n  Less: Taxes (EBIT √ó Tax Rate)\n= NOPAT (Net Operating Profit After Tax)\n  Add: Depreciation & Amortization\n  Less: Capital Expenditures\n  Less: Change in Net Working Capital\n= Unlevered Free Cash Flow\n```\n\n### Sheet 3: Valuation\n```\nPresent Value of FCF (Years 1-5)\n  Year 1 FCF / (1 + WACC)^1\n  Year 2 FCF / (1 + WACC)^2\n  ...\n  Year 5 FCF / (1 + WACC)^5\n= Sum of PV(FCF)\n\nTerminal Value Calculation\n  Terminal FCF = Year 5 FCF √ó (1 + Terminal Growth Rate)\n  Terminal Value = Terminal FCF / (WACC - Terminal Growth Rate)\n  PV of Terminal Value = Terminal Value / (1 + WACC)^5\n\nEnterprise Value\n  = Sum of PV(FCF) + PV(Terminal Value)\n\nEquity Value\n  = Enterprise Value\n  - Net Debt\n  + Non-Operating Assets\n\nEquity Value per Share\n  = Equity Value / Shares Outstanding\n```\n\n### Sheet 4: Sensitivity Analysis\nTwo-way sensitivity table showing Enterprise Value sensitivity to:\n- **Rows**: WACC (ranging from -2% to +2% of base case)\n- **Columns**: Terminal Growth Rate (ranging from 1.5% to 3.5%)\n- **Output**: Enterprise Value at each combination\n\n## Step-by-Step Workflow\n\n### 1. Gather Inputs\nAsk the user for the following information (provide defaults based on industry averages if user is uncertain):\n\n**Required Inputs:**\n- Company name and ticker symbol\n- Base year revenue (most recent fiscal year)\n- Revenue growth rates for Years 1-5 (e.g., 15%, 12%, 10%, 8%, 6%)\n- EBITDA margin % (e.g., 20%)\n- Tax rate % (e.g., 21% for US corporations)\n\n**Optional Inputs (use defaults if not provided):**\n- D&A as % of revenue (default: 5%)\n- CapEx as % of revenue (default: 4%)\n- NWC as % of revenue (default: 10%)\n- Terminal growth rate (default: 2.5%)\n- WACC/discount rate (default: 10%)\n- Net debt amount (default: $0)\n- Shares outstanding (if calculating per-share value)\n\n### 2. Validate Inputs\nEnsure the following before building the model:\n- Revenue growth rates are reasonable (typically 0-30%)\n- EBITDA margin is positive\n- Tax rate is between 0-40%\n- Terminal growth < WACC (model won't work if g >= WACC)\n- WACC is reasonable (typically 7-15%)\n\n### 3. Build Excel Model\nUse the Excel MCP server to:\n1. Create new workbook\n2. Create 4 sheets: \"Assumptions\", \"FCF Projections\", \"Valuation\", \"Sensitivity\"\n3. Populate assumptions in Sheet 1\n4. Build FCF projection formulas in Sheet 2 (link to assumptions)\n5. Calculate PV of FCF and Terminal Value in Sheet 3\n6. Create sensitivity table in Sheet 4\n7. Apply professional formatting:\n   - Currency format for monetary values\n   - Percentage format for rates\n   - Conditional formatting on sensitivity table (green = high value, red = low value)\n   - Bold headers, freeze panes\n   - Cell borders for clarity\n\n### 4. Calculate Results\nReturn to the user:\n- Enterprise Value (in millions/billions)\n- Equity Value (if net debt provided)\n- Equity Value per Share (if shares outstanding provided)\n- Key assumptions used\n- Link to sensitivity analysis\n\n### 5. Provide Insights\nAfter building the model, provide brief commentary:\n- Is the implied valuation reasonable given industry comparables?\n- What are the key value drivers? (usually revenue growth and margins)\n- What's the sensitivity to WACC? (typically highest sensitivity)\n- Recommend focus areas for further analysis\n\n## Example Formulas\n\n### Free Cash Flow Calculation\n```excel\n// Revenue (Year 1)\n=Assumptions!B5 * (1 + Assumptions!B10)\n\n// EBITDA\n=Revenue * Assumptions!B15\n\n// EBIT\n=EBITDA - (Revenue * Assumptions!B16)\n\n// NOPAT\n=EBIT * (1 - Assumptions!B18)\n\n// Free Cash Flow\n=NOPAT + (Revenue * Assumptions!B16) - (Revenue * Assumptions!B17) - ŒîNWCapital\n```\n\n### Present Value\n```excel\n// PV of Year 1 FCF\n=FCF_Year1 / (1 + WACC)^1\n\n// Terminal Value\n=(FCF_Year5 * (1 + Terminal_Growth)) / (WACC - Terminal_Growth)\n\n// PV of Terminal Value\n=Terminal_Value / (1 + WACC)^5\n```\n\n### Sensitivity Table\n```excel\n// Use Excel's data table feature\n// Row input: Terminal Growth Rate\n// Column input: WACC\n// Formula: =Enterprise_Value\n```\n\n## Example Interaction\n\n**User:** \"Create a DCF model for Tesla\"\n\n**Skill Response:**\n```\nI'll build a DCF valuation model for Tesla. Let me gather some inputs.\n\nWhat base year revenue should we use for Tesla?\n(For reference, Tesla's 2023 revenue was approximately $96.8B)\n\nUser: $96.8 billion\n\nGreat! What revenue growth rates should we assume for Years 1-5?\n(Recommended: Start high and taper down, e.g., 20%, 18%, 15%, 12%, 10%)\n\nUser: 25%, 20%, 15%, 12%, 10%\n\nWhat EBITDA margin should we assume?\n(Tesla's recent EBITDA margin has been around 15-17%)\n\nUser: 16%\n\nPerfect! I'll use the following assumptions:\n- Base year revenue: $96.8B\n- Growth rates: 25%, 20%, 15%, 12%, 10%\n- EBITDA margin: 16%\n- Tax rate: 21% (standard US rate)\n- WACC: 10% (typical for automotive/tech)\n- Terminal growth: 2.5%\n- D&A: 5% of revenue\n- CapEx: 4% of revenue\n- NWC: 10% of revenue\n\nBuilding your DCF model...\n\n‚úÖ DCF Model Complete!\n\nüìä Valuation Results:\n- Enterprise Value: $847.3 billion\n- Terminal Value represents: 68% of EV\n- Implied equity value per share: $243 (assuming 3.48B shares)\n\nüéØ Key Insights:\n- High growth assumptions drive significant value creation\n- Model is sensitive to WACC (see sensitivity table)\n- Terminal value is major component - validate terminal assumptions\n- Revenue growth is the primary value driver\n\nüìÅ Model saved to: Tesla_DCF_Model.xlsx\n\nThe sensitivity table shows Enterprise Value ranges from $612B (high WACC, low growth) to $1,124B (low WACC, high growth).\n\nWould you like me to adjust any assumptions or add additional analysis?\n```\n\n## Common Variations\n\n### Request: \"Build a DCF but I don't have all the numbers\"\n\n**Response:**\n```\nNo problem! I can build a DCF model with industry-average assumptions, and you can adjust later.\n\nWhat company are you valuing?\n\nUser: Amazon\n\nWhat's the approximate current revenue? (or I can look up recent financials)\n\nI'll use industry-standard assumptions for a large-cap tech company:\n- Revenue growth: 15%, 12%, 10%, 8%, 6% (tapering)\n- EBITDA margin: 25% (tech average)\n- WACC: 9%\n- Terminal growth: 2.5%\n\nI'll build the model with these defaults, and you can easily adjust the assumptions in the \"Assumptions\" sheet.\n```\n\n### Request: \"Explain the DCF model you built\"\n\n**Response:**\n```\nSure! Let me walk through the DCF model:\n\nüìä What is a DCF?\nA DCF values a company based on its future cash flows, discounted to present value.\n\nüî¢ The Model:\n1. Projects free cash flow for 5 years\n2. Calculates a \"terminal value\" for years beyond Year 5\n3. Discounts all cash flows to today using WACC\n4. Sums everything to get Enterprise Value\n\nüí∞ Key Components:\n- Free Cash Flow = Cash the company generates after all investments\n- WACC = Cost of capital (what investors expect to earn)\n- Terminal Value = Value of cash flows beyond Year 5 (usually 60-80% of total)\n\n‚ö†Ô∏è Important Notes:\n- This is a simplified DCF (doesn't include debt schedules, detailed NWC)\n- Garbage in, garbage out: Assumptions drive everything\n- Use this as a starting point, not final answer\n- Always compare to comparable company analysis (comps)\n\nWould you like me to explain any specific part in more detail?\n```\n\n## Error Handling\n\n### Invalid Inputs\nIf user provides invalid inputs:\n```\n‚ö†Ô∏è Issue detected: Terminal growth rate (4%) is >= WACC (3.5%)\n\nThis will cause a mathematical error in the terminal value calculation.\nThe terminal growth rate must be less than WACC.\n\nTypical ranges:\n- WACC: 7-15%\n- Terminal growth: 2-3%\n\nWould you like me to adjust these values?\n```\n\n### Missing Information\nIf critical information is missing:\n```\nI need at least the following to build a DCF:\n- Company name or industry\n- Approximate current revenue (or I can use industry average)\n\nAll other assumptions can use industry defaults.\n\nWould you like me to proceed with defaults, or would you prefer to provide specific assumptions?\n```\n\n## Best Practices Embedded\n\nThis Skill follows investment banking best practices:\n\n1. **Conservative Terminal Growth**: Default to 2.5% (GDP growth rate)\n2. **Tapering Growth Rates**: Revenue growth declines over projection period\n3. **Sensitivity Analysis**: Always include WACC and terminal growth sensitivity\n4. **Clear Labeling**: All assumptions clearly labeled and linked\n5. **Professional Formatting**: Currency/percentage formats, frozen panes, borders\n6. **Audit Trail**: Formulas link back to assumptions (no hard-coded values)\n7. **Reasonableness Checks**: Validate inputs before building model\n\n## Resources\n\nSee the resources folder for:\n- `dcf-template.xlsx`: Pre-built DCF template\n- `REFERENCE.md`: Financial modeling best practices\n- `formulas.txt`: Common DCF formulas for reference\n\n## Limitations\n\nThis Skill creates a simplified DCF model suitable for:\n- Initial valuation analysis\n- Pitch decks and presentations\n- Academic exercises\n- Quick \"back of envelope\" valuations\n\nFor detailed investment committee presentations or official fairness opinions, you should:\n- Add detailed debt schedules\n- Include multiple scenarios (base, bull, bear)\n- Add more granular operating assumptions\n- Validate with third-party data\n- Have a finance professional review\n\n## Version History\n\n- v1.0.0 (2025-10-27): Initial release with core DCF functionality",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-dcf-modeler/SKILL.md"
    },
    {
      "slug": "excel-lbo-modeler",
      "name": "excel-lbo-modeler",
      "description": "Create leveraged buyout (LBO) models in Excel with sources & uses, debt",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Excel LBO Modeler\n\nBuilds comprehensive LBO models for private equity transactions following industry-standard practices.\n\n## When to Invoke This Skill\n\nAutomatically load this Skill when the user asks to:\n- \"Create an LBO model\"\n- \"Build a leveraged buyout model\"\n- \"Private equity analysis for [company]\"\n- \"Calculate IRR for acquisition\"\n- \"LBO for [company]\"\n- \"Buyout model\"\n- \"What returns can we get on this deal?\"\n\n## Model Structure\n\nThis Skill creates a complete 6-sheet Excel LBO model:\n\n### Sheet 1: Transaction Summary\n- **Deal Terms**: Purchase price, entry multiple, equity check\n- **Sources & Uses**: How the deal is financed\n- **Returns Summary**: IRR, MoM, hold period\n\n### Sheet 2: Sources & Uses\n\n**Uses of Funds:**\n```\nPurchase Equity Value\n+ Estimated Net Debt\n= Enterprise Value\n+ Transaction Fees (2-3%)\n+ Financing Fees (2-3%)\n= Total Uses\n```\n\n**Sources of Funds:**\n```\nRevolver (typically 0% at close)\n+ Term Loan A (2-3x EBITDA)\n+ Term Loan B (2-3x EBITDA)\n+ Subordinated Debt (1-2x EBITDA)\n+ Preferred Equity (optional)\n+ Sponsor Equity (remainder)\n= Total Sources\n```\n\n### Sheet 3: Operating Model (5 Years)\n```\nRevenue\n  √ó Revenue Growth %\n  √ó EBITDA Margin %\n= EBITDA\n  - CapEx\n  - Change in NWC\n  - Cash Taxes\n= Cash Flow Available for Debt Service\n```\n\n### Sheet 4: Debt Schedule\n\n**For Each Debt Tranche:**\n```\nBeginning Balance\n+ Draws (if revolver)\n- Mandatory Amortization\n- Excess Cash Flow Sweep\n- Optional Prepayment\n= Ending Balance\n\nInterest Expense = Avg Balance √ó Interest Rate\n```\n\n**Debt Paydown Waterfall:**\n1. Revolver paydown\n2. Term Loan A amortization\n3. Term Loan B amortization\n4. Excess cash ‚Üí Revolver\n5. Remaining excess ‚Üí Optional prepayments\n\n### Sheet 5: Returns Analysis\n\n**Exit Valuation:**\n```\nExit Year EBITDA\n  √ó Exit Multiple\n= Exit Enterprise Value\n  - Net Debt at Exit\n= Exit Equity Value\n```\n\n**Returns Calculation:**\n```\nExit Equity Value\n  √∑ Initial Equity Investment\n= Money-on-Money Multiple (MoM)\n\nIRR = ((Exit Value / Entry Value)^(1/Years)) - 1\n```\n\n**Sensitivity Tables:**\n- Exit Multiple vs Hold Period ‚Üí IRR\n- Exit Multiple vs Entry Multiple ‚Üí IRR\n- EBITDA Growth vs Exit Multiple ‚Üí MoM\n\n### Sheet 6: Debt Covenants\n\n**Leverage Covenants:**\n```\nTotal Debt / EBITDA (typically ‚â§ 6.0x at entry, step down over time)\nSenior Debt / EBITDA (typically ‚â§ 4.0x)\n```\n\n**Coverage Covenants:**\n```\nEBITDA / Interest Expense (typically ‚â• 2.0x)\n(EBITDA - CapEx) / Debt Service (typically ‚â• 1.2x)\n```\n\n## Step-by-Step Workflow\n\n### 1. Gather Transaction Inputs\n\n**Required Inputs:**\n- Target company name\n- Current year EBITDA (or trailing twelve months)\n- Entry valuation multiple (EV/EBITDA, typically 8-12x)\n- Expected revenue growth (Years 1-5)\n- Expected EBITDA margin expansion (if any)\n- Exit multiple assumption (typically = entry multiple or slight premium)\n- Hold period (typically 5 years)\n\n**Optional Inputs (use defaults):**\n- CapEx as % of revenue (default: 3%)\n- NWC as % of revenue (default: 10%)\n- Tax rate (default: 25%)\n- Transaction fees (default: 2.5%)\n\n### 2. Structure Financing\n\n**Typical LBO Debt Structure:**\n- **Revolver**: 1-2x EBITDA, undrawn at close (cash buffer)\n- **Term Loan A**: 2-2.5x EBITDA, 5-7 year amortization\n- **Term Loan B**: 2-3x EBITDA, minimal amortization\n- **Subordinated/Mezzanine**: 1-2x EBITDA (if needed)\n- **Total Debt**: 5-6x EBITDA\n- **Equity**: Remainder (typically 30-40% of purchase price)\n\n**Interest Rates (as of 2025):**\n- Revolver: SOFR + 3.00% (default: 8.0%)\n- Term Loan A: SOFR + 3.50% (default: 8.5%)\n- Term Loan B: SOFR + 4.50% (default: 9.5%)\n- Subordinated: 12-14% (default: 13.0%)\n\n### 3. Build Operating Projections\n\nProject 5 years of operations:\n```\nYear 0 Revenue\n  √ó (1 + Growth Rate) for each year\n= Projected Revenue\n\nProjected Revenue\n  √ó EBITDA Margin\n= Projected EBITDA\n\nEBITDA\n  - CapEx (Revenue √ó CapEx %)\n  - Œî NWC (Change in Revenue √ó NWC %)\n  - Cash Taxes (assume % of EBITDA)\n= Cash Flow to Equity (before debt service)\n```\n\n### 4. Calculate Debt Paydown\n\nFor each year:\n1. Calculate cash available after operations\n2. Pay mandatory debt amortization\n3. Pay interest on all tranches\n4. Use excess cash to pay down revolver first\n5. Then pay down term loans (highest rate first)\n6. Track ending debt balances\n\n### 5. Calculate Returns\n\nAt exit (typically Year 5):\n```\nExit EBITDA\n  √ó Exit Multiple\n= Exit Enterprise Value\n  - Remaining Net Debt\n  + Excess Cash\n= Exit Equity Value\n\nMoney-on-Money = Exit Equity Value / Initial Equity\nIRR = (MoM)^(1/Years) - 1\n```\n\n### 6. Create Sensitivity Tables\n\nBuild 3 sensitivity analyses:\n1. **IRR Sensitivity**: Exit Multiple (rows) vs Hold Period (columns)\n2. **MoM Sensitivity**: Exit Multiple (rows) vs Entry Multiple (columns)\n3. **Value Creation**: EBITDA Growth (rows) vs Margin Expansion (columns)\n\n## Example Formulas\n\n### Sources & Uses\n```excel\n// Enterprise Value\n=EBITDA * Entry_Multiple\n\n// Transaction Fees\n=Enterprise_Value * 2.5%\n\n// Term Loan A\n=EBITDA * 2.0  // 2.0x EBITDA\n\n// Sponsor Equity\n=Total_Uses - Total_Debt\n```\n\n### Debt Schedule\n```excel\n// Beginning Balance Year 1\n=Sources_and_Uses!Term_Loan_A\n\n// Interest Expense\n=AVERAGE(Beginning_Balance, Ending_Balance) * Interest_Rate\n\n// Mandatory Amortization\n=Original_Balance / Amortization_Period\n\n// Ending Balance\n=Beginning_Balance - Mandatory_Amortization - Excess_Paydown\n```\n\n### Returns\n```excel\n// Exit Enterprise Value\n=Exit_EBITDA * Exit_Multiple\n\n// Exit Equity Value\n=Exit_Enterprise_Value - Exit_Net_Debt\n\n// IRR\n=IRR(Initial_Equity (negative), Year1_CF, Year2_CF, ..., Final_CF_with_Exit)\n\n// Money-on-Money\n=Exit_Equity_Value / Initial_Equity_Investment\n```\n\n## Example Interaction\n\n**User:** \"Build an LBO model for a $50M EBITDA software company\"\n\n**Skill Response:**\n```\nI'll create an LBO model for a software company with $50M EBITDA.\n\nLet me gather a few inputs:\n\nWhat entry multiple should we assume (EV/EBITDA)?\n(Software companies typically trade at 10-15x EBITDA)\n\nUser: 12x\n\nGreat! What revenue growth should we assume for Years 1-5?\n(Software companies often grow 15-25% annually)\n\nUser: 20%, 18%, 15%, 12%, 10%\n\nWhat EBITDA margin should we assume?\n(Current or target margin)\n\nUser: Start at 25%, expand to 30% by Year 5\n\nPerfect! I'll use these assumptions:\n\nüìä Transaction:\n- Entry EBITDA: $50M\n- Entry Multiple: 12.0x EV/EBITDA\n- Enterprise Value: $600M\n- Equity Check: ~$240M (40%)\n\nüí∞ Financing Structure:\n- Revolver: $50M (undrawn)\n- Term Loan A: $100M @ 8.5%\n- Term Loan B: $150M @ 9.5%\n- Subordinated: $50M @ 13.0%\n- Total Debt: $350M (7.0x EBITDA)\n- Sponsor Equity: $265M\n\nüìà Operating Assumptions:\n- Revenue growth: 20% ‚Üí 10% (tapering)\n- EBITDA margin: 25% ‚Üí 30% (expansion)\n- CapEx: 3% of revenue\n- NWC: 15% of revenue (working capital heavy)\n\nBuilding LBO model...\n\n‚úÖ LBO Model Complete!\n\nüìä Base Case Returns (5-year hold, 12x exit):\n- Exit EBITDA: $97.8M\n- Exit Enterprise Value: $1,174M\n- Debt Paydown: $350M ‚Üí $89M\n- Exit Equity Value: $1,124M\n- Money-on-Money: 4.2x\n- IRR: 34.2%\n\nüéØ Key Insights:\n- Strong returns driven by EBITDA growth (95% increase)\n- Significant deleveraging (7.0x ‚Üí 0.9x)\n- Margin expansion adds $24M in exit EBITDA\n- Multiple arbitrage: None (flat entry/exit multiple)\n\nüìÅ Model saved to: Software_LBO_Model.xlsx\n\n‚ö†Ô∏è Sensitivity Check:\n- At 10x exit: IRR = 26.8% (still attractive)\n- At 14x exit: IRR = 41.1% (exceptional)\n- Downside protected by strong cash generation\n\nWould you like me to adjust any assumptions or run additional scenarios?\n```\n\n## Common Variations\n\n### Request: \"What returns do we need to hit our 25% IRR hurdle?\"\n\n**Response:**\n```\nLet me build a reverse sensitivity to find the required exit conditions.\n\nWith current assumptions (12x entry, 5-year hold):\n\nTo achieve 25% IRR, you need:\n1. 10.5x exit multiple (13% discount to entry), OR\n2. 15% EBITDA CAGR (vs current 14%), OR\n3. 4.2 year hold period (vs 5 years)\n\nThe deal has multiple paths to your hurdle rate.\n\nWould you like me to add a scenario analysis showing these paths?\n```\n\n### Request: \"Model a dividend recap in Year 3\"\n\n**Response:**\n```\nI'll add a dividend recapitalization to the model.\n\nIn Year 3, after debt paydown, we could refinance and distribute a dividend.\n\nAssumptions:\n- Refi to 5.0x EBITDA (from current 3.8x)\n- Distribute ~$150M to equity sponsors\n- Reset debt maturity clocks\n\nThis would:\n- Boost IRR by ~300bps (partial liquidity in Year 3)\n- Reduce final exit proceeds\n- De-risk investment with early cash return\n\nShould I add this to the model?\n```\n\n## Error Handling\n\n### Over-Leveraged Structure\n```\n‚ö†Ô∏è Warning: Total debt of 8.5x EBITDA exceeds typical LBO leverage (5-7x).\n\nThis financing structure may:\n- Not be achievable in current market\n- Violate debt covenants\n- Leave insufficient cash flow for operations\n\nRecommended: Reduce debt to 6.0x EBITDA maximum.\n\nWould you like me to adjust the capital structure?\n```\n\n### Negative Cash Flow\n```\n‚ö†Ô∏è Issue: Model shows negative cash flow in Year 2.\n\nCauses:\n- Interest expense ($62M) + Debt amortization ($25M) > Cash Flow ($78M)\n- Insufficient EBITDA growth to service debt\n\nSolutions:\n1. Reduce entry leverage (currently 7.0x)\n2. Increase revenue growth assumptions\n3. Extend amortization schedule\n4. Add PIK interest option\n\nWould you like me to adjust the model?\n```\n\n## Best Practices Embedded\n\nThis Skill follows PE industry standards:\n\n1. **Debt Structure**: Typical 5-7x EBITDA total leverage\n2. **Conservative Assumptions**: Exit multiple ‚â§ entry multiple\n3. **Covenant Headroom**: Maintain >15% cushion on covenants\n4. **Cash Flow Sweep**: Model 75-100% excess cash flow to debt paydown\n5. **Multiple Scenarios**: Always include sensitivity tables\n6. **Professional Formatting**: Clear sections, color-coding, audit trail\n7. **Reasonableness Checks**: Validate leverage, coverage, growth rates\n\n## Resources\n\nSee the resources folder for:\n- `lbo-template.xlsx`: Pre-built LBO template\n- `REFERENCE.md`: Private equity modeling best practices\n- `debt-structures.txt`: Common debt structures by industry\n\n## Limitations\n\nThis Skill creates a standard LBO model suitable for:\n- Initial investment committee presentations\n- First-round analysis\n- Learning/training purposes\n- Quick deal screening\n\nFor detailed IC memos or final investment decisions, add:\n- Multiple scenarios (base, downside, upside)\n- Management option pool\n- Detailed working capital build\n- Quarterly debt schedules\n- Covenant compliance analysis throughout hold period\n- Transaction expense detail\n\n## Version History\n\n- v1.0.0 (2025-10-27): Initial release with core LBO functionality",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-lbo-modeler/SKILL.md"
    },
    {
      "slug": "excel-pivot-wizard",
      "name": "excel-pivot-wizard",
      "description": "Generate pivot tables and charts from raw data using natural language",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Excel Pivot Wizard\n\nCreates pivot tables and visualizations from raw data using natural language commands.\n\n## When to Invoke This Skill\n\nAutomatically load this Skill when the user asks to:\n- \"Create a pivot table\"\n- \"Analyze [data] by [dimension]\"\n- \"Summarize sales by region\"\n- \"Show revenue breakdown\"\n- \"Group data by category\"\n- \"Cross-tab analysis\"\n- \"Compare [X] across [Y]\"\n\n## Capabilities\n\n### Pivot Table Generation\n- **Rows**: Group data by one or more fields\n- **Columns**: Cross-tabulate across another dimension\n- **Values**: Aggregate functions (sum, average, count, min, max)\n- **Filters**: Slice data by specific criteria\n- **Calculated Fields**: Create custom formulas\n\n### Visualization\n- Column/bar charts for comparisons\n- Line charts for trends over time\n- Pie charts for composition\n- Combo charts for multiple metrics\n- Conditional formatting for heatmaps\n\n## Common Analysis Patterns\n\n### Pattern 1: Single Dimension Summary\n**Request:** \"Show total sales by region\"\n\n**Output:**\n```\n| Region    | Total Sales |\n|-----------|-------------|\n| Northeast | $1,250,000  |\n| Southeast | $980,000    |\n| Midwest   | $1,100,000  |\n| West      | $1,450,000  |\n| Total     | $4,780,000  |\n```\n\n### Pattern 2: Cross-Tabulation\n**Request:** \"Sales by region and product category\"\n\n**Output:**\n```\n| Region    | Electronics | Clothing | Home Goods | Total     |\n|-----------|-------------|----------|------------|-----------|\n| Northeast | $400K       | $500K    | $350K      | $1,250K   |\n| Southeast | $300K       | $380K    | $300K      | $980K     |\n| Midwest   | $450K       | $350K    | $300K      | $1,100K   |\n| West      | $550K       | $500K    | $400K      | $1,450K   |\n| Total     | $1,700K     | $1,730K  | $1,350K    | $4,780K   |\n```\n\n### Pattern 3: Time-Based Trending\n**Request:** \"Monthly revenue trend for 2024\"\n\n**Output:**\n```\nLine chart showing:\n- X-axis: Jan, Feb, Mar, ..., Dec\n- Y-axis: Revenue\n- Line: Monthly revenue with data labels\n```\n\n### Pattern 4: Top N Analysis\n**Request:** \"Top 10 products by revenue\"\n\n**Output:**\n```\n| Rank | Product       | Revenue   | % of Total |\n|------|---------------|-----------|------------|\n| 1    | Product A     | $450,000  | 9.4%       |\n| 2    | Product B     | $380,000  | 7.9%       |\n| 3    | Product C     | $350,000  | 7.3%       |\n| ...  | ...           | ...       | ...        |\n| 10   | Product J     | $180,000  | 3.8%       |\n|      | Top 10 Total  | $2,850,000| 59.6%      |\n|      | All Others    | $1,930,000| 40.4%      |\n|      | Grand Total   | $4,780,000| 100.0%     |\n```\n\n## Step-by-Step Workflow\n\n### 1. Understand the Data\n\nAsk clarifying questions if needed:\n- What does each column represent?\n- What grain is the data? (transaction-level, daily summary, etc.)\n- What fields should be aggregated vs grouped?\n\n### 2. Interpret the Request\n\nParse natural language into pivot table structure:\n\n**\"Show sales by region and month\"** ‚Üí\n```\nRows: Region\nColumns: Month\nValues: Sum of Sales\n```\n\n**\"Average order value by customer segment\"** ‚Üí\n```\nRows: Customer Segment\nValues: Average of Order Value\n```\n\n**\"Count of orders by product category and sales rep\"** ‚Üí\n```\nRows: Product Category\nColumns: Sales Rep\nValues: Count of Orders\n```\n\n### 3. Build the Pivot Table\n\nUse Excel MCP server to:\n1. Create pivot table on new sheet\n2. Set row fields\n3. Set column fields (if cross-tab)\n4. Set value fields with aggregation function\n5. Add filters/slicers if requested\n6. Apply formatting (currency, thousands separator, etc.)\n\n### 4. Add Enhancements\n\n**Conditional Formatting:**\n```\n- Top 10% values: Dark green\n- Bottom 10% values: Dark red\n- Color scale for heatmap effect\n```\n\n**Calculated Fields:**\n```\n- Profit Margin % = (Revenue - Cost) / Revenue\n- Growth % = (This Year - Last Year) / Last Year\n- % of Total = Value / Grand Total\n```\n\n**Sorting:**\n```\n- Largest to smallest by default\n- Or chronological for dates\n```\n\n### 5. Create Visualization\n\nIf visualization is requested or would be helpful:\n\n**For Comparisons:** Column chart\n**For Trends:** Line chart\n**For Composition:** Pie/donut chart\n**For Correlations:** Scatter plot\n**For Multiple Metrics:** Combo chart (column + line)\n\n## Example Interactions\n\n### Example 1: Sales Analysis\n\n**User:** \"Analyze sales by region and product category\"\n\n**Skill Response:**\n```\nI'll create a pivot table to analyze sales by region and product category.\n\nLooking at your data, I see:\n- 5,234 transaction rows\n- Date range: Jan 1 - Oct 27, 2025\n- 4 regions, 12 product categories\n\nCreating pivot table...\n\n‚úÖ Pivot Table Created!\n\nüìä Sales by Region and Product Category:\n\nTop Insights:\n1. West region leads with $1.45M (30.3% of total)\n2. Electronics is #1 category at $1.70M (35.6%)\n3. Northeast + Electronics = $400K (highest cell)\n4. Southeast underperforming in Home Goods ($300K vs avg $338K)\n\nVisualization: Added column chart comparing regions\n\nWould you like me to:\n- Add a slicer to filter by quarter?\n- Calculate % of total for each cell?\n- Create a separate view for top products?\n```\n\n### Example 2: Time Trend\n\n**User:** \"Show monthly revenue trend\"\n\n**Skill Response:**\n```\nCreating monthly revenue trend analysis...\n\nI see data spanning 10 months (Jan-Oct 2025).\n\n‚úÖ Trend Analysis Complete!\n\nüìà Monthly Revenue:\n\nKey Findings:\n- Strong start: Q1 averaged $520K/month\n- Dip in summer: Jun-Aug averaged $420K/month\n- Recovery: Sep-Oct back to $510K/month\n- YTD Total: $4.78M\n- Average: $478K/month\n\nCreated line chart with:\n- Trend line showing slight upward trajectory\n- Data labels for easy reading\n- Target line at $500K (if provided)\n\nüìä Growth Rates:\n- Jan-Oct: +8.3% overall\n- Best month: March ($575K)\n- Weakest month: August ($385K)\n\nRecommendation: Investigate summer seasonality (Jun-Aug dip)\n```\n\n### Example 3: Customer Segmentation\n\n**User:** \"Show average order value by customer segment\"\n\n**Skill Response:**\n```\nAnalyzing average order value (AOV) across customer segments...\n\n‚úÖ Customer Segmentation Analysis Complete!\n\n| Segment       | Orders | Total Revenue | Avg Order Value | % of Revenue |\n|---------------|--------|---------------|-----------------|--------------|\n| Enterprise    | 245    | $1,850,000    | $7,551         | 38.7%        |\n| Mid-Market    | 892    | $1,920,000    | $2,152         | 40.2%        |\n| Small Business| 2,456  | $835,000      | $340           | 17.5%        |\n| Consumer      | 1,641  | $175,000      | $107           | 3.7%         |\n| Total         | 5,234  | $4,780,000    | $913           | 100.0%       |\n\nüéØ Key Insights:\n- Enterprise: Highest AOV ($7.5K) but only 4.7% of orders\n- Mid-Market: Sweet spot (40% of revenue, $2.2K AOV)\n- Small Business: High volume (47% of orders) but low AOV\n- Consumer: Explore if worth serving (low AOV, high volume)\n\nCreated bar chart showing AOV by segment with order count overlay.\n\nüí° Strategic Questions:\n1. Should we focus more resources on Enterprise (high AOV)?\n2. Can we up-sell Small Business to Mid-Market?\n3. Is Consumer segment profitable at $107 AOV?\n```\n\n### Example 4: Product Performance\n\n**User:** \"Which products are driving the most revenue?\"\n\n**Skill Response:**\n```\nCreating product revenue ranking...\n\n‚úÖ Top Products Analysis Complete!\n\nüìä Top 10 Products (out of 147 total):\n\n| Rank | Product          | Revenue   | Orders | AOV    | % of Total |\n|------|------------------|-----------|--------|--------|------------|\n| 1    | Pro Laptop 15\"   | $450,000  | 385    | $1,169 | 9.4%       |\n| 2    | Business Suite   | $380,000  | 892    | $426   | 7.9%       |\n| 3    | Smart Display    | $350,000  | 1,245  | $281   | 7.3%       |\n| 4    | Wireless Buds    | $310,000  | 2,156  | $144   | 6.5%       |\n| 5    | Office Chair Pro | $295,000  | 428    | $689   | 6.2%       |\n| 6    | Standing Desk    | $280,000  | 356    | $787   | 5.9%       |\n| 7    | Monitor 27\"      | $265,000  | 487    | $544   | 5.5%       |\n| 8    | Keyboard Mech    | $245,000  | 1,823  | $134   | 5.1%       |\n| 9    | Webcam 4K        | $220,000  | 678    | $324   | 4.6%       |\n| 10   | Mouse Wireless   | $195,000  | 2,145  | $91    | 4.1%       |\n|      | **Top 10 Total** | **$2,990,000** | | | **62.5%** |\n|      | All Others (137) | $1,790,000 |    |        | 37.5%       |\n|      | **Grand Total**  | **$4,780,000** | | | **100.0%** |\n\nüéØ Insights:\n- Top 10 products = 62.5% of revenue (strong concentration)\n- Pro Laptop alone = 9.4% of total revenue\n- High-volume low-AOV products: Wireless Buds, Keyboard, Mouse\n- High-AOV low-volume products: Pro Laptop, Standing Desk\n\nCreated:\n- Bar chart: Top 10 products by revenue\n- Pareto chart: Cumulative % of revenue\n\nüí° Strategic Implications:\n1. Focus inventory on Top 20 products (likely 80% of revenue)\n2. Bundle high-volume + high-margin items\n3. Wireless Buds: Explore upsell to Pro Buds (higher margin?)\n```\n\n## Advanced Features\n\n### Calculated Fields\n\n**Request:** \"Show profit margin by product category\"\n\n**Response:**\n```\nI'll add a calculated field for profit margin.\n\nAssuming you have Cost and Revenue columns, I'll calculate:\nProfit Margin % = (Revenue - Cost) / Revenue √ó 100%\n\n| Category    | Revenue | Cost    | Profit  | Margin % |\n|-------------|---------|---------|---------|----------|\n| Electronics | $1,700K | $1,020K | $680K   | 40.0%    |\n| Clothing    | $1,730K | $1,125K | $605K   | 35.0%    |\n| Home Goods  | $1,350K | $945K   | $405K   | 30.0%    |\n| Total       | $4,780K | $3,090K | $1,690K | 35.4%    |\n\nElectronics has the highest margin (40%) despite not being the highest revenue category.\n```\n\n### Slicers & Filters\n\n**Request:** \"Add filters for date range and region\"\n\n**Response:**\n```\nI've added slicers for:\n1. Date Range (select quarters or specific months)\n2. Region (select one or multiple regions)\n3. Product Category (filter to specific categories)\n\nThese slicers are linked to the pivot table - any selection will update the data automatically.\n\nTry clicking Q3 in the date slicer to see summer performance!\n```\n\n### Multiple Aggregations\n\n**Request:** \"Show sum and average of sales by region\"\n\n**Response:**\n```\n| Region    | Total Sales | Avg Sale | Order Count |\n|-----------|-------------|----------|-------------|\n| Northeast | $1,250,000  | $625     | 2,000       |\n| Southeast | $980,000    | $490     | 2,000       |\n| Midwest   | $1,100,000  | $524     | 2,100       |\n| West      | $1,450,000  | $690     | 2,100       |\n\nInteresting: Southeast has lowest average sale despite same order count as Northeast.\nThis suggests smaller basket sizes in Southeast - worth investigating!\n```\n\n## Formatting Best Practices\n\nThe Skill applies professional formatting:\n\n### Numbers\n```\nRevenue: $1,250,000 or $1.25M (use M for millions)\nCounts: 2,000 (thousands separator)\nPercentages: 35.0% (1 decimal)\n```\n\n### Conditional Formatting\n```\nTop performers: Green highlight\nBottom performers: Red highlight\nHeatmap: Color gradient from red (low) to green (high)\n```\n\n### Layout\n```\n- Bold headers\n- Freeze top row and left column\n- Subtotals and grand totals\n- Alternating row colors for readability\n```\n\n## Resources\n\nSee resources folder for:\n- `REFERENCE.md`: Pivot table best practices\n- `examples/`: Sample pivot tables for common analyses\n\n## Limitations\n\nThis Skill creates standard pivot tables for:\n- Summarization and aggregation\n- Cross-tabulation\n- Basic calculations (sum, average, count)\n\nFor advanced analysis, you may need:\n- Power Pivot (for complex data models)\n- Pivot charts with custom formatting\n- Integration with external data sources\n- Real-time data refresh\n\n## Version History\n\n- v1.0.0 (2025-10-27): Initial release with core pivot table generation",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-pivot-wizard/SKILL.md"
    },
    {
      "slug": "excel-variance-analyzer",
      "name": "excel-variance-analyzer",
      "description": "Automate budget vs actual variance analysis in excel with flagging, commentary,",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Excel Variance Analyzer\n\nAutomates variance analysis for monthly/quarterly financial reporting and budget reviews.\n\n## When to Invoke This Skill\n\nAutomatically load this Skill when the user asks to:\n- \"Analyze budget variance\"\n- \"Compare actual vs forecast\"\n- \"Create variance report\"\n- \"Explain budget differences\"\n- \"Why are we over/under budget?\"\n- \"Variance analysis for [period]\"\n- \"Budget vs actual\"\n\n## Report Structure\n\nCreates a comprehensive variance report with 3 sheets:\n\n### Sheet 1: Variance Summary\n```\n| Line Item       | Budget  | Actual  | Variance | % Var | Flag | Commentary |\n|-----------------|---------|---------|----------|-------|------|------------|\n| Revenue         | $1,000K | $950K   | $(50K)   | -5.0% | ‚ö†Ô∏è   | Below plan |\n| COGS            | $600K   | $580K   | $(20K)   | -3.3% | ‚úÖ   | Favorable  |\n| Gross Profit    | $400K   | $370K   | $(30K)   | -7.5% | üî¥   | Investigate|\n| Operating Exp   | $250K   | $280K   | $30K     | 12.0% | üî¥   | Over budget|\n| EBITDA          | $150K   | $90K    | $(60K)   | -40.0%| üî¥   | Miss       |\n```\n\n### Sheet 2: Executive Summary\n```\nüìä Performance Highlights\n- Total Revenue: $950K (5.0% below budget)\n- EBITDA: $90K (40.0% below budget)\n- Key Driver: Operating expenses 12% over budget\n\nüî¥ Top 5 Unfavorable Variances:\n1. EBITDA: $(60K) / -40.0%\n2. Revenue: $(50K) / -5.0%\n3. Operating Expenses: $30K / 12.0%\n4. Gross Profit: $(30K) / -7.5%\n5. Marketing: $25K / 25.0%\n\n‚úÖ Top 5 Favorable Variances:\n1. COGS: $(20K) / -3.3%\n2. Rent: $(5K) / -10.0%\n3. Utilities: $(2K) / -8.0%\n```\n\n### Sheet 3: Trend Analysis (if multiple periods)\n```\n| Line Item | Jan Var% | Feb Var% | Mar Var% | Q1 Var% | Trend |\n|-----------|----------|----------|----------|---------|-------|\n| Revenue   | -3%      | -5%      | -7%      | -5%     | ‚¨áÔ∏è    |\n| COGS      | -2%      | -4%      | -3%      | -3%     | ‚û°Ô∏è    |\n```\n\n## Step-by-Step Workflow\n\n### 1. Load Data\n\nAsk the user for:\n- **Budget data**: Can be Excel file, CSV, or pasted table\n- **Actual data**: Same format as budget\n- **Period**: Month, quarter, YTD\n- **Threshold settings** (or use defaults):\n  - Percentage threshold: 10% (flag items >10% variance)\n  - Dollar threshold: $50K (flag items >$50K absolute variance)\n  - Categories to exclude: (e.g., non-cash items like depreciation)\n\n### 2. Validate Data\n\nBefore analysis, check:\n- Budget and actual have matching line items\n- All values are numeric\n- No missing data for key categories (revenue, expenses, profit)\n- Budget data is reasonable (no zeros where there should be values)\n\n### 3. Calculate Variances\n\nFor each line item:\n```\nAbsolute Variance = Actual - Budget\nPercentage Variance = (Actual - Budget) / Budget √ó 100%\n\nSign Convention:\n- Positive variance for revenue/profit = Favorable (‚úÖ)\n- Negative variance for revenue/profit = Unfavorable (üî¥)\n- Positive variance for expenses = Unfavorable (üî¥)\n- Negative variance for expenses = Favorable (‚úÖ)\n```\n\n### 4. Flag Material Items\n\nApply flagging rules:\n```\nüî¥ Red Flag (Critical):\n- Revenue/profit >10% below budget\n- Expenses >10% over budget\n- Absolute variance >$100K\n\n‚ö†Ô∏è Yellow Flag (Warning):\n- Revenue/profit 5-10% below budget\n- Expenses 5-10% over budget\n- Absolute variance $50K-$100K\n\n‚úÖ Green (On Track):\n- Variance within ¬±5%\n- Absolute variance <$50K\n```\n\n### 5. Generate Commentary\n\nFor each flagged item, provide automated commentary:\n\n**Revenue Below Budget:**\n```\nRevenue is $50K (5%) below budget. Possible drivers:\n- Lower than expected sales volume\n- Pricing pressure\n- Delayed revenue recognition\n- Seasonal factors\nRecommend: Review sales pipeline and backlog\n```\n\n**Expenses Over Budget:**\n```\nOperating expenses are $30K (12%) over budget. Breakdown:\n- Marketing: $25K over (new campaign)\n- Travel: $8K over\n- Utilities: $2K under\nRecommend: Review Q2 marketing spend and ROI\n```\n\n**Favorable Variances:**\n```\nCOGS is $20K (3.3%) under budget. Drivers:\n- Lower raw material costs\n- Improved operational efficiency\n- Volume discounts from suppliers\n```\n\n### 6. Create Executive Summary\n\nSummarize for leadership:\n```\nüìä [Period] Financial Performance\n\nBottom Line:\n- Revenue: [X%] vs budget ([favorable/unfavorable])\n- EBITDA: [X%] vs budget ([favorable/unfavorable])\n- Net Variance Impact: $[XXX]K\n\nKey Takeaways:\n1. [Primary driver of miss/beat]\n2. [Secondary factor]\n3. [Outlook for next period]\n\nAction Items:\n1. [Specific action needed]\n2. [Follow-up required]\n```\n\n## Example Interaction\n\n**User:** \"Analyze Q1 budget vs actual\"\n\n**Skill Response:**\n```\nI'll analyze your Q1 variance. Let me load the data.\n\nDo you have budget and actual data files, or would you like to paste them?\n\nUser: *pastes Excel data*\n\nPerfect! I see data for Q1 2025. Setting analysis parameters:\n- Period: Q1 2025 (Jan-Mar)\n- Line items: 15 categories detected\n- Materiality threshold: ¬±10% or ¬±$50K\n\nRunning variance analysis...\n\n‚úÖ Variance Analysis Complete!\n\nüìä Q1 2025 PERFORMANCE SUMMARY\n\nüéØ Overall Results:\n- Revenue: $2,850K vs budget $3,000K (-5.0%) üî¥\n- Gross Profit: $1,110K vs budget $1,200K (-7.5%) üî¥\n- EBITDA: $270K vs budget $450K (-40.0%) üî¥\n\nüìâ Key Drivers of Underperformance:\n1. Revenue Miss: $(150K) - 5% below plan\n   ‚Üí Slower than expected customer acquisition\n   ‚Üí Delayed Q1 enterprise deals to Q2\n\n2. Operating Expenses Over: $90K - 12% above plan\n   ‚Üí Marketing campaign: $75K incremental spend\n   ‚Üí Headcount additions brought forward to Q1\n\nüîç Detailed Findings:\n\nüî¥ CRITICAL VARIANCES (>10%):\n- Operating Expenses: $840K vs $750K (+12.0%) - $90K over\n- EBITDA: $270K vs $450K (-40.0%) - $180K under\n- Marketing: $325K vs $260K (+25.0%) - $65K over\n\n‚ö†Ô∏è WARNING VARIANCES (5-10%):\n- Revenue: $2,850K vs $3,000K (-5.0%) - $150K under\n- Gross Profit: $1,110K vs $1,200K (-7.5%) - $90K under\n\n‚úÖ FAVORABLE VARIANCES:\n- COGS: $1,740K vs $1,800K (-3.3%) - $60K favorable\n- Facilities: $45K vs $50K (-10.0%) - $5K favorable\n\nüí° RECOMMENDATIONS:\n\nImmediate Actions:\n1. Review Q2 sales pipeline - need $150K catch-up + Q2 plan\n2. Evaluate marketing campaign ROI (Q1 spend +$65K)\n3. Assess if Q1 revenue timing shift impacts full-year plan\n\nStrategic Questions:\n1. Is Q1 revenue miss timing (shifts to Q2) or lost?\n2. Will marketing investment drive Q2-Q4 revenue?\n3. Should we adjust full-year budget guidance?\n\nüìÅ Report saved to: Q1_2025_Variance_Analysis.xlsx\n\nWould you like me to:\n- Add trend analysis vs Q4 2024?\n- Create forecast scenarios for Q2?\n- Drill into specific expense categories?\n```\n\n## Common Variations\n\n### Request: \"Why is marketing over budget?\"\n\n**Response:**\n```\nLet me drill into marketing expenses.\n\nMarketing Variance Detail:\nBudget: $260K\nActual: $325K\nVariance: $65K (25% over)\n\nBreakdown by subcategory:\n- Digital Ads: $180K vs $150K (+$30K)\n  ‚Üí Google Ads campaign expanded\n- Events: $75K vs $60K (+$15K)\n  ‚Üí Added trade show booth\n- Content: $45K vs $40K (+$5K)\n  ‚Üí Video production\n- Agencies: $25K vs $10K (+$15K)\n  ‚Üí New creative agency retainer\n\nPrimary driver: Digital ads campaign expansion (+$30K)\n\nWas this spend planned but not budgeted, or truly incremental?\nWould you like me to analyze marketing ROI?\n```\n\n### Request: \"Compare last 3 months\"\n\n**Response:**\n```\nI'll create a 3-month trend analysis.\n\nRevenue Trend (Budget vs Actual):\n| Month | Budget | Actual | Variance | Var% | Trend |\n|-------|--------|--------|----------|------|-------|\n| Jan   | $1,000K| $970K  | $(30K)   | -3%  | ‚¨áÔ∏è    |\n| Feb   | $1,000K| $950K  | $(50K)   | -5%  | ‚¨áÔ∏è    |\n| Mar   | $1,000K| $930K  | $(70K)   | -7%  | ‚¨áÔ∏è    |\n| Q1    | $3,000K| $2,850K| $(150K)  | -5%  | ‚¨áÔ∏è    |\n\n‚ö†Ô∏è ALERT: Revenue variance is worsening month-over-month\n\nThis suggests a structural issue, not timing:\n- Jan: 97% of budget\n- Feb: 95% of budget\n- Mar: 93% of budget\n\nRecommendation: Immediate deep-dive on sales execution\n```\n\n## Formatting Rules\n\nThe Skill applies professional formatting:\n\n### Conditional Formatting\n```\n- Green cells: Favorable variances (>5% better than budget)\n- Yellow cells: Variances within ¬±5%\n- Red cells: Unfavorable variances (>5% worse than budget)\n```\n\n### Number Formatting\n```\n- Currency: $1,000K or $1.0M (use K for thousands, M for millions)\n- Percentages: 1 decimal place (5.0%)\n- Variance: Show sign ($(50K) or $50K)\n```\n\n### Icons\n```\n‚úÖ = On track / Favorable\n‚ö†Ô∏è = Warning / Needs attention\nüî¥ = Critical / Unfavorable\n‚¨ÜÔ∏è = Improving trend\n‚¨áÔ∏è = Worsening trend\n‚û°Ô∏è = Flat trend\n```\n\n## Best Practices Embedded\n\n1. **Materiality Thresholds**: Don't flag every small variance\n2. **Commentary Not Just Numbers**: Explain \"why\", not just \"what\"\n3. **Action-Oriented**: Recommend next steps\n4. **Executive Summary**: Leadership wants top 5-10 items\n5. **Trend Analysis**: Show if variance is new or ongoing\n6. **Sign Conventions**: Consistent favorable/unfavorable labeling\n7. **Audit Trail**: Show calculations and formulas\n\n## Resources\n\nSee resources folder for:\n- `REFERENCE.md`: Variance analysis best practices\n- `templates/`: Sample variance reports\n\n## Limitations\n\nThis Skill provides automated variance analysis for:\n- Standard income statement formats\n- Monthly/quarterly reporting\n- Budget vs actual comparisons\n\nFor more complex analysis, you may need:\n- Statistical variance analysis (standard deviations)\n- Multi-year trend analysis\n- Driver-based variance decomposition\n- Forecast vs forecast comparisons\n\n## Version History\n\n- v1.0.0 (2025-10-27): Initial release with core variance analysis functionality",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-variance-analyzer/SKILL.md"
    },
    {
      "slug": "explaining-machine-learning-models",
      "name": "explaining-machine-learning-models",
      "description": "This skill enables claude to provide interpretability and explainability",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to analyze and explain machine learning models. It helps users understand why a model makes certain predictions, identify the most influential features, and gain insights into the model's overall behavior.\n\n## How It Works\n\n1. **Analyze Context**: Claude analyzes the user's request and the available model data.\n2. **Select Explanation Technique**: Claude chooses the most appropriate explanation technique (e.g., SHAP, LIME) based on the model type and the user's needs.\n3. **Generate Explanations**: Claude uses the selected technique to generate explanations for model predictions.\n4. **Present Results**: Claude presents the explanations in a clear and concise format, highlighting key insights and feature importances.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Understand why a machine learning model made a specific prediction.\n- Identify the most important features influencing a model's output.\n- Debug model performance issues by identifying unexpected feature interactions.\n- Communicate model insights to non-technical stakeholders.\n- Ensure fairness and transparency in model predictions.\n\n## Examples\n\n### Example 1: Understanding Loan Application Decisions\n\nUser request: \"Explain why this loan application was rejected.\"\n\nThe skill will:\n1. Analyze the loan application data and the model's prediction.\n2. Calculate SHAP values to determine the contribution of each feature to the rejection decision.\n3. Present the results, highlighting the features that most strongly influenced the outcome, such as credit score or debt-to-income ratio.\n\n### Example 2: Identifying Key Factors in Customer Churn\n\nUser request: \"Interpret the customer churn model and identify the most important factors.\"\n\nThe skill will:\n1. Analyze the customer churn model and its predictions.\n2. Use LIME to generate local explanations for individual customer churn predictions.\n3. Aggregate the LIME explanations to identify the most important features driving churn, such as customer tenure or service usage.\n\n## Best Practices\n\n- **Model Type**: Choose the explanation technique that is most appropriate for the model type (e.g., tree-based models, neural networks).\n- **Data Preprocessing**: Ensure that the data used for explanation is properly preprocessed and aligned with the model's input format.\n- **Visualization**: Use visualizations to effectively communicate model insights and feature importances.\n\n## Integration\n\nThis skill integrates with other data analysis and visualization plugins to provide a comprehensive model understanding workflow. It can be used in conjunction with data cleaning and preprocessing plugins to ensure data quality and with visualization tools to present the explanation results in an informative way.",
      "parentPlugin": {
        "name": "model-explainability-tool",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-explainability-tool",
        "version": "1.0.0",
        "description": "Model interpretability and explainability"
      },
      "filePath": "plugins/ai-ml/model-explainability-tool/skills/model-explainability-tool/SKILL.md"
    },
    {
      "slug": "exploring-blockchain-data",
      "name": "exploring-blockchain-data",
      "description": "Query and analyze blockchain data including blocks, transactions, and smart contracts. Use when querying blockchain data and transactions. Trigger with phrases like \"explore blockchain\", \"query transactions\", or \"check on-chain data\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:explorer-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:explorer-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "blockchain-explorer-cli",
        "category": "crypto",
        "path": "plugins/crypto/blockchain-explorer-cli",
        "version": "1.0.0",
        "description": "Command-line blockchain explorer for transactions, addresses, and contracts"
      },
      "filePath": "plugins/crypto/blockchain-explorer-cli/skills/blockchain-explorer-cli/SKILL.md"
    },
    {
      "slug": "fairdb-backup-manager",
      "name": "fairdb-backup-manager",
      "description": "Use when you need to work with backup and recovery. This skill provides backup automation and disaster recovery with comprehensive guidance and automation. Trigger with phrases like \"create backups\", \"automate backups\", or \"implement disaster recovery\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/fairdb-backup-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/fairdb-backup-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/fairdb-backup-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/fairdb-backup-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/fairdb-backup-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/fairdb-backup-manager-dashboard.json`",
      "parentPlugin": {
        "name": "fairdb-operations-kit",
        "category": "devops",
        "path": "plugins/devops/fairdb-operations-kit",
        "version": "1.0.0",
        "description": "Complete operations kit for FairDB PostgreSQL as a Service - VPS setup, PostgreSQL management, customer provisioning, monitoring, and backup automation"
      },
      "filePath": "plugins/devops/fairdb-operations-kit/skills/fairdb-backup-manager/SKILL.md"
    },
    {
      "slug": "finding-arbitrage-opportunities",
      "name": "finding-arbitrage-opportunities",
      "description": "Identify profitable arbitrage opportunities across exchanges and DEXs in real-time. Use when discovering profitable arbitrage across exchanges. Trigger with phrases like \"find arbitrage\", \"scan for arb opportunities\", or \"check arbitrage\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:arbitrage-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:arbitrage-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "arbitrage-opportunity-finder",
        "category": "crypto",
        "path": "plugins/crypto/arbitrage-opportunity-finder",
        "version": "1.0.0",
        "description": "Find and analyze arbitrage opportunities across exchanges and DeFi protocols"
      },
      "filePath": "plugins/crypto/arbitrage-opportunity-finder/skills/arbitrage-opportunity-finder/SKILL.md"
    },
    {
      "slug": "finding-security-misconfigurations",
      "name": "finding-security-misconfigurations",
      "description": "Identify security misconfigurations in infrastructure-as-code, application settings, and system configurations. Use when you need to audit Terraform/CloudFormation templates, check application config files, validate system security settings, or ensure compliance with security best practices. Trigger with phrases like \"find security misconfigurations\", \"audit infrastructure security\", \"check config security\", or \"scan for misconfigured settings\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Configuration files accessible in {baseDir}/ (Terraform, CloudFormation, YAML, JSON)\n- Infrastructure-as-code files (.tf, .yaml, .json, .template)\n- Application configuration files (application.yml, config.json, .env.example)\n- System configuration exports available\n- Write permissions for findings report in {baseDir}/security-findings/\n\n## Instructions\n\n### 1. Configuration Discovery Phase\n\nLocate configuration files to analyze:\n- Infrastructure-as-code: Terraform (.tf), CloudFormation (.yaml/.json), Ansible, Kubernetes\n- Application configs: application.yml, config.json, web.config, .properties\n- Cloud provider configs: AWS, GCP, Azure resource definitions\n- Container configs: Dockerfile, docker-compose.yml, Kubernetes manifests\n- Web server configs: nginx.conf, httpd.conf, .htaccess\n\n### 2. IaC Misconfiguration Checks\n\n**Cloud Storage**:\n- S3 buckets with public read/write access\n- Storage accounts without encryption\n- Publicly accessible blob containers\n- Missing versioning on data stores\n\n**Network Security**:\n- Security groups allowing 0.0.0.0/0 on sensitive ports (22, 3389, 3306, 5432)\n- Network ACLs with overly permissive rules\n- VPCs without flow logs enabled\n- Missing network segmentation\n\n**Identity and Access**:\n- IAM policies with wildcard (*) permissions\n- Service accounts with admin privileges\n- Missing MFA enforcement\n- Overly broad role assignments\n- Hardcoded credentials in code\n\n**Compute Resources**:\n- EC2 instances with public IPs unnecessarily\n- Unencrypted EBS volumes\n- Missing instance metadata service v2\n- Outdated AMIs/images\n\n**Database Security**:\n- RDS instances publicly accessible\n- Databases without encryption at rest\n- Missing automated backups\n- Weak password policies\n- Default ports exposed\n\n### 3. Application Configuration Checks\n\n**Authentication/Authorization**:\n- Debug mode enabled in production\n- Default credentials present\n- Weak session timeout values\n- Missing CSRF protection\n- Insecure password policies\n\n**API Security**:\n- API keys in configuration files\n- CORS configured with wildcard (*)\n- Missing rate limiting\n- Unencrypted API endpoints\n- Disabled authentication\n\n**Data Protection**:\n- Sensitive data in plain text\n- Missing encryption configuration\n- Insecure cookie settings (no HttpOnly, Secure flags)\n- Logging sensitive information\n\n**Dependencies**:\n- Outdated library versions with CVEs\n- Unmaintained packages\n- Unnecessary dependencies\n\n### 4. System Configuration Checks\n\n**Operating System**:\n- Unnecessary services enabled\n- Weak SSH configurations\n- Missing security updates\n- Insecure file permissions\n- Disabled firewalls\n\n**Web Servers**:\n- Directory listing enabled\n- Server tokens exposed\n- Missing security headers\n- Weak TLS configurations\n- Default error pages revealing information\n\n### 5. Severity Classification\n\nRate findings by severity:\n- **Critical**: Immediate exploitation risk (public S3, hardcoded secrets)\n- **High**: Significant security impact (weak auth, missing encryption)\n- **Medium**: Configuration weaknesses (overly permissive, missing logs)\n- **Low**: Best practice violations (information disclosure, outdated configs)\n\n### 6. Generate Findings Report\n\nDocument all misconfigurations with:\n- Severity and category\n- Specific configuration issue\n- Security impact explanation\n- Remediation steps with code examples\n- Compliance implications (CIS, NIST, PCI-DSS)\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Security misconfigurations report saved to {baseDir}/security-findings/misconfig-YYYYMMDD.md\n\n**Report Structure**:\n```\n# Security Misconfiguration Findings\nScan Date: 2024-01-15\nFiles Analyzed: 42\nFindings: 15 (3 Critical, 5 High, 4 Medium, 3 Low)\n\n## Critical Findings\n\n### 1. Publicly Accessible S3 Bucket\n**File**: {baseDir}/terraform/storage.tf\n**Line**: 23\n**Issue**: S3 bucket allows public read access\n**Code**:\n```hcl\nresource \"aws_s3_bucket\" \"data\" {\n  acl = \"public-read\"  # CRITICAL: Public access\n}\n```\n**Impact**: Sensitive data exposed to internet\n**Remediation**:\n```hcl\nresource \"aws_s3_bucket\" \"data\" {\n  acl = \"private\"\n\n  public_access_block {\n    block_public_acls       = true\n    block_public_policy     = true\n    ignore_public_acls      = true\n    restrict_public_buckets = true\n  }\n}\n```\n**Compliance**: Violates CIS AWS 2.1.5, NIST AC-3\n\n## High Findings\n\n### 2. Security Group Allows SSH from Anywhere\n**File**: {baseDir}/terraform/network.tf\n**Line**: 45\n**Issue**: Port 22 open to 0.0.0.0/0\n**Impact**: Brute force attack surface\n**Remediation**: Restrict to specific IP ranges or use bastion host\n\n[Additional findings follow similar structure]\n\n## Summary by Category\n- IAM/Access Control: 4 findings\n- Network Security: 6 findings\n- Data Protection: 3 findings\n- Logging/Monitoring: 2 findings\n\n## Compliance Impact\n- CIS Benchmarks: 8 violations\n- NIST 800-53: 5 controls affected\n- PCI-DSS: 3 requirements unmet\n```\n\n**Secondary Outputs**:\n- JSON for CI/CD integration: {baseDir}/security-findings/misconfig-YYYYMMDD.json\n- CSV for spreadsheet tracking\n- SARIF format for GitHub Security tab\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Unable to Parse Configuration File**\n   - Error: \"Syntax error in {baseDir}/terraform/main.tf\"\n   - Resolution: Validate file syntax first, report parse errors separately\n   - Fallback: Skip malformed files, note in report\n\n2. **Missing Cloud Provider Context**\n   - Error: \"Cannot determine cloud provider from configuration\"\n   - Resolution: Look for provider blocks, file naming conventions\n   - Fallback: Apply generic security checks only\n\n3. **Encrypted or Binary Configuration Files**\n   - Error: \"Cannot read encrypted configuration\"\n   - Resolution: Request decrypted version or configuration export\n   - Note: Document inability to audit in report\n\n4. **Large Configuration Sets**\n   - Error: \"Too many files to analyze ({baseDir}/ has 500+ configs)\"\n   - Resolution: Prioritize by file type and location\n   - Strategy: Start with IaC, then app configs, then system configs\n\n5. **False Positives**\n   - Error: \"Flagged configuration is intentional (dev environment)\"\n   - Resolution: Allow environment-specific exceptions\n   - Enhancement: Support ignore/exception rules file\n\n## Resources\n\n**Security Benchmarks**:\n- CIS Benchmarks: https://www.cisecurity.org/cis-benchmarks/\n- OWASP Configuration Guide: https://cheatsheetseries.owasp.org/cheatsheets/Infrastructure_as_Code_Security_Cheatsheet.html\n- Cloud Security Alliance: https://cloudsecurityalliance.org/\n\n**IaC Security Tools**:\n- tfsec (Terraform): https://github.com/aquasecurity/tfsec\n- Checkov (Multi-cloud): https://www.checkov.io/\n- cfn-nag (CloudFormation): https://github.com/stelligent/cfn_nag\n- kube-bench (Kubernetes): https://github.com/aquasecurity/kube-bench\n\n**Configuration Best Practices**:\n- AWS Security Best Practices: https://aws.amazon.com/architecture/security-identity-compliance/\n- Azure Security Baseline: https://docs.microsoft.com/en-us/security/benchmark/azure/\n- GCP Security Best Practices: https://cloud.google.com/security/best-practices\n\n**Compliance Frameworks**:\n- CIS Controls: https://www.cisecurity.org/controls/\n- NIST 800-53: https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final\n- PCI-DSS Requirements: https://www.pcisecuritystandards.org/\n\n**Remediation Examples**:\n- Terraform security modules: {baseDir}/templates/terraform-secure/\n- CloudFormation secure templates: {baseDir}/templates/cfn-secure/\n- Kubernetes security policies: {baseDir}/templates/k8s-policies/",
      "parentPlugin": {
        "name": "security-misconfiguration-finder",
        "category": "security",
        "path": "plugins/security/security-misconfiguration-finder",
        "version": "1.0.0",
        "description": "Find security misconfigurations"
      },
      "filePath": "plugins/security/security-misconfiguration-finder/skills/security-misconfiguration-finder/SKILL.md"
    },
    {
      "slug": "firestore-operations-manager",
      "name": "firestore-operations-manager",
      "description": "Manages Firebase/Firestore operations including CRUD, queries, batch",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Firestore Operations Manager\n\n## Overview\n\nThis skill manages Firebase/Firestore operations for both regular web/mobile applications and AI agent-to-agent (A2A) frameworks. It handles:\n\n- **Basic Operations**: CRUD, queries, batch processing for standard applications\n- **A2A Framework**: Agent-to-agent communication patterns using Firestore as state store\n- **MCP Integration**: Model Context Protocol server communication via Firestore\n- **Cloud Run Services**: Integration patterns for Cloud Run services accessing Firestore\n- **Security**: Proper authentication, validation, and security rules for both humans and agents\n\n## Core Capabilities\n\n### For Everyone (Basic Firestore)\n- Create, read, update, delete documents\n- Complex queries with filters and ordering\n- Batch operations for efficiency\n- Collection management and organization\n- Security rules generation and validation\n- Data migrations and transformations\n\n### For AI Power Users (A2A/MCP)\n- Agent session management with Firestore state\n- Agent-to-agent messaging and task coordination\n- MCP server communication patterns\n- Agent memory and context storage\n- Cloud Run service integration\n- Multi-agent workflow orchestration\n\n## When to Use This Skill\n\nThis skill activates when users mention:\n\n- **Basic operations**: \"create a firestore document\", \"query users collection\", \"batch update documents\"\n- **A2A patterns**: \"setup agent communication\", \"A2A task queue\", \"agent-to-agent messaging\"\n- **MCP integration**: \"MCP server firestore\", \"agent memory storage\", \"session management\"\n- **Cloud Run**: \"Cloud Run firestore integration\", \"service account access\"\n- **Security**: \"firestore security rules\", \"agent authentication\", \"service account permissions\"\n\n## Workflow\n\n### Phase 1: Setup and Initialization\n\n**For basic users:**\n1. Check if Firebase Admin SDK is installed\n2. Guide through credential setup (service account JSON)\n3. Initialize Firestore connection\n4. Run connection test\n5. Create basic usage examples\n\n**For A2A/MCP users:**\n1. Perform basic setup (above)\n2. Install additional dependencies (@google-cloud/firestore)\n3. Create A2A collection structure (sessions, memory, tasks, messages, logs)\n4. Configure service account whitelisting\n5. Setup security rules for agent access\n6. Create MCP service wrapper classes\n\nExample setup:\n```bash\n# Basic setup\nnpm install firebase-admin\n\n# A2A/MCP setup\nnpm install firebase-admin @google-cloud/firestore dotenv\n\n# Set credentials\nexport GOOGLE_APPLICATION_CREDENTIALS=\"./serviceAccountKey.json\"\n\n# Run setup command\n/firestore-setup\n```\n\n### Phase 2: Basic CRUD Operations\n\nFor standard database operations:\n\n**Create documents:**\n```javascript\nconst { db, admin } = require('./src/firebase');\n\n// Single document\nawait db.collection('users').add({\n  name: 'John Doe',\n  email: '[email protected]',\n  createdAt: admin.firestore.FieldValue.serverTimestamp()\n});\n\n// With custom ID\nawait db.collection('users').doc('user123').set({\n  name: 'Jane Doe',\n  email: '[email protected]'\n});\n```\n\n**Read documents:**\n```javascript\n// Single document\nconst doc = await db.collection('users').doc('user123').get();\nconst userData = doc.data();\n\n// Query\nconst snapshot = await db.collection('users')\n  .where('status', '==', 'active')\n  .orderBy('createdAt', 'desc')\n  .limit(10)\n  .get();\n\nsnapshot.forEach(doc => console.log(doc.data()));\n```\n\n**Update documents:**\n```javascript\n// Partial update\nawait db.collection('users').doc('user123').update({\n  status: 'active',\n  updatedAt: admin.firestore.FieldValue.serverTimestamp()\n});\n\n// Increment counter\nawait db.collection('stats').doc('views').update({\n  count: admin.firestore.FieldValue.increment(1)\n});\n```\n\n**Delete documents:**\n```javascript\n// Single delete\nawait db.collection('users').doc('user123').delete();\n\n// Batch delete\nconst batch = db.batch();\nconst docs = await db.collection('temp').limit(500).get();\ndocs.forEach(doc => batch.delete(doc.ref));\nawait batch.commit();\n```\n\n### Phase 3: A2A Framework Operations\n\nFor agent-to-agent communication patterns:\n\n**1. Create Agent Session:**\n```javascript\nconst { MCPService } = require('./src/mcp-service');\nconst mcp = new MCPService('mcp-server@project.iam.gserviceaccount.com');\n\n// Create session for agent workflow\nconst sessionId = await mcp.createSession({\n  task: 'process_user_data',\n  priority: 'high',\n  metadata: { userId: 'user123' }\n});\n\nconsole.log(`Session created: ${sessionId}`);\n```\n\n**2. Store Agent Context:**\n```javascript\n// Store agent memory/context in Firestore\nawait mcp.storeContext(sessionId, {\n  conversation: [...messages],\n  userPreferences: { theme: 'dark' },\n  currentStep: 'data_validation'\n});\n\n// Retrieve context later\nconst context = await db\n  .collection('agent_memory')\n  .doc('mcp-server@project.iam.gserviceaccount.com')\n  .collection('contexts')\n  .doc(sessionId)\n  .get();\n```\n\n**3. Agent-to-Agent Messaging:**\n```javascript\n// Send message from one agent to another\nawait mcp.sendMessage(\n  'agent-engine@project.iam.gserviceaccount.com',\n  {\n    action: 'analyze_data',\n    data: { userId: 'user123', fields: ['name', 'email'] }\n  }\n);\n\n// Receive messages (in receiving agent)\nconst messages = await mcp.receiveMessages();\nmessages.forEach(msg => {\n  console.log(`From: ${msg.from}, Payload:`, msg.payload);\n});\n```\n\n**4. Task Queue Management:**\n```javascript\n// Create task for another agent\nawait db.collection('a2a_tasks').add({\n  taskType: 'data_processing',\n  assignedTo: 'worker-agent@project.iam.gserviceaccount.com',\n  status: 'pending',\n  priority: 1,\n  payload: { userId: 'user123' },\n  createdAt: admin.firestore.FieldValue.serverTimestamp()\n});\n\n// Agent claims and processes task\nconst taskQuery = await db.collection('a2a_tasks')\n  .where('assignedTo', '==', 'worker-agent@project.iam.gserviceaccount.com')\n  .where('status', '==', 'pending')\n  .orderBy('priority', 'asc')\n  .limit(1)\n  .get();\n\nif (!taskQuery.empty) {\n  const task = taskQuery.docs[0];\n  await task.ref.update({ status: 'in_progress' });\n  // Process task...\n  await task.ref.update({ status: 'completed' });\n}\n```\n\n**5. Agent Activity Logging:**\n```javascript\n// Log agent activities for audit trail\nawait mcp.logActivity({\n  action: 'processed_data',\n  userId: 'user123',\n  duration: 1500, // ms\n  result: 'success'\n}, 'info');\n```\n\n### Phase 4: Cloud Run Integration\n\nFor Cloud Run services accessing Firestore:\n\n**Setup Cloud Run service class:**\n```javascript\nconst { CloudRunService } = require('./src/cloudrun-service');\nconst cloudrun = new CloudRunService();\n\n// In your Cloud Run endpoint\napp.post('/api/users/:userId/data', async (req, res) => {\n  const { userId } = req.params;\n\n  try {\n    // Log request\n    await cloudrun.logRequest('/api/users/data', 'POST', userId);\n\n    // Get user data from Firestore\n    const userData = await cloudrun.getUserData(userId);\n\n    // Store response\n    await cloudrun.storeResponse(req.id, {\n      userId,\n      data: userData,\n      status: 'success'\n    });\n\n    res.json({ success: true, data: userData });\n  } catch (error) {\n    await cloudrun.storeResponse(req.id, {\n      userId,\n      error: error.message,\n      status: 'error'\n    });\n\n    res.status(500).json({ error: error.message });\n  }\n});\n```\n\n### Phase 5: Security Rules Management\n\nGenerate and deploy security rules for both users and agents:\n\n**For basic users:**\n```javascript\nrules_version = '2';\nservice cloud.firestore {\n  match /databases/{database}/documents {\n    match /users/{userId} {\n      allow read, write: if request.auth != null && request.auth.uid == userId;\n    }\n  }\n}\n```\n\n**For A2A/MCP (service accounts):**\n```javascript\nrules_version = '2';\nservice cloud.firestore {\n  match /databases/{database}/documents {\n    function isServiceAccount() {\n      return request.auth.token.email.matches('.*@.*\\\\.iam\\\\.gserviceaccount\\\\.com$');\n    }\n\n    function isAuthorizedAgent() {\n      return isServiceAccount() && request.auth.token.email in [\n        'mcp-server@project-id.iam.gserviceaccount.com',\n        'agent-engine@project-id.iam.gserviceaccount.com'\n      ];\n    }\n\n    // Agent sessions\n    match /agent_sessions/{sessionId} {\n      allow read, write: if isAuthorizedAgent();\n    }\n\n    // Agent memory\n    match /agent_memory/{agentId}/{document=**} {\n      allow read, write: if isAuthorizedAgent();\n    }\n\n    // A2A messages\n    match /a2a_messages/{messageId} {\n      allow create: if isAuthorizedAgent();\n      allow read: if isAuthorizedAgent() &&\n                     (resource.data.from == request.auth.token.email ||\n                      resource.data.to == request.auth.token.email);\n    }\n  }\n}\n```\n\nDeploy rules:\n```bash\nfirebase deploy --only firestore:rules\n```\n\n## Advanced Patterns\n\n### Pattern 1: Multi-Agent Workflow Orchestration\n\n```javascript\n// Coordinator agent creates workflow\nconst workflowId = await db.collection('workflows').add({\n  name: 'user_data_processing',\n  steps: [\n    { agent: 'validator@project.iam.gserviceaccount.com', status: 'pending' },\n    { agent: 'processor@project.iam.gserviceaccount.com', status: 'pending' },\n    { agent: 'notifier@project.iam.gserviceaccount.com', status: 'pending' }\n  ],\n  createdAt: admin.firestore.FieldValue.serverTimestamp()\n});\n\n// Each agent listens for their step\nconst unsubscribe = db.collection('workflows')\n  .doc(workflowId)\n  .onSnapshot(async (doc) => {\n    const workflow = doc.data();\n    const myStep = workflow.steps.find(s => s.agent === myEmail && s.status === 'pending');\n\n    if (myStep) {\n      // Process step\n      await processStep(myStep);\n\n      // Mark complete and notify next agent\n      myStep.status = 'completed';\n      await doc.ref.update({ steps: workflow.steps });\n    }\n  });\n```\n\n### Pattern 2: Agent Context Sharing\n\n```javascript\n// Agent 1 stores context\nawait db.collection('shared_context').doc('task_abc').set({\n  sharedBy: 'agent1@project.iam.gserviceaccount.com',\n  sharedWith: ['agent2@project.iam.gserviceaccount.com'],\n  context: {\n    userId: 'user123',\n    analysis: { sentiment: 'positive', score: 0.85 }\n  },\n  expiresAt: new Date(Date.now() + 3600000) // 1 hour\n});\n\n// Agent 2 retrieves context\nconst contextDoc = await db.collection('shared_context').doc('task_abc').get();\nif (contextDoc.exists && contextDoc.data().sharedWith.includes(myEmail)) {\n  const context = contextDoc.data().context;\n  // Use context...\n}\n```\n\n### Pattern 3: Rate Limiting for Agents\n\n```javascript\n// Check and enforce rate limits\nconst rateLimitRef = db.collection('rate_limits').doc(agentEmail);\nconst rateLimitDoc = await rateLimitRef.get();\n\nif (rateLimitDoc.exists) {\n  const { count, resetAt } = rateLimitDoc.data();\n\n  if (Date.now() < resetAt && count >= 100) {\n    throw new Error('Rate limit exceeded');\n  }\n\n  if (Date.now() >= resetAt) {\n    // Reset counter\n    await rateLimitRef.set({\n      count: 1,\n      resetAt: Date.now() + 60000 // 1 minute\n    });\n  } else {\n    // Increment counter\n    await rateLimitRef.update({\n      count: admin.firestore.FieldValue.increment(1)\n    });\n  }\n} else {\n  // First request\n  await rateLimitRef.set({\n    count: 1,\n    resetAt: Date.now() + 60000\n  });\n}\n```\n\n## Performance Optimization\n\n### For Basic Users:\n1. **Use batch operations** - Write/update 500 docs at once\n2. **Create indexes** - Required for complex queries\n3. **Paginate results** - Use cursor-based pagination\n4. **Cache frequently read data** - Reduce read costs\n\n### For A2A/MCP Users:\n1. **Connection pooling** - Reuse Firestore connections\n2. **Batch agent messages** - Combine multiple messages\n3. **TTL for agent data** - Clean up expired sessions automatically\n4. **Denormalize agent state** - Avoid cross-collection queries\n\n## Cost Optimization\n\nFirestore costs:\n- Document reads: $0.06 per 100k\n- Document writes: $0.18 per 100k\n- Document deletes: $0.02 per 100k\n\nReduce costs:\n- Use batch writes (1 operation vs 500)\n- Cache agent context locally\n- Archive old agent logs to Cloud Storage\n- Set up billing alerts\n\n## Security Best Practices\n\n1. **Never allow open access** - Always require authentication\n2. **Whitelist service accounts** - Don't allow all service accounts\n3. **Validate all inputs** - Check types, formats, required fields\n4. **Make logs immutable** - Prevent tampering with audit trails\n5. **Rotate credentials** - Change service account keys every 90 days\n6. **Monitor usage** - Set up Firebase console alerts\n7. **Test rules** - Use Firebase Emulator before deploying\n\n## Troubleshooting\n\n### Issue: Permission Denied\n- Check security rules allow the operation\n- Verify service account is whitelisted\n- Ensure GOOGLE_APPLICATION_CREDENTIALS is set\n\n### Issue: A2A Messages Not Delivered\n- Verify recipient agent email is correct\n- Check message status in Firestore console\n- Ensure security rules allow cross-agent messaging\n\n### Issue: Rate Limit Errors\n- Implement exponential backoff\n- Use batch operations\n- Increase rate limits in configuration\n\n### Issue: Cloud Run Connection Fails\n- Verify service account has Firestore permissions\n- Check VPC connectivity if using private IP\n- Ensure project ID matches in code and credentials\n\n## Examples\n\nSee `examples/firestore-usage.js` for complete code examples covering:\n- Basic CRUD operations\n- Complex queries and pagination\n- Batch operations\n- A2A agent communication\n- MCP server integration\n- Cloud Run service patterns\n- Security rules testing\n\n## Resources\n\n- [Firestore Documentation](https://firebase.google.com/docs/firestore)\n- [A2A Protocol Specification](https://github.com/google/vertex-ai-agents)\n- [MCP Documentation](https://github.com/anthropics/mcp)\n- [Cloud Run Integration](https://cloud.google.com/run/docs/tutorials)\n\n## Summary\n\nThis skill provides comprehensive Firestore operations for:\n- **Everyone**: Standard database CRUD, queries, batch ops, security\n- **AI Power Users**: A2A communication, MCP integration, Cloud Run services, multi-agent workflows\n\nUse `/firestore-setup` to initialize, then leverage the agents and commands for specific operations!",
      "parentPlugin": {
        "name": "jeremy-firestore",
        "category": "community",
        "path": "plugins/community/jeremy-firestore",
        "version": "1.0.0",
        "description": "Firestore database specialist for schema design, queries, and real-time sync"
      },
      "filePath": "plugins/community/jeremy-firestore/skills/firestore-manager/SKILL.md"
    },
    {
      "slug": "forecasting-time-series-data",
      "name": "forecasting-time-series-data",
      "description": "This skill enables claude to forecast future values based on historical",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to perform time series forecasting, providing insights into future trends and patterns. It automates the process of data analysis, model selection, and prediction generation, delivering valuable information for decision-making.\n\n## How It Works\n\n1. **Data Analysis**: Claude analyzes the provided time series data, identifying key characteristics such as trends, seasonality, and autocorrelation.\n2. **Model Selection**: Based on the data characteristics, Claude selects an appropriate forecasting model (e.g., ARIMA, Prophet).\n3. **Prediction Generation**: The selected model is trained on the historical data, and future values are predicted along with confidence intervals.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Forecast future sales based on past sales data.\n- Predict website traffic for the next month.\n- Analyze trends in stock prices over the past year.\n\n## Examples\n\n### Example 1: Forecasting Sales\n\nUser request: \"Forecast sales for the next quarter based on the past 3 years of monthly sales data.\"\n\nThe skill will:\n1. Analyze the historical sales data to identify trends and seasonality.\n2. Select and train a suitable forecasting model (e.g., ARIMA or Prophet).\n3. Generate a forecast of sales for the next quarter, including confidence intervals.\n\n### Example 2: Predicting Website Traffic\n\nUser request: \"Predict weekly website traffic for the next month based on the last 6 months of data.\"\n\nThe skill will:\n1. Analyze the website traffic data to identify patterns and seasonality.\n2. Choose an appropriate time series forecasting model.\n3. Generate a forecast of weekly website traffic for the next month.\n\n## Best Practices\n\n- **Data Quality**: Ensure the time series data is clean, complete, and accurate for optimal forecasting results.\n- **Model Selection**: Choose a forecasting model appropriate for the characteristics of the data (e.g., ARIMA for stationary data, Prophet for data with strong seasonality).\n- **Evaluation**: Evaluate the performance of the forecasting model using appropriate metrics (e.g., Mean Absolute Error, Root Mean Squared Error).\n\n## Integration\n\nThis skill can be integrated with other data analysis and visualization tools within the Claude Code ecosystem to provide a comprehensive solution for time series analysis and forecasting.",
      "parentPlugin": {
        "name": "time-series-forecaster",
        "category": "ai-ml",
        "path": "plugins/ai-ml/time-series-forecaster",
        "version": "1.0.0",
        "description": "Time series forecasting and analysis"
      },
      "filePath": "plugins/ai-ml/time-series-forecaster/skills/time-series-forecaster/SKILL.md"
    },
    {
      "slug": "fuzzing-apis",
      "name": "fuzzing-apis",
      "description": "Perform API fuzzing to discover edge cases, crashes, and security vulnerabilities. Use when performing specialized testing. Trigger with phrases like \"fuzz the API\", \"run fuzzing tests\", or \"discover edge cases\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:fuzz-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:fuzz-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "api-fuzzer",
        "category": "testing",
        "path": "plugins/testing/api-fuzzer",
        "version": "1.0.0",
        "description": "Fuzz testing for APIs with malformed inputs, edge cases, and security vulnerability detection"
      },
      "filePath": "plugins/testing/api-fuzzer/skills/api-fuzzer/SKILL.md"
    },
    {
      "slug": "gcp-examples-expert",
      "name": "gcp-examples-expert",
      "description": "Provide production-ready Google Cloud code examples from official repositories",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## What This Skill Does\n\nExpert aggregator of production-ready code examples from official Google Cloud repositories. Provides battle-tested starter kits, templates, and best practices for building AI agents, workflows, and applications on Google Cloud Platform.\n\n## When This Skill Activates\n\n### Trigger Phrases\n- \"Show me ADK sample code\"\n- \"Genkit starter template\"\n- \"Vertex AI code example\"\n- \"Agent Starter Pack template\"\n- \"Gemini function calling example\"\n- \"Multi-agent orchestration pattern\"\n- \"Google Cloud starter kit\"\n- \"Production agent template\"\n- \"How to implement RAG with Genkit\"\n- \"A2A protocol code example\"\n\n### Use Cases\n- Quick access to official Google Cloud code examples\n- Production-ready agent templates\n- Genkit flow patterns (RAG, multi-step workflows, tool calling)\n- Vertex AI training and deployment code\n- Gemini API integration examples\n- Multi-agent system orchestration\n- Infrastructure as Code (Terraform) templates\n\n## Code Example Categories\n\n### 1. ADK (Agent Development Kit) Samples\n\n**Source**: google/adk-samples\n\n**Examples Provided**:\n- Basic agent creation with Code Execution Sandbox\n- Memory Bank configuration for stateful agents\n- A2A protocol implementation for inter-agent communication\n- Multi-tool agent configuration\n- VPC Service Controls integration\n- IAM least privilege patterns\n\n**Sample Pattern**:\n```python\nfrom google.cloud.aiplatform import agent_builder\n\ndef create_adk_agent(project_id: str, location: str):\n    agent_config = {\n        \"display_name\": \"production-agent\",\n        \"model\": \"gemini-2.5-flash\",\n        \"code_execution_config\": {\n            \"enabled\": True,\n            \"state_ttl_days\": 14\n        },\n        \"memory_bank_config\": {\n            \"enabled\": True\n        }\n    }\n    # Implementation from google/adk-samples\n```\n\n### 2. Agent Starter Pack\n\n**Source**: GoogleCloudPlatform/agent-starter-pack\n\n**Examples Provided**:\n- Production agent with monitoring and observability\n- Auto-scaling configuration\n- Security best practices (Model Armor, VPC-SC)\n- Cloud Monitoring dashboards\n- Alerting policies\n- Error tracking setup\n\n**Sample Pattern**:\n```python\ndef production_agent_with_observability(project_id: str):\n    agent = aiplatform.Agent.create(\n        config={\n            \"auto_scaling\": {\n                \"min_instances\": 2,\n                \"max_instances\": 10\n            },\n            \"vpc_service_controls\": {\"enabled\": True},\n            \"model_armor\": {\"enabled\": True}\n        }\n    )\n    # Full implementation from agent-starter-pack\n```\n\n### 3. Firebase Genkit\n\n**Source**: firebase/genkit\n\n**Examples Provided**:\n- RAG flows with vector search\n- Multi-step workflows\n- Tool calling integration\n- Prompt templates\n- Evaluation frameworks\n- Deployment patterns (Cloud Run, Functions)\n\n**Sample Pattern**:\n```typescript\nimport { genkit, z } from 'genkit';\nimport { googleAI, gemini15ProLatest } from '@genkit-ai/googleai';\n\nconst ragFlow = ai.defineFlow({\n  name: 'ragSearchFlow',\n  inputSchema: z.object({ query: z.string() }),\n  outputSchema: z.object({ answer: z.string() })\n}, async (input) => {\n  // Implementation from firebase/genkit examples\n});\n```\n\n### 4. Vertex AI Samples\n\n**Source**: GoogleCloudPlatform/vertex-ai-samples\n\n**Examples Provided**:\n- Custom model training with Gemini\n- Batch prediction jobs\n- Hyperparameter tuning\n- Model evaluation\n- Endpoint deployment with auto-scaling\n- A/B testing patterns\n\n**Sample Pattern**:\n```python\ndef fine_tune_gemini_model(project_id: str, training_data_uri: str):\n    job = aiplatform.CustomTrainingJob(\n        training_config={\n            \"base_model\": \"gemini-2.5-flash\",\n            \"learning_rate\": 0.001,\n            \"adapter_size\": 8  # LoRA\n        }\n    )\n    # Full implementation from vertex-ai-samples\n```\n\n### 5. Generative AI Examples\n\n**Source**: GoogleCloudPlatform/generative-ai\n\n**Examples Provided**:\n- Gemini multimodal analysis (text, images, video)\n- Function calling with live APIs\n- Structured output generation\n- Grounding with Google Search\n- Safety filters and content moderation\n- Token counting and cost optimization\n\n**Sample Pattern**:\n```python\nfrom vertexai.generative_models import GenerativeModel, Part\n\ndef analyze_multimodal_content(video_uri: str, question: str):\n    model = GenerativeModel(\"gemini-2.5-pro\")\n    video_part = Part.from_uri(video_uri, mime_type=\"video/mp4\")\n    response = model.generate_content([video_part, question])\n    # Implementation from generative-ai examples\n```\n\n### 6. AgentSmithy\n\n**Source**: GoogleCloudPlatform/agentsmithy\n\n**Examples Provided**:\n- Multi-agent orchestration\n- Supervisory agent patterns\n- Agent-to-agent communication\n- Workflow coordination (sequential, parallel, conditional)\n- Task delegation strategies\n- Error handling and retry logic\n\n**Sample Pattern**:\n```python\nfrom agentsmithy import Agent, Orchestrator, Task\n\ndef create_multi_agent_system(project_id: str):\n    orchestrator = Orchestrator(\n        agents=[research_agent, analysis_agent, writer_agent],\n        strategy=\"sequential\"\n    )\n    # Full implementation from agentsmithy\n```\n\n## Workflow\n\n### Phase 1: Identify Use Case\n```\n1. Listen for trigger phrases in user request\n2. Determine which repository has relevant examples\n3. Identify specific code pattern needed\n4. Select appropriate framework (ADK, Genkit, Vertex AI)\n```\n\n### Phase 2: Provide Code Example\n```\n1. Fetch relevant code snippet from knowledge base\n2. Adapt to user's specific requirements\n3. Include imports and dependencies\n4. Add configuration details\n5. Cite source repository\n```\n\n### Phase 3: Explain Best Practices\n```\n1. Highlight security considerations (IAM, VPC-SC, Model Armor)\n2. Show monitoring and observability setup\n3. Demonstrate error handling patterns\n4. Include infrastructure deployment code\n5. Provide cost optimization tips\n```\n\n### Phase 4: Deployment Guidance\n```\n1. Provide Terraform/IaC templates\n2. Show Cloud Build CI/CD configuration\n3. Include testing strategies\n4. Document environment variables\n5. Link to official documentation\n```\n\n## Tool Permissions\n\nThis skill uses the following tools:\n- **Read**: Access code examples and documentation\n- **Write**: Create starter template files\n- **Edit**: Modify templates for user's project\n- **Grep**: Search for specific patterns in examples\n- **Glob**: Find related code files\n- **Bash**: Run setup commands and validation\n\n## Example Interactions\n\n### Example 1: ADK Agent Creation\n**User**: \"Show me how to create an ADK agent with Code Execution\"\n\n**Skill Activates**:\n- Provides code example from google/adk-samples\n- Includes Code Execution Sandbox configuration\n- Shows 14-day state persistence setup\n- Demonstrates security best practices\n- Links to official ADK documentation\n\n### Example 2: Genkit RAG Flow\n**User**: \"I need a Genkit starter template for RAG\"\n\n**Skill Activates**:\n- Provides RAG flow code from firebase/genkit\n- Shows vector search integration\n- Demonstrates embedding generation\n- Includes context retrieval logic\n- Provides deployment configuration\n\n### Example 3: Production Agent Template\n**User**: \"What's the best way to deploy a production agent?\"\n\n**Skill Activates**:\n- Provides Agent Starter Pack template\n- Shows auto-scaling configuration\n- Includes monitoring dashboard setup\n- Demonstrates alerting policies\n- Provides Terraform deployment code\n\n### Example 4: Gemini Multimodal\n**User**: \"How do I analyze video with Gemini?\"\n\n**Skill Activates**:\n- Provides multimodal code from generative-ai repo\n- Shows video part creation\n- Demonstrates prompt engineering\n- Includes error handling\n- Provides cost optimization tips\n\n### Example 5: Multi-Agent System\n**User**: \"I want to build a multi-agent system\"\n\n**Skill Activates**:\n- Provides AgentSmithy orchestration code\n- Shows supervisory agent pattern\n- Demonstrates A2A protocol usage\n- Includes workflow coordination\n- Provides testing strategies\n\n## Best Practices Applied\n\n### Security\n‚úÖ IAM least privilege service accounts\n‚úÖ VPC Service Controls for enterprise isolation\n‚úÖ Model Armor for prompt injection protection\n‚úÖ Encrypted data at rest and in transit\n‚úÖ No hardcoded credentials (use Secret Manager)\n\n### Performance\n‚úÖ Auto-scaling configuration (min/max instances)\n‚úÖ Appropriate machine types and accelerators\n‚úÖ Caching strategies for repeated queries\n‚úÖ Batch processing for high throughput\n‚úÖ Token optimization for cost efficiency\n\n### Observability\n‚úÖ Cloud Monitoring dashboards\n‚úÖ Alerting policies for errors and latency\n‚úÖ Structured logging with severity levels\n‚úÖ Distributed tracing with Cloud Trace\n‚úÖ Error tracking with Cloud Error Reporting\n\n### Reliability\n‚úÖ Multi-region deployment for high availability\n‚úÖ Circuit breaker patterns for fault tolerance\n‚úÖ Retry logic with exponential backoff\n‚úÖ Health check endpoints\n‚úÖ Graceful degradation strategies\n\n### Cost Optimization\n‚úÖ Use Gemini 2.5 Flash for simple tasks (cheaper)\n‚úÖ Gemini 2.5 Pro for complex reasoning (higher quality)\n‚úÖ Batch predictions for bulk processing\n‚úÖ Preemptible instances for non-critical workloads\n‚úÖ Token counting to estimate costs\n\n## Integration with Other Plugins\n\n### Works with jeremy-genkit-pro\n- Provides Genkit code examples\n- Complements Genkit flow architect agent\n- Shares Genkit production best practices\n\n### Works with jeremy-adk-orchestrator\n- Provides ADK sample code\n- Shows A2A protocol implementation\n- Demonstrates multi-agent patterns\n\n### Works with jeremy-vertex-validator\n- Provides production-ready code that passes validation\n- Follows security and performance best practices\n- Includes monitoring from the start\n\n### Works with jeremy-*-terraform plugins\n- Provides infrastructure code examples\n- Shows Terraform module patterns\n- Demonstrates resource configuration\n\n## Version History\n\n- **1.0.0** (2025): Initial release with 6 official Google Cloud repository integrations\n\n## References\n\n- **google/adk-samples**: https://github.com/google/adk-samples\n- **GoogleCloudPlatform/agent-starter-pack**: https://github.com/GoogleCloudPlatform/agent-starter-pack\n- **firebase/genkit**: https://github.com/firebase/genkit\n- **GoogleCloudPlatform/vertex-ai-samples**: https://github.com/GoogleCloudPlatform/vertex-ai-samples\n- **GoogleCloudPlatform/generative-ai**: https://github.com/GoogleCloudPlatform/generative-ai\n- **GoogleCloudPlatform/agentsmithy**: https://github.com/GoogleCloudPlatform/agentsmithy",
      "parentPlugin": {
        "name": "jeremy-gcp-starter-examples",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-gcp-starter-examples",
        "version": "1.0.0",
        "description": "Google Cloud starter kits and example code aggregator with ADK samples"
      },
      "filePath": "plugins/ai-ml/jeremy-gcp-starter-examples/skills/gcp-examples-expert/SKILL.md"
    },
    {
      "slug": "generating-api-contracts",
      "name": "generating-api-contracts",
      "description": "Generate API contracts and OpenAPI specifications from code or design documents. Use when documenting API contracts and specifications. Trigger with phrases like \"generate API contract\", \"create OpenAPI spec\", or \"document API contract\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:contract-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:contract-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-contract-generator",
        "category": "api-development",
        "path": "plugins/api-development/api-contract-generator",
        "version": "1.0.0",
        "description": "Generate API contracts for consumer-driven contract testing"
      },
      "filePath": "plugins/api-development/api-contract-generator/skills/api-contract-generator/SKILL.md"
    },
    {
      "slug": "generating-api-docs",
      "name": "generating-api-docs",
      "description": "Create comprehensive API documentation with examples, authentication guides, and SDKs. Use when creating comprehensive API documentation. Trigger with phrases like \"generate API docs\", \"create API documentation\", or \"document the API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:docs-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:docs-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-documentation-generator",
        "category": "api-development",
        "path": "plugins/api-development/api-documentation-generator",
        "version": "1.0.0",
        "description": "Generate comprehensive API documentation from OpenAPI/Swagger specs"
      },
      "filePath": "plugins/api-development/api-documentation-generator/skills/api-documentation-generator/SKILL.md"
    },
    {
      "slug": "generating-api-sdks",
      "name": "generating-api-sdks",
      "description": "Generate client SDKs in multiple languages from OpenAPI specifications. Use when generating client libraries for API consumption. Trigger with phrases like \"generate SDK\", \"create client library\", or \"build API SDK\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:sdk-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:sdk-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-sdk-generator",
        "category": "api-development",
        "path": "plugins/api-development/api-sdk-generator",
        "version": "1.0.0",
        "description": "Generate client SDKs from OpenAPI specs for multiple languages"
      },
      "filePath": "plugins/api-development/api-sdk-generator/skills/api-sdk-generator/SKILL.md"
    },
    {
      "slug": "generating-compliance-reports",
      "name": "generating-compliance-reports",
      "description": "Generate comprehensive compliance reports for security standards. Use when creating compliance documentation. Trigger with 'generate compliance report', 'compliance status', or 'audit compliance'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to create detailed compliance reports, saving time and ensuring accuracy in documenting security practices. It automates the process of gathering information and formatting it into a standardized report, making compliance audits easier and more efficient.\n\n## How It Works\n\n1. **Identify Report Type**: Claude analyzes the user's request to determine the required compliance standard (e.g., PCI DSS, HIPAA).\n2. **Gather Data**: The plugin collects relevant data from the system or prompts the user for necessary information.\n3. **Generate Report**: The plugin formats the collected data into a comprehensive compliance report, including necessary sections and documentation.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Generate a report for a specific compliance standard (e.g., \"generate a HIPAA compliance report\").\n- Create a security audit report.\n- Document adherence to a security policy.\n- Prepare for a compliance audit.\n\n## Examples\n\n### Example 1: Generating a PCI DSS Compliance Report\n\nUser request: \"Generate a PCI DSS compliance report for our e-commerce platform.\"\n\nThe skill will:\n1. Activate the compliance-report-generator plugin.\n2. Prompt the user for information about their e-commerce platform's security controls and processes.\n3. Generate a detailed PCI DSS compliance report based on the provided information.\n\n### Example 2: Creating a HIPAA Compliance Report\n\nUser request: \"Create a HIPAA compliance report to demonstrate our adherence to privacy regulations.\"\n\nThe skill will:\n1. Activate the compliance-report-generator plugin.\n2. Guide the user through a series of questions related to HIPAA requirements.\n3. Compile the answers into a structured HIPAA compliance report.\n\n## Best Practices\n\n- **Specificity**: Be specific about the compliance standard you need a report for (e.g., \"SOC 2 report\").\n- **Completeness**: Provide all the necessary information requested by the plugin to ensure a comprehensive and accurate report.\n- **Review**: Always review the generated report to ensure its accuracy and completeness before submitting it for an audit.\n\n## Integration\n\nThis skill can be integrated with other plugins that provide security assessment or vulnerability scanning capabilities. The results from those plugins can be incorporated into the compliance reports generated by this skill, providing a more comprehensive view of the organization's security posture.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "compliance-report-generator",
        "category": "security",
        "path": "plugins/security/compliance-report-generator",
        "version": "1.0.0",
        "description": "Generate compliance reports"
      },
      "filePath": "plugins/security/compliance-report-generator/skills/compliance-report-generator/SKILL.md"
    },
    {
      "slug": "generating-conventional-commits",
      "name": "generating-conventional-commits",
      "description": "Generates conventional commit messages using AI. It analyzes code changes",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill helps you create well-formatted, informative commit messages that follow the conventional commits standard, improving collaboration and automation in your Git workflow. It saves you time and ensures consistency across your project.\n\n## How It Works\n\n1. **Analyze Changes**: The skill analyzes the staged changes in your Git repository.\n2. **Generate Suggestion**: It uses AI to generate a commit message based on the analyzed changes, adhering to the conventional commits format (e.g., `feat: add new feature`, `fix: correct bug`).\n3. **Present to User**: The generated commit message is presented to you for review and acceptance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a commit message after making code changes.\n- Ensure your commit messages follow the conventional commits standard.\n- Save time writing commit messages manually.\n\n## Examples\n\n### Example 1: Adding a New Feature\n\nUser request: \"Generate a commit message for these changes.\"\n\nThe skill will:\n1. Analyze the staged changes related to a new feature.\n2. Generate a commit message like `feat: Implement user authentication`.\n\n### Example 2: Fixing a Bug\n\nUser request: \"Create a commit for the bug fix.\"\n\nThe skill will:\n1. Analyze the staged changes related to a bug fix.\n2. Generate a commit message like `fix: Resolve issue with incorrect password reset`.\n\n## Best Practices\n\n- **Stage Changes**: Ensure all relevant changes are staged before using the skill.\n- **Review Carefully**: Always review the generated commit message before accepting it.\n- **Customize if Needed**: Feel free to customize the generated message to provide more context.\n\n## Integration\n\nThis skill integrates with your Git workflow, providing a convenient way to generate commit messages directly within Claude Code. It complements other Git-related skills in the DevOps Automation Pack, such as `/branch-create` and `/pr-create`.",
      "parentPlugin": {
        "name": "devops-automation-pack",
        "category": "packages",
        "path": "plugins/packages/devops-automation-pack",
        "version": "1.0.0",
        "description": "25 professional DevOps plugins for CI/CD, deployment, Docker, Kubernetes, and infrastructure automation. Save 20+ hours of manual work."
      },
      "filePath": "plugins/packages/devops-automation-pack/skills/devops-automation-pack/SKILL.md"
    },
    {
      "slug": "generating-database-seed-data",
      "name": "generating-database-seed-data",
      "description": "This skill enables claude to generate realistic test data and database",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the creation of database seed scripts, populating your database with realistic and consistent test data. It leverages Faker libraries to generate diverse and believable data, ensuring relational integrity and configurable data volumes.\n\n## How It Works\n\n1. **Analyze Schema**: Claude analyzes the database schema to understand table structures and relationships.\n2. **Generate Data**: Using Faker libraries, Claude generates realistic data for each table, respecting data types and constraints.\n3. **Maintain Relationships**: Claude ensures foreign key relationships are maintained, creating consistent and valid data across tables.\n4. **Create Seed Script**: Claude generates a database seed script (e.g., SQL, JavaScript) containing the generated data.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Populate a development database with realistic data.\n- Create a seed script for automated database setup.\n- Generate test data for application testing.\n- Demonstrate an application with pre-populated data.\n\n## Examples\n\n### Example 1: Populating a User Database\n\nUser request: \"Create a seed script to populate my users table with 50 realistic users.\"\n\nThe skill will:\n1. Analyze the 'users' table schema (name, email, password, etc.).\n2. Generate 50 sets of realistic user data using Faker libraries.\n3. Create a SQL seed script to insert the generated user data into the 'users' table.\n\n### Example 2: Seeding a Blog Database\n\nUser request: \"Generate test data for my blog database, including posts, comments, and users.\"\n\nThe skill will:\n1. Analyze the 'posts', 'comments', and 'users' table schemas and their relationships.\n2. Generate realistic data for each table, ensuring foreign key relationships are maintained (e.g., comments linked to posts, posts linked to users).\n3. Create a seed script (e.g., JavaScript with TypeORM) to insert the generated data into the database.\n\n## Best Practices\n\n- **Data Volume**: Start with a small data volume and gradually increase it to avoid performance issues.\n- **Data Consistency**: Ensure the Faker libraries used are appropriate for the data types and formats required by your database.\n- **Idempotency**: Design your seed scripts to be idempotent, so they can be run multiple times without causing errors or duplicate data.\n\n## Integration\n\nThis skill integrates well with database migration tools and frameworks, allowing you to automate the entire database setup process, including schema creation and data seeding. It can also be used in conjunction with testing frameworks to generate realistic test data for automated testing.",
      "parentPlugin": {
        "name": "data-seeder-generator",
        "category": "database",
        "path": "plugins/database/data-seeder-generator",
        "version": "1.0.0",
        "description": "Generate realistic test data and database seed scripts for development and testing environments"
      },
      "filePath": "plugins/database/data-seeder-generator/skills/data-seeder-generator/SKILL.md"
    },
    {
      "slug": "generating-docker-compose-files",
      "name": "generating-docker-compose-files",
      "description": "Use when you need to work with Docker Compose. This skill provides Docker Compose file generation with comprehensive guidance and automation. Trigger with phrases like \"generate docker-compose\", \"create compose file\", or \"configure multi-container app\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/docker-compose-generator/`\n\n**Documentation and Guides**: `{baseDir}/docs/docker-compose-generator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/docker-compose-generator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/docker-compose-generator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/docker-compose-generator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/docker-compose-generator-dashboard.json`",
      "parentPlugin": {
        "name": "docker-compose-generator",
        "category": "devops",
        "path": "plugins/devops/docker-compose-generator",
        "version": "1.0.0",
        "description": "Generate Docker Compose configurations for multi-container applications with best practices"
      },
      "filePath": "plugins/devops/docker-compose-generator/skills/docker-compose-generator/SKILL.md"
    },
    {
      "slug": "generating-grpc-services",
      "name": "generating-grpc-services",
      "description": "Generate gRPC service definitions, stubs, and implementations from Protocol Buffers. Use when creating high-performance gRPC services. Trigger with phrases like \"generate gRPC service\", \"create gRPC API\", or \"build gRPC server\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:grpc-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:grpc-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "grpc-service-generator",
        "category": "api-development",
        "path": "plugins/api-development/grpc-service-generator",
        "version": "1.0.0",
        "description": "Generate gRPC services with Protocol Buffers and streaming support"
      },
      "filePath": "plugins/api-development/grpc-service-generator/skills/grpc-service-generator/SKILL.md"
    },
    {
      "slug": "generating-helm-charts",
      "name": "generating-helm-charts",
      "description": "Use when generating Helm charts for Kubernetes applications. Trigger with phrases like \"create Helm chart\", \"generate chart for app\", \"package Kubernetes deployment\", or \"helm template\". Produces production-ready charts with Chart.yaml, values.yaml, templates, and best practices for multi-environment deployments.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(helm:*)",
        "Bash(kubectl:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Helm 3+ is installed on the system\n- Kubernetes cluster access is configured\n- Application container images are available\n- Understanding of application resource requirements\n- Chart repository access (if publishing)\n\n## Instructions\n\n1. **Gather Requirements**: Identify application type, dependencies, configuration needs\n2. **Create Chart Structure**: Generate Chart.yaml with metadata and version info\n3. **Define Values**: Create values.yaml with configurable parameters and defaults\n4. **Build Templates**: Generate deployment, service, ingress, and configmap templates\n5. **Add Helpers**: Create _helpers.tpl for reusable template functions\n6. **Configure Resources**: Set resource limits, security contexts, and health checks\n7. **Test Chart**: Validate with `helm lint` and `helm template` commands\n8. **Document Usage**: Add README with installation instructions and configuration options\n\n## Output\n\nGenerates complete Helm chart structure:\n\n```\n{baseDir}/helm-charts/app-name/\n‚îú‚îÄ‚îÄ Chart.yaml          # Chart metadata\n‚îú‚îÄ‚îÄ values.yaml         # Default configuration\n‚îú‚îÄ‚îÄ templates/\n‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ service.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ _helpers.tpl    # Template helpers\n‚îÇ   ‚îî‚îÄ‚îÄ NOTES.txt       # Post-install notes\n‚îú‚îÄ‚îÄ charts/             # Dependencies\n‚îî‚îÄ‚îÄ README.md\n```\n\n**Example Chart.yaml:**\n```yaml\napiVersion: v2\nname: my-app\ndescription: Production-ready application chart\ntype: application\nversion: 1.0.0\nappVersion: \"1.0.0\"\n```\n\n**Example values.yaml:**\n```yaml\nreplicaCount: 3\nimage:\n  repository: registry/app\n  tag: \"1.0.0\"\n  pullPolicy: IfNotPresent\nresources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 250m\n    memory: 256Mi\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Chart Validation Errors**\n- Error: \"Chart.yaml: version is required\"\n- Solution: Ensure Chart.yaml contains valid apiVersion, name, and version fields\n\n**Template Rendering Failures**\n- Error: \"parse error in deployment.yaml\"\n- Solution: Validate template syntax with `helm template` and check Go template formatting\n\n**Missing Dependencies**\n- Error: \"dependency not found\"\n- Solution: Run `helm dependency update` in chart directory\n\n**Values Override Issues**\n- Error: \"failed to render values\"\n- Solution: Check values.yaml syntax and ensure proper YAML indentation\n\n**Installation Failures**\n- Error: \"release failed: timed out waiting for condition\"\n- Solution: Increase timeout or check pod logs for application startup issues\n\n## Resources\n\n- Helm documentation: https://helm.sh/docs/\n- Chart best practices guide: https://helm.sh/docs/chart_best_practices/\n- Template function reference: https://helm.sh/docs/chart_template_guide/\n- Example charts repository: https://github.com/helm/charts\n- Chart testing guide in {baseDir}/docs/helm-testing.md",
      "parentPlugin": {
        "name": "helm-chart-generator",
        "category": "devops",
        "path": "plugins/devops/helm-chart-generator",
        "version": "1.0.0",
        "description": "Generate Helm charts for Kubernetes applications"
      },
      "filePath": "plugins/devops/helm-chart-generator/skills/helm-chart-generator/SKILL.md"
    },
    {
      "slug": "generating-infrastructure-as-code",
      "name": "generating-infrastructure-as-code",
      "description": "Use when generating infrastructure as code configurations. Trigger with phrases like \"create Terraform config\", \"generate CloudFormation template\", \"write Pulumi code\", or \"IaC for AWS/GCP/Azure\". Produces production-ready code for Terraform, CloudFormation, Pulumi, ARM templates, and CDK across multiple cloud providers.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(terraform:*)",
        "Bash(aws:*)",
        "Bash(gcloud:*)",
        "Bash(az:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Target cloud provider CLI is installed (aws-cli, gcloud, az)\n- IaC tool is installed (Terraform, Pulumi, AWS CDK)\n- Cloud credentials are configured locally\n- Understanding of target infrastructure architecture\n- Version control system for IaC storage\n\n## Instructions\n\n1. **Identify Platform**: Determine IaC tool (Terraform, CloudFormation, Pulumi, ARM, CDK)\n2. **Define Resources**: Specify cloud resources needed (compute, network, storage, database)\n3. **Establish Structure**: Create modular file structure for maintainability\n4. **Generate Code**: Write IaC configurations with proper syntax and formatting\n5. **Add Variables**: Define input variables for environment-specific values\n6. **Configure Outputs**: Specify outputs for resource references and integrations\n7. **Implement State**: Set up remote state storage for team collaboration\n8. **Document Usage**: Add README with deployment instructions and prerequisites\n\n## Output\n\nGenerates infrastructure as code files:\n\n**Terraform Example:**\n```hcl\n# {baseDir}/terraform/main.tf\nterraform {\n  required_version = \">= 1.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nresource \"aws_vpc\" \"main\" {\n  cidr_block = var.vpc_cidr\n  enable_dns_hostnames = true\n\n  tags = {\n    Name = \"${var.project}-vpc\"\n    Environment = var.environment\n  }\n}\n```\n\n**CloudFormation Example:**\n```yaml\n# {baseDir}/cloudformation/template.yaml\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: Production VPC infrastructure\n\nParameters:\n  VpcCidr:\n    Type: String\n    Default: 10.0.0.0/16\n\nResources:\n  VPC:\n    Type: AWS::EC2::VPC\n    Properties:\n      CidrBlock: !Ref VpcCidr\n      EnableDnsHostnames: true\n```\n\n**Pulumi Example:**\n```typescript\n// {baseDir}/pulumi/index.ts\nimport * as aws from \"@pulumi/aws\";\n\nconst vpc = new aws.ec2.Vpc(\"main\", {\n    cidrBlock: \"10.0.0.0/16\",\n    enableDnsHostnames: true,\n    tags: {\n        Name: \"production-vpc\"\n    }\n});\n\nexport const vpcId = vpc.id;\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Syntax Errors**\n- Error: \"Invalid resource syntax in configuration\"\n- Solution: Validate syntax with `terraform validate` or respective tool linter\n\n**Provider Authentication**\n- Error: \"Unable to authenticate with cloud provider\"\n- Solution: Configure credentials via environment variables or CLI login\n\n**Resource Conflicts**\n- Error: \"Resource already exists\"\n- Solution: Import existing resources or use data sources instead of creating new ones\n\n**State Lock Issues**\n- Error: \"Error acquiring state lock\"\n- Solution: Ensure no other process is running, or force unlock if safe\n\n**Dependency Errors**\n- Error: \"Resource depends on resource that does not exist\"\n- Solution: Check resource references and ensure proper dependency ordering\n\n## Resources\n\n- Terraform documentation: https://www.terraform.io/docs/\n- AWS CloudFormation guide: https://docs.aws.amazon.com/cloudformation/\n- Pulumi documentation: https://www.pulumi.com/docs/\n- Azure ARM templates: https://docs.microsoft.com/azure/azure-resource-manager/\n- IaC best practices guide in {baseDir}/docs/iac-standards.md",
      "parentPlugin": {
        "name": "infrastructure-as-code-generator",
        "category": "devops",
        "path": "plugins/devops/infrastructure-as-code-generator",
        "version": "1.0.0",
        "description": "Generate Infrastructure as Code for Terraform, CloudFormation, Pulumi, and more"
      },
      "filePath": "plugins/devops/infrastructure-as-code-generator/skills/infrastructure-as-code-generator/SKILL.md"
    },
    {
      "slug": "generating-orm-code",
      "name": "generating-orm-code",
      "description": "Use when you need to work with ORM code generation. This skill provides ORM model and code generation with comprehensive guidance and automation. Trigger with phrases like \"generate ORM models\", \"create entity classes\", or \"scaffold database models\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/orm-code-generator/`\n\n**Documentation and Guides**: `{baseDir}/docs/orm-code-generator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/orm-code-generator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/orm-code-generator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/orm-code-generator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/orm-code-generator-dashboard.json`",
      "parentPlugin": {
        "name": "orm-code-generator",
        "category": "database",
        "path": "plugins/database/orm-code-generator",
        "version": "1.0.0",
        "description": "Generate ORM models from database schemas or create database schemas from models for TypeORM, Prisma, Sequelize, SQLAlchemy, and more"
      },
      "filePath": "plugins/database/orm-code-generator/skills/orm-code-generator/SKILL.md"
    },
    {
      "slug": "generating-rest-apis",
      "name": "generating-rest-apis",
      "description": "Generate complete REST API implementations from OpenAPI specifications or database schemas. Use when generating RESTful API implementations. Trigger with phrases like \"generate REST API\", \"create RESTful API\", or \"build REST endpoints\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:rest-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:rest-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "rest-api-generator",
        "category": "api-development",
        "path": "plugins/api-development/rest-api-generator",
        "version": "1.0.0",
        "description": "Generate RESTful APIs from schemas with proper routing, validation, and documentation"
      },
      "filePath": "plugins/api-development/rest-api-generator/skills/rest-api-generator/SKILL.md"
    },
    {
      "slug": "generating-security-audit-reports",
      "name": "generating-security-audit-reports",
      "description": "Generate comprehensive security audit reports for applications and systems. Use when you need to assess security posture, identify vulnerabilities, evaluate compliance status, or create formal security documentation. Trigger with phrases like \"create security audit report\", \"generate security assessment\", \"audit security posture\", or \"PCI-DSS compliance report\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Security scan data or logs are available in {baseDir}/security/\n- Access to application configuration files\n- Security tool outputs (e.g., vulnerability scanners, SAST/DAST results)\n- Compliance framework documentation (if applicable)\n- Write permissions for generating report files\n\n## Instructions\n\n### 1. Data Collection Phase\n\nGather security information from available sources:\n- Read vulnerability scan results\n- Analyze security configurations\n- Review access control policies\n- Check encryption implementations\n- Examine authentication mechanisms\n\n### 2. Analysis Phase\n\nProcess collected data to identify:\n- Critical vulnerabilities (CVSS scores, exploitability)\n- Security misconfigurations\n- Compliance gaps against standards (PCI-DSS, GDPR, HIPAA, SOC 2)\n- Access control weaknesses\n- Data protection issues\n\n### 3. Report Generation Phase\n\nCreate structured audit report with:\n- Executive summary with risk overview\n- Detailed vulnerability findings with severity ratings\n- Compliance status matrix\n- Risk assessment and prioritization\n- Remediation recommendations with timelines\n- Technical appendices with evidence\n\n### 4. Output Formatting\n\nGenerate report in requested format:\n- Markdown for version control\n- HTML for stakeholder review\n- JSON for integration with ticketing systems\n- PDF-ready structure for formal documentation\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Comprehensive security audit report saved to {baseDir}/reports/security-audit-YYYYMMDD.md\n\n**Report Structure**:\n```\n# Security Audit Report - [System Name]\n## Executive Summary\n- Overall risk rating\n- Critical findings count\n- Compliance status\n\n## Vulnerability Findings\n### Critical (CVSS 9.0+)\n- [CVE-XXXX-XXXX] Description\n- Impact assessment\n- Remediation steps\n\n### High (CVSS 7.0-8.9)\n[Similar structure]\n\n## Compliance Assessment\n- PCI-DSS: 85% compliant (gaps identified)\n- GDPR: 92% compliant\n- SOC 2: In progress\n\n## Remediation Plan\nPriority matrix with timelines\n\n## Technical Appendices\nEvidence and scan outputs\n```\n\n**Secondary Outputs**:\n- Vulnerability tracking JSON for issue systems\n- Executive summary slide deck outline\n- Remediation tracking checklist\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Missing Scan Data**\n   - Error: \"No security scan results found\"\n   - Resolution: Specify alternate data sources or run preliminary scans\n   - Fallback: Generate report from configuration analysis only\n\n2. **Incomplete Compliance Framework**\n   - Error: \"Cannot assess [STANDARD] compliance - requirements unavailable\"\n   - Resolution: Request framework checklist or use general best practices\n   - Fallback: Note limitation in report with partial assessment\n\n3. **Access Denied to Configuration Files**\n   - Error: \"Permission denied reading {baseDir}/config/\"\n   - Resolution: Request elevated permissions or provide configuration exports\n   - Fallback: Generate report with available data, note gaps\n\n4. **Large Dataset Processing**\n   - Error: \"Scan results exceed processing capacity\"\n   - Resolution: Process in batches by severity or component\n   - Fallback: Focus on critical/high findings first\n\n## Resources\n\n**Security Standards References**:\n- OWASP Top 10: https://owasp.org/www-project-top-ten/\n- CWE Top 25: https://cwe.mitre.org/top25/\n- NIST Cybersecurity Framework: https://www.nist.gov/cyberframework\n\n**Compliance Frameworks**:\n- PCI-DSS Requirements: https://www.pcisecuritystandards.org/\n- GDPR Compliance Checklist: https://gdpr.eu/checklist/\n- HIPAA Security Rule: https://www.hhs.gov/hipaa/for-professionals/security/\n\n**Vulnerability Databases**:\n- National Vulnerability Database: https://nvd.nist.gov/\n- CVE Details: https://www.cvedetails.com/\n\n**Report Templates**:\n- Use {baseDir}/templates/security-audit-template.md if available\n- Default structure follows NIST SP 800-115 guidelines\n\n**Integration Points**:\n- Export findings to JIRA/GitHub Issues for tracking\n- Generate compliance evidence for SOC 2 audits\n- Link to SIEM/logging systems for evidence validation",
      "parentPlugin": {
        "name": "security-audit-reporter",
        "category": "security",
        "path": "plugins/security/security-audit-reporter",
        "version": "1.0.0",
        "description": "Generate comprehensive security audit reports"
      },
      "filePath": "plugins/security/security-audit-reporter/skills/security-audit-reporter/SKILL.md"
    },
    {
      "slug": "generating-smart-commits",
      "name": "generating-smart-commits",
      "description": "Use when generating conventional commit messages from staged git changes. Trigger with phrases like \"create commit message\", \"generate smart commit\", \"/commit-smart\", or \"/gc\". Automatically analyzes changes to determine commit type (feat, fix, docs), identifies breaking changes, and formats according to conventional commit standards.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(git:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Git repository is initialized in {baseDir}\n- Changes are staged using `git add`\n- User has permission to create commits\n- Git user name and email are configured\n\n## Instructions\n\n1. **Analyze Staged Changes**: Examine git diff output to understand modifications\n2. **Determine Commit Type**: Classify changes as feat, fix, docs, style, refactor, test, or chore\n3. **Identify Scope**: Extract affected module or component from file paths\n4. **Detect Breaking Changes**: Look for API changes, removed features, or incompatible modifications\n5. **Format Message**: Construct message following pattern: `type(scope): description`\n6. **Present for Review**: Show generated message and ask for confirmation before committing\n\n## Output\n\nGenerates conventional commit messages in this format:\n\n```\ntype(scope): brief description\n\n- Detailed explanation of changes\n- Why the change was necessary\n- Impact on existing functionality\n\nBREAKING CHANGE: description if applicable\n```\n\nExamples:\n- `feat(auth): implement JWT authentication middleware`\n- `fix(api): resolve null pointer exception in user endpoint`\n- `docs(readme): update installation instructions`\n\n## Error Handling\n\nCommon issues and solutions:\n\n**No Staged Changes**\n- Error: \"No changes staged for commit\"\n- Solution: Stage files using `git add <files>` before generating commit message\n\n**Git Not Initialized**\n- Error: \"Not a git repository\"\n- Solution: Initialize git with `git init` or navigate to repository root\n\n**Uncommitted Changes**\n- Warning: \"Unstaged changes detected\"\n- Solution: Stage relevant changes or use `git stash` for unrelated modifications\n\n**Invalid Commit Format**\n- Error: \"Generated message doesn't follow conventional format\"\n- Solution: Review and manually adjust type, scope, or description\n\n## Resources\n\n- Conventional Commits specification: https://www.conventionalcommits.org/\n- Git commit best practices documentation\n- Repository commit history for style consistency\n- Project-specific commit guidelines in {baseDir}/CONTRIBUTING.md",
      "parentPlugin": {
        "name": "git-commit-smart",
        "category": "devops",
        "path": "plugins/devops/git-commit-smart",
        "version": "1.0.0",
        "description": "AI-powered conventional commit message generator with smart analysis"
      },
      "filePath": "plugins/devops/git-commit-smart/skills/git-commit-smart/SKILL.md"
    },
    {
      "slug": "generating-stored-procedures",
      "name": "generating-stored-procedures",
      "description": "Use when you need to work with stored procedure generation. This skill provides stored procedure code generation with comprehensive guidance and automation. Trigger with phrases like \"generate stored procedures\", \"create database functions\", or \"write SQL procedures\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/stored-procedure-generator/`\n\n**Documentation and Guides**: `{baseDir}/docs/stored-procedure-generator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/stored-procedure-generator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/stored-procedure-generator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/stored-procedure-generator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/stored-procedure-generator-dashboard.json`",
      "parentPlugin": {
        "name": "stored-procedure-generator",
        "category": "database",
        "path": "plugins/database/stored-procedure-generator",
        "version": "1.0.0",
        "description": "Database plugin for stored-procedure-generator"
      },
      "filePath": "plugins/database/stored-procedure-generator/skills/stored-procedure-generator/SKILL.md"
    },
    {
      "slug": "generating-test-data",
      "name": "generating-test-data",
      "description": "Generate realistic test data including edge cases and boundary conditions. Use when creating realistic fixtures or edge case test data. Trigger with phrases like \"generate test data\", \"create fixtures\", or \"setup test database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:data-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:data-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "test-data-generator",
        "category": "testing",
        "path": "plugins/testing/test-data-generator",
        "version": "1.0.0",
        "description": "Generate realistic test data including users, products, orders, and custom schemas for comprehensive testing"
      },
      "filePath": "plugins/testing/test-data-generator/skills/test-data-generator/SKILL.md"
    },
    {
      "slug": "generating-test-doubles",
      "name": "generating-test-doubles",
      "description": "Generate mocks, stubs, spies, and fakes for dependency isolation. Use when creating mocks, stubs, or test isolation fixtures. Trigger with phrases like \"generate mocks\", \"create test doubles\", or \"setup stubs\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:doubles-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:doubles-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "test-doubles-generator",
        "category": "testing",
        "path": "plugins/testing/test-doubles-generator",
        "version": "1.0.0",
        "description": "Generate mocks, stubs, spies, and fakes for unit testing with Jest, Sinon, and test frameworks"
      },
      "filePath": "plugins/testing/test-doubles-generator/skills/test-doubles-generator/SKILL.md"
    },
    {
      "slug": "generating-test-reports",
      "name": "generating-test-reports",
      "description": "Generate comprehensive test reports with metrics, coverage, and visualizations. Use when performing specialized testing. Trigger with phrases like \"generate test report\", \"create test documentation\", or \"show test metrics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:report-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:report-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "test-report-generator",
        "category": "testing",
        "path": "plugins/testing/test-report-generator",
        "version": "1.0.0",
        "description": "Generate comprehensive test reports with coverage, trends, and stakeholder-friendly formats"
      },
      "filePath": "plugins/testing/test-report-generator/skills/test-report-generator/SKILL.md"
    },
    {
      "slug": "generating-trading-signals",
      "name": "generating-trading-signals",
      "description": "Generate trading signals using technical indicators and on-chain metrics. Use when receiving trading signals and alerts. Trigger with phrases like \"get trading signals\", \"check indicators\", or \"analyze signals\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:signals-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:signals-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "crypto-signal-generator",
        "category": "crypto",
        "path": "plugins/crypto/crypto-signal-generator",
        "version": "1.0.0",
        "description": "Generate trading signals from technical indicators and market analysis"
      },
      "filePath": "plugins/crypto/crypto-signal-generator/skills/crypto-signal-generator/SKILL.md"
    },
    {
      "slug": "generating-unit-tests",
      "name": "generating-unit-tests",
      "description": "Automatically generate comprehensive unit tests from source code covering happy paths, edge cases, and error conditions. Use when creating test coverage for functions, classes, or modules. Trigger with phrases like \"generate unit tests\", \"create tests for\", or \"add test coverage\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:unit-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Source code files requiring test coverage\n- Testing framework installed (Jest, Mocha, pytest, JUnit, etc.)\n- Understanding of code dependencies and external services to mock\n- Test directory structure established (e.g., `tests/`, `__tests__/`, `spec/`)\n- Package configuration updated with test scripts\n\n## Instructions\n\n### Step 1: Analyze Source Code\nExamine code structure and identify test requirements:\n1. Use Read tool to load source files from {baseDir}/src/\n2. Identify all functions, classes, and methods requiring tests\n3. Document function signatures, parameters, return types, and side effects\n4. Note external dependencies requiring mocking or stubbing\n\n### Step 2: Determine Testing Framework\nSelect appropriate testing framework based on language:\n- JavaScript/TypeScript: Jest, Mocha, Jasmine, Vitest\n- Python: pytest, unittest, nose2\n- Java: JUnit 5, TestNG\n- Go: testing package with testify assertions\n- Ruby: RSpec, Minitest\n\n### Step 3: Generate Test Cases\nCreate comprehensive test suite covering:\n1. Happy path tests with valid inputs and expected outputs\n2. Edge case tests with boundary values (empty arrays, null, zero, max values)\n3. Error condition tests with invalid inputs\n4. Mock external dependencies (databases, APIs, file systems)\n5. Setup and teardown fixtures for test isolation\n\n### Step 4: Write Test File\nGenerate test file in {baseDir}/tests/ with structure:\n- Import statements for code under test and testing framework\n- Mock declarations for external dependencies\n- Describe/context blocks grouping related tests\n- Individual test cases with arrange-act-assert pattern\n- Cleanup logic in afterEach/tearDown hooks\n\n## Output\n\nThe skill generates complete test files:\n\n### Test File Structure\n```javascript\n// Example Jest test file\nimport { validator } from '../src/utils/validator';\n\ndescribe('Validator', () => {\n  describe('validateEmail', () => {\n    it('should accept valid email addresses', () => {\n      expect(validator.validateEmail('test@example.com')).toBe(true);\n    });\n\n    it('should reject invalid email formats', () => {\n      expect(validator.validateEmail('invalid-email')).toBe(false);\n    });\n\n    it('should handle null and undefined', () => {\n      expect(validator.validateEmail(null)).toBe(false);\n      expect(validator.validateEmail(undefined)).toBe(false);\n    });\n  });\n});\n```\n\n### Coverage Metrics\n- Line coverage percentage (target: 80%+)\n- Branch coverage showing tested conditional paths\n- Function coverage ensuring all exports are tested\n- Statement coverage for comprehensive validation\n\n### Mock Implementations\nGenerated mocks for:\n- Database connections and queries\n- HTTP requests to external APIs\n- File system operations (read/write)\n- Environment variables and configuration\n- Time-dependent functions (Date.now(), setTimeout)\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Module Import Errors**\n- Error: Cannot find module or dependencies\n- Solution: Install missing packages; verify import paths match project structure; check TypeScript configuration\n\n**Mock Setup Failures**\n- Error: Mock not properly intercepting calls\n- Solution: Ensure mocks are defined before imports; use proper mocking syntax for framework; clear mocks between tests\n\n**Async Test Timeouts**\n- Error: Test exceeded timeout before completing\n- Solution: Increase timeout for slow operations; ensure async/await or done callbacks are used correctly; check for unresolved promises\n\n**Test Isolation Issues**\n- Error: Tests pass individually but fail when run together\n- Solution: Add proper cleanup in afterEach hooks; avoid shared mutable state; reset mocks between tests\n\n## Resources\n\n### Testing Frameworks\n- Jest documentation for JavaScript testing\n- pytest documentation for Python testing\n- JUnit 5 User Guide for Java testing\n- Go testing package and testify library\n\n### Best Practices\n- Follow AAA pattern (Arrange, Act, Assert) for test structure\n- Write tests before fixing bugs (test-driven bug fixing)\n- Use descriptive test names that explain the scenario\n- Keep tests independent and avoid test interdependencies\n- Mock external dependencies for unit test isolation\n- Aim for 80%+ code coverage on critical paths",
      "parentPlugin": {
        "name": "unit-test-generator",
        "category": "testing",
        "path": "plugins/testing/unit-test-generator",
        "version": "1.0.0",
        "description": "Automatically generate comprehensive unit tests from source code with multiple testing framework support"
      },
      "filePath": "plugins/testing/unit-test-generator/skills/unit-test-generator/SKILL.md"
    },
    {
      "slug": "genkit-infra-expert",
      "name": "genkit-infra-expert",
      "description": "Use when deploying Genkit applications to production with Terraform. Trigger with phrases like \"deploy genkit terraform\", \"provision genkit infrastructure\", \"firebase functions terraform\", \"cloud run deployment\", or \"genkit production infrastructure\". Provisions Firebase Functions, Cloud Run services, GKE clusters, monitoring dashboards, and CI/CD for AI workflows.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(terraform:*)",
        "Bash(gcloud:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Google Cloud project with Firebase enabled\n- Terraform 1.0+ installed\n- gcloud and firebase CLI authenticated\n- Genkit application built and containerized\n- API keys for Gemini or other AI models\n- Understanding of Genkit flows and deployment options\n\n## Instructions\n\n1. **Choose Deployment Target**: Firebase Functions, Cloud Run, or GKE\n2. **Configure Terraform Backend**: Set up remote state in GCS\n3. **Define Variables**: Project ID, region, Genkit app configuration\n4. **Provision Compute**: Deploy functions or containers\n5. **Configure Secrets**: Store API keys in Secret Manager\n6. **Set Up Monitoring**: Create dashboards for token usage and latency\n7. **Enable Auto-scaling**: Configure min/max instances\n8. **Validate Deployment**: Test Genkit flows via HTTP endpoints\n\n## Output\n\n**Firebase Functions:**\n```hcl\n# {baseDir}/terraform/functions.tf\nresource \"google_cloudfunctions2_function\" \"genkit_function\" {\n  name     = \"genkit-ai-flow\"\n  location = var.region\n\n  build_config {\n    runtime     = \"nodejs20\"\n    entry_point = \"genkitFlow\"\n  }\n\n  service_config {\n    max_instance_count = 100\n    available_memory   = \"512Mi\"\n    timeout_seconds    = 300\n  }\n}\n```\n\n**Cloud Run Service:**\n```hcl\nresource \"google_cloud_run_v2_service\" \"genkit_service\" {\n  name     = \"genkit-api\"\n  location = var.region\n\n  template {\n    scaling {\n      min_instance_count = 1\n      max_instance_count = 10\n    }\n    containers {\n      image = \"gcr.io/${var.project_id}/genkit-app:latest\"\n      resources {\n        limits = {\n          cpu = \"2\"\n          memory = \"1Gi\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Error Handling\n\n**Build Failures**\n- Error: \"Cloud Function build failed\"\n- Solution: Check package.json dependencies and Node.js runtime version\n\n**Cold Start Latency**\n- Warning: \"High latency on first request\"\n- Solution: Set min_instance_count >= 1 to keep warm instances\n\n**Secret Access Denied**\n- Error: \"Permission denied accessing secret\"\n- Solution: Grant secretAccessor role to Cloud Run/Functions service account\n\n**Memory Exceeded**\n- Error: \"Container killed: out of memory\"\n- Solution: Increase available_memory or optimize Genkit flow memory usage\n\n## Resources\n\n- Genkit Deployment: https://genkit.dev/docs/deployment\n- Firebase Terraform: https://registry.terraform.io/providers/hashicorp/google/latest\n- Genkit examples in {baseDir}/genkit-examples/",
      "parentPlugin": {
        "name": "jeremy-genkit-terraform",
        "category": "devops",
        "path": "plugins/devops/jeremy-genkit-terraform",
        "version": "1.0.0",
        "description": "Terraform modules for Firebase Genkit infrastructure and deployments"
      },
      "filePath": "plugins/devops/jeremy-genkit-terraform/skills/genkit-infra-expert/SKILL.md"
    },
    {
      "slug": "genkit-production-expert",
      "name": "genkit-production-expert",
      "description": "Build production Firebase Genkit applications including RAG systems,",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## What This Skill Does\n\nThis skill provides comprehensive expertise in building production-ready Firebase Genkit applications across Node.js (1.0), Python (Alpha), and Go (1.0). It handles the complete lifecycle from initialization to deployment with AI monitoring.\n\n### Core Capabilities\n\n1. **Project Initialization**: Set up properly structured Genkit projects with best practices\n2. **Flow Architecture**: Design multi-step AI workflows with proper error handling\n3. **RAG Implementation**: Build retrieval-augmented generation systems with vector search\n4. **Tool Integration**: Implement function calling and custom tools\n5. **Monitoring Setup**: Configure AI monitoring for Firebase Console\n6. **Multi-Language Support**: Expert guidance for TypeScript, Python, and Go implementations\n7. **Production Deployment**: Deploy to Firebase Functions or Google Cloud Run\n\n## When This Skill Activates\n\nThis skill automatically activates when you mention:\n\n### Trigger Phrases\n- \"Create a Genkit flow\"\n- \"Implement RAG with Genkit\"\n- \"Deploy Genkit to Firebase\"\n- \"Set up Gemini integration\"\n- \"Configure AI monitoring\"\n- \"Build Genkit application\"\n- \"Design AI workflow\"\n- \"Genkit tool calling\"\n- \"Vector search with Genkit\"\n- \"Genkit production deployment\"\n\n### Use Case Patterns\n- Setting up new Genkit projects\n- Implementing RAG systems with embedding models\n- Integrating Gemini 2.5 Pro/Flash models\n- Creating multi-step AI workflows\n- Deploying to production with monitoring\n- Debugging Genkit flows\n- Optimizing token usage and costs\n\n## How It Works\n\n### Phase 1: Requirements Analysis\n```\nUser Request ‚Üí Analyze needs ‚Üí Determine:\n- Target language (Node.js/Python/Go)\n- Flow complexity (simple/multi-step/RAG)\n- Model requirements (Gemini version, custom models)\n- Deployment target (Firebase/Cloud Run/local)\n```\n\n### Phase 2: Project Setup\n```\nCheck existing project ‚Üí If new:\n  - Initialize project structure\n  - Install dependencies\n  - Configure environment variables\n  - Set up TypeScript/Python/Go config\n\nIf existing:\n  - Analyze current structure\n  - Identify integration points\n  - Preserve existing code\n```\n\n### Phase 3: Implementation\n```\nDesign flow architecture ‚Üí Implement:\n  - Input/output schemas (Zod/Pydantic/Go structs)\n  - Model configuration\n  - Tool definitions (if needed)\n  - Retriever setup (for RAG)\n  - Error handling\n  - Tracing configuration\n```\n\n### Phase 4: Testing & Validation\n```\nCreate test cases ‚Üí Run locally:\n  - Genkit Developer UI\n  - Unit tests\n  - Integration tests\n  - Token usage analysis\n```\n\n### Phase 5: Production Deployment\n```\nConfigure deployment ‚Üí Deploy:\n  - Firebase Functions (with AI monitoring)\n  - Cloud Run (with auto-scaling)\n  - Set up monitoring dashboards\n  - Configure alerting\n```\n\n## Workflow Examples\n\n### Example 1: Simple Question-Answering Flow\n\n**User Request**: \"Create a Genkit flow that answers user questions using Gemini 2.5 Flash\"\n\n**Skill Response**:\n1. Creates TypeScript project (default)\n2. Implements flow with input validation:\n```typescript\nconst qaFlow = ai.defineFlow(\n  {\n    name: 'qaFlow',\n    inputSchema: z.object({ question: z.string() }),\n    outputSchema: z.object({ answer: z.string() }),\n  },\n  async (input) => {\n    const { text } = await ai.generate({\n      model: gemini25Flash,\n      prompt: `Answer this question: ${input.question}`,\n      config: { temperature: 0.3 }, // Lower for factual answers\n    });\n    return { answer: text };\n  }\n);\n```\n3. Sets up local testing\n4. Provides deployment instructions\n\n### Example 2: RAG System with Vector Search\n\n**User Request**: \"Implement RAG with Genkit for our documentation search\"\n\n**Skill Response**:\n1. Analyzes document storage needs\n2. Implements retriever with embeddings:\n```typescript\nconst docRetriever = ai.defineRetriever(\n  {\n    name: 'docRetriever',\n    configSchema: z.object({ k: z.number().default(5) }),\n  },\n  async (query, config) => {\n    // Generate embedding\n    const embedding = await ai.embed({\n      embedder: textEmbeddingGecko,\n      content: query,\n    });\n\n    // Search vector database\n    const results = await vectorDB.search(embedding, config.k);\n    return results.map(doc => ({\n      content: doc.text,\n      metadata: { source: doc.source },\n    }));\n  }\n);\n```\n3. Creates RAG flow combining retrieval + generation\n4. Sets up vector database connection\n5. Implements caching for efficiency\n\n### Example 3: Multi-Agent Tool Calling\n\n**User Request**: \"Create a Genkit agent with weather and calendar tools\"\n\n**Skill Response**:\n1. Defines tools with proper schemas:\n```typescript\nconst weatherTool = ai.defineTool({\n  name: 'getWeather',\n  description: 'Get current weather for a location',\n  inputSchema: z.object({ location: z.string() }),\n  outputSchema: z.object({\n    temp: z.number(),\n    conditions: z.string(),\n  }),\n}, async ({ location }) => {\n  // Call weather API\n});\n\nconst calendarTool = ai.defineTool({\n  name: 'checkCalendar',\n  description: 'Check calendar availability',\n  inputSchema: z.object({ date: z.string() }),\n  outputSchema: z.object({ available: z.boolean() }),\n}, async ({ date }) => {\n  // Check calendar API\n});\n```\n2. Creates agent flow with tool access:\n```typescript\nconst agentFlow = ai.defineFlow(async (userQuery) => {\n  const { text } = await ai.generate({\n    model: gemini25Flash,\n    prompt: userQuery,\n    tools: [weatherTool, calendarTool],\n  });\n  return text;\n});\n```\n3. Implements proper error handling\n4. Sets up tool execution tracing\n\n## Production Best Practices Applied\n\n### 1. Schema Validation\n- All inputs/outputs use Zod (TS), Pydantic (Python), or structs (Go)\n- Prevents runtime errors from malformed data\n\n### 2. Error Handling\n```typescript\ntry {\n  const result = await ai.generate({...});\n  return result;\n} catch (error) {\n  if (error.code === 'SAFETY_BLOCK') {\n    // Handle safety filters\n  } else if (error.code === 'QUOTA_EXCEEDED') {\n    // Handle rate limits\n  }\n  throw error;\n}\n```\n\n### 3. Cost Optimization\n- Context caching for repeated prompts\n- Token usage monitoring\n- Temperature tuning for use case\n- Model selection (Flash vs Pro)\n\n### 4. Monitoring\n- OpenTelemetry tracing enabled\n- Custom span attributes\n- Firebase Console integration\n- Alert configuration\n\n### 5. Security\n- Environment variable management\n- API key rotation support\n- Input sanitization\n- Output filtering\n\n## Integration with Other Tools\n\n### Works With ADK Plugin\nWhen complex multi-agent orchestration is needed:\n- Use Genkit for individual specialized flows\n- Use ADK for orchestrating multiple Genkit flows\n- Pass results via A2A protocol\n\n### Works With Vertex AI Validator\nFor production deployment:\n- Genkit implements the flows\n- Validator ensures production readiness\n- Validates monitoring configuration\n- Checks security compliance\n\n## Tool Permissions\n\nThis skill uses the following tools:\n- **Read**: Analyze existing code and configuration\n- **Write**: Create new flow files and configs\n- **Edit**: Modify existing Genkit implementations\n- **Grep**: Search for integration points\n- **Glob**: Find related files\n- **Bash**: Install dependencies, run tests, deploy\n\n## Troubleshooting Guide\n\n### Common Issue 1: API Key Not Found\n**Symptoms**: Error \"API key not provided\"\n**Solution**:\n1. Check `.env` file exists\n2. Verify `GOOGLE_API_KEY` is set\n3. Ensure `dotenv` is loaded\n\n### Common Issue 2: Flow Not Appearing in UI\n**Symptoms**: Flow not visible in Genkit Developer UI\n**Solution**:\n1. Ensure flow is exported\n2. Restart Genkit server\n3. Check console for errors\n\n### Common Issue 3: High Token Usage\n**Symptoms**: Unexpected costs\n**Solution**:\n1. Implement context caching\n2. Use Gemini 2.5 Flash instead of Pro\n3. Lower temperature\n4. Compress prompts\n\n## Version History\n\n- **1.0.0** (2025): Initial release with Node.js 1.0, Python Alpha, Go 1.0 support\n- Supports Gemini 2.5 Pro/Flash\n- AI monitoring integration\n- Production deployment patterns\n\n## References\n\n- Official Docs: https://genkit.dev/\n- Node.js 1.0 Announcement: https://firebase.blog/posts/2025/02/announcing-genkit/\n- Go 1.0 Announcement: https://developers.googleblog.com/en/announcing-genkit-go-10/\n- Vertex AI Plugin: https://genkit.dev/docs/integrations/vertex-ai/",
      "parentPlugin": {
        "name": "jeremy-genkit-pro",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-genkit-pro",
        "version": "1.0.0",
        "description": "Firebase Genkit expert for production-ready AI workflows with RAG and tool calling"
      },
      "filePath": "plugins/ai-ml/jeremy-genkit-pro/skills/genkit-production-expert/SKILL.md"
    },
    {
      "slug": "gh-actions-validator",
      "name": "gh-actions-validator",
      "description": "Use when validating GitHub Actions workflows for Google Cloud and Vertex AI deployments. Trigger with phrases like \"validate github actions\", \"setup workload identity federation\", \"github actions security\", \"deploy agent with ci/cd\", or \"automate vertex ai deployment\". Enforces Workload Identity Federation (WIF), validates OIDC permissions, ensures least privilege IAM, and implements security best practices.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(git:*)",
        "Bash(gcloud:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- GitHub repository with Actions enabled\n- Google Cloud project with billing enabled\n- gcloud CLI authenticated with admin permissions\n- Understanding of Workload Identity Federation concepts\n- GitHub repository secrets configured\n- Appropriate IAM roles for CI/CD automation\n\n## Instructions\n\n1. **Audit Existing Workflows**: Scan .github/workflows/ for security issues\n2. **Validate WIF Usage**: Ensure no JSON service account keys are used\n3. **Check OIDC Permissions**: Verify id-token: write is present\n4. **Review IAM Roles**: Confirm least privilege (no owner/editor roles)\n5. **Add Security Scans**: Include secret detection and vulnerability scanning\n6. **Validate Deployments**: Add post-deployment health checks\n7. **Configure Monitoring**: Set up alerts for deployment failures\n8. **Document WIF Setup**: Provide one-time WIF configuration commands\n\n## Output\n\n**Secure Workflow Template:**\n```yaml\n# {baseDir}/.github/workflows/deploy-vertex-ai.yml\nname: Deploy Vertex AI Agent\n\non:\n  push:\n    branches: [main]\n    paths: ['agent/**']\n\npermissions:\n  contents: read\n  id-token: write  # REQUIRED for WIF\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Authenticate to GCP (WIF)\n        uses: google-github-actions/auth@v2\n        with:\n          workload_identity_provider: ${{ secrets.WIF_PROVIDER }}\n          service_account: ${{ secrets.WIF_SERVICE_ACCOUNT }}\n\n      - name: Deploy to Vertex AI\n        run: |\n          gcloud ai agents deploy \\\n            --project=${{ secrets.GCP_PROJECT_ID }} \\\n            --region=us-central1\n\n      - name: Validate Deployment\n        run: |\n          python scripts/validate-deployment.py\n```\n\n**WIF Setup Commands:**\n```bash\n# One-time WIF configuration\ngcloud iam workload-identity-pools create github-pool \\\n  --location=global \\\n  --display-name=\"GitHub Actions Pool\"\n\ngcloud iam workload-identity-pools providers create-oidc github-provider \\\n  --location=global \\\n  --workload-identity-pool=github-pool \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.repository=assertion.repository\"\n```\n\n**Security Validation Checks:**\n```yaml\n# {baseDir}/.github/workflows/security-check.yml\nname: Security Validation\n\non: [pull_request, push]\n\npermissions:\n  contents: read\n  security-events: write\n\njobs:\n  security:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Scan for secrets\n        uses: trufflesecurity/trufflehog@main\n\n      - name: Vulnerability scan\n        uses: aquasecurity/trivy-action@master\n\n      - name: Validate no JSON keys\n        run: |\n          if find . -name \"*service-account*.json\"; then\n            echo \"ERROR: Service account keys detected\"\n            exit 1\n          fi\n\n      - name: Validate WIF usage\n        run: |\n          if grep -r \"credentials_json\" .github/workflows/; then\n            echo \"ERROR: Use WIF instead of JSON keys\"\n            exit 1\n          fi\n```\n\n## Error Handling\n\n**WIF Authentication Failed**\n- Error: \"Failed to generate Google Cloud access token\"\n- Solution: Verify WIF provider and service account email are correct\n\n**OIDC Token Error**\n- Error: \"Unable to get ACTIONS_ID_TOKEN_REQUEST_URL env variable\"\n- Solution: Add `id-token: write` permission to workflow\n\n**IAM Permission Denied**\n- Error: \"does not have required permission\"\n- Solution: Grant service account minimum required roles (run.admin, aiplatform.user)\n\n**Attribute Condition Failed**\n- Error: \"Token does not match attribute condition\"\n- Solution: Update attribute mapping to include repository restriction\n\n**Deployment Validation Failed**\n- Error: \"Agent not in RUNNING state\"\n- Solution: Check agent configuration and deployment logs\n\n## Resources\n\n- Workload Identity Federation: https://cloud.google.com/iam/docs/workload-identity-federation\n- GitHub OIDC: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments\n- Vertex AI Agent Engine: https://cloud.google.com/vertex-ai/docs/agent-engine\n- google-github-actions/auth: https://github.com/google-github-actions/auth\n- WIF setup guide in {baseDir}/docs/wif-setup.md",
      "parentPlugin": {
        "name": "jeremy-github-actions-gcp",
        "category": "devops",
        "path": "plugins/devops/jeremy-github-actions-gcp",
        "version": "1.0.0",
        "description": "GitHub Actions CI/CD workflows for Google Cloud and Vertex AI deployments"
      },
      "filePath": "plugins/devops/jeremy-github-actions-gcp/skills/gh-actions-validator/SKILL.md"
    },
    {
      "slug": "google-cloud-agent-sdk-master",
      "name": "Google Cloud Agent SDK Master",
      "description": "Automatic activation for all google cloud agent development kit (adk) Use when appropriate context detected. Trigger with relevant phrases based on skill purpose.",
      "allowedTools": [
        "Read",
        "WebFetch",
        "WebSearch",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Google Cloud Agent SDK Master - Production-Ready Agent Systems\n\nThis Agent Skill provides comprehensive mastery of Google's Agent Development Kit (ADK) and Agent Starter Pack for building and deploying production-grade containerized agents.\n\n## Core Capabilities\n\n### ü§ñ Agent Development Kit (ADK)\n\n**Framework Overview:**\n- **Open-source Python framework** from Google\n- Same framework powering Google Agentspace and CES\n- Build production agents in <100 lines of code\n- Model-agnostic (optimized for Gemini)\n- Deployment-agnostic (local, Cloud Run, GKE, Agent Engine)\n\n**Supported Agent Types:**\n1. **LLM Agents**: Dynamic routing with intelligence\n2. **Workflow Agents**:\n   - Sequential: Linear execution\n   - Loop: Iterative processing\n   - Parallel: Concurrent execution\n3. **Custom Agents**: User-defined implementations\n4. **Multi-agent Systems**: Hierarchical coordination\n\n**Key Features:**\n- Flexible orchestration (workflow & LLM-driven)\n- Tool ecosystem (search, code execution, custom functions)\n- Third-party integrations (LangChain, CrewAI)\n- Agents-as-tools capability\n- Built-in evaluation framework\n- Cloud Trace integration\n\n### üì¶ Agent Starter Pack\n\n**Production Templates:**\n1. **adk_base** - ReAct agent using ADK\n2. **agentic_rag** - Document retrieval + Q&A with search\n3. **langgraph_base_react** - LangGraph ReAct implementation\n4. **crewai_coding_crew** - Multi-agent coding system\n5. **adk_live** - Multimodal RAG (audio/video/text)\n\n**Infrastructure Automation:**\n- CI/CD setup with single command\n- GitHub Actions or Cloud Build pipelines\n- Multi-environment support (dev, staging, prod)\n- Automated testing and evaluation\n- Deployment rollback mechanisms\n\n### üöÄ Deployment Targets\n\n**1. Vertex AI Agent Engine**\n- Fully managed runtime\n- Auto-scaling and load balancing\n- Built-in observability\n- Serverless architecture\n- Best for: Production-scale agents\n\n**2. Cloud Run**\n- Containerized serverless\n- Pay-per-use pricing\n- Custom domain support\n- Traffic splitting\n- Best for: Web-facing agents\n\n**3. Google Kubernetes Engine (GKE)**\n- Full container orchestration\n- Advanced networking\n- Resource management\n- Multi-cluster support\n- Best for: Complex multi-agent systems\n\n**4. Local/Docker**\n- Development and testing\n- Custom infrastructure\n- On-premises deployment\n- Best for: POC and debugging\n\n### üîß Technical Implementation\n\n**Installation:**\n```bash\n# Agent Starter Pack (recommended)\npip install agent-starter-pack\n\n# or direct from GitHub\nuvx agent-starter-pack create my-agent\n\n# ADK only\npip install google-cloud-aiplatform[adk,agent_engines]>=1.111\n```\n\n**Create Agent (ADK):**\n```python\nfrom google.cloud.aiplatform import agent\nfrom vertexai.preview.agents import ADKAgent\n\n# Simple ReAct agent\n@agent.adk_agent\nclass MyAgent(ADKAgent):\n    def __init__(self):\n        super().__init__(\n            model=\"gemini-2.5-pro\",\n            tools=[search_tool, code_exec_tool]\n        )\n\n    def run(self, query: str):\n        return self.generate(query)\n\n# Multi-agent orchestration\nclass OrchestratorAgent(ADKAgent):\n    def __init__(self):\n        self.research_agent = ResearchAgent()\n        self.analysis_agent = AnalysisAgent()\n        self.writer_agent = WriterAgent()\n\n    def run(self, task: str):\n        research = self.research_agent.run(task)\n        analysis = self.analysis_agent.run(research)\n        output = self.writer_agent.run(analysis)\n        return output\n```\n\n**Using Agent Starter Pack:**\n```bash\n# Create project with template\nuvx agent-starter-pack create my-rag-agent \\\n    --template agentic_rag \\\n    --deployment cloud_run\n\n# Generates complete structure:\nmy-rag-agent/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ agent.py          # Agent implementation\n‚îÇ   ‚îú‚îÄ‚îÄ tools/            # Custom tools\n‚îÇ   ‚îî‚îÄ‚îÄ config.py         # Configuration\n‚îú‚îÄ‚îÄ deployment/\n‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile\n‚îÇ   ‚îú‚îÄ‚îÄ cloudbuild.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ terraform/\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ unit_tests.py\n‚îÇ   ‚îî‚îÄ‚îÄ integration_tests.py\n‚îî‚îÄ‚îÄ .github/workflows/    # CI/CD pipelines\n```\n\n**Deploy to Cloud Run:**\n```bash\n# Using ADK CLI\nadk deploy \\\n    --target cloud_run \\\n    --region us-central1 \\\n    --service-account sa@project.iam.gserviceaccount.com\n\n# Manual with Docker\ndocker build -t gcr.io/PROJECT/agent:latest .\ndocker push gcr.io/PROJECT/agent:latest\ngcloud run deploy agent \\\n    --image gcr.io/PROJECT/agent:latest \\\n    --region us-central1 \\\n    --allow-unauthenticated\n```\n\n**Deploy to Agent Engine:**\n```bash\n# Using Agent Starter Pack\nasp deploy \\\n    --env production \\\n    --target agent_engine\n\n# Manual deployment\nfrom google.cloud.aiplatform import agent_engines\nagent_engines.deploy_agent(\n    agent_id=\"my-agent\",\n    project=\"PROJECT_ID\",\n    location=\"us-central1\"\n)\n```\n\n### üìä RAG Agent Implementation\n\n**Vector Search Integration:**\n```python\nfrom vertexai.preview.rag import VectorSearchTool\nfrom google.cloud import aiplatform\n\n# Set up vector search\nvector_search = VectorSearchTool(\n    index_endpoint=\"projects/PROJECT/locations/LOCATION/indexEndpoints/INDEX_ID\",\n    deployed_index_id=\"deployed_index\"\n)\n\n# RAG agent with ADK\nclass RAGAgent(ADKAgent):\n    def __init__(self):\n        super().__init__(\n            model=\"gemini-2.5-pro\",\n            tools=[vector_search, web_search_tool]\n        )\n\n    def run(self, query: str):\n        # Retrieves relevant docs automatically\n        response = self.generate(\n            f\"Answer this using retrieved context: {query}\"\n        )\n        return response\n```\n\n**Vertex AI Search Integration:**\n```python\nfrom vertexai.preview.search import VertexAISearchTool\n\n# Enterprise search integration\nvertex_search = VertexAISearchTool(\n    data_store_id=\"DATA_STORE_ID\",\n    project=\"PROJECT_ID\"\n)\n\nagent = ADKAgent(\n    model=\"gemini-2.5-pro\",\n    tools=[vertex_search]\n)\n```\n\n### üîÑ CI/CD Automation\n\n**GitHub Actions (auto-generated):**\n```yaml\nname: Deploy Agent\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Test Agent\n        run: pytest tests/\n      - name: Deploy to Cloud Run\n        run: |\n          gcloud run deploy agent \\\n            --source . \\\n            --region us-central1\n```\n\n**Cloud Build Pipeline:**\n```yaml\nsteps:\n  # Build container\n  - name: 'gcr.io/cloud-builders/docker'\n    args: ['build', '-t', 'gcr.io/$PROJECT_ID/agent', '.']\n\n  # Run tests\n  - name: 'gcr.io/$PROJECT_ID/agent'\n    args: ['pytest', 'tests/']\n\n  # Deploy to Cloud Run\n  - name: 'gcr.io/cloud-builders/gcloud'\n    args:\n      - 'run'\n      - 'deploy'\n      - 'agent'\n      - '--image=gcr.io/$PROJECT_ID/agent'\n      - '--region=us-central1'\n```\n\n### üéØ Multi-Agent Orchestration\n\n**Hierarchical Agents:**\n```python\n# Coordinator agent with specialized sub-agents\nclass ProjectManagerAgent(ADKAgent):\n    def __init__(self):\n        self.researcher = ResearchAgent()\n        self.analyst = AnalysisAgent()\n        self.writer = WriterAgent()\n        self.reviewer = ReviewAgent()\n\n    def run(self, project_brief: str):\n        # Coordinate multiple agents\n        research = self.researcher.run(project_brief)\n        analysis = self.analyst.run(research)\n        draft = self.writer.run(analysis)\n        final = self.reviewer.run(draft)\n        return final\n```\n\n**Parallel Agent Execution:**\n```python\nimport asyncio\n\nclass ParallelResearchAgent(ADKAgent):\n    async def research_topic(self, topics: list[str]):\n        # Run multiple agents concurrently\n        tasks = [\n            self.specialized_agent(topic)\n            for topic in topics\n        ]\n        results = await asyncio.gather(*tasks)\n        return self.synthesize(results)\n```\n\n### üìà Evaluation & Monitoring\n\n**Built-in Evaluation:**\n```python\nfrom google.cloud.aiplatform import agent_evaluation\n\n# Define evaluation metrics\neval_config = agent_evaluation.EvaluationConfig(\n    metrics=[\"accuracy\", \"relevance\", \"safety\"],\n    test_dataset=\"gs://bucket/eval_data.jsonl\"\n)\n\n# Run evaluation\nresults = agent.evaluate(eval_config)\nprint(f\"Accuracy: {results.accuracy}\")\nprint(f\"Relevance: {results.relevance}\")\n```\n\n**Cloud Trace Integration:**\n```python\nfrom google.cloud import trace_v1\n\n# Automatic tracing\n@traced_agent\nclass MonitoredAgent(ADKAgent):\n    def run(self, query: str):\n        # All calls automatically traced\n        with self.trace_span(\"retrieval\"):\n            docs = self.retrieve(query)\n\n        with self.trace_span(\"generation\"):\n            response = self.generate(query, docs)\n\n        return response\n```\n\n### üîí Security & Best Practices\n\n**1. Service Account Management:**\n```bash\n# Create minimal-permission service account\ngcloud iam service-accounts create agent-sa \\\n    --display-name \"Agent Service Account\"\n\n# Grant only required permissions\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n    --member=\"serviceAccount:agent-sa@PROJECT.iam.gserviceaccount.com\" \\\n    --role=\"roles/aiplatform.user\"\n```\n\n**2. Secret Management:**\n```python\nfrom google.cloud import secretmanager\n\ndef get_api_key():\n    client = secretmanager.SecretManagerServiceClient()\n    name = \"projects/PROJECT/secrets/api-key/versions/latest\"\n    response = client.access_secret_version(name=name)\n    return response.payload.data.decode('UTF-8')\n```\n\n**3. VPC Service Controls:**\n```bash\n# Enable VPC SC for data security\ngcloud access-context-manager perimeters create agent-perimeter \\\n    --resources=projects/PROJECT_ID \\\n    --restricted-services=aiplatform.googleapis.com\n```\n\n### üí∞ Cost Optimization\n\n**Strategies:**\n- Use Gemini 2.5 Flash for most operations\n- Cache embeddings for RAG systems\n- Implement request batching\n- Use preemptible GKE nodes\n- Monitor token usage in Cloud Monitoring\n\n**Pricing Examples:**\n- Cloud Run: $0.00024/GB-second\n- Agent Engine: Pay-per-request pricing\n- GKE: Standard cluster costs\n- Gemini API: $3.50/1M tokens (Pro)\n\n### üìö Reference Architecture\n\n**Production Agent System:**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Load Balancer ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇCloud Run‚îÇ (Agent containers)\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ Agent Engine  ‚îÇ (Orchestration)\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ  Vertex AI Search   ‚îÇ (RAG)\n    ‚îÇ  Vector Search      ‚îÇ\n    ‚îÇ  Gemini 2.5 Pro     ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### üéØ Best Practices for Jeremy\n\n**1. Start with Templates:**\n```bash\n# Use Agent Starter Pack templates\nuvx agent-starter-pack create my-agent --template agentic_rag\n```\n\n**2. Local Development:**\n```bash\n# Test locally first\nadk serve --port 8080\ncurl http://localhost:8080/query -d '{\"question\": \"test\"}'\n```\n\n**3. Gradual Deployment:**\n```bash\n# Deploy to dev ‚Üí staging ‚Üí prod\nasp deploy --env dev\n# Test thoroughly\nasp deploy --env staging\n# Final production push\nasp deploy --env production\n```\n\n**4. Monitor Everything:**\n- Enable Cloud Trace\n- Set up error reporting\n- Track token usage\n- Monitor response times\n- Set up alerting\n\n### üìñ Official Documentation\n\n**Core Resources:**\n- ADK Docs: https://google.github.io/adk-docs/\n- Agent Starter Pack: https://github.com/GoogleCloudPlatform/agent-starter-pack\n- Agent Engine: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\n- Agent Builder: https://cloud.google.com/products/agent-builder\n\n**Tutorials:**\n- Building AI Agents: https://codelabs.developers.google.com/devsite/codelabs/building-ai-agents-vertexai\n- Multi-agent Systems: https://cloud.google.com/blog/products/ai-machine-learning/build-and-manage-multi-system-agents-with-vertex-ai\n\n## When This Skill Activates\n\nThis skill automatically activates when you mention:\n- Agent development, ADK, or Agent Starter Pack\n- Multi-agent systems or orchestration\n- Containerized agent deployment\n- Cloud Run, GKE, or Agent Engine deployment\n- RAG agents or ReAct agents\n- Agent templates or scaffolding\n- CI/CD for agents\n- Production agent systems\n\n## Integration with Other Services\n\n**Google Cloud:**\n- Vertex AI (Gemini, Search, Vector Search)\n- Cloud Storage (data storage)\n- Cloud Functions (triggers)\n- Cloud Scheduler (automation)\n- Cloud Logging & Monitoring\n\n**Third-party:**\n- LangChain integration\n- CrewAI orchestration\n- Custom tool frameworks\n\n## Success Metrics\n\n**Track:**\n- Agent response time (target: <2s)\n- Evaluation scores (target: >85% accuracy)\n- Deployment frequency (target: daily)\n- System uptime (target: 99.9%)\n- Cost per query (target: <$0.01)\n\n---\n\n**This skill makes Jeremy a Google Cloud agent architecture expert with instant access to ADK, Agent Starter Pack, and production deployment patterns.**\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nIf execution fails:\n- Verify prerequisites are met\n- Check input parameters and formats\n- Validate file paths and permissions\n- Review error messages for root cause\n- Consult documentation for troubleshooting\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "004-jeremy-google-cloud-agent-sdk",
        "category": "productivity",
        "path": "plugins/productivity/004-jeremy-google-cloud-agent-sdk",
        "version": "1.0.0",
        "description": "Google Cloud Agent Development Kit (ADK) and Agent Starter Pack mastery - build containerized multi-agent systems with production-ready templates, deploy to Cloud Run/GKE/Agent Engine, RAG agents, ReAct agents, and multi-agent orchestration."
      },
      "filePath": "plugins/productivity/004-jeremy-google-cloud-agent-sdk/skills/agent-sdk-master/SKILL.md"
    },
    {
      "slug": "handling-api-errors",
      "name": "handling-api-errors",
      "description": "Implement standardized error handling with proper HTTP status codes and error responses. Use when implementing standardized error handling. Trigger with phrases like \"add error handling\", \"standardize errors\", or \"implement error responses\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:error-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:error-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-error-handler",
        "category": "api-development",
        "path": "plugins/api-development/api-error-handler",
        "version": "1.0.0",
        "description": "Implement standardized error handling with proper HTTP status codes"
      },
      "filePath": "plugins/api-development/api-error-handler/skills/api-error-handler/SKILL.md"
    },
    {
      "slug": "implementing-backup-strategies",
      "name": "implementing-backup-strategies",
      "description": "Use when you need to work with backup and recovery. This skill provides backup automation and disaster recovery with comprehensive guidance and automation. Trigger with phrases like \"create backups\", \"automate backups\", or \"implement disaster recovery\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/backup-strategy-implementor/`\n\n**Documentation and Guides**: `{baseDir}/docs/backup-strategy-implementor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/backup-strategy-implementor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/backup-strategy-implementor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/backup-strategy-implementor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/backup-strategy-implementor-dashboard.json`",
      "parentPlugin": {
        "name": "backup-strategy-implementor",
        "category": "devops",
        "path": "plugins/devops/backup-strategy-implementor",
        "version": "1.0.0",
        "description": "Implement backup strategies for databases and applications"
      },
      "filePath": "plugins/devops/backup-strategy-implementor/skills/backup-strategy-implementor/SKILL.md"
    },
    {
      "slug": "implementing-database-audit-logging",
      "name": "implementing-database-audit-logging",
      "description": "Use when you need to track database changes for compliance and security monitoring. This skill implements audit logging using triggers, application-level logging, CDC, or native logs. Trigger with phrases like \"implement database audit logging\", \"add audit trails\", \"track database changes\", or \"monitor database activity for compliance\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Database credentials with CREATE TABLE and CREATE TRIGGER permissions\n- Understanding of compliance requirements (GDPR, HIPAA, SOX, PCI-DSS)\n- Sufficient storage for audit logs (estimate 10-30% of data size)\n- Decision on audit log retention period\n- Access to database documentation for table schemas\n- Monitoring tools configured for audit log analysis\n\n## Instructions\n\n### Step 1: Define Audit Requirements\n1. Identify tables requiring audit logging based on compliance needs\n2. Determine events to audit (INSERT, UPDATE, DELETE, SELECT for sensitive data)\n3. Define which columns contain sensitive data requiring audit\n4. Document retention requirements for audit logs\n5. Identify users/roles whose actions need auditing\n\n### Step 2: Choose Audit Strategy\n1. **Trigger-Based Auditing**: Best for comprehensive row-level tracking\n   - Pros: Automatic, no application changes, captures all changes\n   - Cons: Performance overhead, complex trigger maintenance\n2. **Application-Level Auditing**: Best for selective auditing\n   - Pros: Flexible, lower database overhead, easier debugging\n   - Cons: Requires application changes, can miss direct database changes\n3. **Change Data Capture (CDC)**: Best for real-time streaming\n   - Pros: Minimal performance impact, real-time analysis, external processing\n   - Cons: Complex setup, requires CDC infrastructure\n4. **Native Database Logs**: Best for general monitoring\n   - Pros: No setup, captures everything, built-in\n   - Cons: High volume, limited retention, difficult to query\n\n### Step 3: Design Audit Table Schema\n1. Create audit log table with these core columns:\n   - audit_id (primary key), table_name, action (INSERT/UPDATE/DELETE)\n   - record_id (reference to audited record), old_values (JSON), new_values (JSON)\n   - changed_by (user), changed_at (timestamp), client_ip, application_context\n2. Add indexes on table_name, changed_at, changed_by for query performance\n3. Partition audit table by date for efficient archival\n4. Configure tablespace for audit logs separate from primary data\n\n### Step 4: Implement Audit Mechanism\n1. For trigger-based: Create AFTER INSERT/UPDATE/DELETE triggers on each table\n2. Capture old and new row values as JSON in trigger body\n3. Record user context (CURRENT_USER, application user, IP address)\n4. Handle trigger failures gracefully (log but don't block operations)\n5. Test triggers with sample data modifications\n\n### Step 5: Configure Audit Log Management\n1. Set up automated archival of old audit logs to cold storage\n2. Implement audit log analysis queries for common compliance reports\n3. Create alerts for suspicious activities (bulk deletes, off-hours changes)\n4. Document audit log query procedures for compliance auditors\n5. Schedule periodic audit log reviews with security team\n\n### Step 6: Validate Audit Implementation\n1. Perform test operations on audited tables\n2. Verify audit log entries are created with complete data\n3. Test audit log queries for performance\n4. Confirm audit logs cannot be modified by regular users\n5. Document audit implementation for compliance documentation\n\n## Output\n\nThis skill produces:\n\n**Audit Table Schema**: SQL DDL for audit log table with proper indexes and partitioning\n\n**Audit Triggers**: Database triggers for automatic audit log population on data changes\n\n**Audit Log Queries**: Pre-built SQL queries for compliance reports and change tracking\n\n**Implementation Documentation**: Configuration details, trigger logic, and maintenance procedures\n\n**Compliance Report Templates**: SQL queries for GDPR access logs, SOX change reports, etc.\n\n## Error Handling\n\n**Trigger Performance Issues**:\n- Audit only critical tables, not all tables\n- Use asynchronous audit logging with queue systems\n- Batch audit log inserts instead of individual inserts\n- Monitor trigger execution time and optimize trigger logic\n\n**Audit Table Growth**:\n- Implement automated archival of audit logs older than retention period\n- Partition audit table by month or quarter\n- Compress old audit log partitions\n- Move historical audit logs to cheaper storage tiers\n\n**Missing Audit Context**:\n- Set application context in database session before operations\n- Use database session variables to pass user identity\n- Implement connection pooling with session initialization\n- Log application user separately from database user\n\n**Permission Issues**:\n- Ensure audit log table is writable by trigger execution context\n- Grant INSERT on audit table to all database users\n- Protect audit table from modifications (no UPDATE/DELETE grants)\n- Use separate schema for audit tables with restricted access\n\n## Resources\n\n**Audit Table Templates**:\n- PostgreSQL audit trigger: `{baseDir}/templates/postgresql-audit-trigger.sql`\n- MySQL audit trigger: `{baseDir}/templates/mysql-audit-trigger.sql`\n- Audit table schema: `{baseDir}/templates/audit-table-schema.sql`\n\n**Compliance Report Queries**: `{baseDir}/queries/compliance-reports/`\n- GDPR data access report\n- SOX change audit report\n- User activity summary\n- Suspicious activity detection\n\n**Audit Strategy Guide**: `{baseDir}/docs/audit-strategy-selection.md`\n**Performance Tuning**: `{baseDir}/docs/audit-performance-optimization.md`\n**Archival Procedures**: `{baseDir}/scripts/audit-archival.sh`",
      "parentPlugin": {
        "name": "database-audit-logger",
        "category": "database",
        "path": "plugins/database/database-audit-logger",
        "version": "1.0.0",
        "description": "Database plugin for database-audit-logger"
      },
      "filePath": "plugins/database/database-audit-logger/skills/database-audit-logger/SKILL.md"
    },
    {
      "slug": "implementing-database-caching",
      "name": "implementing-database-caching",
      "description": "Use when you need to implement multi-tier caching to improve database performance. This skill sets up Redis, in-memory caching, and CDN layers to reduce database load. Trigger with phrases like \"implement database caching\", \"add Redis cache layer\", \"improve query performance with caching\", or \"reduce database load\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Redis server available or ability to deploy Redis container\n- Understanding of application data access patterns and hotspots\n- Knowledge of which queries/data benefit most from caching\n- Monitoring tools to measure cache hit rates and performance\n- Development environment for testing caching implementation\n- Understanding of cache invalidation requirements for data consistency\n\n## Instructions\n\n### Step 1: Analyze Caching Requirements\n1. Profile database queries to identify slow or frequently executed queries\n2. Determine which data is read-heavy vs write-heavy\n3. Identify data that can tolerate eventual consistency\n4. Calculate expected cache size and Redis memory requirements\n5. Document current database load and target performance metrics\n\n### Step 2: Choose Caching Strategy\n1. **Cache-Aside (Lazy Loading)**: Application checks cache first, loads from DB on miss\n   - Best for: Read-heavy workloads, unpredictable access patterns\n   - Pros: Only caches requested data, simple to implement\n   - Cons: Cache misses incur database hit, stale data possible\n2. **Write-Through**: Application writes to cache and database simultaneously\n   - Best for: Write-heavy workloads needing consistency\n   - Pros: Cache always consistent, no stale data\n   - Cons: Write latency, unnecessary caching of rarely-read data\n3. **Write-Behind (Write-Back)**: Application writes to cache, async writes to database\n   - Best for: High write throughput requirements\n   - Pros: Low write latency, batched database writes\n   - Cons: Risk of data loss, complexity in implementation\n\n### Step 3: Design Cache Architecture\n1. Set up Redis as distributed cache layer (L2 cache)\n2. Implement in-memory LRU cache in application (L1 cache)\n3. Configure CDN for static assets (images, CSS, JS)\n4. Design cache key naming convention (e.g., `user:123:profile`)\n5. Define TTL (Time To Live) for different data types\n\n### Step 4: Implement Caching Code\n1. Add Redis client library to application dependencies\n2. Create cache wrapper functions (get, set, delete, invalidate)\n3. Modify database query code to check cache before DB query\n4. Implement cache population on cache miss\n5. Add error handling for cache failures (fail gracefully to database)\n\n### Step 5: Configure Cache Invalidation\n1. Implement TTL-based expiration for time-sensitive data\n2. Add explicit cache invalidation on data updates/deletes\n3. Use cache tags or patterns for bulk invalidation\n4. Implement cache warming for critical data after deployments\n5. Set up cache stampede prevention (lock/queue on miss)\n\n### Step 6: Monitor and Optimize\n1. Track cache hit rate, miss rate, and eviction rate\n2. Monitor Redis memory usage and eviction policy\n3. Analyze query performance improvements\n4. Adjust TTLs based on data update frequency\n5. Identify and cache additional hot data\n\n## Output\n\nThis skill produces:\n\n**Redis Configuration**: Docker Compose or config files for Redis deployment with appropriate memory and eviction settings\n\n**Caching Code**: Application code implementing cache-aside, write-through, or write-behind patterns\n\n**Cache Key Schema**: Documentation of cache key naming conventions and TTL settings\n\n**Monitoring Dashboards**: Metrics for cache hit rates, memory usage, and performance improvements\n\n**Cache Invalidation Logic**: Code for explicit and implicit cache invalidation on data changes\n\n## Error Handling\n\n**Cache Connection Failures**:\n- Implement circuit breaker pattern to prevent cascading failures\n- Fall back to database when cache is unavailable\n- Log cache connection errors for monitoring\n- Retry cache connections with exponential backoff\n- Consider read-replica or cache cluster for high availability\n\n**Cache Stampede**:\n- Implement probabilistic early expiration (PER) for TTLs\n- Use distributed locks (Redis SETNX) to prevent concurrent cache population\n- Queue cache refresh requests instead of parallel execution\n- Add jitter to TTLs to spread expiration times\n- Use stale-while-revalidate pattern for acceptable delays\n\n**Stale Data Issues**:\n- Implement versioning in cache keys (e.g., `user:123:v2`)\n- Use cache tags for related data invalidation\n- Set aggressive TTLs for frequently changing data\n- Implement active cache invalidation on data updates\n- Monitor data consistency between cache and database\n\n**Memory Pressure**:\n- Configure Redis eviction policy (allkeys-lru recommended)\n- Monitor Redis memory usage and set max memory limits\n- Implement tiered caching (hot data in Redis, warm data in DB)\n- Reduce TTLs for less critical data\n- Scale Redis horizontally with cluster mode\n\n## Resources\n\n**Redis Configuration Templates**:\n- Docker Compose: `{baseDir}/docker/redis-compose.yml`\n- Redis config: `{baseDir}/config/redis.conf`\n- Cluster config: `{baseDir}/config/redis-cluster.conf`\n\n**Caching Code Examples**: `{baseDir}/examples/caching/`\n- Cache-aside pattern (Node.js, Python, Java)\n- Write-through pattern\n- Cache invalidation strategies\n- Distributed locking\n\n**Cache Key Design Guide**: `{baseDir}/docs/cache-key-design.md`\n**Performance Tuning**: `{baseDir}/docs/cache-performance-tuning.md`\n**Monitoring Setup**: `{baseDir}/monitoring/redis-dashboard.json`",
      "parentPlugin": {
        "name": "database-cache-layer",
        "category": "database",
        "path": "plugins/database/database-cache-layer",
        "version": "1.0.0",
        "description": "Database plugin for database-cache-layer"
      },
      "filePath": "plugins/database/database-cache-layer/skills/database-cache-layer/SKILL.md"
    },
    {
      "slug": "implementing-real-user-monitoring",
      "name": "implementing-real-user-monitoring",
      "description": "Implement Real User Monitoring (RUM) to capture actual user performance data including Core Web Vitals and page load times. Use when setting up user experience monitoring or tracking custom performance events. Trigger with phrases like \"setup RUM\", \"track Core Web Vitals\", or \"monitor real user performance\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(npm:*)",
        "Bash(rum:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill streamlines the process of setting up Real User Monitoring (RUM) for web applications. It guides you through the essential steps of choosing a platform, defining metrics, and implementing the tracking code to capture valuable user experience data.\n\n## How It Works\n\n1. **Platform Selection**: Helps you consider available RUM platforms (e.g., Google Analytics, Datadog RUM, New Relic).\n2. **Instrumentation Design**: Guides you in defining the key performance metrics to track, including Core Web Vitals and custom events.\n3. **Tracking Code Implementation**: Assists in implementing the necessary JavaScript code to collect and transmit performance data.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement Real User Monitoring on a website or web application.\n- Track Core Web Vitals (LCP, FID, CLS) to improve user experience.\n- Monitor page load times (FCP, TTI, TTFB) for performance optimization.\n\n## Examples\n\n### Example 1: Setting up RUM for a new website\n\nUser request: \"setup RUM for my new website\"\n\nThe skill will:\n1. Guide the user through selecting a RUM platform.\n2. Provide code snippets for implementing basic tracking.\n\n### Example 2: Tracking custom performance metrics\n\nUser request: \"I want to track how long it takes users to complete a purchase\"\n\nThe skill will:\n1. Help define a custom performance metric for purchase completion time.\n2. Generate JavaScript code to track the metric.\n\n## Best Practices\n\n- **Privacy Compliance**: Ensure compliance with privacy regulations (e.g., GDPR, CCPA) when collecting user data.\n- **Sampling**: Implement sampling to reduce data volume and impact on performance.\n- **Error Handling**: Implement robust error handling to prevent tracking code from breaking the website.\n\n## Integration\n\nThis skill can be used in conjunction with other monitoring and analytics tools to provide a comprehensive view of application performance.\n\n## Prerequisites\n\n- Access to web application frontend code in {baseDir}/\n- RUM platform account (Google Analytics, Datadog, New Relic)\n- Understanding of Core Web Vitals metrics\n- Privacy compliance documentation (GDPR, CCPA)\n\n## Instructions\n\n1. Select appropriate RUM platform for requirements\n2. Define key metrics to track (Core Web Vitals, custom events)\n3. Implement tracking code in application frontend\n4. Configure data sampling and privacy settings\n5. Set up dashboards for metric visualization\n6. Define alerts for performance degradation\n\n## Output\n\n- RUM implementation code snippets\n- Platform configuration documentation\n- Custom event tracking examples\n- Dashboard definitions for key metrics\n- Privacy compliance checklist\n\n## Error Handling\n\nIf RUM implementation fails:\n- Verify platform API credentials\n- Check JavaScript bundle integration\n- Validate metric collection permissions\n- Review privacy consent configuration\n- Ensure network connectivity for data transmission\n\n## Resources\n\n- Core Web Vitals measurement guide\n- RUM platform documentation\n- Privacy compliance best practices\n- Performance monitoring strategies",
      "parentPlugin": {
        "name": "real-user-monitoring",
        "category": "performance",
        "path": "plugins/performance/real-user-monitoring",
        "version": "1.0.0",
        "description": "Implement Real User Monitoring for actual performance data"
      },
      "filePath": "plugins/performance/real-user-monitoring/skills/real-user-monitoring/SKILL.md"
    },
    {
      "slug": "integrating-secrets-managers",
      "name": "integrating-secrets-managers",
      "description": "This skill enables claude to seamlessly integrate with various secrets",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to automate the integration of secrets managers into your infrastructure. It generates the necessary configuration files and setup code, ensuring a secure and efficient workflow for managing sensitive credentials.\n\n## How It Works\n\n1. **Identify Requirements**: Claude analyzes the user's request to determine the specific secrets manager and desired configurations.\n2. **Generate Configuration**: Based on the identified requirements, Claude generates the appropriate configuration files (e.g., Vault policies, AWS IAM roles) and setup code.\n3. **Provide Instructions**: Claude provides clear instructions on how to deploy and configure the generated code and integrate it into the existing infrastructure.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Integrate HashiCorp Vault into your infrastructure.\n- Set up AWS Secrets Manager for secure credential storage.\n- Generate configuration files for managing secrets.\n- Implement best practices for secrets management.\n\n## Examples\n\n### Example 1: Integrating Vault with a Kubernetes Cluster\n\nUser request: \"Integrate Vault with my Kubernetes cluster for managing database credentials.\"\n\nThe skill will:\n1. Generate Vault policies for accessing database credentials.\n2. Create Kubernetes service accounts with appropriate annotations for Vault integration.\n3. Provide instructions for deploying the Vault agent injector to the Kubernetes cluster.\n\n### Example 2: Setting up AWS Secrets Manager for API Keys\n\nUser request: \"Set up AWS Secrets Manager to securely store API keys for my application.\"\n\nThe skill will:\n1. Generate an IAM role with permissions to access AWS Secrets Manager.\n2. Create a Secrets Manager secret containing the API keys.\n3. Provide code snippets for retrieving the API keys from Secrets Manager within the application.\n\n## Best Practices\n\n- **Least Privilege**: Generate configurations that grant only the necessary permissions for accessing secrets.\n- **Secure Storage**: Ensure that secrets are stored securely within the chosen secrets manager.\n- **Regular Rotation**: Implement a strategy for regularly rotating secrets to minimize the impact of potential breaches.\n\n## Integration\n\nThis skill can be used in conjunction with other skills for deploying applications, configuring infrastructure, and automating DevOps workflows. It provides a secure foundation for managing sensitive information across your entire infrastructure.",
      "parentPlugin": {
        "name": "secrets-manager-integrator",
        "category": "devops",
        "path": "plugins/devops/secrets-manager-integrator",
        "version": "1.0.0",
        "description": "Integrate with secrets managers (Vault, AWS Secrets Manager, etc)"
      },
      "filePath": "plugins/devops/secrets-manager-integrator/skills/secrets-manager-integrator/SKILL.md"
    },
    {
      "slug": "load-testing-apis",
      "name": "load-testing-apis",
      "description": "Execute comprehensive load and stress testing to validate API performance and scalability. Use when validating API performance under load. Trigger with phrases like \"load test the API\", \"stress test API\", or \"benchmark API performance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:load-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:load-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-load-tester",
        "category": "api-development",
        "path": "plugins/api-development/api-load-tester",
        "version": "1.0.0",
        "description": "Load test APIs with k6, Gatling, or Artillery"
      },
      "filePath": "plugins/api-development/api-load-tester/skills/api-load-tester/SKILL.md"
    },
    {
      "slug": "logging-api-requests",
      "name": "logging-api-requests",
      "description": "Log API requests with correlation IDs, performance metrics, and security audit trails. Use when auditing API requests and responses. Trigger with phrases like \"log API requests\", \"add API logging\", or \"track API calls\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:log-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:log-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-request-logger",
        "category": "api-development",
        "path": "plugins/api-development/api-request-logger",
        "version": "1.0.0",
        "description": "Log API requests with structured logging and correlation IDs"
      },
      "filePath": "plugins/api-development/api-request-logger/skills/api-request-logger/SKILL.md"
    },
    {
      "slug": "managing-api-cache",
      "name": "managing-api-cache",
      "description": "Implement intelligent API response caching with Redis, Memcached, and CDN integration. Use when optimizing API performance with caching. Trigger with phrases like \"add caching\", \"optimize API performance\", or \"implement cache layer\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:cache-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:cache-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-cache-manager",
        "category": "api-development",
        "path": "plugins/api-development/api-cache-manager",
        "version": "1.0.0",
        "description": "Implement caching strategies with Redis, CDN, and HTTP headers"
      },
      "filePath": "plugins/api-development/api-cache-manager/skills/api-cache-manager/SKILL.md"
    },
    {
      "slug": "managing-autonomous-development",
      "name": "managing-autonomous-development",
      "description": "Enables claude to manage sugar's autonomous development workflows. it",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to orchestrate and monitor autonomous development processes within the Sugar environment. It provides a set of commands to create, manage, and execute tasks, ensuring efficient and automated software development workflows.\n\n## How It Works\n\n1. **Command Recognition**: Claude identifies the appropriate Sugar command (e.g., `/sugar-task`, `/sugar-status`, `/sugar-review`, `/sugar-run`).\n2. **Parameter Extraction**: Claude extracts relevant parameters from the user's request, such as task type, priority, and execution flags.\n3. **Execution**: Claude executes the corresponding Sugar command with the extracted parameters, interacting with the Sugar plugin.\n4. **Response Generation**: Claude presents the results of the command execution to the user in a clear and informative manner.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new development task with specific requirements.\n- Check the current status of the Sugar system and task queue.\n- Review and manage pending tasks in the queue.\n- Start or manage the autonomous execution mode.\n\n## Examples\n\n### Example 1: Creating a New Feature Task\n\nUser request: \"/sugar-task Implement user authentication --type feature --priority 4\"\n\nThe skill will:\n1. Parse the request and identify the command as `/sugar-task` with parameters \"Implement user authentication\", `--type feature`, and `--priority 4`.\n2. Execute the `sugar` command to create a new task with the specified parameters.\n3. Confirm the successful creation of the task to the user.\n\n### Example 2: Checking System Status\n\nUser request: \"/sugar-status\"\n\nThe skill will:\n1. Identify the command as `/sugar-status`.\n2. Execute the `sugar` command to retrieve the system status.\n3. Display the system status, including task queue information, to the user.\n\n## Best Practices\n\n- **Clarity**: Always confirm the parameters before executing a command to ensure accuracy.\n- **Safety**: When using `/sugar-run`, strongly advise the user to use `--dry-run --once` first.\n- **Validation**: Recommend validating the Sugar configuration before starting autonomous mode.\n\n## Integration\n\nThis skill integrates directly with the Sugar plugin, leveraging its command-line interface to manage autonomous development workflows. It can be combined with other skills to provide a more comprehensive development experience.",
      "parentPlugin": {
        "name": "sugar",
        "category": "devops",
        "path": "plugins/devops/sugar",
        "version": "unknown",
        "description": ""
      },
      "filePath": "plugins/devops/sugar/skills/sugar/SKILL.md"
    },
    {
      "slug": "managing-container-registries",
      "name": "managing-container-registries",
      "description": "Use when you need to work with containerization. This skill provides container management and orchestration with comprehensive guidance and automation. Trigger with phrases like \"containerize app\", \"manage containers\", or \"orchestrate deployment\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/container-registry-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/container-registry-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/container-registry-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/container-registry-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/container-registry-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/container-registry-manager-dashboard.json`",
      "parentPlugin": {
        "name": "container-registry-manager",
        "category": "devops",
        "path": "plugins/devops/container-registry-manager",
        "version": "1.0.0",
        "description": "Manage container registries (ECR, GCR, Harbor)"
      },
      "filePath": "plugins/devops/container-registry-manager/skills/container-registry-manager/SKILL.md"
    },
    {
      "slug": "managing-database-migrations",
      "name": "managing-database-migrations",
      "description": "Use when you need to work with database migrations. This skill provides schema migration management with comprehensive guidance and automation. Trigger with phrases like \"create migration\", \"run migrations\", or \"manage schema versions\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-migration-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-migration-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-migration-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-migration-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-migration-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-migration-manager-dashboard.json`",
      "parentPlugin": {
        "name": "database-migration-manager",
        "category": "database",
        "path": "plugins/database/database-migration-manager",
        "version": "1.0.0",
        "description": "Manage database migrations with version control, rollback capabilities, and automated schema evolution tracking"
      },
      "filePath": "plugins/database/database-migration-manager/skills/database-migration-manager/SKILL.md"
    },
    {
      "slug": "managing-database-partitions",
      "name": "managing-database-partitions",
      "description": "Use when you need to work with database partitioning. This skill provides table partitioning strategies with comprehensive guidance and automation. Trigger with phrases like \"partition tables\", \"implement partitioning\", or \"optimize large tables\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-partition-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-partition-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-partition-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-partition-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-partition-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-partition-manager-dashboard.json`",
      "parentPlugin": {
        "name": "database-partition-manager",
        "category": "database",
        "path": "plugins/database/database-partition-manager",
        "version": "1.0.0",
        "description": "Database plugin for database-partition-manager"
      },
      "filePath": "plugins/database/database-partition-manager/skills/database-partition-manager/SKILL.md"
    },
    {
      "slug": "managing-database-recovery",
      "name": "managing-database-recovery",
      "description": "Use when you need to work with database operations. This skill provides database management and optimization with comprehensive guidance and automation. Trigger with phrases like \"manage database\", \"optimize database\", or \"configure database\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-recovery-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-recovery-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-recovery-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-recovery-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-recovery-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-recovery-manager-dashboard.json`",
      "parentPlugin": {
        "name": "database-recovery-manager",
        "category": "database",
        "path": "plugins/database/database-recovery-manager",
        "version": "1.0.0",
        "description": "Database plugin for database-recovery-manager"
      },
      "filePath": "plugins/database/database-recovery-manager/skills/database-recovery-manager/SKILL.md"
    },
    {
      "slug": "managing-database-replication",
      "name": "managing-database-replication",
      "description": "Use when you need to work with database scalability. This skill provides replication and sharding with comprehensive guidance and automation. Trigger with phrases like \"set up replication\", \"implement sharding\", or \"scale database\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-replication-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-replication-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-replication-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-replication-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-replication-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-replication-manager-dashboard.json`",
      "parentPlugin": {
        "name": "database-replication-manager",
        "category": "database",
        "path": "plugins/database/database-replication-manager",
        "version": "1.0.0",
        "description": "Manage database replication, failover, and high availability configurations"
      },
      "filePath": "plugins/database/database-replication-manager/skills/database-replication-manager/SKILL.md"
    },
    {
      "slug": "managing-database-sharding",
      "name": "managing-database-sharding",
      "description": "Use when you need to work with database sharding. This skill provides horizontal sharding strategies with comprehensive guidance and automation. Trigger with phrases like \"implement sharding\", \"shard database\", or \"distribute data\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-sharding-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-sharding-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-sharding-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-sharding-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-sharding-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-sharding-manager-dashboard.json`",
      "parentPlugin": {
        "name": "database-sharding-manager",
        "category": "database",
        "path": "plugins/database/database-sharding-manager",
        "version": "1.0.0",
        "description": "Database plugin for database-sharding-manager"
      },
      "filePath": "plugins/database/database-sharding-manager/skills/database-sharding-manager/SKILL.md"
    },
    {
      "slug": "managing-database-tests",
      "name": "managing-database-tests",
      "description": "Database testing including fixtures, transactions, and rollback management. Use when performing specialized testing. Trigger with phrases like \"test the database\", \"run database tests\", or \"validate data integrity\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:db-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:db-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "database-test-manager",
        "category": "testing",
        "path": "plugins/testing/database-test-manager",
        "version": "1.0.0",
        "description": "Database testing utilities with test data setup, transaction rollback, and schema validation"
      },
      "filePath": "plugins/testing/database-test-manager/skills/database-test-manager/SKILL.md"
    },
    {
      "slug": "managing-deployment-rollbacks",
      "name": "managing-deployment-rollbacks",
      "description": "Use when you need to work with deployment and CI/CD. This skill provides deployment automation and orchestration with comprehensive guidance and automation. Trigger with phrases like \"deploy application\", \"create pipeline\", or \"automate deployment\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/deployment-rollback-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/deployment-rollback-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/deployment-rollback-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/deployment-rollback-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/deployment-rollback-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/deployment-rollback-manager-dashboard.json`",
      "parentPlugin": {
        "name": "deployment-rollback-manager",
        "category": "devops",
        "path": "plugins/devops/deployment-rollback-manager",
        "version": "1.0.0",
        "description": "Manage and execute deployment rollbacks with safety checks"
      },
      "filePath": "plugins/devops/deployment-rollback-manager/skills/deployment-rollback-manager/SKILL.md"
    },
    {
      "slug": "managing-environment-configurations",
      "name": "managing-environment-configurations",
      "description": "Use when you need to work with environment configuration. This skill provides environment and configuration management with comprehensive guidance and automation. Trigger with phrases like \"manage environments\", \"configure environments\", or \"sync configurations\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/environment-config-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/environment-config-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/environment-config-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/environment-config-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/environment-config-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/environment-config-manager-dashboard.json`",
      "parentPlugin": {
        "name": "environment-config-manager",
        "category": "devops",
        "path": "plugins/devops/environment-config-manager",
        "version": "1.0.0",
        "description": "Manage environment configurations and secrets across deployments"
      },
      "filePath": "plugins/devops/environment-config-manager/skills/environment-config-manager/SKILL.md"
    },
    {
      "slug": "managing-network-policies",
      "name": "managing-network-policies",
      "description": "Use when managing Kubernetes network policies and firewall rules. Trigger with phrases like \"create network policy\", \"configure firewall rules\", \"restrict pod communication\", or \"setup ingress/egress rules\". Generates Kubernetes NetworkPolicy manifests following least privilege and zero-trust principles.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(kubectl:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Kubernetes cluster has network policy support enabled\n- Network plugin supports policies (Calico, Cilium, Weave)\n- Pod labels are properly defined for policy selectors\n- Understanding of application communication patterns\n- Namespace isolation strategy is defined\n\n## Instructions\n\n1. **Identify Requirements**: Determine which pods need to communicate\n2. **Define Selectors**: Use pod/namespace labels for policy targeting\n3. **Configure Ingress**: Specify allowed incoming traffic sources and ports\n4. **Configure Egress**: Define allowed outgoing traffic destinations\n5. **Test Policies**: Verify connectivity works as expected\n6. **Monitor Denials**: Check for blocked traffic in network plugin logs\n7. **Iterate**: Refine policies based on application behavior\n\n## Output\n\n**Network Policy Examples:**\n```yaml\n# {baseDir}/network-policies/allow-frontend-to-backend.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n      - podSelector:\n          matchLabels:\n            app: frontend\n      ports:\n      - protocol: TCP\n        port: 8080\n---\n# Deny all ingress by default\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n    - Ingress\n```\n\n**Egress Policy:**\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-external-api\nspec:\n  podSelector:\n    matchLabels:\n      app: api-client\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n      - namespaceSelector:\n          matchLabels:\n            name: external-services\n      ports:\n      - protocol: TCP\n        port: 443\n```\n\n## Error Handling\n\n**Policy Not Applied**\n- Error: Traffic still blocked/allowed contrary to policy\n- Solution: Verify network plugin supports policies and policy is applied to correct namespace\n\n**DNS Resolution Fails**\n- Error: Pods cannot resolve DNS after applying policy\n- Solution: Add egress rule allowing DNS traffic to kube-dns/coredns\n\n**No Communication After Policy**\n- Error: All traffic blocked unexpectedly\n- Solution: Check for default-deny policies and ensure explicit allow rules exist\n\n**Label Mismatch**\n- Error: Policy not targeting intended pods\n- Solution: Verify pod labels match policy selectors using `kubectl get pods --show-labels`\n\n## Resources\n\n- Kubernetes NetworkPolicy: https://kubernetes.io/docs/concepts/services-networking/network-policies/\n- Calico documentation: https://docs.projectcalico.org/\n- Example policies in {baseDir}/network-policy-examples/",
      "parentPlugin": {
        "name": "network-policy-manager",
        "category": "devops",
        "path": "plugins/devops/network-policy-manager",
        "version": "1.0.0",
        "description": "Manage Kubernetes network policies and firewall rules"
      },
      "filePath": "plugins/devops/network-policy-manager/skills/network-policy-manager/SKILL.md"
    },
    {
      "slug": "managing-snapshot-tests",
      "name": "managing-snapshot-tests",
      "description": "Create and validate component snapshots for UI regression testing. Use when performing specialized testing. Trigger with phrases like \"update snapshots\", \"test UI snapshots\", or \"validate component snapshots\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:snapshot-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:snapshot-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "snapshot-test-manager",
        "category": "testing",
        "path": "plugins/testing/snapshot-test-manager",
        "version": "1.0.0",
        "description": "Manage and update snapshot tests with intelligent diff analysis and selective updates"
      },
      "filePath": "plugins/testing/snapshot-test-manager/skills/snapshot-test-manager/SKILL.md"
    },
    {
      "slug": "managing-ssltls-certificates",
      "name": "managing-ssltls-certificates",
      "description": "This skill enables claude to manage and monitor ssl/tls certificates",
      "allowedTools": [
        "Read",
        "Bash",
        "Grep",
        "Glob"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to seamlessly interact with the ssl-certificate-manager plugin, facilitating efficient management and monitoring of SSL/TLS certificates. It allows for quick checks of certificate expiry dates, automated renewal processes, and comprehensive listings of installed certificates.\n\n## How It Works\n\n1. **Identify Intent**: Claude analyzes the user's request for keywords related to SSL/TLS certificate management.\n2. **Plugin Activation**: The ssl-certificate-manager plugin is automatically activated.\n3. **Command Execution**: Based on the user's request, Claude executes the appropriate command within the plugin (e.g., checking expiry, renewing certificate, listing certificates).\n4. **Result Presentation**: Claude presents the results of the command execution to the user in a clear and concise format.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Check the expiry date of an SSL/TLS certificate.\n- Renew an SSL/TLS certificate.\n- List all installed SSL/TLS certificates.\n- Investigate SSL/TLS certificate issues.\n\n## Examples\n\n### Example 1: Checking Certificate Expiry\n\nUser request: \"Check the expiry date of my SSL certificate for example.com\"\n\nThe skill will:\n1. Activate the ssl-certificate-manager plugin.\n2. Execute the command to check the expiry date for the specified domain.\n3. Display the expiry date to the user.\n\n### Example 2: Renewing a Certificate\n\nUser request: \"Renew my SSL certificate for api.example.org\"\n\nThe skill will:\n1. Activate the ssl-certificate-manager plugin.\n2. Execute the command to renew the SSL certificate for the specified domain.\n3. Confirm the renewal process to the user.\n\n## Best Practices\n\n- **Specificity**: Provide the full domain name when requesting certificate checks or renewals.\n- **Context**: If encountering errors, provide the full error message to Claude for better troubleshooting.\n- **Verification**: After renewal, always verify the new certificate is correctly installed and functioning.\n\n## Integration\n\nThis skill can be used in conjunction with other security-related plugins to provide a comprehensive security overview. For example, it can be integrated with vulnerability scanning tools to identify potential weaknesses related to outdated or misconfigured certificates.",
      "parentPlugin": {
        "name": "ssl-certificate-manager",
        "category": "security",
        "path": "plugins/security/ssl-certificate-manager",
        "version": "1.0.0",
        "description": "Manage and monitor SSL/TLS certificates"
      },
      "filePath": "plugins/security/ssl-certificate-manager/skills/ssl-certificate-manager/SKILL.md"
    },
    {
      "slug": "managing-test-environments",
      "name": "managing-test-environments",
      "description": "Provision and manage isolated test environments with configuration and data. Use when performing specialized testing. Trigger with phrases like \"manage test environment\", \"provision test env\", or \"setup test infrastructure\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:env-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:env-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "test-environment-manager",
        "category": "testing",
        "path": "plugins/testing/test-environment-manager",
        "version": "1.0.0",
        "description": "Manage test environments with Docker Compose, Testcontainers, and environment isolation"
      },
      "filePath": "plugins/testing/test-environment-manager/skills/test-environment-manager/SKILL.md"
    },
    {
      "slug": "memory",
      "name": "memory",
      "description": "Access and use project memories from previous sessions for context-aware assistance. Use when recalling past decisions, checking project conventions, or understanding user preferences. Trigger with phrases like \"remember when\", \"like before\", or \"what was our decision about\". allowed-tools: Read, Write license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "yldrmahmet",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Project memory file at `{baseDir}/.memories/project_memory.json`\n- Read permissions for the memory storage location\n- Understanding that memories persist across sessions\n- Knowledge of slash commands for manual memory management\n\n## Instructions\n\n### Step 1: Access Project Memories\nRetrieve stored memories from previous sessions:\n1. Locate memory file using Read tool\n2. Parse JSON structure containing memory entries\n3. Identify relevant memories based on current context\n4. Extract applicable decisions, conventions, or preferences\n\n### Step 2: Apply Memories to Current Context\nIntegrate past decisions into current work:\n- Use remembered library/tool choices when making similar decisions\n- Apply architectural patterns established in prior sessions\n- Reference user preferences for coding style or conventions\n- Consider past decisions as context for new features\n\n### Step 3: Update Memories When Needed\nStore new decisions for future reference:\n- Add significant architectural choices\n- Document tool or library selections with rationale\n- Record user preferences and conventions\n- Update changed decisions to avoid conflicts\n\n### Step 4: Resolve Memory Conflicts\nHandle situations where memories conflict with current requests:\n- Prioritize current explicit user requests over stored memories\n- Flag conflicts for user awareness when appropriate\n- Update memories that have become outdated\n- Remove memories that are no longer relevant\n\n## Output\n\nThe skill provides seamless memory-enhanced responses:\n\n### Silent Integration\n- Memories applied automatically without announcement\n- Decisions informed by historical context\n- Consistent behavior aligned with past choices\n- Natural incorporation of established patterns\n\n### Memory Status\nWhen using slash commands:\n- List of all stored memories with timestamps\n- Confirmation of newly added memories\n- Notification of removed or updated memories\n- Summary of applicable memories for current task\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Memory File Not Found**\n- Error: Cannot locate project memory file\n- Solution: Initialize new memory file in standard location, prompt user to set up memory persistence\n\n**Conflicting Memories**\n- Error: Multiple memories contradict each other\n- Solution: Apply most recent memory, allow current request to override, suggest cleanup\n\n**Invalid Memory Format**\n- Error: Memory file corrupted or improperly formatted\n- Solution: Backup existing file, recreate with valid JSON structure, restore recoverable entries\n\n**Permission Denied**\n- Error: Cannot read or write memory file\n- Solution: Check file permissions, request necessary access, use alternative storage location\n\n## Resources\n\n### Memory Management Commands\n- `/remember [text]` - Add new memory to manual_memories array\n- `/forget [text]` - Remove matching memory from storage\n- `/memories` - Display all currently stored memories\n\n### Best Practices\n- Apply memories silently without announcing to user\n- Current explicit requests always override stored memories\n- Store significant decisions that affect future work\n- Regularly review and clean up outdated memories\n- Use memories as context, not rigid constraints\n\n### Memory Categories\n- **Architecture decisions**: Framework choices, design patterns\n- **Tool selections**: Libraries, dependencies, build tools\n- **Code conventions**: Style preferences, naming patterns\n- **User preferences**: Communication style, detail level\n- **Project constraints**: Performance targets, compatibility requirements\n\n### Integration Guidelines\n- Memory retrieval happens automatically during task analysis\n- Memories inform recommendations and implementation choices\n- User can override any memory-based decision at any time\n- Regular memory updates keep context current and relevant",
      "parentPlugin": {
        "name": "claude-never-forgets",
        "category": "community",
        "path": "plugins/community/claude-never-forgets",
        "version": "1.0.0",
        "description": "Persistent memory across sessions. Learns preferences, conventions, and corrections automatically."
      },
      "filePath": "plugins/community/claude-never-forgets/skills/memory/SKILL.md"
    },
    {
      "slug": "migrating-apis",
      "name": "migrating-apis",
      "description": "Migrate APIs between versions, platforms, or frameworks with minimal downtime. Use when upgrading APIs between versions. Trigger with phrases like \"migrate the API\", \"upgrade API version\", or \"migrate to new API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:migrate-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:migrate-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-migration-tool",
        "category": "api-development",
        "path": "plugins/api-development/api-migration-tool",
        "version": "1.0.0",
        "description": "Migrate APIs between versions with backward compatibility"
      },
      "filePath": "plugins/api-development/api-migration-tool/skills/api-migration-tool/SKILL.md"
    },
    {
      "slug": "mocking-apis",
      "name": "mocking-apis",
      "description": "Generate mock API servers for testing and development with realistic response data. Use when creating mock APIs for development and testing. Trigger with phrases like \"create mock API\", \"generate API mock\", or \"setup mock server\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:mock-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:mock-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-mock-server",
        "category": "api-development",
        "path": "plugins/api-development/api-mock-server",
        "version": "1.0.0",
        "description": "Create mock API servers from OpenAPI specs for testing"
      },
      "filePath": "plugins/api-development/api-mock-server/skills/api-mock-server/SKILL.md"
    },
    {
      "slug": "modeling-nosql-data",
      "name": "modeling-nosql-data",
      "description": "Use when you need to work with NoSQL data modeling. This skill provides NoSQL database design with comprehensive guidance and automation. Trigger with phrases like \"model NoSQL data\", \"design document structure\", or \"optimize NoSQL schema\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/nosql-data-modeler/`\n\n**Documentation and Guides**: `{baseDir}/docs/nosql-data-modeler/`\n\n**Example Scripts and Code**: `{baseDir}/examples/nosql-data-modeler/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/nosql-data-modeler-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/nosql-data-modeler-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/nosql-data-modeler-dashboard.json`",
      "parentPlugin": {
        "name": "nosql-data-modeler",
        "category": "database",
        "path": "plugins/database/nosql-data-modeler",
        "version": "1.0.0",
        "description": "Database plugin for nosql-data-modeler"
      },
      "filePath": "plugins/database/nosql-data-modeler/skills/nosql-data-modeler/SKILL.md"
    },
    {
      "slug": "monitoring-apis",
      "name": "monitoring-apis",
      "description": "Build real-time API monitoring dashboards with metrics, alerts, and health checks. Use when tracking API health and performance metrics. Trigger with phrases like \"monitor the API\", \"add API metrics\", or \"setup API monitoring\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:monitor-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:monitor-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-monitoring-dashboard",
        "category": "api-development",
        "path": "plugins/api-development/api-monitoring-dashboard",
        "version": "1.0.0",
        "description": "Create monitoring dashboards for API health, metrics, and alerts"
      },
      "filePath": "plugins/api-development/api-monitoring-dashboard/skills/api-monitoring-dashboard/SKILL.md"
    },
    {
      "slug": "monitoring-cpu-usage",
      "name": "monitoring-cpu-usage",
      "description": "This skill enables claude to monitor and analyze cpu usage patterns within",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to analyze code for CPU-intensive operations, offering detailed optimization recommendations to improve processor utilization. By pinpointing areas of high CPU usage, it facilitates targeted improvements for enhanced application performance.\n\n## How It Works\n\n1. **Initiate CPU Monitoring**: Claude activates the `cpu-usage-monitor` plugin.\n2. **Code Analysis**: The plugin analyzes the codebase for computationally expensive operations, synchronous blocking calls, inefficient loops, and regex patterns.\n3. **Optimization Recommendations**: Claude provides a detailed report outlining areas for optimization, including suggestions for algorithmic improvements, asynchronous processing, and regex optimization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify CPU bottlenecks in your application.\n- Optimize application performance by reducing CPU load.\n- Analyze code for computationally intensive operations.\n\n## Examples\n\n### Example 1: Identifying CPU Hotspots\n\nUser request: \"Monitor CPU usage in my Python script and suggest optimizations.\"\n\nThe skill will:\n1. Analyze the provided Python script for CPU-intensive functions.\n2. Identify potential bottlenecks such as inefficient loops or complex regex patterns.\n3. Provide recommendations for optimizing the code, such as using more efficient algorithms or asynchronous operations.\n\n### Example 2: Analyzing Algorithmic Complexity\n\nUser request: \"Analyze the CPU load of this Java code and identify areas with high algorithmic complexity.\"\n\nThe skill will:\n1. Analyze the provided Java code, focusing on algorithmic complexity (e.g., O(n^2) or worse).\n2. Pinpoint specific methods or sections of code with high complexity.\n3. Suggest alternative algorithms or data structures to improve performance.\n\n## Best Practices\n\n- **Targeted Analysis**: Focus the analysis on specific sections of code known to be CPU-intensive.\n- **Asynchronous Operations**: Consider using asynchronous operations to prevent blocking the main thread.\n- **Regex Optimization**: Carefully review and optimize regular expressions for performance.\n\n## Integration\n\nThis skill can be used in conjunction with other code analysis and refactoring tools to implement the suggested optimizations. It can also be integrated into CI/CD pipelines to automatically monitor CPU usage and identify performance regressions.",
      "parentPlugin": {
        "name": "cpu-usage-monitor",
        "category": "performance",
        "path": "plugins/performance/cpu-usage-monitor",
        "version": "1.0.0",
        "description": "Monitor and analyze CPU usage patterns in applications"
      },
      "filePath": "plugins/performance/cpu-usage-monitor/skills/cpu-usage-monitor/SKILL.md"
    },
    {
      "slug": "monitoring-cross-chain-bridges",
      "name": "monitoring-cross-chain-bridges",
      "description": "Monitor cross-chain bridge security, liquidity, and transaction status across networks. Use when monitoring cross-chain asset transfers. Trigger with phrases like \"monitor bridges\", \"check cross-chain\", or \"track bridge transfers\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:bridge-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:bridge-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "cross-chain-bridge-monitor",
        "category": "crypto",
        "path": "plugins/crypto/cross-chain-bridge-monitor",
        "version": "1.0.0",
        "description": "Monitor cross-chain bridge activity, track transfers, analyze security, and detect bridge exploits"
      },
      "filePath": "plugins/crypto/cross-chain-bridge-monitor/skills/cross-chain-bridge-monitor/SKILL.md"
    },
    {
      "slug": "monitoring-database-health",
      "name": "monitoring-database-health",
      "description": "Use when you need to work with monitoring and observability. This skill provides health monitoring and alerting with comprehensive guidance and automation. Trigger with phrases like \"monitor system health\", \"set up alerts\", or \"track metrics\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-health-monitor/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-health-monitor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-health-monitor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-health-monitor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-health-monitor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-health-monitor-dashboard.json`",
      "parentPlugin": {
        "name": "database-health-monitor",
        "category": "database",
        "path": "plugins/database/database-health-monitor",
        "version": "1.0.0",
        "description": "Database plugin for database-health-monitor"
      },
      "filePath": "plugins/database/database-health-monitor/skills/database-health-monitor/SKILL.md"
    },
    {
      "slug": "monitoring-database-transactions",
      "name": "monitoring-database-transactions",
      "description": "Use when you need to work with monitoring and observability. This skill provides health monitoring and alerting with comprehensive guidance and automation. Trigger with phrases like \"monitor system health\", \"set up alerts\", or \"track metrics\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-transaction-monitor/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-transaction-monitor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-transaction-monitor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-transaction-monitor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-transaction-monitor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-transaction-monitor-dashboard.json`",
      "parentPlugin": {
        "name": "database-transaction-monitor",
        "category": "database",
        "path": "plugins/database/database-transaction-monitor",
        "version": "1.0.0",
        "description": "Database plugin for database-transaction-monitor"
      },
      "filePath": "plugins/database/database-transaction-monitor/skills/database-transaction-monitor/SKILL.md"
    },
    {
      "slug": "monitoring-error-rates",
      "name": "monitoring-error-rates",
      "description": "Monitor and analyze application error rates to improve reliability. Use when tracking errors in applications including HTTP errors, exceptions, and database issues. Trigger with phrases like \"monitor error rates\", \"track application errors\", or \"analyze error patterns\".",
      "allowedTools": [
        "Read",
        "Bash(monitoring:*)",
        "Bash(metrics:*)",
        "Bash(logs:*)",
        "Grep",
        "Glob"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the process of setting up comprehensive error monitoring and alerting for various components of an application. It helps identify, track, and analyze different types of errors, enabling proactive identification and resolution of issues before they impact users.\n\n## How It Works\n\n1. **Analyze Error Sources**: Identifies potential error sources within the application architecture, including HTTP endpoints, database queries, external APIs, background jobs, and client-side code.\n2. **Define Monitoring Criteria**: Establishes specific error types and thresholds for each source, such as HTTP status codes (4xx, 5xx), exception types, query timeouts, and API response failures.\n3. **Configure Alerting**: Sets up alerts to trigger when error rates exceed defined thresholds, notifying relevant teams or individuals for investigation and remediation.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Set up error monitoring for a new application.\n- Analyze existing error rates and identify areas for improvement.\n- Configure alerts to be notified of critical errors in real-time.\n- Establish error budgets and track progress towards reliability goals.\n\n## Examples\n\n### Example 1: Setting up Error Monitoring for a Web Application\n\nUser request: \"Monitor errors in my web application, especially 500 errors and database connection issues.\"\n\nThe skill will:\n1. Analyze the web application's architecture to identify potential error sources (e.g., HTTP endpoints, database connections).\n2. Configure monitoring for 500 errors and database connection failures, setting appropriate thresholds and alerts.\n\n### Example 2: Analyzing Error Rates in a Background Job Processor\n\nUser request: \"Analyze error rates for my background job processor. I'm seeing a lot of failed jobs.\"\n\nThe skill will:\n1. Focus on the background job processor and identify the types of errors occurring (e.g., task failures, timeouts, resource exhaustion).\n2. Analyze the frequency and patterns of these errors to identify potential root causes.\n\n## Best Practices\n\n- **Granularity**: Monitor errors at a granular level to identify specific problem areas.\n- **Thresholding**: Set appropriate alert thresholds to avoid alert fatigue and focus on critical issues.\n- **Context**: Include relevant context in error messages and alerts to facilitate troubleshooting.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools, such as Prometheus, Grafana, and PagerDuty, to provide a comprehensive view of application health and performance. It can also be used in conjunction with incident management tools to streamline incident response workflows.\n\n## Prerequisites\n\n- Access to application logs and metrics\n- Monitoring infrastructure (Prometheus, Grafana, or similar)\n- Read permissions for log files in {baseDir}/logs/\n- Network access to monitoring endpoints\n\n## Instructions\n\n1. Identify error sources by analyzing application architecture\n2. Define error types and monitoring thresholds\n3. Configure alerting rules with appropriate severity levels\n4. Set up dashboards for error rate visualization\n5. Establish notification channels for critical errors\n6. Document error baselines and SLO targets\n\n## Output\n\n- Error rate metrics and trends\n- Alert configurations for critical thresholds\n- Dashboard definitions for error monitoring\n- Reports on error patterns and root causes\n- Recommendations for error reduction strategies\n\n## Error Handling\n\nIf monitoring setup fails:\n- Verify log file permissions and paths\n- Check monitoring service connectivity\n- Validate metric export configurations\n- Review alert rule syntax\n- Ensure notification channels are configured\n\n## Resources\n\n- Monitoring platform documentation (Prometheus, Grafana)\n- Application log format specifications\n- Error taxonomy and classification guides\n- SLO/SLI definition best practices",
      "parentPlugin": {
        "name": "error-rate-monitor",
        "category": "performance",
        "path": "plugins/performance/error-rate-monitor",
        "version": "1.0.0",
        "description": "Monitor and analyze application error rates"
      },
      "filePath": "plugins/performance/error-rate-monitor/skills/error-rate-monitor/SKILL.md"
    },
    {
      "slug": "monitoring-whale-activity",
      "name": "monitoring-whale-activity",
      "description": "Track large crypto transactions and whale wallet movements across blockchains. Use when tracking large holder movements. Trigger with phrases like \"track whales\", \"monitor large transfers\", or \"check whale activity\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:whale-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:whale-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "whale-alert-monitor",
        "category": "crypto",
        "path": "plugins/crypto/whale-alert-monitor",
        "version": "1.0.0",
        "description": "Monitor large crypto transactions and whale wallet movements in real-time"
      },
      "filePath": "plugins/crypto/whale-alert-monitor/skills/whale-alert-monitor/SKILL.md"
    },
    {
      "slug": "ollama-setup",
      "name": "ollama-setup",
      "description": "Auto-configure Ollama when user needs local LLM deployment, free AI alternatives, or wants to eliminate OpenAI/Anthropic API costs. Trigger phrases: \"install ollama\", \"local AI\", \"free LLM\", \"self-hosted AI\", \"replace OpenAI\", \"no API costs\" allowed-tools: Read, Write, Bash version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Ollama Setup Skill\n\n## Purpose\n\nAutomatically detect when users need local AI deployment and guide them through Ollama installation. Eliminates paid API dependencies.\n\n## Activation Triggers\n\n- \"install ollama\"\n- \"local AI models\"\n- \"free alternative to OpenAI\"\n- \"self-hosted LLM\"\n- \"run AI locally\"\n- \"eliminate API costs\"\n- \"privacy-first AI\"\n- \"offline AI models\"\n\n## Skill Workflow\n\n### 1. Detect Need for Local AI\n\nWhen user mentions:\n- High API costs\n- Privacy concerns\n- Offline requirements\n- OpenAI/Anthropic alternatives\n- Self-hosted infrastructure\n\n**‚Üí Activate this skill**\n\n### 2. Assess System Requirements\n\n```bash\n# Check OS\nuname -s\n\n# Check available memory\nfree -h  # Linux\nvm_stat  # macOS\n\n# Check GPU\nnvidia-smi  # NVIDIA\nsystem_profiler SPDisplaysDataType  # macOS\n```\n\n### 3. Recommend Appropriate Models\n\n**8GB RAM:**\n- llama3.2:7b (4GB)\n- mistral:7b (4GB)\n- phi3:14b (8GB)\n\n**16GB RAM:**\n- codellama:13b (7GB)\n- mixtral:8x7b (26GB quantized)\n\n**32GB+ RAM:**\n- llama3.2:70b (40GB)\n- codellama:34b (20GB)\n\n### 4. Installation Process\n\n**macOS:**\n```bash\nbrew install ollama\nbrew services start ollama\nollama pull llama3.2\n```\n\n**Linux:**\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\nsudo systemctl start ollama\nollama pull llama3.2\n```\n\n**Docker:**\n```bash\ndocker run -d \\\\\n  -v ollama:/root/.ollama \\\\\n  -p 11434:11434 \\\\\n  --name ollama \\\\\n  ollama/ollama\n\ndocker exec -it ollama ollama pull llama3.2\n```\n\n### 5. Verify Installation\n\n```bash\nollama list\nollama run llama3.2 \"Say hello\"\ncurl http://localhost:11434/api/tags\n```\n\n### 6. Integration Examples\n\n**Python:**\n```python\nimport ollama\n\nresponse = ollama.chat(\n    model='llama3.2',\n    messages=[{'role': 'user', 'content': 'Hello!'}]\n)\nprint(response['message']['content'])\n```\n\n**Node.js:**\n```javascript\nconst ollama = require('ollama')\n\nconst response = await ollama.chat({\n  model: 'llama3.2',\n  messages: [{ role: 'user', content: 'Hello!' }]\n})\n```\n\n**cURL:**\n```bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Hello!\"\n}'\n```\n\n## Common Use Cases\n\n### Replace OpenAI\n```python\n# Before (Paid)\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"...\")\n\n# After (Free)\nimport ollama\nresponse = ollama.chat(model='llama3.2', ...)\n```\n\n### Replace Anthropic Claude\n```python\n# Before (Paid)\nfrom anthropic import Anthropic\nclient = Anthropic(api_key=\"...\")\n\n# After (Free)\nimport ollama\nresponse = ollama.chat(model='mistral', ...)\n```\n\n### Code Generation\n```bash\nollama pull codellama\nollama run codellama \"Write a Python REST API\"\n```\n\n## Cost Comparison\n\n**OpenAI GPT-4:**\n- Input: $0.03/1K tokens\n- Output: $0.06/1K tokens\n- 1M tokens/month = $30-60\n\n**Ollama:**\n- Setup: Free\n- Usage: $0 (hardware you already own)\n- Savings: $30-60/month ‚úì\n\n## Performance Expectations\n\n**With GPU (NVIDIA/Metal):**\n- 7B models: 50-100 tokens/sec\n- 13B models: 30-60 tokens/sec\n- 34B models: 20-40 tokens/sec\n\n**CPU Only:**\n- 7B models: 10-20 tokens/sec\n- 13B models: 5-10 tokens/sec\n- 34B models: 2-5 tokens/sec\n\n## Troubleshooting\n\n### Out of Memory\n```bash\n# Use quantized models\nollama pull llama3.2:7b-q4  # 4-bit (smaller)\n```\n\n### Slow Performance\n```bash\n# Use smaller model\nollama pull mistral:7b  # Faster than 70B\n```\n\n### Model Not Found\n```bash\n# Pull model first\nollama pull llama3.2\nollama list  # Verify\n```\n\n## Success Metrics\n\nAfter skill execution, user should have:\n- ‚úÖ Ollama installed and running\n- ‚úÖ At least one model downloaded\n- ‚úÖ Successful test inference\n- ‚úÖ Integration code examples\n- ‚úÖ Zero ongoing API costs\n\n## Privacy Benefits\n\n- Data never leaves local machine\n- No API keys required\n- No usage tracking\n- GDPR/HIPAA compliant (local only)\n- Offline capable\n\n## When NOT to Use This Skill\n\n- User needs latest GPT-4 specifically\n- User has <8GB RAM\n- User needs real-time updates (like web search)\n- User wants Claude Code itself (requires Anthropic API)\n\n## Related Skills\n\n- `local-llm-wrapper` - Generic local LLM integration\n- `ai-sdk-agents` - AI SDK with Ollama support\n- `privacy-first-ai` - Privacy-focused AI workflows",
      "parentPlugin": {
        "name": "ollama-local-ai",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ollama-local-ai",
        "version": "1.0.0",
        "description": "Run AI models locally with Ollama - free alternative to OpenAI, Anthropic, and other paid LLM APIs. Zero-cost, privacy-first AI infrastructure."
      },
      "filePath": "plugins/ai-ml/ollama-local-ai/skills/ollama-setup/SKILL.md"
    },
    {
      "slug": "optimizing-cache-performance",
      "name": "optimizing-cache-performance",
      "description": "This skill enables claude to analyze and improve application caching",
      "allowedTools": [
        "Read",
        "Write",
        "Bash",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to diagnose and resolve caching-related performance issues. It guides users through a comprehensive optimization process, ensuring efficient use of caching resources.\n\n## How It Works\n\n1. **Identify Caching Implementation**: Locates the caching implementation within the project (e.g., Redis, Memcached, in-memory caches).\n2. **Analyze Cache Configuration**: Examines the existing cache configuration, including TTL values, eviction policies, and key structures.\n3. **Recommend Optimizations**: Suggests improvements to cache hit rates, TTLs, key design, invalidation strategies, and memory usage.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Improve application performance by optimizing caching mechanisms.\n- Identify and resolve caching-related bottlenecks.\n- Review and improve cache key design for better hit rates.\n\n## Examples\n\n### Example 1: Optimizing Redis Cache\n\nUser request: \"Optimize Redis cache performance.\"\n\nThe skill will:\n1. Analyze the Redis configuration, including TTLs and memory usage.\n2. Recommend optimal TTL values based on data access patterns.\n\n### Example 2: Improving Cache Hit Rate\n\nUser request: \"Improve cache hit rate in my application.\"\n\nThe skill will:\n1. Analyze cache key design and identify potential areas for improvement.\n2. Suggest more effective cache key structures to increase hit rates.\n\n## Best Practices\n\n- **TTL Management**: Set appropriate TTL values to balance data freshness and cache hit rates.\n- **Key Design**: Use consistent and well-structured cache keys for efficient retrieval.\n- **Invalidation Strategies**: Implement proper cache invalidation strategies to avoid serving stale data.\n\n## Integration\n\nThis skill can integrate with code analysis tools to automatically identify caching implementations and configuration. It can also work with monitoring tools to track cache hit rates and performance metrics.",
      "parentPlugin": {
        "name": "cache-performance-optimizer",
        "category": "performance",
        "path": "plugins/performance/cache-performance-optimizer",
        "version": "1.0.0",
        "description": "Optimize caching strategies for improved performance"
      },
      "filePath": "plugins/performance/cache-performance-optimizer/skills/cache-performance-optimizer/SKILL.md"
    },
    {
      "slug": "optimizing-cloud-costs",
      "name": "optimizing-cloud-costs",
      "description": "Use when you need to work with cloud cost optimization. This skill provides cost analysis and optimization with comprehensive guidance and automation. Trigger with phrases like \"optimize costs\", \"analyze spending\", or \"reduce costs\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/cloud-cost-optimizer/`\n\n**Documentation and Guides**: `{baseDir}/docs/cloud-cost-optimizer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/cloud-cost-optimizer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/cloud-cost-optimizer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/cloud-cost-optimizer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/cloud-cost-optimizer-dashboard.json`",
      "parentPlugin": {
        "name": "cloud-cost-optimizer",
        "category": "devops",
        "path": "plugins/devops/cloud-cost-optimizer",
        "version": "1.0.0",
        "description": "Optimize cloud costs and generate cost reports"
      },
      "filePath": "plugins/devops/cloud-cost-optimizer/skills/cloud-cost-optimizer/SKILL.md"
    },
    {
      "slug": "optimizing-database-connection-pooling",
      "name": "optimizing-database-connection-pooling",
      "description": "Use when you need to work with connection management. This skill provides connection pooling and management with comprehensive guidance and automation. Trigger with phrases like \"manage connections\", \"configure pooling\", or \"optimize connection usage\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-connection-pooler/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-connection-pooler/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-connection-pooler/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-connection-pooler-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-connection-pooler-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-connection-pooler-dashboard.json`",
      "parentPlugin": {
        "name": "database-connection-pooler",
        "category": "database",
        "path": "plugins/database/database-connection-pooler",
        "version": "1.0.0",
        "description": "Implement and optimize database connection pooling for improved performance and resource management"
      },
      "filePath": "plugins/database/database-connection-pooler/skills/database-connection-pooler/SKILL.md"
    },
    {
      "slug": "optimizing-deep-learning-models",
      "name": "optimizing-deep-learning-models",
      "description": "Optimize deep learning models using Adam, SGD, and learning rate scheduling",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to automatically optimize deep learning models, enhancing their performance and efficiency. It intelligently applies various optimization techniques based on the model's characteristics and the user's objectives.\n\n## How It Works\n\n1. **Analyze Model**: Examines the deep learning model's architecture, training data, and performance metrics.\n2. **Identify Optimizations**: Determines the most effective optimization strategies based on the analysis, such as adjusting the learning rate, applying regularization techniques, or modifying the optimizer.\n3. **Apply Optimizations**: Generates optimized code that implements the chosen strategies.\n4. **Evaluate Performance**: Assesses the impact of the optimizations on model performance, providing metrics like accuracy, training time, and resource consumption.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Optimize the performance of a deep learning model.\n- Reduce the training time of a deep learning model.\n- Improve the accuracy of a deep learning model.\n- Optimize the learning rate for a deep learning model.\n- Reduce resource consumption during deep learning model training.\n\n## Examples\n\n### Example 1: Improving Model Accuracy\n\nUser request: \"Optimize this deep learning model for improved image classification accuracy.\"\n\nThe skill will:\n1. Analyze the model and identify potential areas for improvement, such as adjusting the learning rate or adding regularization.\n2. Apply the selected optimization techniques and generate optimized code.\n3. Evaluate the model's performance and report the improved accuracy.\n\n### Example 2: Reducing Training Time\n\nUser request: \"Reduce the training time of this deep learning model.\"\n\nThe skill will:\n1. Analyze the model and identify bottlenecks in the training process.\n2. Apply techniques like batch size adjustment or optimizer selection to reduce training time.\n3. Evaluate the model's performance and report the reduced training time.\n\n## Best Practices\n\n- **Optimizer Selection**: Experiment with different optimizers (e.g., Adam, SGD) to find the best fit for the model and dataset.\n- **Learning Rate Scheduling**: Implement learning rate scheduling to dynamically adjust the learning rate during training.\n- **Regularization**: Apply regularization techniques (e.g., L1, L2 regularization) to prevent overfitting.\n\n## Integration\n\nThis skill can be integrated with other plugins that provide model building and data preprocessing capabilities. It can also be used in conjunction with monitoring tools to track the performance of optimized models.",
      "parentPlugin": {
        "name": "deep-learning-optimizer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/deep-learning-optimizer",
        "version": "1.0.0",
        "description": "Deep learning optimization techniques"
      },
      "filePath": "plugins/ai-ml/deep-learning-optimizer/skills/deep-learning-optimizer/SKILL.md"
    },
    {
      "slug": "optimizing-defi-yields",
      "name": "optimizing-defi-yields",
      "description": "Find and compare DeFi yield opportunities across protocols with APY calculations. Use when finding optimal DeFi yield opportunities. Trigger with phrases like \"find yield\", \"optimize returns\", or \"compare APY\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:yield-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:yield-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "defi-yield-optimizer",
        "category": "crypto",
        "path": "plugins/crypto/defi-yield-optimizer",
        "version": "1.0.0",
        "description": "Optimize DeFi yield farming strategies across protocols with APY tracking and risk assessment"
      },
      "filePath": "plugins/crypto/defi-yield-optimizer/skills/defi-yield-optimizer/SKILL.md"
    },
    {
      "slug": "optimizing-gas-fees",
      "name": "optimizing-gas-fees",
      "description": "Predict optimal gas prices and transaction timing to minimize blockchain transaction costs. Use when optimizing blockchain transaction costs. Trigger with phrases like \"optimize gas\", \"check gas prices\", or \"minimize fees\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:gas-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:gas-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "gas-fee-optimizer",
        "category": "crypto",
        "path": "plugins/crypto/gas-fee-optimizer",
        "version": "1.0.0",
        "description": "Optimize transaction gas fees with timing and routing recommendations"
      },
      "filePath": "plugins/crypto/gas-fee-optimizer/skills/gas-fee-optimizer/SKILL.md"
    },
    {
      "slug": "optimizing-prompts",
      "name": "optimizing-prompts",
      "description": "This skill optimizes prompts for large language models (llms) to reduce",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to refine prompts for optimal LLM performance. It streamlines prompts to minimize token count, thereby reducing costs and enhancing response speed, all while maintaining or improving output quality.\n\n## How It Works\n\n1. **Analyzing Prompt**: The skill analyzes the input prompt to identify areas of redundancy, verbosity, and potential for simplification.\n2. **Rewriting Prompt**: It rewrites the prompt using techniques like concise language, targeted instructions, and efficient phrasing.\n3. **Suggesting Alternatives**: The skill provides the optimized prompt along with an explanation of the changes made and their expected impact.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Reduce the cost of using an LLM.\n- Improve the speed of LLM responses.\n- Enhance the quality or clarity of LLM outputs by refining the prompt.\n\n## Examples\n\n### Example 1: Reducing LLM Costs\n\nUser request: \"Optimize this prompt for cost and quality: 'I would like you to create a detailed product description for a new ergonomic office chair, highlighting its features, benefits, and target audience, and also include information about its warranty and return policy.'\"\n\nThe skill will:\n1. Analyze the prompt for redundancies and areas for simplification.\n2. Rewrite the prompt to be more concise: \"Create a product description for an ergonomic office chair. Include features, benefits, target audience, warranty, and return policy.\"\n3. Provide the optimized prompt and explain the token reduction achieved.\n\n### Example 2: Improving Prompt Performance\n\nUser request: \"Optimize this prompt for better summarization: 'Please read the following document and provide a comprehensive summary of all the key points, main arguments, supporting evidence, and overall conclusion, ensuring that the summary is accurate, concise, and easy to understand.'\"\n\nThe skill will:\n1. Identify areas for improvement in the prompt's clarity and focus.\n2. Rewrite the prompt to be more direct: \"Summarize this document, including key points, arguments, evidence, and the conclusion.\"\n3. Present the optimized prompt and explain how it enhances summarization performance.\n\n## Best Practices\n\n- **Clarity**: Ensure the original prompt is clear and well-defined before optimization.\n- **Context**: Provide sufficient context to the skill so it can understand the prompt's purpose.\n- **Iteration**: Iterate on the optimized prompt based on the LLM's output to fine-tune performance.\n\n## Integration\n\nThis skill integrates with the `prompt-architect` agent to leverage advanced prompt engineering techniques. It can also be used in conjunction with the `llm-integration-expert` to optimize prompts for specific LLM APIs.",
      "parentPlugin": {
        "name": "ai-ml-engineering-pack",
        "category": "packages",
        "path": "plugins/packages/ai-ml-engineering-pack",
        "version": "1.0.0",
        "description": "Professional AI/ML Engineering toolkit: Prompt engineering, LLM integration, RAG systems, AI safety with 12 expert plugins"
      },
      "filePath": "plugins/packages/ai-ml-engineering-pack/skills/ai-ml-engineering-pack/SKILL.md"
    },
    {
      "slug": "optimizing-sql-queries",
      "name": "optimizing-sql-queries",
      "description": "Use when you need to work with query optimization. This skill provides query performance analysis with comprehensive guidance and automation. Trigger with phrases like \"optimize queries\", \"analyze performance\", or \"improve query speed\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/sql-query-optimizer/`\n\n**Documentation and Guides**: `{baseDir}/docs/sql-query-optimizer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/sql-query-optimizer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/sql-query-optimizer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/sql-query-optimizer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/sql-query-optimizer-dashboard.json`",
      "parentPlugin": {
        "name": "sql-query-optimizer",
        "category": "database",
        "path": "plugins/database/sql-query-optimizer",
        "version": "1.0.0",
        "description": "Analyze and optimize SQL queries for better performance, suggesting indexes, query rewrites, and execution plan improvements"
      },
      "filePath": "plugins/database/sql-query-optimizer/skills/sql-query-optimizer/SKILL.md"
    },
    {
      "slug": "optimizing-staking-rewards",
      "name": "optimizing-staking-rewards",
      "description": "Compare staking rewards across validators and networks with ROI calculations. Use when optimizing proof-of-stake rewards. Trigger with phrases like \"optimize staking\", \"compare validators\", or \"calculate rewards\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:staking-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:staking-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "staking-rewards-optimizer",
        "category": "crypto",
        "path": "plugins/crypto/staking-rewards-optimizer",
        "version": "1.0.0",
        "description": "Optimize staking rewards across multiple protocols and chains"
      },
      "filePath": "plugins/crypto/staking-rewards-optimizer/skills/staking-rewards-optimizer/SKILL.md"
    },
    {
      "slug": "orchestrating-deployment-pipelines",
      "name": "orchestrating-deployment-pipelines",
      "description": "Use when you need to work with deployment and CI/CD. This skill provides deployment automation and orchestration with comprehensive guidance and automation. Trigger with phrases like \"deploy application\", \"create pipeline\", or \"automate deployment\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/deployment-pipeline-orchestrator/`\n\n**Documentation and Guides**: `{baseDir}/docs/deployment-pipeline-orchestrator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/deployment-pipeline-orchestrator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/deployment-pipeline-orchestrator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/deployment-pipeline-orchestrator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/deployment-pipeline-orchestrator-dashboard.json`",
      "parentPlugin": {
        "name": "deployment-pipeline-orchestrator",
        "category": "devops",
        "path": "plugins/devops/deployment-pipeline-orchestrator",
        "version": "1.0.0",
        "description": "Orchestrate complex multi-stage deployment pipelines"
      },
      "filePath": "plugins/devops/deployment-pipeline-orchestrator/skills/deployment-pipeline-orchestrator/SKILL.md"
    },
    {
      "slug": "orchestrating-multi-agent-systems",
      "name": "orchestrating-multi-agent-systems",
      "description": "Orchestrate multi-agent systems with handoffs, routing, and workflows across AI providers. Use when building complex AI systems requiring agent collaboration, task delegation, or workflow coordination. Trigger with phrases like \"create multi-agent system\", \"orchestrate agents\", or \"coordinate agent workflows\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(npm:*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Node.js 18+ installed for TypeScript agent development\n- AI SDK v5 package installed (`npm install ai`)\n- API keys for AI providers (OpenAI, Anthropic, Google, etc.)\n- Understanding of agent-based architecture patterns\n- TypeScript knowledge for agent implementation\n- Project directory structure for multi-agent systems\n\n## Instructions\n\n### Step 1: Initialize Project Structure\nSet up the foundation for your multi-agent system:\n1. Create project directory with necessary subdirectories\n2. Initialize npm project with TypeScript configuration\n3. Install AI SDK v5 and provider-specific packages\n4. Set up configuration files for agent orchestration\n\n### Step 2: Define Agent Roles\nIdentify and specify specialized agents needed:\n- Determine agent responsibilities and capabilities\n- Define agent system prompts with clear instructions\n- Specify tools each agent can access\n- Establish agent communication protocols\n\n### Step 3: Implement Agents\nCreate individual agent files with proper configuration:\n1. Write agent initialization code with AI SDK\n2. Configure system prompts for agent behavior\n3. Define tool functions for agent capabilities\n4. Implement handoff rules for inter-agent delegation\n\n### Step 4: Configure Orchestration\nSet up coordination between agents:\n- Define workflow sequences for task processing\n- Implement routing logic for task distribution\n- Configure handoff mechanisms between agents\n- Set up state management for multi-step workflows\n\n### Step 5: Test and Refine\nValidate the multi-agent system functionality:\n- Test individual agent responses and behaviors\n- Verify handoff execution between agents\n- Validate routing logic with different input scenarios\n- Monitor coordination and identify bottlenecks\n\n## Output\n\nThe skill generates a complete multi-agent system including:\n\n### Project Structure\n```\n{baseDir}/\n‚îú‚îÄ‚îÄ agents/\n‚îÇ   ‚îú‚îÄ‚îÄ coordinator.ts       # Main orchestration agent\n‚îÇ   ‚îú‚îÄ‚îÄ specialist-1.ts      # Domain-specific agent\n‚îÇ   ‚îú‚îÄ‚îÄ specialist-2.ts      # Domain-specific agent\n‚îÇ   ‚îî‚îÄ‚îÄ [additional agents]\n‚îú‚îÄ‚îÄ orchestration/\n‚îÇ   ‚îú‚îÄ‚îÄ workflow.ts          # Workflow definitions\n‚îÇ   ‚îú‚îÄ‚îÄ routing.ts           # Routing logic\n‚îÇ   ‚îî‚îÄ‚îÄ handoffs.ts          # Handoff configurations\n‚îú‚îÄ‚îÄ tools/\n‚îÇ   ‚îî‚îÄ‚îÄ [agent tools]        # Shared tool implementations\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îî‚îÄ‚îÄ agents.config.ts     # Agent configurations\n‚îî‚îÄ‚îÄ package.json             # Dependencies\n```\n\n### Agent Implementation Files\n- TypeScript files with AI SDK v5 integration\n- System prompts tailored to each agent role\n- Tool definitions and implementations\n- Handoff rules and coordination logic\n\n### Orchestration Configuration\n- Workflow definitions for task sequences\n- Routing rules for intelligent task distribution\n- State management for multi-step processes\n- Error handling and fallback mechanisms\n\n### Documentation\n- Agent role descriptions and capabilities\n- Workflow diagrams showing agent interactions\n- API documentation for agent endpoints\n- Usage examples for common scenarios\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Agent Initialization Failures**\n- Error: AI SDK provider configuration invalid\n- Solution: Verify API keys in environment variables, check provider-specific setup requirements\n\n**Handoff Execution Errors**\n- Error: Agent handoff fails or creates circular dependencies\n- Solution: Review handoff rules for clarity, implement handoff depth limits, add fallback agents\n\n**Routing Logic Failures**\n- Error: Tasks routed to incorrect agent or no agent\n- Solution: Refine routing criteria, add default routing rules, implement topic classification improvement\n\n**Tool Access Violations**\n- Error: Agent attempts to use unauthorized tools\n- Solution: Review tool permissions per agent, implement proper access control, validate tool configurations\n\n**Workflow Deadlocks**\n- Error: Multi-agent workflow stalls without completion\n- Solution: Implement timeout mechanisms, add workflow monitoring, design escape conditions for stuck states\n\n## Resources\n\n### AI SDK Documentation\n- AI SDK v5 official documentation for agent creation\n- Provider-specific integration guides (OpenAI, Anthropic, Google)\n- Tool definition and implementation examples\n- Handoff and routing pattern references\n\n### Multi-Agent Architecture Patterns\n- Coordinator-worker pattern for task distribution\n- Pipeline pattern for sequential processing\n- Hub-and-spoke pattern for centralized coordination\n- Peer-to-peer pattern for collaborative agents\n\n### Agent Design Best Practices\n- Single responsibility principle for agent specialization\n- Clear handoff criteria and routing rules\n- Comprehensive error handling and fallbacks\n- State management for complex workflows\n- Testing strategies for multi-agent systems\n\n### Example Use Cases\n- Code generation pipelines with specialized agents\n- Customer support routing systems\n- Research and analysis workflows\n- Content creation and review pipelines\n- Data processing and validation systems",
      "parentPlugin": {
        "name": "ai-sdk-agents",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ai-sdk-agents",
        "version": "1.0.0",
        "description": "Multi-agent orchestration with AI SDK v5 - handoffs, routing, and coordination for any AI provider (OpenAI, Anthropic, Google)"
      },
      "filePath": "plugins/ai-ml/ai-sdk-agents/skills/ai-sdk-agents/SKILL.md"
    },
    {
      "slug": "orchestrating-test-execution",
      "name": "orchestrating-test-execution",
      "description": "Coordinate parallel test execution across multiple environments and frameworks. Use when performing specialized testing. Trigger with phrases like \"orchestrate tests\", \"run parallel tests\", or \"coordinate test execution\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:orchestrate-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:orchestrate-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "test-orchestrator",
        "category": "testing",
        "path": "plugins/testing/test-orchestrator",
        "version": "1.0.0",
        "description": "Orchestrate complex test workflows with dependencies, parallel execution, and smart test selection"
      },
      "filePath": "plugins/testing/test-orchestrator/skills/test-orchestrator/SKILL.md"
    },
    {
      "slug": "overnight-development",
      "name": "overnight-development",
      "description": "Automates software development overnight using git hooks to enforce test-driven Use when appropriate context detected. Trigger with relevant phrases based on skill purpose.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(general:*)",
        "Bash(util:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Overnight Development\n\n## Overview\n\nThis skill automates software development overnight by leveraging Git hooks to enforce test-driven development (TDD). It ensures that all code changes are fully tested and meet specified quality standards before being committed. This approach allows Claude to work autonomously, building new features, refactoring existing code, or fixing bugs while adhering to a rigorous TDD process.\n\n## Core Capabilities\n\n-   Enforces test-driven development (TDD) using Git hooks.\n-   Automates debugging and code fixing until all tests pass.\n-   Tracks progress and logs activities during overnight sessions.\n-   Supports flexible configuration for various testing frameworks and languages.\n-   Provides guidance and support through the `overnight-dev-coach` agent.\n\n## Workflow\n\n### Phase 1: Project Setup and Configuration\n\nTo prepare the project for overnight development:\n\n1.  **Verify Prerequisites:** Ensure the project is a Git repository, has a configured test framework, and includes at least one passing test.\n    ```bash\n    git init\n    npm install --save-dev jest # Example for Node.js\n    ```\n\n2.  **Install the Plugin:** Add the Claude Code Plugin marketplace and install the `overnight-dev` plugin.\n    ```bash\n    /plugin marketplace add jeremylongshore/claude-code-plugins\n    /plugin install overnight-dev@claude-code-plugins-plus\n    ```\n\n3.  **Run Setup Command:** Execute the `/overnight-setup` command to create necessary Git hooks and configuration files.\n    ```bash\n    /overnight-setup\n    ```\n\n### Phase 2: Task Definition and Planning\n\nTo define the task for the overnight session:\n\n1.  **Define a Clear Goal:** Specify a clear and testable goal for the overnight session, such as \"Build user authentication with JWT (90% coverage).\"\n    ```text\n    Task: Build user authentication with JWT (90% coverage)\n    Success: All tests pass, 90%+ coverage, fully documented\n    ```\n\n2.  **Start Coding:** Begin implementing the feature by writing tests first, following the TDD approach.\n    ```javascript\n    // Example test case (Node.js with Jest)\n    it('should authenticate a user with valid credentials', async () => {\n      // Test implementation\n    });\n    ```\n\n3.  **Attempt to Commit:** Try to commit the changes, which will trigger the Git hooks and run the tests.\n    ```bash\n    git commit -m \"feat: implement user authentication\"\n    ```\n\n### Phase 3: Autonomous Development and Debugging\n\nTo allow Claude to work autonomously:\n\n1.  **Git Hooks Enforcement:** The Git hooks will block the commit if any tests fail, providing Claude with the error messages.\n    ```text\n    Overnight Dev: Running pre-commit checks...\n    Running linting...\n    Linting passed\n    Running tests...\n    12 tests failing\n    Commit blocked!\n    ```\n\n2.  **Automated Debugging:** Claude analyzes the error messages, identifies the issues, and attempts to fix the code.\n    ```text\n    Claude: Fixing test failures in user authentication module.\n    ```\n\n3.  **Retry Commits:** Claude retries the commit after making the necessary fixes, repeating the process until all tests pass.\n    ```bash\n    git commit -m \"fix: address test failures in user authentication\"\n    ```\n\n### Phase 4: Progress Tracking and Completion\n\nTo monitor the progress and finalize the session:\n\n1.  **Monitor Progress:** Track the progress of the overnight session by viewing the log file.\n    ```bash\n    cat .overnight-dev-log.txt\n    ```\n\n2.  **Review Results:** Wake up to fully tested code, complete features, and a clean Git history.\n    ```text\n    7 AM: You wake up to:\n    - 47 passing tests (0 failing)\n    - 94% test coverage\n    - Clean conventional commit history\n    - Fully documented JWT authentication\n    - Production-ready code\n    ```\n\n3.  **Session Completion:** The session completes when all tests pass, the code meets the specified quality standards, and the changes are committed.\n\n## Using Bundled Resources\n\n### Scripts\n\nTo automate the setup process, use the `overnight-setup.sh` script:\n\n```bash\n./scripts/overnight-setup.sh\n```\n\nTo track the progress of the overnight session, use the `progress-tracker.py` script:\n\n```bash\n./scripts/progress-tracker.py --log .overnight-dev-log.txt\n```\n\n### References\n\nFor detailed configuration options, load:\n\n-   [Configuration Reference](./references/configuration_reference.md)\n\nFor best practices on writing effective tests, load:\n\n-   [Testing Best Practices](./references/testing_best_practices.md)\n\n### Assets\n\nAvailable templates:\n\n-   `assets/commit-template.txt` - Template for generating commit messages.\n-   `assets/readme-template.md` - Template for generating README files.\n\n## Examples\n\n### Example 1: Building JWT Authentication\n\nUser request: \"Implement JWT authentication with 90% test coverage overnight.\"\n\nWorkflow:\n\n1.  Claude writes failing authentication tests (TDD).\n2.  Claude implements JWT signing (tests still failing).\n3.  Claude debugs token generation (commit blocked, keeps trying).\n4.  Tests pass! Commit succeeds.\n5.  Claude adds middleware (writes tests first).\n6.  Integration tests (debugging edge cases).\n7.  All tests green (Coverage: 94%).\n8.  Claude adds docs, refactors, still green.\n9.  Session complete.\n\n### Example 2: Refactoring Database Layer\n\nUser request: \"Refactor the database layer to use the repository pattern overnight.\"\n\nWorkflow:\n\n1.  Claude analyzes existing tests to ensure no regression.\n2.  Claude implements the repository pattern.\n3.  Tests are run; some fail due to changes in data access.\n4.  Claude updates tests to align with the new repository pattern.\n5.  All tests pass; commit succeeds.\n6.  Claude documents the refactored database layer.\n7.  Session complete.\n\n### Example 3: Fixing a Bug in Payment Processing\n\nUser request: \"Fix the bug in payment processing that causes incorrect amounts to be charged overnight.\"\n\nWorkflow:\n\n1.  Claude reproduces the bug and writes a failing test case.\n2.  Claude analyzes the code and identifies the root cause of the bug.\n3.  Claude fixes the bug and runs the tests.\n4.  The failing test case now passes; all other tests also pass.\n5.  Commit succeeds.\n6.  Claude adds a comment to the code explaining the fix.\n7.  Session complete.\n\n## Best Practices\n\n-   Ensure that the task is well-defined and testable.\n-   Follow the TDD approach by writing tests before implementing features.\n-   Monitor the progress of the overnight session by viewing the log file.\n-   Configure the Git hooks and settings appropriately for the project.\n-   Use the `overnight-dev-coach` agent for guidance and support.\n\n## Troubleshooting\n\n**Issue:** Hooks are not running.\n\n**Solution:** Make sure the hooks are executable:\n\n```bash\nchmod +x .git/hooks/pre-commit\nchmod +x .git/hooks/commit-msg\n```\n\n**Issue:** Tests are failing immediately.\n\n**Solution:** Ensure you have at least one passing test:\n\n```bash\nnpm test # Should see: Tests passed\n```\n\n**Issue:** Lint errors are blocking everything.\n\n**Solution:** Enable auto-fix:\n\n```json\n{\n  \"autoFix\": true\n}\n```\n\nOr fix manually:\n\n```bash\nnpm run lint -- --fix\n```\n\n## Integration\n\nThis skill integrates with Git repositories and various testing frameworks. It uses Git hooks to enforce TDD and ensure that all code changes are fully tested. The `overnight-dev-coach` agent provides guidance and support throughout the process.\n\n```bash\n# Example integration with Jest (Node.js)\n{\n  \"testCommand\": \"npm test -- --coverage --watchAll=false\",\n  \"lintCommand\": \"npm run lint\",\n  \"autoFix\": true\n}\n```\n\n```bash\n# Example integration with pytest (Python)\n{\n  \"testCommand\": \"pytest --cov=. --cov-report=term-missing\",\n  \"lintCommand\": \"flake8 . && black --check .\",\n  \"autoFix\": false\n}\n```\n```\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nIf execution fails:\n- Verify prerequisites are met\n- Check input parameters and formats\n- Validate file paths and permissions\n- Review error messages for root cause\n- Consult documentation for troubleshooting\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "overnight-dev",
        "category": "productivity",
        "path": "plugins/productivity/overnight-dev",
        "version": "1.0.0",
        "description": "Run Claude autonomously for 6-8 hours overnight using Git hooks that enforce TDD - wake up to fully tested features"
      },
      "filePath": "plugins/productivity/overnight-dev/skills/overnight-dev/SKILL.md"
    },
    {
      "slug": "performing-penetration-testing",
      "name": "performing-penetration-testing",
      "description": "Perform security penetration testing to identify vulnerabilities. Use when conducting security assessments. Trigger with 'run pentest', 'security testing', or 'find vulnerabilities'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the process of penetration testing for web applications, identifying vulnerabilities and suggesting exploitation techniques. It leverages the penetration-tester plugin to assess web application security posture.\n\n## How It Works\n\n1. **Target Identification**: Analyzes the user's request to identify the target web application or API endpoint.\n2. **Vulnerability Scanning**: Executes automated scans to discover potential vulnerabilities, covering OWASP Top 10 risks.\n3. **Reporting**: Generates a detailed penetration test report, including identified vulnerabilities, risk ratings, and remediation recommendations.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform a penetration test on a web application.\n- Identify vulnerabilities in a web application or API.\n- Assess the security posture of a web application.\n- Generate a report detailing security flaws and remediation steps.\n\n## Examples\n\n### Example 1: Performing a Full Penetration Test\n\nUser request: \"Run a penetration test on example.com\"\n\nThe skill will:\n1. Initiate a comprehensive penetration test on the specified domain.\n2. Generate a detailed report outlining identified vulnerabilities, including SQL injection, XSS, and CSRF.\n\n### Example 2: Assessing API Security\n\nUser request: \"Perform vulnerability assessment on the /api/users endpoint\"\n\nThe skill will:\n1. Target the specified API endpoint for vulnerability scanning.\n2. Identify potential security flaws in the API, such as authentication bypass or authorization issues, and provide remediation advice.\n\n## Best Practices\n\n- **Authorization**: Always ensure you have explicit authorization before performing penetration testing on any system.\n- **Scope Definition**: Clearly define the scope of the penetration test to avoid unintended consequences.\n- **Safe Exploitation**: Use exploitation techniques carefully to demonstrate vulnerabilities without causing damage.\n\n## Integration\n\nThis skill can be integrated with other security tools and plugins to enhance vulnerability management and remediation efforts. For example, findings can be exported to vulnerability tracking systems.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "penetration-tester",
        "category": "security",
        "path": "plugins/security/penetration-tester",
        "version": "1.0.0",
        "description": "Automated penetration testing for web applications with OWASP Top 10 coverage"
      },
      "filePath": "plugins/security/penetration-tester/skills/penetration-tester/SKILL.md"
    },
    {
      "slug": "performing-regression-analysis",
      "name": "performing-regression-analysis",
      "description": "This skill empowers claude to perform regression analysis and modeling",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill enables Claude to analyze data, build regression models, and provide insights into the relationships between variables. It leverages the regression-analysis-tool plugin to automate the process and ensure best practices are followed.\n\n## How It Works\n\n1. **Data Analysis**: Claude analyzes the provided data to understand its structure and identify potential relationships between variables.\n2. **Model Generation**: Based on the data, Claude generates appropriate regression models (e.g., linear, polynomial).\n3. **Model Validation**: Claude validates the generated models to ensure their accuracy and reliability.\n4. **Performance Reporting**: Claude provides performance metrics and insights into the model's effectiveness.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform regression analysis on a given dataset.\n- Predict future values based on existing data using regression models.\n- Understand the relationship between independent and dependent variables.\n- Evaluate the performance of a regression model.\n\n## Examples\n\n### Example 1: Predicting House Prices\n\nUser request: \"Can you build a regression model to predict house prices based on square footage and number of bedrooms?\"\n\nThe skill will:\n1. Analyze the provided data on house prices, square footage, and number of bedrooms.\n2. Generate a regression model (likely multiple to compare) to predict house prices.\n3. Provide performance metrics such as R-squared and RMSE.\n\n### Example 2: Analyzing Sales Trends\n\nUser request: \"I need to analyze the sales data for the past year and identify any trends using regression analysis.\"\n\nThe skill will:\n1. Analyze the provided sales data.\n2. Generate a regression model to identify trends and patterns in the sales data.\n3. Visualize the trend and report the equation and R-squared value.\n\n## Best Practices\n\n- **Data Preparation**: Ensure the data is clean and preprocessed before performing regression analysis.\n- **Model Selection**: Choose the appropriate regression model based on the data and the problem.\n- **Validation**: Always validate the model to ensure its accuracy and reliability.\n\n## Integration\n\nThis skill works independently using the regression-analysis-tool plugin. It can be used in conjunction with other data analysis and visualization tools to provide a comprehensive understanding of the data.",
      "parentPlugin": {
        "name": "regression-analysis-tool",
        "category": "ai-ml",
        "path": "plugins/ai-ml/regression-analysis-tool",
        "version": "1.0.0",
        "description": "Regression analysis and modeling"
      },
      "filePath": "plugins/ai-ml/regression-analysis-tool/skills/regression-analysis-tool/SKILL.md"
    },
    {
      "slug": "performing-security-audits",
      "name": "performing-security-audits",
      "description": "This skill allows claude to conduct comprehensive security audits of",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to perform in-depth security audits across various domains, from code vulnerability scanning to compliance verification and infrastructure security assessment. It utilizes the specialized tools within the security-pro-pack to provide a comprehensive security posture analysis.\n\n## How It Works\n\n1. **Analysis Selection**: Claude determines the appropriate security-pro-pack tool (e.g., `Security Auditor Expert`, `Compliance Checker`, `Crypto Audit`) based on the user's request and the context of the code or system being analyzed.\n2. **Execution**: Claude executes the selected tool, providing it with the relevant code, configuration files, or API endpoints.\n3. **Reporting**: Claude aggregates and presents the findings in a clear, actionable report, highlighting vulnerabilities, compliance issues, and potential security risks, along with suggested remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess the security of code for vulnerabilities like those in the OWASP Top 10.\n- Evaluate compliance with standards such as HIPAA, PCI DSS, GDPR, or SOC 2.\n- Review cryptographic implementations for weaknesses.\n- Perform container security scans or API security audits.\n\n## Examples\n\n### Example 1: Vulnerability Assessment\n\nUser request: \"Please perform a security audit on this authentication code to find any potential vulnerabilities.\"\n\nThe skill will:\n1. Invoke the `Security Auditor Expert` agent.\n2. Analyze the provided authentication code for common vulnerabilities.\n3. Generate a report detailing any identified vulnerabilities, their severity, and recommended fixes.\n\n### Example 2: Compliance Check\n\nUser request: \"Check this application against GDPR compliance requirements.\"\n\nThe skill will:\n1. Invoke the `Compliance Checker` agent.\n2. Evaluate the application's architecture and code against GDPR guidelines.\n3. Generate a report highlighting any non-compliant areas and suggesting necessary changes.\n\n## Best Practices\n\n- **Specificity**: Provide clear and specific instructions about the scope of the audit (e.g., \"audit this specific function\" instead of \"audit the whole codebase\").\n- **Context**: Include relevant context about the application, infrastructure, or data being audited to enable more accurate and relevant results.\n- **Iteration**: Use the skill iteratively, addressing the most critical findings first and then progressively improving the overall security posture.\n\n## Integration\n\nThis skill seamlessly integrates with all other components of the security-pro-pack plugin. It also works well with Claude's existing code analysis capabilities, allowing for a holistic and integrated security review process.",
      "parentPlugin": {
        "name": "security-pro-pack",
        "category": "packages",
        "path": "plugins/packages/security-pro-pack",
        "version": "1.0.0",
        "description": "Professional security tools for Claude Code: vulnerability scanning, compliance, cryptography audit, container & API security"
      },
      "filePath": "plugins/packages/security-pro-pack/skills/security-pro-pack/SKILL.md"
    },
    {
      "slug": "performing-security-code-review",
      "name": "performing-security-code-review",
      "description": "This skill enables claude to conduct a security-focused code review using",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to act as a security expert, identifying and explaining potential vulnerabilities within code. It leverages the security-agent plugin to provide detailed security analysis, helping developers improve the security posture of their applications.\n\n## How It Works\n\n1. **Receiving Request**: Claude identifies a user's request for a security review or audit of code.\n2. **Activating Security Agent**: Claude invokes the security-agent plugin to analyze the provided code.\n3. **Generating Security Report**: The security-agent produces a structured report detailing identified vulnerabilities, their severity, affected code locations, and recommended remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Review code for security vulnerabilities.\n- Perform a security audit of a codebase.\n- Identify potential security risks in a software application.\n\n## Examples\n\n### Example 1: Identifying SQL Injection Vulnerability\n\nUser request: \"Please review this database query code for SQL injection vulnerabilities.\"\n\nThe skill will:\n1. Activate the security-agent plugin to analyze the database query code.\n2. Generate a report identifying potential SQL injection vulnerabilities, including the vulnerable code snippet, its severity, and suggested remediation, such as using parameterized queries.\n\n### Example 2: Checking for Insecure Dependencies\n\nUser request: \"Can you check this project's dependencies for known security vulnerabilities?\"\n\nThe skill will:\n1. Utilize the security-agent plugin to scan the project's dependencies against known vulnerability databases.\n2. Produce a report listing any vulnerable dependencies, their Common Vulnerabilities and Exposures (CVE) identifiers, and recommendations for updating to secure versions.\n\n## Best Practices\n\n- **Specificity**: Provide the exact code or project you want reviewed.\n- **Context**: Clearly state the security concerns you have regarding the code.\n- **Iteration**: Use the findings to address vulnerabilities and request further reviews.\n\n## Integration\n\nThis skill integrates with Claude's code understanding capabilities and leverages the security-agent plugin to provide specialized security analysis. It can be used in conjunction with other code analysis tools to provide a comprehensive assessment of code quality and security.",
      "parentPlugin": {
        "name": "security-agent",
        "category": "examples",
        "path": "plugins/examples/security-agent",
        "version": "1.0.0",
        "description": "Specialized security review subagent"
      },
      "filePath": "plugins/examples/security-agent/skills/security-agent/SKILL.md"
    },
    {
      "slug": "performing-security-testing",
      "name": "performing-security-testing",
      "description": "Automate security vulnerability testing covering OWASP Top 10, SQL injection, XSS, CSRF, and authentication issues. Use when performing security assessments, penetration tests, or vulnerability scans. Trigger with phrases like \"scan for vulnerabilities\", \"test security\", or \"run penetration test\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:security-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Target application or API endpoint URLs accessible for testing\n- Authentication credentials if testing protected resources\n- Appropriate authorization to perform security testing on the target system\n- Test environment configured (avoid production without explicit approval)\n- Security testing tools installed (OWASP ZAP, sqlmap, or equivalent)\n\n## Instructions\n\n### Step 1: Define Test Scope\nIdentify the security testing parameters:\n- Target URLs and endpoints to scan\n- Authentication requirements and test credentials\n- Specific vulnerability types to focus on (OWASP Top 10, injection, XSS, etc.)\n- Testing depth level (passive scan vs. active exploitation)\n\n### Step 2: Execute Security Scan\nRun automated vulnerability detection:\n1. Use Read tool to analyze application structure and identify entry points\n2. Execute security testing tools via Bash(test:security-*) with proper scope\n3. Monitor scan progress and capture all findings\n4. Document identified vulnerabilities with severity ratings\n\n### Step 3: Analyze Vulnerabilities\nProcess scan results to identify:\n- SQL injection vulnerabilities in database queries\n- Cross-Site Scripting (XSS) in user input fields\n- Cross-Site Request Forgery (CSRF) token weaknesses\n- Authentication and authorization bypass opportunities\n- Security misconfigurations and exposed sensitive data\n\n### Step 4: Generate Security Report\nCreate comprehensive documentation in {baseDir}/security-reports/:\n- Executive summary with risk overview\n- Detailed vulnerability findings with CVSS scores\n- Proof-of-concept exploit examples where applicable\n- Prioritized remediation recommendations\n- Compliance assessment against security standards\n\n## Output\n\nThe skill generates structured security assessment reports:\n\n### Vulnerability Summary\n- Total vulnerabilities discovered by severity (Critical, High, Medium, Low)\n- OWASP Top 10 category mapping for each finding\n- Attack surface analysis showing exposed endpoints\n\n### Detailed Findings\nEach vulnerability includes:\n- Unique identifier and CVSS score\n- Affected URLs, parameters, and HTTP methods\n- Technical description of the security weakness\n- Proof-of-concept demonstration or reproduction steps\n- Potential impact on confidentiality, integrity, and availability\n\n### Remediation Guidance\n- Specific code fixes or configuration changes required\n- Secure coding best practices to prevent recurrence\n- Priority recommendations based on risk and effort\n- Verification testing procedures after remediation\n\n### Compliance Assessment\n- Alignment with OWASP Application Security Verification Standard (ASVS)\n- PCI DSS requirements if applicable to payment processing\n- General Data Protection Regulation (GDPR) security considerations\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Access Denied**\n- Error: HTTP 403 or authentication failures during scan\n- Solution: Verify credentials are valid and have sufficient permissions; use provided test accounts\n\n**Rate Limiting**\n- Error: Too many requests blocked by WAF or rate limiter\n- Solution: Configure scan throttling to reduce request rate; use authenticated sessions to increase limits\n\n**False Positives**\n- Error: Reported vulnerabilities that cannot be exploited\n- Solution: Manually verify each finding; adjust scanner sensitivity; whitelist known safe patterns\n\n**Tool Installation Missing**\n- Error: Security testing tools not found on system\n- Solution: Install required tools using Bash(test:security-install) with package manager\n\n## Resources\n\n### Security Testing Tools\n- OWASP ZAP for automated vulnerability scanning\n- Burp Suite for manual penetration testing\n- sqlmap for SQL injection detection and exploitation\n- Nikto for web server vulnerability scanning\n\n### Vulnerability Databases\n- Common Vulnerabilities and Exposures (CVE) database\n- National Vulnerability Database (NVD) for CVSS scoring\n- OWASP Top 10 documentation and remediation guides\n\n### Secure Coding Guidelines\n- OWASP Secure Coding Practices checklist\n- CWE (Common Weakness Enumeration) catalog\n- SANS Top 25 Most Dangerous Software Errors\n\n### Best Practices\n- Always test in non-production environments first\n- Obtain written authorization before security testing\n- Document all testing activities for audit trails\n- Validate remediation effectiveness with regression testing",
      "parentPlugin": {
        "name": "security-test-scanner",
        "category": "testing",
        "path": "plugins/testing/security-test-scanner",
        "version": "1.0.0",
        "description": "Automated security vulnerability testing covering OWASP Top 10, SQL injection, XSS, CSRF, and authentication issues"
      },
      "filePath": "plugins/testing/security-test-scanner/skills/security-test-scanner/SKILL.md"
    },
    {
      "slug": "planning-disaster-recovery",
      "name": "planning-disaster-recovery",
      "description": "Use when you need to work with backup and recovery. This skill provides backup automation and disaster recovery with comprehensive guidance and automation. Trigger with phrases like \"create backups\", \"automate backups\", or \"implement disaster recovery\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/disaster-recovery-planner/`\n\n**Documentation and Guides**: `{baseDir}/docs/disaster-recovery-planner/`\n\n**Example Scripts and Code**: `{baseDir}/examples/disaster-recovery-planner/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/disaster-recovery-planner-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/disaster-recovery-planner-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/disaster-recovery-planner-dashboard.json`",
      "parentPlugin": {
        "name": "disaster-recovery-planner",
        "category": "devops",
        "path": "plugins/devops/disaster-recovery-planner",
        "version": "1.0.0",
        "description": "Plan and implement disaster recovery procedures"
      },
      "filePath": "plugins/devops/disaster-recovery-planner/skills/disaster-recovery-planner/SKILL.md"
    },
    {
      "slug": "plugin-auditor",
      "name": "plugin-auditor",
      "description": "Automatically audits claude code plugins for security vulnerabilities,",
      "allowedTools": [
        "Read",
        "Grep",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Plugin Auditor\n\n## Purpose\nAutomatically audits Claude Code plugins for security vulnerabilities, best practice violations, CLAUDE.md compliance, and quality standards - optimized for claude-code-plugins repository requirements.\n\n## Trigger Keywords\n- \"audit plugin\"\n- \"security review\" or \"security audit\"\n- \"best practices check\"\n- \"plugin quality\"\n- \"compliance check\"\n- \"plugin security\"\n\n## Audit Categories\n\n### 1. Security Audit\n\n**Critical Checks:**\n- ‚ùå No hardcoded secrets (passwords, API keys, tokens)\n- ‚ùå No AWS keys (AKIA...)\n- ‚ùå No private keys (BEGIN PRIVATE KEY)\n- ‚ùå No dangerous commands (rm -rf /, eval(), exec())\n- ‚ùå No command injection vectors\n- ‚ùå No suspicious URLs (IP addresses, non-HTTPS)\n- ‚ùå No obfuscated code (base64 decode, hex encoding)\n\n**Security Patterns:**\n```bash\n# Check for hardcoded secrets\ngrep -r \"password\\s*=\\s*['\\\"]\" --exclude-dir=node_modules\ngrep -r \"api_key\\s*=\\s*['\\\"]\" --exclude-dir=node_modules\ngrep -r \"secret\\s*=\\s*['\\\"]\" --exclude-dir=node_modules\n\n# Check for AWS keys\ngrep -r \"AKIA[0-9A-Z]{16}\" --exclude=README.md\n\n# Check for private keys\ngrep -r \"BEGIN.*PRIVATE KEY\" --exclude=README.md\n\n# Check for dangerous patterns\ngrep -r \"rm -rf /\" | grep -v \"/var/\" | grep -v \"{baseDir}/tmp/\"\ngrep -r \"eval\\s*\\(\" --exclude=README.md\n```\n\n### 2. Best Practices Audit\n\n**Plugin Structure:**\n- ‚úÖ Proper directory hierarchy\n- ‚úÖ Required files present\n- ‚úÖ Semantic versioning (x.y.z)\n- ‚úÖ Clear, concise descriptions\n- ‚úÖ Proper LICENSE file (MIT/Apache-2.0)\n- ‚úÖ Comprehensive README\n- ‚úÖ At least 5 keywords\n\n**Code Quality:**\n- ‚úÖ No TODO/FIXME without issue links\n- ‚úÖ No console.log() in production code\n- ‚úÖ No hardcoded paths (/home/, /Users/)\n- ‚úÖ Uses `${CLAUDE_PLUGIN_ROOT}` in hooks\n- ‚úÖ Scripts have proper shebangs\n- ‚úÖ All scripts are executable\n\n**Documentation:**\n- ‚úÖ README has installation section\n- ‚úÖ README has usage examples\n- ‚úÖ README has clear description\n- ‚úÖ Commands have proper frontmatter\n- ‚úÖ Agents have model specified\n- ‚úÖ Skills have trigger keywords\n\n### 3. CLAUDE.md Compliance\n\n**Repository Standards:**\n- ‚úÖ Follows plugin structure from CLAUDE.md\n- ‚úÖ Uses correct marketplace slug\n- ‚úÖ Proper category assignment\n- ‚úÖ Valid plugin.json schema\n- ‚úÖ Marketplace catalog entry exists\n- ‚úÖ Version consistency\n\n**Skills Compliance (if applicable):**\n- ‚úÖ SKILL.md has proper frontmatter\n- ‚úÖ Description includes trigger keywords\n- ‚úÖ allowed-tools specified (if restricted)\n- ‚úÖ Clear purpose and instructions\n- ‚úÖ Examples provided\n\n### 4. Marketplace Compliance\n\n**Catalog Requirements:**\n- ‚úÖ Plugin listed in marketplace.extended.json\n- ‚úÖ Source path matches actual location\n- ‚úÖ Version matches plugin.json\n- ‚úÖ Category is valid\n- ‚úÖ No duplicate plugin names\n- ‚úÖ Author information complete\n\n### 5. Git Hygiene\n\n**Repository Practices:**\n- ‚úÖ No large binary files\n- ‚úÖ No node_modules/ committed\n- ‚úÖ No .env files\n- ‚úÖ Proper .gitignore\n- ‚úÖ No merge conflicts\n- ‚úÖ Clean commit history\n\n### 6. MCP Plugin Audit (if applicable)\n\n**MCP-Specific Checks:**\n- ‚úÖ Valid package.json with @modelcontextprotocol/sdk\n- ‚úÖ TypeScript configured correctly\n- ‚úÖ dist/ in .gitignore\n- ‚úÖ Proper mcp/*.json configuration\n- ‚úÖ Build scripts present\n- ‚úÖ No dependency vulnerabilities\n\n### 7. Performance Audit\n\n**Efficiency Checks:**\n- ‚úÖ No unnecessary file reads\n- ‚úÖ Efficient glob patterns\n- ‚úÖ No recursive loops\n- ‚úÖ Reasonable timeout values\n- ‚úÖ No memory leaks (event listeners)\n\n### 8. Accessibility & UX\n\n**User Experience:**\n- ‚úÖ Clear error messages\n- ‚úÖ Helpful command descriptions\n- ‚úÖ Proper usage examples\n- ‚úÖ Good README formatting\n- ‚úÖ Working demo commands\n\n## Audit Process\n\nWhen activated, I will:\n\n1. **Security Scan**\n   ```bash\n   # Run security checks\n   grep -r \"password\\|secret\\|api_key\" plugins/plugin-name/\n   grep -r \"AKIA[0-9A-Z]{16}\" plugins/plugin-name/\n   grep -r \"BEGIN.*PRIVATE KEY\" plugins/plugin-name/\n   grep -r \"rm -rf /\" plugins/plugin-name/\n   grep -r \"eval\\(\" plugins/plugin-name/\n   ```\n\n2. **Structure Validation**\n   ```bash\n   # Check required files\n   test -f .claude-plugin/plugin.json\n   test -f README.md\n   test -f LICENSE\n\n   # Check component directories\n   ls -d commands/ agents/ skills/ hooks/ mcp/ 2>/dev/null\n   ```\n\n3. **Best Practices Check**\n   ```bash\n   # Check for TODO/FIXME\n   grep -r \"TODO\\|FIXME\" --exclude=README.md\n\n   # Check for console.log\n   grep -r \"console\\.log\" --exclude=README.md\n\n   # Check script permissions\n   find . -name \"*.sh\" ! -perm -u+x\n   ```\n\n4. **Compliance Verification**\n   ```bash\n   # Check marketplace entry\n   jq '.plugins[] | select(.name == \"plugin-name\")' .claude-plugin/marketplace.extended.json\n\n   # Verify version consistency\n   plugin_version=$(jq -r '.version' .claude-plugin/plugin.json)\n   market_version=$(jq -r '.plugins[] | select(.name == \"plugin-name\") | .version' .claude-plugin/marketplace.extended.json)\n   ```\n\n5. **Generate Audit Report**\n\n## Audit Report Format\n\n```\nüîç PLUGIN AUDIT REPORT\nPlugin: plugin-name\nVersion: 1.0.0\nCategory: security\nAudit Date: 2025-10-16\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüîí SECURITY AUDIT\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n‚úÖ PASSED (7/7)\n- No hardcoded secrets\n- No AWS keys\n- No private keys\n- No dangerous commands\n- No command injection vectors\n- HTTPS URLs only\n- No obfuscated code\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüìã BEST PRACTICES\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n‚úÖ PASSED (10/12)\n- Proper directory structure\n- Required files present\n- Semantic versioning\n- Clear descriptions\n- Comprehensive README\n\n‚ö†Ô∏è  WARNINGS (2)\n- 3 scripts missing execute permission\n  Fix: chmod +x scripts/*.sh\n\n- 2 TODO items without issue links\n  Location: commands/scan.md:45, agents/analyzer.md:67\n  Recommendation: Create GitHub issues or remove TODOs\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚úÖ CLAUDE.MD COMPLIANCE\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n‚úÖ PASSED (6/6)\n- Follows plugin structure\n- Uses correct marketplace slug\n- Proper category assignment\n- Valid plugin.json schema\n- Marketplace entry exists\n- Version consistency\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüìä QUALITY SCORE\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nSecurity:        10/10 ‚úÖ\nBest Practices:   8/10 ‚ö†Ô∏è\nCompliance:      10/10 ‚úÖ\nDocumentation:   10/10 ‚úÖ\n\nOVERALL SCORE: 9.5/10 (EXCELLENT)\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüéØ RECOMMENDATIONS\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nPriority: MEDIUM\n1. Fix script permissions (2 min)\n2. Resolve TODO items (10 min)\n\nOptional Improvements:\n- Add more usage examples in README\n- Include troubleshooting section\n- Add GIF/video demo\n\n‚úÖ AUDIT COMPLETE\nPlugin is production-ready with minor improvements needed.\n```\n\n## Severity Levels\n\n**Critical (üî¥):**\n- Security vulnerabilities\n- Hardcoded secrets\n- Dangerous commands\n- Missing required files\n\n**High (üü†):**\n- Best practice violations\n- Missing documentation\n- Broken functionality\n- Schema violations\n\n**Medium (üü°):**\n- Code quality issues\n- Missing optional features\n- Performance concerns\n- UX improvements\n\n**Low (üü¢):**\n- Style inconsistencies\n- Minor documentation gaps\n- Nice-to-have features\n\n## Auto-Fix Capabilities\n\nI can automatically fix:\n- ‚úÖ Script permissions\n- ‚úÖ JSON formatting\n- ‚úÖ Markdown formatting\n- ‚úÖ Version sync issues\n\n## Repository-Specific Checks\n\n**For claude-code-plugins repo:**\n- Validates against CLAUDE.md standards\n- Checks marketplace integration\n- Verifies category structure\n- Ensures quality for featured plugins\n- Checks contributor guidelines compliance\n\n## Examples\n\n**User says:** \"Audit the security-scanner plugin\"\n\n**I automatically:**\n1. Run full security scan\n2. Check best practices\n3. Verify CLAUDE.md compliance\n4. Generate comprehensive report\n5. Provide recommendations\n\n**User says:** \"Is this plugin safe to publish?\"\n\n**I automatically:**\n1. Security audit (critical)\n2. Marketplace compliance\n3. Quality score calculation\n4. Publish readiness assessment\n\n**User says:** \"Quality review before featured status\"\n\n**I automatically:**\n1. Full audit (all categories)\n2. Higher quality thresholds\n3. Featured plugin requirements\n4. Recommendation: approve/reject",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/plugin-auditor/SKILL.md"
    },
    {
      "slug": "plugin-creator",
      "name": "plugin-creator",
      "description": "Automatically creates new claude code plugins with proper structure,",
      "allowedTools": [
        "Write",
        "Read",
        "Grep",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Plugin Creator\n\n## Purpose\nAutomatically scaffolds new Claude Code plugins with complete directory structure, required files, proper formatting, and marketplace catalog integration - specifically optimized for the claude-code-plugins repository.\n\n## Trigger Keywords\n- \"create plugin\" or \"new plugin\"\n- \"plugin from template\"\n- \"scaffold plugin\"\n- \"generate plugin\"\n- \"add new plugin to marketplace\"\n\n## Plugin Creation Process\n\nWhen activated, I will:\n\n1. **Gather Requirements**\n   - Plugin name (kebab-case)\n   - Category (productivity, security, devops, etc.)\n   - Type (commands, agents, skills, MCP, or combination)\n   - Description and keywords\n   - Author information\n\n2. **Create Directory Structure**\n   ```\n   plugins/[category]/[plugin-name]/\n   ‚îú‚îÄ‚îÄ .claude-plugin/\n   ‚îÇ   ‚îî‚îÄ‚îÄ plugin.json\n   ‚îú‚îÄ‚îÄ README.md\n   ‚îú‚îÄ‚îÄ LICENSE\n   ‚îî‚îÄ‚îÄ [commands|agents|skills|hooks|mcp]/\n   ```\n\n3. **Generate Required Files**\n   - **plugin.json** with proper schema (name, version, description, author)\n   - **README.md** with comprehensive documentation\n   - **LICENSE** (MIT by default)\n   - Component files based on type\n\n4. **Add to Marketplace Catalog**\n   - Update `.claude-plugin/marketplace.extended.json`\n   - Run `npm run sync-marketplace` automatically\n   - Validate catalog schema\n\n5. **Validate Everything**\n   - Run `./scripts/validate-all.sh` on new plugin\n   - Check JSON syntax with `jq`\n   - Verify frontmatter in markdown files\n   - Ensure scripts are executable\n\n## Plugin Types Supported\n\n### Commands Plugin\n- Creates `commands/` directory\n- Generates example command with proper frontmatter\n- Includes `/demo-command` example\n\n### Agents Plugin\n- Creates `agents/` directory\n- Generates example agent with capabilities\n- Includes model specification\n\n### Skills Plugin\n- Creates `skills/skill-name/` directory\n- Generates SKILL.md with proper format\n- Includes trigger keywords and allowed-tools\n\n### MCP Plugin\n- Creates `src/`, `dist/`, `mcp/` directories\n- Generates TypeScript boilerplate\n- Includes package.json with MCP SDK\n- Adds to pnpm workspace\n\n### Full Plugin\n- Combines all types\n- Creates complete example structure\n- Ready for customization\n\n## File Templates\n\n### plugin.json Template\n```json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Clear description\",\n  \"author\": {\n    \"name\": \"Author Name\",\n    \"email\": \"[email protected]\"\n  },\n  \"repository\": \"https://github.com/jeremylongshore/claude-code-plugins\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"keyword1\", \"keyword2\"]\n}\n```\n\n### Command Template\n```markdown\n---\nname: command-name\ndescription: What this command does\nmodel: sonnet\n---\n\n# Command Title\n\nInstructions for Claude...\n```\n\n### Skill Template\n```markdown\n---\nname: Skill Name\ndescription: What it does AND when to use it\nallowed-tools: Read, Write, Grep\n---\n\n# Skill Name\n\n## Purpose\n[What this skill does]\n\n## Trigger Keywords\n- keyword1\n- keyword2\n\n## Instructions\n[Step-by-step for Claude]\n```\n\n## Marketplace Integration\n\nI automatically:\n1. Add plugin entry to `marketplace.extended.json`\n2. Run `npm run sync-marketplace` to update CLI catalog\n3. Validate both catalogs with `jq`\n4. Check for duplicate names\n5. Verify source paths exist\n\n## Validation Steps\n\nAfter creation:\n- ‚úÖ All required files present\n- ‚úÖ Valid JSON (plugin.json, catalogs)\n- ‚úÖ Proper frontmatter in markdown\n- ‚úÖ Scripts executable (`chmod +x`)\n- ‚úÖ No duplicate plugin names\n- ‚úÖ Category is valid\n- ‚úÖ Keywords present\n\n## Repository-Specific Features\n\n**For claude-code-plugins repo:**\n- Follows exact directory structure\n- Uses correct marketplace slug (`claude-code-plugins-plus`)\n- Includes proper LICENSE file\n- Adds to correct category folder\n- Validates against existing plugins\n- Updates version in marketplace\n\n## Output\n\nI provide:\n```\n‚úÖ Created plugin: plugin-name\nüìÅ Location: plugins/category/plugin-name/\nüìù Files created: 8\nüîç Validation: PASSED\nüì¶ Marketplace: UPDATED\n‚ú® Ready to commit!\n\nNext steps:\n1. Review files in plugins/category/plugin-name/\n2. Customize README.md and component files\n3. Run: git add plugins/category/plugin-name/\n4. Run: git commit -m \"feat: Add plugin-name plugin\"\n```\n\n## Examples\n\n**User says:** \"Create a new security plugin called 'owasp-scanner' with commands\"\n\n**I automatically:**\n1. Create directory: `plugins/security/owasp-scanner/`\n2. Generate plugin.json, README, LICENSE\n3. Create `commands/` with example\n4. Add to marketplace.extended.json\n5. Sync marketplace.json\n6. Validate all files\n7. Report success\n\n**User says:** \"Scaffold a Skills plugin for code review\"\n\n**I automatically:**\n1. Create directory with `skills/` subdirectories\n2. Generate SKILL.md templates\n3. Add trigger keywords for code review\n4. Add to marketplace\n5. Validate and report",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/plugin-creator/SKILL.md"
    },
    {
      "slug": "plugin-validator",
      "name": "plugin-validator",
      "description": "Automatically validates claude code plugin structure, schemas, and compliance",
      "allowedTools": [
        "Read",
        "Grep",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Plugin Validator\n\n## Purpose\nAutomatically validates Claude Code plugins against repository standards, checking structure, JSON schemas, frontmatter, permissions, security, and marketplace compliance - optimized for claude-code-plugins repository.\n\n## Trigger Keywords\n- \"validate plugin\"\n- \"check plugin\"\n- \"plugin validation\"\n- \"plugin errors\"\n- \"lint plugin\"\n- \"verify plugin\"\n\n## Validation Checks\n\n### 1. Required Files\n- ‚úÖ `.claude-plugin/plugin.json` exists\n- ‚úÖ `README.md` exists and not empty\n- ‚úÖ `LICENSE` file exists\n- ‚úÖ At least one component directory (commands/, agents/, skills/, hooks/, mcp/)\n\n### 2. Plugin.json Schema\n```bash\n# Required fields:\n- name (kebab-case, lowercase, hyphens only)\n- version (semantic versioning x.y.z)\n- description (clear, concise)\n- author.name\n- author.email\n- license (MIT, Apache-2.0, etc.)\n- keywords (array, at least 2)\n\n# Optional but recommended:\n- repository (GitHub URL)\n- homepage (docs URL)\n```\n\n### 3. Frontmatter Validation\n**For Commands (commands/*.md):**\n```yaml\n---\nname: command-name\ndescription: Brief description\nmodel: sonnet|opus|haiku\n---\n```\n\n**For Agents (agents/*.md):**\n```yaml\n---\nname: agent-name\ndescription: Agent purpose\nmodel: sonnet|opus|haiku\n---\n```\n\n**For Skills (skills/*/SKILL.md):**\n```yaml\n---\nname: Skill Name\ndescription: What it does AND when to use it\nallowed-tools: Tool1, Tool2, Tool3  # optional\n---\n```\n\n### 4. Directory Structure\nValidates proper hierarchy:\n```\nplugin-name/\n‚îú‚îÄ‚îÄ .claude-plugin/          # Required\n‚îÇ   ‚îî‚îÄ‚îÄ plugin.json          # Required\n‚îú‚îÄ‚îÄ README.md                 # Required\n‚îú‚îÄ‚îÄ LICENSE                   # Required\n‚îú‚îÄ‚îÄ commands/                 # Optional\n‚îÇ   ‚îî‚îÄ‚îÄ *.md\n‚îú‚îÄ‚îÄ agents/                   # Optional\n‚îÇ   ‚îî‚îÄ‚îÄ *.md\n‚îú‚îÄ‚îÄ skills/                   # Optional\n‚îÇ   ‚îî‚îÄ‚îÄ skill-name/\n‚îÇ       ‚îî‚îÄ‚îÄ SKILL.md\n‚îú‚îÄ‚îÄ hooks/                    # Optional\n‚îÇ   ‚îî‚îÄ‚îÄ hooks.json\n‚îî‚îÄ‚îÄ mcp/                      # Optional\n    ‚îî‚îÄ‚îÄ *.json\n```\n\n### 5. Script Permissions\n```bash\n# All .sh files must be executable\nfind . -name \"*.sh\" ! -perm -u+x\n# Should return empty\n```\n\n### 6. JSON Validation\n```bash\n# All JSON must be valid\njq empty plugin.json\njq empty marketplace.extended.json\njq empty hooks/hooks.json\n```\n\n### 7. Security Scans\n- ‚ùå No hardcoded secrets (API keys, tokens, passwords)\n- ‚ùå No AWS keys (AKIA...)\n- ‚ùå No private keys (BEGIN PRIVATE KEY)\n- ‚ùå No dangerous commands (rm -rf /, eval())\n- ‚ùå No suspicious URLs (non-HTTPS, IP addresses)\n\n### 8. Marketplace Compliance\n- ‚úÖ Plugin listed in marketplace.extended.json\n- ‚úÖ Source path matches actual location\n- ‚úÖ Version matches between plugin.json and catalog\n- ‚úÖ Category is valid\n- ‚úÖ No duplicate plugin names\n\n### 9. README Requirements\n- ‚úÖ Has installation instructions\n- ‚úÖ Has usage examples\n- ‚úÖ Has description section\n- ‚úÖ Proper markdown formatting\n- ‚úÖ No broken links\n\n### 10. Path Variables\nFor hooks:\n- ‚úÖ Uses `${CLAUDE_PLUGIN_ROOT}` not absolute paths\n- ‚úÖ No hardcoded /home/ or /Users/ paths\n\n## Validation Process\n\nWhen activated, I will:\n\n1. **Identify Plugin**\n   - Detect plugin directory from context\n   - Or ask user which plugin to validate\n\n2. **Run Comprehensive Checks**\n   ```bash\n   # Structure validation\n   ./scripts/validate-all.sh plugins/category/plugin-name/\n\n   # JSON validation\n   jq empty .claude-plugin/plugin.json\n\n   # Frontmatter check\n   python3 scripts/check-frontmatter.py\n\n   # Permission check\n   find . -name \"*.sh\" ! -perm -u+x\n\n   # Security scan\n   grep -r \"password\\|secret\\|api_key\" | grep -v placeholder\n   ```\n\n3. **Generate Report**\n   - List all issues by severity (critical, high, medium, low)\n   - Provide fix commands for each issue\n   - Summary: PASSED or FAILED\n\n## Validation Report Format\n\n```\nüîç PLUGIN VALIDATION REPORT\nPlugin: plugin-name\nLocation: plugins/category/plugin-name/\n\n‚úÖ PASSED CHECKS (8/10)\n- Required files present\n- Valid plugin.json schema\n- Proper frontmatter format\n- Directory structure correct\n- No security issues\n- Marketplace compliance\n- README complete\n- JSON valid\n\n‚ùå FAILED CHECKS (2/10)\n- Script permissions: 3 .sh files not executable\n  Fix: chmod +x scripts/*.sh\n\n- Marketplace version mismatch\n  plugin.json: v1.2.0\n  marketplace.extended.json: v1.1.0\n  Fix: Update marketplace.extended.json to v1.2.0\n\n‚ö†Ô∏è  WARNINGS (1)\n- README missing usage examples\n  Recommendation: Add ## Usage section with examples\n\nOVERALL: FAILED (2 critical issues)\nFix issues above before committing.\n```\n\n## Auto-Fix Capabilities\n\nI can automatically fix:\n- ‚úÖ Script permissions (`chmod +x`)\n- ‚úÖ JSON formatting (`jq` reformat)\n- ‚úÖ Marketplace version sync\n- ‚úÖ Missing LICENSE (copy from root)\n\n## Repository-Specific Checks\n\n**For claude-code-plugins repo:**\n- Validates against `.claude-plugin/marketplace.extended.json`\n- Checks category folder matches catalog entry\n- Ensures marketplace slug is `claude-code-plugins-plus`\n- Validates against other plugins (no duplicates)\n- Checks compliance with CLAUDE.md standards\n\n## Integration with CI\n\nValidation results match GitHub Actions:\n- Same checks as `.github/workflows/validate-plugins.yml`\n- Compatible with CI error format\n- Can be run locally before pushing\n\n## Examples\n\n**User says:** \"Validate the skills-powerkit plugin\"\n\n**I automatically:**\n1. Run all validation checks\n2. Identify 2 issues (permissions, version mismatch)\n3. Provide fix commands\n4. Report overall status: FAILED\n\n**User says:** \"Check if my plugin is ready to commit\"\n\n**I automatically:**\n1. Detect plugin from context\n2. Run comprehensive validation\n3. Check marketplace compliance\n4. Report: PASSED or list issues\n\n**User says:** \"Why is my plugin failing CI?\"\n\n**I automatically:**\n1. Run same checks as CI\n2. Identify exact failure\n3. Provide fix command\n4. Validate fix works",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/plugin-validator/SKILL.md"
    },
    {
      "slug": "preprocessing-data-with-automated-pipelines",
      "name": "preprocessing-data-with-automated-pipelines",
      "description": "Automate data cleaning, transformation, and validation for ML tasks.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill enables Claude to construct and execute automated data preprocessing pipelines, ensuring data quality and readiness for machine learning. It streamlines the data preparation process by automating common tasks such as data cleaning, transformation, and validation.\n\n## How It Works\n\n1. **Analyze Requirements**: Claude analyzes the user's request to understand the specific data preprocessing needs, including data sources, target format, and desired transformations.\n2. **Generate Pipeline Code**: Based on the requirements, Claude generates Python code for an automated data preprocessing pipeline using relevant libraries and best practices. This includes data validation and error handling.\n3. **Execute Pipeline**: The generated code is executed, performing the data preprocessing steps.\n4. **Provide Metrics and Insights**: Claude provides performance metrics and insights about the pipeline's execution, including data quality reports and potential issues encountered.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Prepare raw data for machine learning models.\n- Automate data cleaning and transformation processes.\n- Implement a robust ETL (Extract, Transform, Load) pipeline.\n\n## Examples\n\n### Example 1: Cleaning Customer Data\n\nUser request: \"Preprocess the customer data from the CSV file to remove duplicates and handle missing values.\"\n\nThe skill will:\n1. Generate a Python script to read the CSV file, remove duplicate entries, and impute missing values using appropriate techniques (e.g., mean imputation).\n2. Execute the script and provide a summary of the changes made, including the number of duplicates removed and the number of missing values imputed.\n\n### Example 2: Transforming Sensor Data\n\nUser request: \"Create an ETL pipeline to transform the sensor data from the database into a format suitable for time series analysis.\"\n\nThe skill will:\n1. Generate a Python script to extract sensor data from the database, transform it into a time series format (e.g., resampling to a fixed frequency), and load it into a suitable storage location.\n2. Execute the script and provide performance metrics, such as the time taken for each step of the pipeline and the size of the transformed data.\n\n## Best Practices\n\n- **Data Validation**: Always include data validation steps to ensure data quality and catch potential errors early in the pipeline.\n- **Error Handling**: Implement robust error handling to gracefully handle unexpected issues during pipeline execution.\n- **Performance Optimization**: Optimize the pipeline for performance by using efficient algorithms and data structures.\n\n## Integration\n\nThis skill can be integrated with other Claude Code skills for data analysis, model training, and deployment. It provides a standardized way to prepare data for these tasks, ensuring consistency and reliability.",
      "parentPlugin": {
        "name": "data-preprocessing-pipeline",
        "category": "ai-ml",
        "path": "plugins/ai-ml/data-preprocessing-pipeline",
        "version": "1.0.0",
        "description": "Automated data preprocessing and cleaning pipelines"
      },
      "filePath": "plugins/ai-ml/data-preprocessing-pipeline/skills/data-preprocessing-pipeline/SKILL.md"
    },
    {
      "slug": "processing-api-batches",
      "name": "processing-api-batches",
      "description": "Process bulk API requests efficiently with batching, throttling, and parallel execution. Use when processing bulk API operations efficiently. Trigger with phrases like \"process bulk requests\", \"batch API calls\", or \"handle batch operations\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:batch-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:batch-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-batch-processor",
        "category": "api-development",
        "path": "plugins/api-development/api-batch-processor",
        "version": "1.0.0",
        "description": "Implement batch API operations with bulk processing and job queues"
      },
      "filePath": "plugins/api-development/api-batch-processor/skills/api-batch-processor/SKILL.md"
    },
    {
      "slug": "processing-computer-vision-tasks",
      "name": "processing-computer-vision-tasks",
      "description": "Process images using object detection, classification, and segmentation.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to leverage the computer-vision-processor plugin to analyze images, detect objects, and extract meaningful information. It automates computer vision workflows, optimizes performance, and provides detailed insights based on image content.\n\n## How It Works\n\n1. **Analyzing the Request**: Claude identifies the need for computer vision processing based on the user's request and trigger terms.\n2. **Generating Code**: Claude generates the appropriate Python code to interact with the computer-vision-processor plugin, specifying the desired analysis type (e.g., object detection, image classification).\n3. **Executing the Task**: The generated code is executed using the `/process-vision` command, which processes the image and returns the results.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze an image for specific objects or features.\n- Classify an image into predefined categories.\n- Segment an image to identify different regions or objects.\n\n## Examples\n\n### Example 1: Object Detection\n\nUser request: \"Analyze this image and identify all the cars and pedestrians.\"\n\nThe skill will:\n1. Generate code to perform object detection on the provided image using the computer-vision-processor plugin.\n2. Return a list of bounding boxes and labels for each detected car and pedestrian.\n\n### Example 2: Image Classification\n\nUser request: \"Classify this image. Is it a cat or a dog?\"\n\nThe skill will:\n1. Generate code to perform image classification on the provided image using the computer-vision-processor plugin.\n2. Return the classification result (e.g., \"cat\" or \"dog\") along with a confidence score.\n\n## Best Practices\n\n- **Data Validation**: Always validate the input image to ensure it's in a supported format and resolution.\n- **Error Handling**: Implement robust error handling to gracefully manage potential issues during image processing.\n- **Performance Optimization**: Choose the appropriate computer vision techniques and parameters to optimize performance for the specific task.\n\n## Integration\n\nThis skill utilizes the `/process-vision` command provided by the computer-vision-processor plugin. It can be integrated with other skills to further process the results of the computer vision analysis, such as generating reports or triggering actions based on detected objects.",
      "parentPlugin": {
        "name": "computer-vision-processor",
        "category": "ai-ml",
        "path": "plugins/ai-ml/computer-vision-processor",
        "version": "1.0.0",
        "description": "Computer vision image processing and analysis"
      },
      "filePath": "plugins/ai-ml/computer-vision-processor/skills/computer-vision-processor/SKILL.md"
    },
    {
      "slug": "profiling-application-performance",
      "name": "profiling-application-performance",
      "description": "This skill enables claude to profile application performance, analyzing",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to analyze application performance, pinpoint bottlenecks, and recommend optimizations. By leveraging the application-profiler plugin, it provides insights into CPU usage, memory allocation, and execution time, enabling targeted improvements.\n\n## How It Works\n\n1. **Identify Application Stack**: Determines the application's technology (e.g., Node.js, Python, Java).\n2. **Locate Entry Points**: Identifies main application entry points and critical execution paths.\n3. **Analyze Performance Metrics**: Examines CPU usage, memory allocation, and execution time to detect bottlenecks.\n4. **Generate Profile**: Compiles the analysis into a comprehensive performance profile, highlighting areas for optimization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze application performance for bottlenecks.\n- Identify CPU-intensive operations and memory leaks.\n- Optimize application execution time.\n\n## Examples\n\n### Example 1: Identifying Memory Leaks\n\nUser request: \"Analyze my Node.js application for memory leaks.\"\n\nThe skill will:\n1. Activate the application-profiler plugin.\n2. Analyze the application's memory allocation patterns.\n3. Generate a profile highlighting potential memory leaks.\n\n### Example 2: Optimizing CPU Usage\n\nUser request: \"Profile my Python script and find the most CPU-intensive functions.\"\n\nThe skill will:\n1. Activate the application-profiler plugin.\n2. Analyze the script's CPU usage.\n3. Generate a profile identifying the functions consuming the most CPU time.\n\n## Best Practices\n\n- **Code Instrumentation**: Ensure the application code is instrumented for accurate profiling.\n- **Realistic Workloads**: Use realistic workloads during profiling to simulate real-world scenarios.\n- **Iterative Optimization**: Apply optimizations iteratively and re-profile to measure improvements.\n\n## Integration\n\nThis skill can be used in conjunction with code editing plugins to implement the recommended optimizations directly within the application's source code. It can also integrate with monitoring tools to track performance improvements over time.",
      "parentPlugin": {
        "name": "application-profiler",
        "category": "performance",
        "path": "plugins/performance/application-profiler",
        "version": "1.0.0",
        "description": "Profile application performance with CPU, memory, and execution time analysis"
      },
      "filePath": "plugins/performance/application-profiler/skills/application-profiler/SKILL.md"
    },
    {
      "slug": "providing-performance-optimization-advice",
      "name": "providing-performance-optimization-advice",
      "description": "Provide comprehensive prioritized performance optimization recommendations for frontend, backend, and infrastructure. Use when analyzing bottlenecks or seeking improvement strategies. Trigger with phrases like \"optimize performance\", \"improve speed\", or \"performance recommendations\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(profiling:*)",
        "Bash(analysis:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to act as a performance optimization advisor, delivering a detailed report of potential improvements across various layers of a software application. It prioritizes recommendations based on impact and effort, allowing for a focused and efficient optimization strategy.\n\n## How It Works\n\n1. **Analyze Project**: Claude uses the plugin to analyze the project's codebase, infrastructure configuration, and architecture.\n2. **Identify Optimization Areas**: The plugin identifies potential optimization areas in the frontend, backend, and infrastructure.\n3. **Prioritize Recommendations**: The plugin prioritizes recommendations based on estimated performance gains and implementation effort.\n4. **Generate Report**: Claude presents a comprehensive report with actionable advice, performance gain estimates, and a phased implementation roadmap.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in a software application.\n- Get recommendations for improving website loading speed.\n- Optimize database query performance.\n- Improve API response times.\n- Reduce infrastructure costs.\n\n## Examples\n\n### Example 1: Optimizing a Slow Website\n\nUser request: \"My website is loading very slowly. Can you help me optimize its performance?\"\n\nThe skill will:\n1. Analyze the website's frontend code, backend APIs, and infrastructure configuration.\n2. Identify issues such as unoptimized images, inefficient database queries, and lack of CDN usage.\n3. Generate a report with prioritized recommendations, including image optimization, database query optimization, and CDN implementation.\n\n### Example 2: Improving API Response Time\n\nUser request: \"The API response time is too slow. What can I do to improve it?\"\n\nThe skill will:\n1. Analyze the API code, database queries, and caching strategies.\n2. Identify issues such as inefficient database queries, lack of caching, and slow processing logic.\n3. Generate a report with prioritized recommendations, including database query optimization, caching implementation, and asynchronous processing.\n\n## Best Practices\n\n- **Specificity**: Provide specific details about the project and its performance issues to get more accurate and relevant recommendations.\n- **Context**: Explain the context of the performance problem, such as the expected user load or the specific use case.\n- **Iteration**: Review the recommendations and provide feedback to refine the optimization strategy.\n\n## Integration\n\nThis skill integrates well with other plugins that provide code analysis, infrastructure management, and deployment automation capabilities. For example, it can be used in conjunction with a code linting plugin to identify code-level performance issues or with an infrastructure-as-code plugin to automate infrastructure optimization tasks.\n\n## Prerequisites\n\n- Access to application codebase in {baseDir}/\n- Infrastructure configuration files\n- Performance profiling tools\n- Current performance metrics and baselines\n\n## Instructions\n\n1. Analyze frontend code for rendering and asset optimization\n2. Review backend code for query and processing efficiency\n3. Examine infrastructure for scaling and resource usage\n4. Identify high-impact optimization opportunities\n5. Prioritize recommendations by effort vs impact\n6. Generate phased implementation roadmap\n\n## Output\n\n- Comprehensive optimization report by layer (frontend/backend/infra)\n- Prioritized recommendations with impact estimates\n- Code examples for suggested improvements\n- Performance gain projections\n- Implementation effort estimates and timeline\n\n## Error Handling\n\nIf optimization analysis fails:\n- Verify codebase access permissions\n- Check profiling tool installation\n- Validate configuration file formats\n- Ensure sufficient analysis resources\n- Review project structure completeness\n\n## Resources\n\n- Web performance optimization guides\n- Database query optimization best practices\n- Infrastructure scaling patterns\n- Caching strategies and CDN usage",
      "parentPlugin": {
        "name": "performance-optimization-advisor",
        "category": "performance",
        "path": "plugins/performance/performance-optimization-advisor",
        "version": "1.0.0",
        "description": "Get comprehensive performance optimization recommendations"
      },
      "filePath": "plugins/performance/performance-optimization-advisor/skills/performance-optimization-advisor/SKILL.md"
    },
    {
      "slug": "rate-limiting-apis",
      "name": "rate-limiting-apis",
      "description": "Implement sophisticated rate limiting with sliding windows, token buckets, and quotas. Use when protecting APIs from excessive requests. Trigger with phrases like \"add rate limiting\", \"limit API requests\", or \"implement rate limits\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:ratelimit-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:ratelimit-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-rate-limiter",
        "category": "api-development",
        "path": "plugins/api-development/api-rate-limiter",
        "version": "1.0.0",
        "description": "Implement rate limiting with token bucket, sliding window, and Redis"
      },
      "filePath": "plugins/api-development/api-rate-limiter/skills/api-rate-limiter/SKILL.md"
    },
    {
      "slug": "responding-to-security-incidents",
      "name": "responding-to-security-incidents",
      "description": "Guide security incident response, investigation, and remediation processes. Use when you need to handle security breaches, classify incidents, develop response playbooks, gather forensic evidence, or coordinate remediation efforts. Trigger with phrases like \"security incident response\", \"ransomware attack response\", \"data breach investigation\", \"incident playbook\", or \"security forensics\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Access to system and application logs in {baseDir}/logs/\n- Network traffic captures or SIEM data available\n- Incident response team contact information\n- Backup systems operational and accessible\n- Write permissions for incident documentation in {baseDir}/incidents/\n- Communication channels established for stakeholder updates\n\n## Instructions\n\n### 1. Incident Detection and Triage\n\nClassify the security incident:\n- Incident type (ransomware, data breach, DDoS, insider threat, phishing)\n- Severity level (Critical, High, Medium, Low)\n- Scope assessment (affected systems, data, users)\n- Initial timestamp and detection method\n- Potential business impact\n\n### 2. Immediate Containment Actions\n\nPrevent further damage:\n- Isolate affected systems from network\n- Disable compromised user accounts\n- Block malicious IP addresses at firewall\n- Preserve system state for forensics\n- Activate incident response team\n- Document all containment actions with timestamps\n\n### 3. Evidence Collection Phase\n\nGather forensic data systematically:\n\n**System Evidence**:\n- Memory dumps from affected systems\n- Disk images for forensic analysis\n- Running process listings\n- Network connection states\n- Registry modifications (Windows)\n\n**Log Evidence**:\n- Authentication logs (successful/failed logins)\n- Application logs with error patterns\n- Network traffic logs (firewall, IDS/IPS)\n- Database access logs\n- Web server access/error logs\n\n**Network Evidence**:\n- Packet captures (PCAP files)\n- DNS query logs\n- Proxy server logs\n- Network flow data (NetFlow)\n\n### 4. Investigation and Analysis\n\nReconstruct the attack timeline:\n- Identify initial access vector (how attackers got in)\n- Map lateral movement within network\n- Determine data exfiltration attempts\n- Identify persistence mechanisms\n- Assess privilege escalation methods\n- Document indicators of compromise (IOCs)\n\n### 5. Eradication Phase\n\nRemove threat from environment:\n- Remove malware and backdoors\n- Close exploited vulnerabilities\n- Reset compromised credentials\n- Apply security patches\n- Update firewall rules\n- Verify threat elimination\n\n### 6. Recovery and Restoration\n\nRestore normal operations:\n- Restore systems from clean backups\n- Rebuild compromised systems from scratch\n- Verify system integrity\n- Monitor for reinfection attempts\n- Gradually restore services\n- Validate business operations\n\n### 7. Post-Incident Documentation\n\nCreate comprehensive incident report:\n- Executive summary\n- Detailed timeline\n- Root cause analysis\n- Lessons learned\n- Remediation recommendations\n- Cost impact assessment\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Incident response playbook saved to {baseDir}/incidents/incident-YYYYMMDD-HHMM.md\n\n**Playbook Structure**:\n```\n# Security Incident Response - [Incident Type]\nDate: YYYY-MM-DD HH:MM\nSeverity: CRITICAL\nStatus: Contained\n\n## Executive Summary\n- Incident type: Ransomware attack\n- Detection time: 2024-01-15 08:30 UTC\n- Affected systems: 15 servers, 200 workstations\n- Business impact: Production halted\n- Current status: Contained, recovery in progress\n\n## Timeline of Events\n08:30 - Initial detection via EDR alert\n08:35 - IT team confirms ransomware encryption\n08:40 - Network isolation initiated\n09:00 - Incident response team activated\n[Detailed timeline continues]\n\n## Containment Actions Taken\n‚úÖ Isolated affected network segments\n‚úÖ Disabled compromised accounts\n‚úÖ Blocked C2 server IPs\n‚úÖ Preserved forensic evidence\n\n## Evidence Collected\n- Memory dumps: 15 systems\n- Log files: {baseDir}/incidents/evidence/logs/\n- Network captures: {baseDir}/incidents/evidence/pcaps/\n- Malware samples: Quarantined\n\n## IOCs (Indicators of Compromise)\n- IP addresses: 203.0.113.45, 198.51.100.78\n- File hashes: SHA256 values listed\n- Domain names: malicious-c2.example\n- Registry keys: HKLM\\Software\\[malware]\n\n## Remediation Plan\nPriority 1 (Immediate):\n- Remove ransomware from all systems\n- Reset all domain credentials\n- Patch vulnerable RDP service\n\nPriority 2 (24 hours):\n- Deploy endpoint protection updates\n- Implement network segmentation\n- Enable MFA for all accounts\n\nPriority 3 (1 week):\n- Security awareness training\n- Update incident response procedures\n- Conduct tabletop exercise\n\n## Recovery Status\n- Clean backups identified: 2024-01-14 backup\n- Systems rebuilt: 5/15 servers complete\n- Services restored: Email, file servers online\n- Estimated full recovery: 48 hours\n\n## Communication Log\n- 08:45 - Executive team notified\n- 09:30 - Legal counsel engaged\n- 10:00 - Cyber insurance contacted\n- 12:00 - Customer notification prepared\n```\n\n**Secondary Outputs**:\n- IOC list for threat intelligence sharing (JSON/STIX format)\n- Evidence chain of custody log\n- Stakeholder communication templates\n- Post-incident review agenda\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Incomplete Log Data**\n   - Error: \"Critical logs missing from {baseDir}/logs/\"\n   - Resolution: Work with available data, note gaps in report\n   - Action: Improve logging for future incidents\n\n2. **Evidence Contamination**\n   - Error: \"System state modified before evidence collection\"\n   - Resolution: Document contamination, collect remaining evidence\n   - Best Practice: Immediately isolate before investigation\n\n3. **Ongoing Active Threat**\n   - Error: \"Attacker still has access during investigation\"\n   - Resolution: Prioritize containment over investigation\n   - Action: Implement emergency containment procedures first\n\n4. **Insufficient Access for Forensics**\n   - Error: \"Permission denied accessing system memory\"\n   - Resolution: Escalate to obtain necessary privileges\n   - Fallback: Use available logs and network data\n\n5. **Backup Corruption**\n   - Error: \"Backups also encrypted by ransomware\"\n   - Resolution: Identify offline/air-gapped backups\n   - Contingency: Assess rebuild from scratch vs ransom payment\n\n## Resources\n\n**Incident Response Frameworks**:\n- NIST Computer Security Incident Handling Guide: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf\n- SANS Incident Handler's Handbook: https://www.sans.org/white-papers/33901/\n- CISA Incident Response Guide: https://www.cisa.gov/incident-response\n\n**Forensic Tools**:\n- Memory analysis: Volatility Framework\n- Disk forensics: Autopsy, FTK Imager\n- Network analysis: Wireshark, tcpdump\n- Log analysis: ELK Stack, Splunk\n\n**Threat Intelligence**:\n- MITRE ATT&CK Framework: https://attack.mitre.org/\n- AlienVault OTX: https://otx.alienvault.com/\n- VirusTotal: https://www.virustotal.com/\n\n**Communication Templates**:\n- Breach notification requirements by jurisdiction\n- Customer communication guidelines\n- Media response templates\n- Regulatory reporting formats (GDPR, HIPAA, etc.)\n\n**Playbook Templates**:\n- Ransomware response: {baseDir}/templates/playbook-ransomware.md\n- Data breach response: {baseDir}/templates/playbook-breach.md\n- DDoS response: {baseDir}/templates/playbook-ddos.md\n\n**Legal and Compliance**:\n- Chain of custody documentation\n- eDiscovery preparation\n- Cyber insurance claim procedures\n- Law enforcement coordination",
      "parentPlugin": {
        "name": "security-incident-responder",
        "category": "security",
        "path": "plugins/security/security-incident-responder",
        "version": "1.0.0",
        "description": "Assist with security incident response"
      },
      "filePath": "plugins/security/security-incident-responder/skills/security-incident-responder/SKILL.md"
    },
    {
      "slug": "routing-dex-trades",
      "name": "routing-dex-trades",
      "description": "Route trades across multiple DEXs to find optimal prices and minimize slippage. Use when routing trades for best execution. Trigger with phrases like \"find best price\", \"route trade\", or \"check DEX prices\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:dex-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:dex-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "dex-aggregator-router",
        "category": "crypto",
        "path": "plugins/crypto/dex-aggregator-router",
        "version": "1.0.0",
        "description": "Find optimal DEX routes for token swaps across multiple exchanges"
      },
      "filePath": "plugins/crypto/dex-aggregator-router/skills/dex-aggregator-router/SKILL.md"
    },
    {
      "slug": "running-chaos-tests",
      "name": "running-chaos-tests",
      "description": "Execute chaos engineering experiments to test system resilience. Use when performing specialized testing. Trigger with phrases like \"run chaos tests\", \"test resilience\", or \"inject failures\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:chaos-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:chaos-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "chaos-engineering-toolkit",
        "category": "testing",
        "path": "plugins/testing/chaos-engineering-toolkit",
        "version": "1.0.0",
        "description": "Chaos testing for resilience with failure injection, latency simulation, and system resilience validation"
      },
      "filePath": "plugins/testing/chaos-engineering-toolkit/skills/chaos-engineering-toolkit/SKILL.md"
    },
    {
      "slug": "running-clustering-algorithms",
      "name": "running-clustering-algorithms",
      "description": "Execute clustering algorithms (K-means, DBSCAN, hierarchical) to identify",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to perform clustering analysis on provided datasets. It allows for automated execution of various clustering algorithms, providing insights into data groupings and structures.\n\n## How It Works\n\n1. **Analyzing the Context**: Claude analyzes the user's request to determine the dataset, desired clustering algorithm (if specified), and any specific requirements.\n2. **Generating Code**: Claude generates Python code using appropriate ML libraries (e.g., scikit-learn) to perform the clustering task, including data loading, preprocessing, algorithm execution, and result visualization.\n3. **Executing Clustering**: The generated code is executed, and the clustering algorithm is applied to the dataset.\n4. **Providing Results**: Claude presents the results, including cluster assignments, performance metrics (e.g., silhouette score, Davies-Bouldin index), and visualizations (e.g., scatter plots with cluster labels).\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify distinct groups within a dataset.\n- Perform a cluster analysis to understand data structure.\n- Run K-means, DBSCAN, or hierarchical clustering on a given dataset.\n\n## Examples\n\n### Example 1: Customer Segmentation\n\nUser request: \"Run clustering on this customer data to identify customer segments. The data is in customer_data.csv.\"\n\nThe skill will:\n1. Load the customer_data.csv dataset.\n2. Perform K-means clustering to identify distinct customer segments based on their attributes.\n3. Provide a visualization of the customer segments and their characteristics.\n\n### Example 2: Anomaly Detection\n\nUser request: \"Perform DBSCAN clustering on this network traffic data to identify anomalies. The data is available at network_traffic.txt.\"\n\nThe skill will:\n1. Load the network_traffic.txt dataset.\n2. Perform DBSCAN clustering to identify outliers representing anomalous network traffic.\n3. Report the identified anomalies and their characteristics.\n\n## Best Practices\n\n- **Data Preprocessing**: Always preprocess the data (e.g., scaling, normalization) before applying clustering algorithms to improve performance and accuracy.\n- **Algorithm Selection**: Choose the appropriate clustering algorithm based on the data characteristics and the desired outcome. K-means is suitable for spherical clusters, while DBSCAN is better for non-spherical clusters and anomaly detection.\n- **Parameter Tuning**: Tune the parameters of the clustering algorithm (e.g., number of clusters in K-means, epsilon and min_samples in DBSCAN) to optimize the results.\n\n## Integration\n\nThis skill can be integrated with data loading skills to retrieve datasets from various sources. It can also be combined with visualization skills to generate insightful visualizations of the clustering results.",
      "parentPlugin": {
        "name": "clustering-algorithm-runner",
        "category": "ai-ml",
        "path": "plugins/ai-ml/clustering-algorithm-runner",
        "version": "1.0.0",
        "description": "Run clustering algorithms on datasets"
      },
      "filePath": "plugins/ai-ml/clustering-algorithm-runner/skills/clustering-algorithm-runner/SKILL.md"
    },
    {
      "slug": "running-e2e-tests",
      "name": "running-e2e-tests",
      "description": "Execute end-to-end tests covering full user workflows across frontend and backend. Use when performing specialized testing. Trigger with phrases like \"run end-to-end tests\", \"test user flows\", or \"execute E2E suite\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:e2e-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:e2e-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "e2e-test-framework",
        "category": "testing",
        "path": "plugins/testing/e2e-test-framework",
        "version": "1.0.0",
        "description": "End-to-end test automation with Playwright, Cypress, and Selenium for browser-based testing"
      },
      "filePath": "plugins/testing/e2e-test-framework/skills/e2e-test-framework/SKILL.md"
    },
    {
      "slug": "running-integration-tests",
      "name": "running-integration-tests",
      "description": "Execute integration tests validating component interactions and system integration. Use when performing specialized testing. Trigger with phrases like \"run integration tests\", \"test integration\", or \"validate component interactions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:integration-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:integration-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "integration-test-runner",
        "category": "testing",
        "path": "plugins/testing/integration-test-runner",
        "version": "1.0.0",
        "description": "Run and manage integration test suites with environment setup, database seeding, and cleanup"
      },
      "filePath": "plugins/testing/integration-test-runner/skills/integration-test-runner/SKILL.md"
    },
    {
      "slug": "running-load-tests",
      "name": "running-load-tests",
      "description": "Create and execute load tests for performance validation using k6, JMeter, and Artillery. Use when validating application performance under load conditions or identifying bottlenecks. Trigger with phrases like \"run load test\", \"create stress test\", or \"validate performance under load\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(k6:*)",
        "Bash(jmeter:*)",
        "Bash(artillery:*)",
        "Bash(performance:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to automate the creation and execution of load tests, ensuring applications can handle expected traffic and identify potential performance bottlenecks. It streamlines the process of defining test scenarios, generating scripts, and executing tests for comprehensive performance validation.\n\n## How It Works\n\n1. **Analyze Application**: Claude analyzes the user's request to understand the application's endpoints and critical paths.\n2. **Identify Test Scenarios**: Claude identifies relevant test scenarios, such as baseline load, stress test, spike test, soak test, or scalability test, based on the user's requirements.\n3. **Generate Load Test Scripts**: Claude generates load test scripts (k6, JMeter, Artillery, etc.) based on the selected scenarios and application details.\n4. **Define Performance Thresholds**: Claude defines performance thresholds and provides execution instructions for the generated scripts.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create load tests for a web application or API.\n- Validate the performance of an application under different load conditions.\n- Identify performance bottlenecks and breaking points.\n\n## Examples\n\n### Example 1: Creating a Stress Test\n\nUser request: \"Create a stress test for the /api/users endpoint to simulate 1000 concurrent users.\"\n\nThe skill will:\n1. Analyze the request and identify the need for a stress test on the /api/users endpoint.\n2. Generate a k6 script that simulates 1000 concurrent users hitting the /api/users endpoint.\n\n### Example 2: Validating Performance After a Code Change\n\nUser request: \"Validate the performance of the application after the recent code changes with a baseline load test.\"\n\nThe skill will:\n1. Identify the need for a baseline load test to validate performance.\n2. Generate a JMeter script that simulates normal traffic patterns for the application.\n\n## Best Practices\n\n- **Realistic Scenarios**: Define load test scenarios that accurately reflect real-world usage patterns.\n- **Threshold Definition**: Establish clear performance thresholds to identify potential issues.\n- **Iterative Testing**: Run load tests iteratively to identify and address performance bottlenecks early in the development cycle.\n\n## Integration\n\nThis skill can be integrated with CI/CD pipelines to automate performance testing as part of the deployment process. It can also be used in conjunction with monitoring tools to correlate performance metrics with application behavior.\n\n## Prerequisites\n\n- Load testing tools installed (k6, JMeter, or Artillery)\n- Access to target application endpoints\n- Test scenario definitions and expected load patterns\n- Results storage location at {baseDir}/load-tests/\n\n## Instructions\n\n1. Analyze application architecture and identify critical endpoints\n2. Define test scenarios (baseline, stress, spike, soak, scalability)\n3. Generate appropriate load test scripts using selected tool\n4. Configure performance thresholds and acceptance criteria\n5. Execute load tests and capture metrics\n6. Analyze results and identify performance bottlenecks\n\n## Output\n\n- Load test scripts (k6, JMeter, or Artillery format)\n- Test execution logs and metrics\n- Performance reports with response times and throughput\n- Threshold violation alerts\n- Recommendations for performance improvements\n\n## Error Handling\n\nIf load test execution fails:\n- Verify tool installation and configuration\n- Check network connectivity to target endpoints\n- Validate authentication and authorization\n- Review test script syntax and parameters\n- Ensure sufficient system resources for test execution\n\n## Resources\n\n- k6 documentation and examples\n- JMeter user manual and best practices\n- Artillery load testing guides\n- Performance testing methodology references",
      "parentPlugin": {
        "name": "load-test-runner",
        "category": "performance",
        "path": "plugins/performance/load-test-runner",
        "version": "1.0.0",
        "description": "Create and execute load tests for performance validation"
      },
      "filePath": "plugins/performance/load-test-runner/skills/load-test-runner/SKILL.md"
    },
    {
      "slug": "running-mutation-tests",
      "name": "running-mutation-tests",
      "description": "Execute mutation testing to evaluate test suite effectiveness. Use when performing specialized testing. Trigger with phrases like \"run mutation tests\", \"test the tests\", or \"validate test effectiveness\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:mutation-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:mutation-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "mutation-test-runner",
        "category": "testing",
        "path": "plugins/testing/mutation-test-runner",
        "version": "1.0.0",
        "description": "Mutation testing to validate test quality by introducing code changes and verifying tests catch them"
      },
      "filePath": "plugins/testing/mutation-test-runner/skills/mutation-test-runner/SKILL.md"
    },
    {
      "slug": "running-performance-tests",
      "name": "running-performance-tests",
      "description": "Execute load testing, stress testing, and performance benchmarking. Use when performing specialized testing. Trigger with phrases like \"run load tests\", \"test performance\", or \"benchmark the system\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:perf-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:perf-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "performance-test-suite",
        "category": "testing",
        "path": "plugins/testing/performance-test-suite",
        "version": "1.0.0",
        "description": "Load testing and performance benchmarking with metrics analysis and bottleneck identification"
      },
      "filePath": "plugins/testing/performance-test-suite/skills/performance-test-suite/SKILL.md"
    },
    {
      "slug": "running-smoke-tests",
      "name": "running-smoke-tests",
      "description": "Execute fast smoke tests validating critical functionality after deployment. Use when performing specialized testing. Trigger with phrases like \"run smoke tests\", \"quick validation\", or \"test critical paths\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:smoke-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:smoke-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "smoke-test-runner",
        "category": "testing",
        "path": "plugins/testing/smoke-test-runner",
        "version": "1.0.0",
        "description": "Quick smoke test suites to verify critical functionality after deployments"
      },
      "filePath": "plugins/testing/smoke-test-runner/skills/smoke-test-runner/SKILL.md"
    },
    {
      "slug": "scanning-accessibility",
      "name": "scanning-accessibility",
      "description": "Validate WCAG compliance and accessibility standards (ARIA, keyboard navigation). Use when auditing WCAG compliance or screen reader compatibility. Trigger with phrases like \"scan accessibility\", \"check WCAG compliance\", or \"validate screen readers\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:a11y-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:a11y-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "accessibility-test-scanner",
        "category": "testing",
        "path": "plugins/testing/accessibility-test-scanner",
        "version": "1.0.0",
        "description": "A11y compliance testing with WCAG 2.1/2.2 validation, screen reader compatibility, and automated accessibility audits"
      },
      "filePath": "plugins/testing/accessibility-test-scanner/skills/accessibility-test-scanner/SKILL.md"
    },
    {
      "slug": "scanning-api-security",
      "name": "scanning-api-security",
      "description": "Scan APIs for security vulnerabilities including injection, broken auth, and data exposure. Use when scanning APIs for security vulnerabilities. Trigger with phrases like \"scan API security\", \"check for vulnerabilities\", or \"audit API security\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:security-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:security-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-security-scanner",
        "category": "api-development",
        "path": "plugins/api-development/api-security-scanner",
        "version": "1.0.0",
        "description": "Scan APIs for security vulnerabilities and OWASP API Top 10"
      },
      "filePath": "plugins/api-development/api-security-scanner/skills/api-security-scanner/SKILL.md"
    },
    {
      "slug": "scanning-container-security",
      "name": "scanning-container-security",
      "description": "Use when you need to work with security and compliance. This skill provides security scanning and vulnerability detection with comprehensive guidance and automation. Trigger with phrases like \"scan for vulnerabilities\", \"implement security controls\", or \"audit security\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/container-security-scanner/`\n\n**Documentation and Guides**: `{baseDir}/docs/container-security-scanner/`\n\n**Example Scripts and Code**: `{baseDir}/examples/container-security-scanner/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/container-security-scanner-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/container-security-scanner-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/container-security-scanner-dashboard.json`",
      "parentPlugin": {
        "name": "container-security-scanner",
        "category": "devops",
        "path": "plugins/devops/container-security-scanner",
        "version": "1.0.0",
        "description": "Scan containers for vulnerabilities using Trivy, Snyk, and other security tools"
      },
      "filePath": "plugins/devops/container-security-scanner/skills/container-security-scanner/SKILL.md"
    },
    {
      "slug": "scanning-database-security",
      "name": "scanning-database-security",
      "description": "Use when you need to work with security and compliance. This skill provides security scanning and vulnerability detection with comprehensive guidance and automation. Trigger with phrases like \"scan for vulnerabilities\", \"implement security controls\", or \"audit security\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-security-scanner/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-security-scanner/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-security-scanner/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-security-scanner-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-security-scanner-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-security-scanner-dashboard.json`",
      "parentPlugin": {
        "name": "database-security-scanner",
        "category": "database",
        "path": "plugins/database/database-security-scanner",
        "version": "1.0.0",
        "description": "Database plugin for database-security-scanner"
      },
      "filePath": "plugins/database/database-security-scanner/skills/database-security-scanner/SKILL.md"
    },
    {
      "slug": "scanning-for-data-privacy-issues",
      "name": "scanning-for-data-privacy-issues",
      "description": "Scan for data privacy issues and sensitive information exposure. Use when reviewing data handling practices. Trigger with 'scan privacy issues', 'check sensitive data', or 'validate data protection'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the process of identifying data privacy risks within a codebase. By leveraging the data-privacy-scanner plugin, Claude can quickly pinpoint potential vulnerabilities, helping developers proactively address compliance requirements and protect sensitive user data.\n\n## How It Works\n\n1. **Initiate Scan**: Upon detecting a privacy-related trigger phrase, Claude activates the data-privacy-scanner plugin.\n2. **Analyze Codebase**: The plugin analyzes the specified files or the entire project for potential data privacy violations.\n3. **Report Findings**: The plugin generates a detailed report outlining identified risks, including the location of the vulnerability and a description of the potential impact.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify potential data privacy vulnerabilities in a codebase.\n- Ensure compliance with data privacy regulations such as GDPR, CCPA, or HIPAA.\n- Perform a privacy audit of a project involving sensitive user data.\n\n## Examples\n\n### Example 1: Identifying PII Leaks\n\nUser request: \"Scan this project for PII leaks.\"\n\nThe skill will:\n1. Activate the data-privacy-scanner plugin to analyze the project.\n2. Generate a report highlighting potential Personally Identifiable Information (PII) leaks, such as exposed email addresses or phone numbers.\n\n### Example 2: Checking GDPR Compliance\n\nUser request: \"Check this configuration file for GDPR compliance issues.\"\n\nThe skill will:\n1. Activate the data-privacy-scanner plugin to analyze the specified configuration file.\n2. Generate a report identifying potential GDPR violations, such as insufficient data anonymization or improper consent management.\n\n## Best Practices\n\n- **Scope**: Specify the relevant files or directories to narrow the scope of the scan and improve performance.\n- **Context**: Provide context about the type of data being processed to help the plugin identify relevant privacy risks.\n- **Review**: Carefully review the generated report to understand the identified vulnerabilities and implement appropriate remediation measures.\n\n## Integration\n\nThis skill can be integrated with other security and compliance tools to provide a comprehensive approach to data privacy. For example, it can be combined with vulnerability scanning tools to identify related security risks or with reporting tools to track progress on remediation efforts.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "data-privacy-scanner",
        "category": "security",
        "path": "plugins/security/data-privacy-scanner",
        "version": "1.0.0",
        "description": "Scan for data privacy issues"
      },
      "filePath": "plugins/security/data-privacy-scanner/skills/data-privacy-scanner/SKILL.md"
    },
    {
      "slug": "scanning-for-gdpr-compliance",
      "name": "scanning-for-gdpr-compliance",
      "description": "Scan for GDPR compliance issues in data handling and privacy practices. Use when ensuring EU data protection compliance. Trigger with 'scan GDPR compliance', 'check data privacy', or 'validate GDPR'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill allows Claude to automatically assess an application's GDPR compliance posture. It provides a comprehensive scan, identifying potential violations and offering actionable recommendations to improve compliance. The skill simplifies the complex process of GDPR auditing, making it easier to identify and address critical gaps.\n\n## How It Works\n\n1. **Initiate Scan**: The user requests a GDPR compliance scan using natural language.\n2. **Plugin Activation**: Claude activates the `gdpr-compliance-scanner` plugin.\n3. **Compliance Assessment**: The plugin scans the application or system based on GDPR requirements.\n4. **Report Generation**: A detailed report is generated, highlighting compliance scores, critical gaps, and recommended actions.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess an application's GDPR compliance.\n- Identify potential GDPR violations.\n- Generate a report outlining compliance gaps and recommendations.\n- Audit data processing activities for adherence to GDPR principles.\n\n## Examples\n\n### Example 1: Assess GDPR Compliance of a Web Application\n\nUser request: \"Scan my web application for GDPR compliance.\"\n\nThe skill will:\n1. Activate the `gdpr-compliance-scanner` plugin.\n2. Scan the web application for GDPR compliance issues related to data collection, storage, and processing.\n3. Generate a report highlighting compliance scores, critical gaps such as missing cookie consent mechanisms, and actionable recommendations like implementing a cookie consent banner.\n\n### Example 2: Audit Data Processing Activities\n\nUser request: \"Check our data processing activities for GDPR compliance.\"\n\nThe skill will:\n1. Activate the `gdpr-compliance-scanner` plugin.\n2. Analyze data processing activities, including data collection methods, storage practices, and security measures.\n3. Generate a report identifying potential violations, such as inadequate data encryption or missing data processing agreements, along with recommendations for remediation.\n\n## Best Practices\n\n- **Specificity**: Provide as much context as possible about the application or system being scanned to improve the accuracy of the assessment.\n- **Regularity**: Schedule regular GDPR compliance scans to ensure ongoing adherence to regulatory requirements.\n- **Actionable Insights**: Prioritize addressing the critical gaps identified in the report to mitigate potential risks.\n\n## Integration\n\nThis skill can be integrated with other security and compliance tools to provide a holistic view of an application's security posture. It can also be used in conjunction with code generation tools to automatically implement recommended changes and improve GDPR compliance.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "gdpr-compliance-scanner",
        "category": "security",
        "path": "plugins/security/gdpr-compliance-scanner",
        "version": "1.0.0",
        "description": "Scan for GDPR compliance issues"
      },
      "filePath": "plugins/security/gdpr-compliance-scanner/skills/gdpr-compliance-scanner/SKILL.md"
    },
    {
      "slug": "scanning-for-secrets",
      "name": "scanning-for-secrets",
      "description": "Scan for exposed secrets, API keys, and credentials in code. Use when auditing for secret leaks. Trigger with 'scan for secrets', 'find exposed keys', or 'check credentials'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill enables Claude to scan your codebase for exposed secrets, API keys, passwords, and other sensitive credentials. It helps you identify and remediate potential security vulnerabilities before they are committed or deployed.\n\n## How It Works\n\n1. **Initiate Scan**: Claude activates the `secret-scanner` plugin.\n2. **Codebase Analysis**: The plugin scans the codebase using pattern matching and entropy analysis.\n3. **Report Generation**: A detailed report is generated, highlighting identified secrets, their locations, and suggested remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Scan your codebase for exposed API keys (e.g., AWS, Google, Azure).\n- Check for hardcoded passwords in configuration files.\n- Identify potential private keys (SSH, PGP) accidentally committed to the repository.\n- Proactively find secrets before committing changes.\n\n## Examples\n\n### Example 1: Identifying Exposed AWS Keys\n\nUser request: \"Scan for AWS keys in the codebase\"\n\nThe skill will:\n1. Activate the `secret-scanner` plugin.\n2. Scan the codebase for patterns matching AWS Access Keys (AKIA[0-9A-Z]{16}).\n3. Generate a report listing any found keys, their file locations, and remediation steps (e.g., revoking the key).\n\n### Example 2: Checking for Hardcoded Passwords\n\nUser request: \"Check for exposed credentials in config files\"\n\nThe skill will:\n1. Activate the `secret-scanner` plugin.\n2. Scan configuration files (e.g., `database.yml`, `.env`) for password patterns.\n3. Generate a report detailing any found passwords and suggesting the use of environment variables.\n\n## Best Practices\n\n- **Regular Scanning**: Schedule regular scans to catch newly introduced secrets.\n- **Pre-Commit Hooks**: Integrate the `secret-scanner` into your pre-commit hooks to prevent committing secrets.\n- **Review Entropy Analysis**: Carefully review results from entropy analysis, as they may indicate potential secrets not caught by pattern matching.\n\n## Integration\n\nThis skill can be integrated with other security tools, such as vulnerability scanners, to provide a comprehensive security assessment of your codebase. It can also be combined with notification plugins to alert you when new secrets are detected.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "secret-scanner",
        "category": "security",
        "path": "plugins/security/secret-scanner",
        "version": "1.0.0",
        "description": "Scan codebase for exposed secrets, API keys, passwords, and sensitive credentials"
      },
      "filePath": "plugins/security/secret-scanner/skills/secret-scanner/SKILL.md"
    },
    {
      "slug": "scanning-for-vulnerabilities",
      "name": "scanning-for-vulnerabilities",
      "description": "This skill enables comprehensive vulnerability scanning using the vulnerability-scanner",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to automatically scan your codebase for security vulnerabilities. It leverages the vulnerability-scanner plugin to identify potential risks, including code-level flaws, vulnerable dependencies, and insecure configurations.\n\n## How It Works\n\n1. **Initiate Scan**: The skill activates the vulnerability-scanner plugin based on user input.\n2. **Perform Analysis**: The plugin scans the codebase, dependencies, and configurations for vulnerabilities, including CVE detection.\n3. **Generate Report**: The plugin creates a detailed vulnerability report with findings, severity levels, and remediation guidance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify security vulnerabilities in your code.\n- Check your project's dependencies for known CVEs.\n- Review your project's configurations for security weaknesses.\n\n## Examples\n\n### Example 1: Identifying SQL Injection Risks\n\nUser request: \"Scan my code for SQL injection vulnerabilities.\"\n\nThe skill will:\n1. Activate the vulnerability-scanner plugin.\n2. Analyze the codebase for potential SQL injection flaws.\n3. Generate a report highlighting any identified SQL injection risks and providing remediation steps.\n\n### Example 2: Checking for Vulnerable npm Packages\n\nUser request: \"Check my project's npm dependencies for known vulnerabilities.\"\n\nThe skill will:\n1. Activate the vulnerability-scanner plugin.\n2. Scan the project's `package.json` file and identify any npm packages with known CVEs.\n3. Generate a report listing the vulnerable packages, their CVE identifiers, and recommended updates.\n\n## Best Practices\n\n- **Regular Scanning**: Run vulnerability scans regularly, especially before deployments.\n- **Prioritize Remediation**: Focus on addressing critical and high-severity vulnerabilities first.\n- **Validate Fixes**: After applying fixes, run another scan to ensure the vulnerabilities are resolved.\n\n## Integration\n\nThis skill integrates with the core Claude Code environment by providing automated vulnerability scanning capabilities. It can be used in conjunction with other plugins to create a comprehensive security workflow, such as integrating with a ticketing system to automatically create tickets for identified vulnerabilities.",
      "parentPlugin": {
        "name": "vulnerability-scanner",
        "category": "security",
        "path": "plugins/security/vulnerability-scanner",
        "version": "1.0.0",
        "description": "Comprehensive vulnerability scanning for code, dependencies, and configurations with CVE detection"
      },
      "filePath": "plugins/security/vulnerability-scanner/skills/vulnerability-scanner/SKILL.md"
    },
    {
      "slug": "scanning-for-xss-vulnerabilities",
      "name": "scanning-for-xss-vulnerabilities",
      "description": "This skill enables claude to automatically scan for xss (cross-site scripting)",
      "allowedTools": [
        "Read",
        "WebFetch",
        "WebSearch",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to proactively identify and report XSS vulnerabilities within your codebase. By leveraging advanced detection techniques, including context-aware analysis and WAF bypass testing, this skill ensures your web applications are resilient against common XSS attack vectors. It provides detailed insights into vulnerability types and offers guidance on remediation strategies.\n\n## How It Works\n\n1. **Activation**: Claude recognizes the user's intent to scan for XSS vulnerabilities through specific trigger phrases like \"scan for XSS\" or the shortcut \"/xss\".\n2. **Code Analysis**: The plugin analyzes the codebase, identifying potential XSS vulnerabilities across different contexts (HTML, JavaScript, CSS, URL).\n3. **Vulnerability Detection**: The plugin detects reflected, stored, and DOM-based XSS vulnerabilities by injecting various payloads and analyzing the responses.\n4. **Reporting**: The plugin generates a report highlighting identified vulnerabilities, their location in the code, and recommended remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform a security audit of your web application.\n- Review code for potential XSS vulnerabilities.\n- Ensure compliance with security standards.\n- Test the effectiveness of your Content Security Policy (CSP).\n- Identify and mitigate XSS vulnerabilities before deploying to production.\n\n## Examples\n\n### Example 1: Detecting Reflected XSS\n\nUser request: \"scan for XSS vulnerabilities in the search functionality\"\n\nThe skill will:\n1. Analyze the code related to the search functionality.\n2. Identify a reflected XSS vulnerability in how search queries are displayed.\n3. Report the vulnerability, including the affected code snippet and a suggested fix using proper sanitization.\n\n### Example 2: Identifying Stored XSS\n\nUser request: \"/xss check the comment submission form\"\n\nThe skill will:\n1. Analyze the comment submission form and its associated backend code.\n2. Detect a stored XSS vulnerability where user comments are saved to the database without sanitization.\n3. Report the vulnerability, highlighting the unsanitized comment storage and suggesting the use of a sanitization library like `sanitizeHtml`.\n\n## Best Practices\n\n- **Sanitization**: Always sanitize user input before displaying it on the page. Use appropriate escaping functions for the specific context (HTML, JavaScript, URL).\n- **Content Security Policy (CSP)**: Implement a strong CSP to restrict the sources from which the browser can load resources, mitigating the impact of XSS vulnerabilities.\n- **Regular Updates**: Keep your web application framework and libraries up to date to patch known XSS vulnerabilities.\n\n## Integration\n\nThis skill complements other security-focused plugins by providing targeted XSS vulnerability detection. It can be integrated with code review tools to automate security checks and provide developers with immediate feedback on potential XSS issues.",
      "parentPlugin": {
        "name": "xss-vulnerability-scanner",
        "category": "security",
        "path": "plugins/security/xss-vulnerability-scanner",
        "version": "1.0.0",
        "description": "Scan for XSS vulnerabilities"
      },
      "filePath": "plugins/security/xss-vulnerability-scanner/skills/xss-vulnerability-scanner/SKILL.md"
    },
    {
      "slug": "scanning-input-validation-practices",
      "name": "scanning-input-validation-practices",
      "description": "Scan for input validation vulnerabilities and injection risks. Use when reviewing user input handling. Trigger with 'scan input validation', 'check injection vulnerabilities', or 'validate sanitization'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the process of identifying potential input validation flaws within a codebase. By analyzing how user-provided data is handled, it helps developers proactively address security vulnerabilities before they can be exploited. This skill streamlines security audits and improves the overall security posture of applications.\n\n## How It Works\n\n1. **Initiate Scan**: The user requests an input validation scan, triggering the skill.\n2. **Code Analysis**: The skill uses the input-validation-scanner plugin to analyze the specified codebase or file.\n3. **Vulnerability Identification**: The plugin identifies instances where input validation may be missing or insufficient.\n4. **Report Generation**: The skill presents a report highlighting potential vulnerabilities and their locations in the code.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Audit a codebase for input validation vulnerabilities.\n- Review newly written code for potential XSS or SQL injection flaws.\n- Harden an application against common web security exploits.\n- Ensure compliance with security best practices related to input handling.\n\n## Examples\n\n### Example 1: Identifying XSS Vulnerabilities\n\nUser request: \"Scan the user profile module for potential XSS vulnerabilities.\"\n\nThe skill will:\n1. Activate the input-validation-scanner plugin on the specified module.\n2. Generate a report highlighting areas where user input is directly rendered without proper sanitization, indicating potential XSS vulnerabilities.\n\n### Example 2: Checking for SQL Injection Risks\n\nUser request: \"Check the database access layer for potential SQL injection risks.\"\n\nThe skill will:\n1. Use the input-validation-scanner plugin to examine the database access code.\n2. Identify instances where user input is used directly in SQL queries without proper parameterization or escaping, indicating potential SQL injection vulnerabilities.\n\n## Best Practices\n\n- **Regular Scanning**: Integrate input validation scanning into your regular development workflow.\n- **Contextual Analysis**: Always review the identified vulnerabilities in context to determine their actual impact and severity.\n- **Comprehensive Validation**: Ensure that all user-supplied data is validated, including data from forms, APIs, and external sources.\n\n## Integration\n\nThis skill can be used in conjunction with other security-related skills to provide a more comprehensive security assessment. For example, it can be combined with a static analysis skill to identify other types of vulnerabilities or with a dependency scanning skill to identify vulnerable third-party libraries.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "input-validation-scanner",
        "category": "security",
        "path": "plugins/security/input-validation-scanner",
        "version": "1.0.0",
        "description": "Scan input validation practices"
      },
      "filePath": "plugins/security/input-validation-scanner/skills/input-validation-scanner/SKILL.md"
    },
    {
      "slug": "scanning-market-movers",
      "name": "scanning-market-movers",
      "description": "Identify significant price movements and unusual volume across crypto markets. Use when tracking significant price movements. Trigger with phrases like \"scan market movers\", \"check biggest gainers\", or \"find pumps\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:movers-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:movers-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "market-movers-scanner",
        "category": "crypto",
        "path": "plugins/crypto/market-movers-scanner",
        "version": "1.0.0",
        "description": "Scan for top market movers - gainers, losers, volume spikes, and unusual activity"
      },
      "filePath": "plugins/crypto/market-movers-scanner/skills/market-movers-scanner/SKILL.md"
    },
    {
      "slug": "setting-up-distributed-tracing",
      "name": "setting-up-distributed-tracing",
      "description": "This skill automates the setup of distributed tracing for microservices.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill streamlines the process of setting up distributed tracing in a microservices environment. It guides you through the key steps of instrumenting your services, configuring trace context propagation, and selecting a backend for trace collection and analysis, enabling comprehensive monitoring and debugging.\n\n## How It Works\n\n1. **Backend Selection**: Determines the preferred tracing backend (e.g., Jaeger, Zipkin, Datadog).\n2. **Instrumentation Strategy**: Designs an instrumentation strategy for each service, focusing on key operations and dependencies.\n3. **Configuration Generation**: Generates the necessary configuration files and code snippets to enable distributed tracing.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement distributed tracing in a microservices application.\n- Gain end-to-end visibility into request flows across multiple services.\n- Troubleshoot performance bottlenecks and latency issues.\n\n## Examples\n\n### Example 1: Adding Tracing to a New Microservice\n\nUser request: \"setup tracing for the new payment service\"\n\nThe skill will:\n1. Prompt for the preferred tracing backend (e.g., Jaeger).\n2. Generate code snippets for OpenTelemetry instrumentation in the payment service.\n\n### Example 2: Troubleshooting Performance Issues\n\nUser request: \"implement distributed tracing to debug slow checkout process\"\n\nThe skill will:\n1. Guide the user through instrumenting relevant services in the checkout flow.\n2. Provide configuration examples for context propagation.\n\n## Best Practices\n\n- **Backend Choice**: Select a tracing backend that aligns with your existing infrastructure and monitoring tools.\n- **Sampling Strategy**: Implement a sampling strategy to manage trace volume and cost, especially in high-traffic environments.\n- **Context Propagation**: Ensure proper context propagation across all services to maintain trace continuity.\n\n## Integration\n\nThis skill can be used in conjunction with other plugins to automate the deployment and configuration of tracing infrastructure. For example, it can integrate with infrastructure-as-code tools to provision Jaeger or Zipkin clusters.",
      "parentPlugin": {
        "name": "distributed-tracing-setup",
        "category": "performance",
        "path": "plugins/performance/distributed-tracing-setup",
        "version": "1.0.0",
        "description": "Set up distributed tracing for microservices"
      },
      "filePath": "plugins/performance/distributed-tracing-setup/skills/distributed-tracing-setup/SKILL.md"
    },
    {
      "slug": "setting-up-experiment-tracking",
      "name": "setting-up-experiment-tracking",
      "description": "Setup machine learning experiment tracking using MLflow or Weights &",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill streamlines the process of setting up experiment tracking for machine learning projects. It automates environment configuration, tool initialization, and provides code examples to get you started quickly.\n\n## How It Works\n\n1. **Analyze Context**: The skill analyzes the current project context to determine the appropriate experiment tracking tool (MLflow or W&B) based on user preference or existing project configuration.\n2. **Configure Environment**: It configures the environment by installing necessary Python packages and setting environment variables.\n3. **Initialize Tracking**: The skill initializes the chosen tracking tool, potentially starting a local MLflow server or connecting to a W&B project.\n4. **Provide Code Snippets**: It provides code snippets demonstrating how to log experiment parameters, metrics, and artifacts within your ML code.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Start tracking machine learning experiments in a new project.\n- Integrate experiment tracking into an existing ML project.\n- Quickly set up MLflow or Weights & Biases for experiment management.\n- Automate the process of logging parameters, metrics, and artifacts.\n\n## Examples\n\n### Example 1: Starting a New Project with MLflow\n\nUser request: \"track experiments using mlflow\"\n\nThe skill will:\n1. Install the `mlflow` Python package.\n2. Generate example code for logging parameters, metrics, and artifacts to an MLflow server.\n\n### Example 2: Integrating W&B into an Existing Project\n\nUser request: \"setup experiment tracking with wandb\"\n\nThe skill will:\n1. Install the `wandb` Python package.\n2. Generate example code for initializing W&B and logging experiment data.\n\n## Best Practices\n\n- **Tool Selection**: Consider the scale and complexity of your project when choosing between MLflow and W&B. MLflow is well-suited for local tracking, while W&B offers cloud-based collaboration and advanced features.\n- **Consistent Logging**: Establish a consistent logging strategy for parameters, metrics, and artifacts to ensure comparability across experiments.\n- **Artifact Management**: Utilize artifact logging to track models, datasets, and other relevant files associated with each experiment.\n\n## Integration\n\nThis skill can be used in conjunction with other skills that generate or modify machine learning code, such as skills for model training or data preprocessing. It ensures that all experiments are properly tracked and documented.",
      "parentPlugin": {
        "name": "experiment-tracking-setup",
        "category": "ai-ml",
        "path": "plugins/ai-ml/experiment-tracking-setup",
        "version": "1.0.0",
        "description": "Set up ML experiment tracking"
      },
      "filePath": "plugins/ai-ml/experiment-tracking-setup/skills/experiment-tracking-setup/SKILL.md"
    },
    {
      "slug": "setting-up-log-aggregation",
      "name": "setting-up-log-aggregation",
      "description": "Use when setting up log aggregation solutions using ELK, Loki, or Splunk. Trigger with phrases like \"setup log aggregation\", \"deploy ELK stack\", \"configure Loki\", or \"install Splunk\". Generates production-ready configurations for data ingestion, processing, storage, and visualization with proper security and scalability.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(docker:*)",
        "Bash(kubectl:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Target infrastructure is identified (Kubernetes, Docker, VMs)\n- Storage requirements are calculated based on log volume\n- Network connectivity between log sources and aggregation platform\n- Authentication mechanism is defined (LDAP, OAuth, basic auth)\n- Resource allocation planned (CPU, memory, disk)\n\n## Instructions\n\n1. **Select Platform**: Choose ELK, Loki, Grafana Loki, or Splunk\n2. **Configure Ingestion**: Set up log shippers (Filebeat, Promtail, Fluentd)\n3. **Define Storage**: Configure retention policies and index lifecycle\n4. **Set Up Processing**: Create parsing rules and field extractions\n5. **Deploy Visualization**: Configure Kibana/Grafana dashboards\n6. **Implement Security**: Enable authentication, encryption, and RBAC\n7. **Test Pipeline**: Verify logs flow from sources to visualization\n\n## Output\n\n**ELK Stack (Docker Compose):**\n```yaml\n# {baseDir}/elk/docker-compose.yml\nversion: '3.8'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=true\n    volumes:\n      - es-data:/usr/share/elasticsearch/data\n    ports:\n      - \"9200:9200\"\n\n  logstash:\n    image: docker.elastic.co/logstash/logstash:8.11.0\n    volumes:\n      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf\n    depends_on:\n      - elasticsearch\n\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.11.0\n    ports:\n      - \"5601:5601\"\n    depends_on:\n      - elasticsearch\n```\n\n**Loki Configuration:**\n```yaml\n# {baseDir}/loki/loki-config.yaml\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n\ningester:\n  lifecycler:\n    ring:\n      kvstore:\n        store: inmemory\n      replication_factor: 1\n  chunk_idle_period: 5m\n  chunk_retain_period: 30s\n\nschema_config:\n  configs:\n    - from: 2024-01-01\n      store: boltdb-shipper\n      object_store: filesystem\n      schema: v11\n      index:\n        prefix: index_\n        period: 24h\n```\n\n## Error Handling\n\n**Out of Memory**\n- Error: \"Elasticsearch heap space exhausted\"\n- Solution: Increase heap size in elasticsearch.yml or add more nodes\n\n**Connection Refused**\n- Error: \"Cannot connect to Elasticsearch\"\n- Solution: Verify network connectivity and firewall rules\n\n**Index Creation Failed**\n- Error: \"Failed to create index\"\n- Solution: Check disk space and index template configuration\n\n**Log Parsing Errors**\n- Error: \"Failed to parse log line\"\n- Solution: Review grok patterns or JSON parsing configuration\n\n## Resources\n\n- ELK Stack guide: https://www.elastic.co/guide/\n- Loki documentation: https://grafana.com/docs/loki/\n- Example configurations in {baseDir}/log-aggregation-examples/",
      "parentPlugin": {
        "name": "log-aggregation-setup",
        "category": "devops",
        "path": "plugins/devops/log-aggregation-setup",
        "version": "1.0.0",
        "description": "Set up log aggregation (ELK, Loki, Splunk)"
      },
      "filePath": "plugins/devops/log-aggregation-setup/skills/log-aggregation-setup/SKILL.md"
    },
    {
      "slug": "setting-up-synthetic-monitoring",
      "name": "setting-up-synthetic-monitoring",
      "description": "Setup synthetic monitoring for proactive performance tracking including uptime checks, transaction monitoring, and API health. Use when implementing availability monitoring or tracking critical user journeys. Trigger with phrases like \"setup synthetic monitoring\", \"monitor uptime\", or \"configure health checks\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(curl:*)",
        "Bash(monitoring:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill streamlines the process of setting up synthetic monitoring, enabling proactive performance tracking for applications. It guides the user through defining key monitoring scenarios and configuring alerts to ensure optimal application performance and availability.\n\n## How It Works\n\n1. **Identify Monitoring Needs**: Determine the critical endpoints, user journeys, and APIs to monitor based on the user's application requirements.\n2. **Design Monitoring Scenarios**: Create specific monitoring scenarios for uptime, transactions, and API performance, including frequency and location.\n3. **Configure Monitoring**: Set up the synthetic monitoring tool with the designed scenarios, including alerts and dashboards for performance visualization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement uptime monitoring for a web application.\n- Track the performance of critical user journeys through transaction monitoring.\n- Monitor the response time and availability of API endpoints.\n\n## Examples\n\n### Example 1: Setting up Uptime Monitoring\n\nUser request: \"Set up uptime monitoring for my website example.com.\"\n\nThe skill will:\n1. Identify example.com as the target endpoint.\n2. Configure uptime monitoring to check the availability of example.com every 5 minutes from multiple locations.\n\n### Example 2: Monitoring API Performance\n\nUser request: \"Configure API monitoring for the /users endpoint of my application.\"\n\nThe skill will:\n1. Identify the /users endpoint as the target for API monitoring.\n2. Set up monitoring to track the response time and status code of the /users endpoint every minute.\n\n## Best Practices\n\n- **Prioritize Critical Endpoints**: Focus on monitoring the most critical endpoints and user journeys that directly impact user experience.\n- **Set Realistic Thresholds**: Configure alerts with realistic thresholds to avoid false positives and ensure timely notifications.\n- **Regularly Review and Adjust**: Periodically review the monitoring configuration and adjust scenarios and thresholds based on application changes and performance trends.\n\n## Integration\n\nThis skill can be integrated with other plugins for incident management and alerting, such as those that handle notifications via Slack or PagerDuty, allowing for automated incident response workflows based on synthetic monitoring results.\n\n## Prerequisites\n\n- Access to synthetic monitoring platform (Pingdom, Datadog, New Relic)\n- List of critical endpoints and user journeys in {baseDir}/monitoring/endpoints.yaml\n- Alerting infrastructure configuration\n- Geographic monitoring location requirements\n\n## Instructions\n\n1. Identify critical endpoints and user journeys to monitor\n2. Design monitoring scenarios (uptime, transactions, API checks)\n3. Configure monitoring frequency and locations\n4. Set up performance and availability thresholds\n5. Configure alerting for failures and degradation\n6. Create dashboards for monitoring visualization\n\n## Output\n\n- Synthetic monitoring configuration files\n- Uptime check definitions for endpoints\n- Transaction monitoring scripts\n- Alert rule configurations\n- Dashboard definitions for monitoring status\n\n## Error Handling\n\nIf synthetic monitoring setup fails:\n- Verify monitoring platform credentials\n- Check endpoint accessibility from monitoring locations\n- Validate transaction script syntax\n- Ensure alert channel configuration\n- Review threshold definitions\n\n## Resources\n\n- Synthetic monitoring best practices\n- Uptime monitoring service documentation\n- Transaction monitoring script examples\n- Alert threshold tuning guides",
      "parentPlugin": {
        "name": "synthetic-monitoring-setup",
        "category": "performance",
        "path": "plugins/performance/synthetic-monitoring-setup",
        "version": "1.0.0",
        "description": "Set up synthetic monitoring for proactive performance tracking"
      },
      "filePath": "plugins/performance/synthetic-monitoring-setup/skills/synthetic-monitoring-setup/SKILL.md"
    },
    {
      "slug": "simulating-flash-loans",
      "name": "simulating-flash-loans",
      "description": "Simulate flash loan arbitrage strategies and profitability across DeFi protocols. Use when performing crypto analysis. Trigger with phrases like \"analyze crypto\", \"check blockchain\", or \"monitor market\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:flashloan-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:flashloan-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "flash-loan-simulator",
        "category": "crypto",
        "path": "plugins/crypto/flash-loan-simulator",
        "version": "1.0.0",
        "description": "Simulate and analyze flash loan strategies including arbitrage, liquidations, and collateral swaps"
      },
      "filePath": "plugins/crypto/flash-loan-simulator/skills/flash-loan-simulator/SKILL.md"
    },
    {
      "slug": "skill-adapter",
      "name": "skill-adapter",
      "description": "Analyzes existing plugins to extract their capabilities, then adapts",
      "allowedTools": [
        "Read",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Skill Adapter - Universal Plugin Capability Extractor\n\n## Purpose\nAnalyzes plugins in the claude-code-plugins marketplace to understand their capabilities, extracts the core patterns and approaches, then adapts those skills to solve the current user's task. Acts as a \"skill chameleon\" that can adopt any plugin's capabilities.\n\n## How It Works\n\n### 1. Task Analysis\nWhen user presents a task:\n- Identify the core capability needed (e.g., \"analyze code quality\", \"generate documentation\", \"automate deployment\")\n- Determine the domain (security, devops, testing, etc.)\n- Extract key requirements and constraints\n\n### 2. Plugin Discovery\nSearch existing plugins for relevant capabilities:\n\n```bash\n# Find plugins in relevant category\nls plugins/community/ plugins/packages/ plugins/examples/\n\n# Search for keywords in plugin descriptions\ngrep -r \"keyword\" --include=\"plugin.json\" plugins/\n\n# Find similar commands/agents\ngrep -r \"capability-name\" --include=\"*.md\" plugins/\n```\n\n### 3. Capability Extraction\n\nFor each relevant plugin found, analyze:\n\n**Commands (commands/*.md):**\n- Read the markdown content\n- Extract the approach/methodology\n- Identify input/output patterns\n- Note any scripts or tools used\n\n**Agents (agents/*.md):**\n- Understand the agent's role\n- Extract problem-solving approach\n- Note decision-making patterns\n- Identify expertise areas\n\n**Skills (skills/*/SKILL.md):**\n- Read the skill instructions\n- Extract core capability\n- Note trigger conditions\n- Understand tool usage patterns\n\n**Scripts (scripts/*.sh, *.py):**\n- Analyze script logic\n- Extract reusable patterns\n- Identify best practices\n- Note error handling approaches\n\n### 4. Pattern Synthesis\n\nCombine learned patterns:\n- Merge multiple approaches if beneficial\n- Adapt to current context and constraints\n- Simplify or enhance based on user needs\n- Ensure compatibility with current environment\n\n### 5. Skill Application\n\nApply the adapted skill:\n- Use the learned approach\n- Follow the extracted patterns\n- Apply best practices discovered\n- Adapt syntax/tools to current context\n\n## Example Workflows\n\n### Example 1: Learning Code Analysis from Security Plugins\n\n**User task:** \"Analyze this codebase for issues\"\n\n**Process:**\n1. Search for security and code-analysis plugins\n2. Find: `owasp-top-10-scanner`, `code-quality-enforcer`, `security-audit-agent`\n3. Extract patterns:\n   - OWASP scanner checks for: SQL injection, XSS, CSRF, auth issues\n   - Quality enforcer looks at: complexity, duplication, standards\n   - Audit agent examines: dependencies, secrets, permissions\n4. Synthesize approach:\n   - Run multi-layer analysis\n   - Check security patterns first\n   - Then code quality metrics\n   - Then dependency issues\n5. Apply to user's codebase with adapted checks\n\n### Example 2: Adopting Documentation Skills\n\n**User task:** \"Generate API documentation\"\n\n**Process:**\n1. Find documentation plugins\n2. Discover: `api-documenter`, `openapi-generator`, `readme-builder`\n3. Extract approaches:\n   - API documenter: parses code, generates OpenAPI spec\n   - OpenAPI generator: creates interactive docs\n   - README builder: structures documentation hierarchically\n4. Synthesize:\n   - Parse code for endpoints\n   - Generate OpenAPI/Swagger spec\n   - Create interactive documentation\n   - Build comprehensive README\n5. Apply combined approach to user's API\n\n### Example 3: Learning Automation from DevOps Plugins\n\n**User task:** \"Automate deployment process\"\n\n**Process:**\n1. Search DevOps category\n2. Find: `deployment-automation`, `ci-cd-pipeline`, `docker-compose-generator`\n3. Extract patterns:\n   - Deployment automation: build ‚Üí test ‚Üí deploy ‚Üí verify\n   - CI/CD pipeline: trigger conditions, parallel jobs, rollback\n   - Docker compose: service orchestration, environment management\n4. Synthesize deployment workflow\n5. Apply to user's specific tech stack\n\n## Reasoning Process\n\n### When to Use Skill Adapter\n\nTrigger when:\n- User needs capability that might exist in marketplace\n- Task could benefit from existing plugin patterns\n- User asks: \"Is there a plugin for this?\"\n- Similar problems have been solved before\n- Multiple approaches could be combined\n\n### Plugin Selection Criteria\n\nChoose plugins based on:\n1. **Relevance**: Matches task domain/requirements\n2. **Quality**: Well-documented, clear approach\n3. **Simplicity**: Not overly complex for the task\n4. **Recency**: Updated plugins preferred\n5. **Popularity**: Featured or well-maintained plugins\n\n### Adaptation Strategy\n\nWhen adapting skills:\n- **Keep**: Core logic and proven patterns\n- **Adapt**: Syntax, tool names, specific commands\n- **Enhance**: Add error handling, user feedback\n- **Simplify**: Remove unnecessary complexity\n- **Contextualize**: Adjust to user's environment\n\n## Limitations and Boundaries\n\n### What Skill Adapter CAN Do:\n‚úÖ Read and analyze plugin source code\n‚úÖ Extract patterns and approaches\n‚úÖ Adapt methodologies to new contexts\n‚úÖ Combine multiple plugin capabilities\n‚úÖ Apply learned skills with reasoning\n\n### What Skill Adapter CANNOT Do:\n‚ùå Execute compiled code (MCP servers)\n‚ùå Access external APIs without credentials\n‚ùå Modify original plugins\n‚ùå Guarantee exact plugin behavior replication\n‚ùå Use plugins that require specific environment setup\n\n## Success Criteria\n\nSkill adaptation is successful when:\n1. User's task is completed effectively\n2. Approach borrowed makes logical sense\n3. Adapted skill is properly contextualized\n4. User understands where the approach came from\n5. Result quality matches or exceeds original plugin\n\n## Transparency\n\nAlways inform user:\n- Which plugins were analyzed\n- What patterns were extracted\n- How the skill was adapted\n- Why this approach was chosen\n- Any limitations of the adaptation\n\n## Example Usage\n\n```\nUser: \"I need to validate JSON schemas in my project\"\n\nSkill Adapter Process:\n1. Searches plugins for JSON validation\n2. Finds: schema-validator, json-lint-enforcer\n3. Extracts: ajv library usage, error formatting patterns\n4. Adapts: Uses available tools (jq, node, python)\n5. Applies: Validates user's schemas with detailed errors\n6. Reports: \"I adapted the schema-validator approach using jq\n   for validation and added custom error formatting\"\n```\n\n## Meta-Learning\n\nSkill Adapter improves by:\n- Tracking which plugins solve which tasks best\n- Learning which patterns are most reusable\n- Noting which adaptations work well\n- Building a mental model of the marketplace\n- Understanding plugin ecosystem relationships\n\n---\n\n**In essence:** Skill Adapter is a meta-skill that makes the entire plugin marketplace available as a learning resource, extracting and applying capabilities on-demand to solve user tasks efficiently.",
      "parentPlugin": {
        "name": "pi-pathfinder",
        "category": "examples",
        "path": "plugins/examples/pi-pathfinder",
        "version": "1.0.0",
        "description": "PI Pathfinder - Finds the path through 229 plugins. Automatically picks the best plugin for your task, extracts its skills, and applies them. You don't pick plugins, PI does."
      },
      "filePath": "plugins/examples/pi-pathfinder/skills/pi-pathfinder/SKILL.md"
    },
    {
      "slug": "spec-writing",
      "name": "spec-writing",
      "description": "This skill should be used when the user asks about \"writing specs\", \"specs.md format\", \"how to write specifications\", \"sprint requirements\", \"testing configuration\", \"scope definition\", or needs guidance on creating effective sprint specifications for agentic development.",
      "allowedTools": [
        "Read"
      ],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Spec Writing\n\nSpecifications drive the entire sprint workflow. Well-written specs lead to focused implementations; vague specs cause wasted iterations. This skill covers writing effective `specs.md` files that guide autonomous agents toward successful outcomes.\n\n## The specs.md File\n\nLocated at `.claude/sprint/[N]/specs.md`, this file is the primary input to the sprint system. It tells the architect what to build and how to test it.\n\n## Essential Structure\n\nA complete specs.md contains:\n\n```markdown\n# Sprint [N]: [Short Title]\n\n## Goal\n[1-2 sentences describing what success looks like]\n\n## Scope\n\n### In Scope\n- [Feature or task 1]\n- [Feature or task 2]\n\n### Out of Scope\n- [What NOT to do]\n\n## Requirements\n[Detailed requirements - can be minimal or extensive]\n\n## Testing\n- QA: required / optional / skip\n- UI Testing: required / optional / skip\n- UI Testing Mode: automated / manual\n```\n\n## Writing Effective Goals\n\nThe goal statement shapes the entire sprint. Make it outcome-focused:\n\n**Good goals:**\n- \"Users can register, login, and reset passwords\"\n- \"API returns paginated product listings with filters\"\n- \"Dashboard displays real-time metrics from backend\"\n\n**Bad goals:**\n- \"Implement authentication\" (too vague)\n- \"Fix the bug\" (which bug?)\n- \"Make it better\" (unmeasurable)\n\nA good goal answers: \"How will we know when we're done?\"\n\n## Defining Scope\n\n### In Scope\n\nList concrete deliverables:\n- Features to implement\n- Endpoints to create\n- UI components to build\n- Tests to write\n\nBe specific. \"Add user profile page\" is better than \"improve user experience\".\n\n### Out of Scope\n\nExplicitly exclude work to prevent scope creep:\n- Related features not in this sprint\n- Refactoring not needed for the goal\n- Nice-to-haves for future sprints\n\nThis prevents agents from over-engineering or adding unrequested features.\n\n## Requirements Depth\n\nSpecs can range from minimal to detailed. The architect adapts accordingly.\n\n### Minimal Spec (One-liner)\n\n```markdown\n# Sprint 5: Add dark mode toggle\n\n## Goal\nUsers can switch between light and dark themes.\n\n## Testing\n- UI Testing: required\n- UI Testing Mode: manual\n```\n\nAppropriate for: Simple features, trusted architect judgment, exploratory work.\n\n### Detailed Spec\n\n```markdown\n# Sprint 12: User Authentication\n\n## Goal\nComplete authentication flow with email verification.\n\n## Scope\n\n### In Scope\n- Registration with email/password\n- Login with session management\n- Password reset via email\n- Email verification flow\n- Protected route middleware\n\n### Out of Scope\n- OAuth providers (future sprint)\n- Two-factor authentication\n- Account deletion\n\n## Requirements\n\n### Registration\n- Email must be unique\n- Password minimum 8 characters\n- Send verification email on signup\n- Users cannot login until verified\n\n### Login\n- Return JWT token on success\n- Rate limit: 5 attempts per minute\n- Lock account after 10 failed attempts\n\n### Password Reset\n- Token expires after 1 hour\n- Invalidate token after use\n- Send confirmation email after reset\n\n## API Endpoints\n\n| Method | Route | Purpose |\n|--------|-------|---------|\n| POST | /auth/register | Create account |\n| POST | /auth/login | Authenticate |\n| POST | /auth/verify | Verify email |\n| POST | /auth/reset-request | Request reset |\n| POST | /auth/reset | Reset password |\n\n## Testing\n- QA: required\n- UI Testing: required\n- UI Testing Mode: automated\n```\n\nAppropriate for: Complex features, specific requirements, team handoffs.\n\n## Testing Configuration\n\nThe Testing section controls which testing agents run.\n\n### Options\n\n| Setting | Values | Meaning |\n|---------|--------|---------|\n| QA | required / optional / skip | API and unit tests |\n| UI Testing | required / optional / skip | Browser-based E2E tests |\n| UI Testing Mode | automated / manual | Auto-run or user-driven |\n\n### When to Use Each\n\n**QA: required**\n- New API endpoints\n- Business logic changes\n- Data validation rules\n\n**QA: skip**\n- Frontend-only changes\n- Documentation updates\n- Configuration changes\n\n**UI Testing: required**\n- User-facing features\n- Form submissions\n- Navigation flows\n\n**UI Testing Mode: manual**\n- Complex interactions\n- Visual verification needed\n- Exploratory testing\n\n**UI Testing Mode: automated**\n- Regression testing\n- Standard CRUD flows\n- Repeatable scenarios\n\n## Common Patterns\n\n### New Feature\n\n```markdown\n## Goal\n[What the feature does]\n\n## Scope\n### In Scope\n- Backend API\n- Frontend UI\n- Integration tests\n\n## Testing\n- QA: required\n- UI Testing: required\n- UI Testing Mode: automated\n```\n\n### Bug Fix\n\n```markdown\n## Goal\nFix [specific issue description]\n\n## Root Cause\n[If known, describe the cause]\n\n## Expected Behavior\n[What should happen]\n\n## Testing\n- QA: required  # Regression test\n- UI Testing: optional\n```\n\n### Refactoring\n\n```markdown\n## Goal\nRefactor [component] for [benefit]\n\n## Constraints\n- No behavior changes\n- Maintain API compatibility\n- All existing tests must pass\n\n## Testing\n- QA: required  # Verify no regressions\n- UI Testing: skip\n```\n\n## Tips for Better Specs\n\n### Be Specific, Not Prescriptive\n\nTell the architect WHAT to build, not HOW to build it:\n- Good: \"Users can filter products by category and price range\"\n- Bad: \"Use a Redux slice with useSelector for filter state\"\n\nThe architect chooses implementation details.\n\n### Include Edge Cases\n\nMention important edge cases in requirements:\n- Empty states\n- Error conditions\n- Boundary values\n- Concurrent access\n\n### Reference Existing Patterns\n\nIf the codebase has conventions, mention them:\n- \"Follow the existing auth middleware pattern\"\n- \"Use the same validation approach as user endpoints\"\n\n### Keep It Maintainable\n\nSpecs should be readable by humans too:\n- Use clear headings\n- Keep bullet points short\n- Include examples where helpful\n\n## Iteration and Updates\n\nSpecs evolve during the sprint:\n\n1. **Initial**: User writes complete specs.md\n2. **Phase 1**: Architect may clarify or expand\n3. **Each iteration**: Architect removes completed items\n4. **Final**: Specs reflect only documented decisions\n\nThis convergent pattern keeps context focused.\n\n## Additional Resources\n\nThe `/sprint:new` command creates specs.md templates interactively. See the command file for implementation details.",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/spec-writing/SKILL.md"
    },
    {
      "slug": "splitting-datasets",
      "name": "splitting-datasets",
      "description": "Split datasets into training, validation, and testing sets for ML model",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill automates the process of dividing a dataset into subsets for training, validating, and testing machine learning models. It ensures proper data preparation and facilitates robust model evaluation.\n\n## How It Works\n\n1. **Analyze Request**: The skill analyzes the user's request to determine the dataset to be split and the desired proportions for each subset.\n2. **Generate Code**: Based on the request, the skill generates Python code utilizing standard ML libraries to perform the data splitting.\n3. **Execute Splitting**: The code is executed to split the dataset into training, validation, and testing sets according to the specified ratios.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Prepare a dataset for machine learning model training.\n- Create training, validation, and testing sets.\n- Partition data to evaluate model performance.\n\n## Examples\n\n### Example 1: Splitting a CSV file\n\nUser request: \"Split the data in 'my_data.csv' into 70% training, 15% validation, and 15% testing sets.\"\n\nThe skill will:\n1. Generate Python code to read the 'my_data.csv' file.\n2. Execute the code to split the data according to the specified proportions, creating 'train.csv', 'validation.csv', and 'test.csv' files.\n\n### Example 2: Creating a Train-Test Split\n\nUser request: \"Create a train-test split of 'large_dataset.csv' with an 80/20 ratio.\"\n\nThe skill will:\n1. Generate Python code to load 'large_dataset.csv'.\n2. Execute the code to split the dataset into 80% training and 20% testing sets, saving them as 'train.csv' and 'test.csv'.\n\n## Best Practices\n\n- **Data Integrity**: Verify that the splitting process maintains the integrity of the data, ensuring no data loss or corruption.\n- **Stratification**: Consider stratification when splitting imbalanced datasets to maintain class distributions in each subset.\n- **Randomization**: Ensure the splitting process is randomized to avoid bias in the resulting datasets.\n\n## Integration\n\nThis skill can be integrated with other data processing and model training tools within the Claude Code ecosystem to create a complete machine learning workflow.",
      "parentPlugin": {
        "name": "dataset-splitter",
        "category": "ai-ml",
        "path": "plugins/ai-ml/dataset-splitter",
        "version": "1.0.0",
        "description": "Split datasets for training, validation, and testing"
      },
      "filePath": "plugins/ai-ml/dataset-splitter/skills/dataset-splitter/SKILL.md"
    },
    {
      "slug": "sprint-workflow",
      "name": "sprint-workflow",
      "description": "This skill should be used when the user asks about \"how sprints work\", \"sprint phases\", \"iteration workflow\", \"convergent development\", \"sprint lifecycle\", \"when to use sprints\", or wants to understand the sprint execution model and its convergent diffusion approach.",
      "allowedTools": [
        "Read"
      ],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Sprint Workflow\n\nSprint implements a convergent development model where autonomous agents iteratively refine implementations until specifications are satisfied. This skill covers the execution lifecycle, phase transitions, and iteration patterns.\n\n## Core Concept: Convergent Diffusion\n\nTraditional AI-assisted development suffers from context bloat - each iteration adds more information, compounding errors and noise. Sprint reverses this pattern:\n\n- **Start noisy**: Initial specs may be vague or incomplete\n- **Converge iteratively**: Each iteration removes completed work from specs\n- **Focus narrows**: Agents receive only remaining tasks, not history\n- **Signal improves**: Less noise means better output quality\n\nThe metaphor is diffusion models in reverse - instead of adding noise to generate, remove noise to refine.\n\n## Sprint Phases\n\nA sprint executes through 6 distinct phases:\n\n### Phase 0: Load Specifications\n\nParse the sprint directory and prepare context:\n- Locate sprint directory (`.claude/sprint/[N]/`)\n- Read `specs.md` for user requirements\n- Read `status.md` if resuming\n- Detect project type for framework-specific agents\n\n### Phase 1: Architectural Planning\n\nThe project-architect agent analyzes requirements:\n- Read existing `project-map.md` for architecture context\n- Read `project-goals.md` for business objectives\n- Create specification files (`api-contract.md`, `backend-specs.md`, etc.)\n- Return SPAWN REQUEST for implementation agents\n\n### Phase 2: Implementation\n\nSpawn implementation agents in parallel:\n- `python-dev` for Python/FastAPI backend\n- `nextjs-dev` for Next.js frontend\n- `cicd-agent` for CI/CD pipelines\n- `allpurpose-agent` for any other technology\n- Collect structured reports from each agent\n\n### Phase 3: Testing\n\nExecute testing agents:\n- `qa-test-agent` runs first (API and unit tests)\n- `ui-test-agent` runs after (browser-based E2E tests)\n- Framework-specific diagnostics agents run in parallel with UI tests\n- Collect test reports\n\n### Phase 4: Review & Iteration\n\nArchitect reviews all reports:\n- Analyze conformity status\n- Update specifications (remove completed, add fixes)\n- Update `status.md` with current state\n- Decide: more implementation, more testing, or finalize\n\n### Phase 5: Finalization\n\nSprint completion:\n- Final `status.md` summary\n- All specs in consistent state\n- **Clean up manual-test-report.md** (no longer relevant)\n- Signal FINALIZE to orchestrator\n\n## Resuming Sprints\n\nWhen running `/sprint` on an existing sprint:\n\n**If status.md shows COMPLETE:**\n- System asks: Run manual testing? Continue with fixes? Create new sprint?\n- Guides user to appropriate action\n\n**If status.md shows IN PROGRESS:**\n- If manual-test-report.md exists: Uses it to inform architect\n- If not: Offers to run manual testing first or continue\n\nThis ensures the user always knows where they are and what options they have.\n\n## Iteration Loop\n\nThe sprint cycles between phases 1-4 until complete:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                                         ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ Phase 1  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Phase 2          ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ Planning ‚îÇ    ‚îÇ Implementation   ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ       ‚ñ≤                   ‚îÇ            ‚îÇ\n‚îÇ       ‚îÇ                   ‚ñº            ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ Phase 4  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ Phase 3          ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ Review   ‚îÇ    ‚îÇ Testing          ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                                         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº (after max 5 iterations or success)\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ Phase 5  ‚îÇ\n    ‚îÇ Finalize ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Maximum 5 iterations**: The system pauses after 5 cycles to prevent infinite loops. User intervention may be needed for complex blockers.\n\n## When to Use Sprints\n\nSprints are ideal for:\n- Multi-component features (backend + frontend + tests)\n- Complex requirements needing architectural planning\n- Tasks requiring coordination between specialized agents\n- Incremental development with testing validation\n\nSprints are overkill for:\n- Simple bug fixes (use direct implementation)\n- Single-file changes\n- Documentation-only updates\n- Quick prototypes without testing needs\n\n## Key Artifacts\n\n### User-Created\n\n| File | Purpose |\n|------|---------|\n| `specs.md` | Requirements, scope, testing config |\n| `project-goals.md` | Business vision and objectives |\n\n### Architect-Created\n\n| File | Purpose |\n|------|---------|\n| `status.md` | Current sprint state (architect maintains) |\n| `project-map.md` | Technical architecture (architect maintains) |\n| `api-contract.md` | Shared interface between agents |\n| `*-specs.md` | Agent-specific implementation guidance |\n\n### Orchestrator-Created\n\n| File | Purpose |\n|------|---------|\n| `*-report-[N].md` | Agent reports per iteration |\n\n## Convergence Principles\n\n### Specs Shrink Over Time\n\nAfter each iteration, the architect:\n- Removes completed tasks from spec files\n- Removes outdated information\n- Keeps only remaining work\n\nThis prevents context bloat and focuses agents on actual gaps.\n\n### Status Stays Current\n\n`status.md` is rewritten each iteration, not appended:\n- Always reflects current truth\n- Maximum ~50 lines\n- No historical log dumps\n\n### Reports Are Structured\n\nAgents return machine-parseable reports:\n- Standard sections (CONFORMITY, DEVIATIONS, ISSUES)\n- No verbose prose\n- Actionable information only\n\n## Manual Testing\n\nThere are two ways to do manual testing:\n\n### Within a Sprint (specs-driven)\n\nSet `UI Testing Mode: manual` in your specs.md:\n```markdown\n## Testing\n- UI Testing: required\n- UI Testing Mode: manual\n```\n\nWhen the architect requests UI testing:\n1. Browser opens pointing to your app\n2. You explore the app manually\n3. Console errors are monitored in the background\n4. **Close the browser tab** when done testing\n5. Agent detects tab close and returns report\n6. Sprint continues with architect review\n\n### Standalone Testing (quick access)\n\nUse `/sprint:test` for quick testing outside of sprints:\n- Opens Chrome browser directly\n- Monitors errors while you explore\n- Say \"finish testing\" when done\n- **Report saved to `.claude/sprint/[N]/manual-test-report.md`**\n\n**Reports feed into sprints:** When you run `/sprint`, the architect reads your manual test report and prioritizes fixing the issues you discovered.\n\nUse manual testing for:\n- Exploratory testing before a sprint\n- Bug hunting and discovery\n- UX validation\n- Edge cases hard to automate\n\n## Additional Resources\n\nFor more details, see the full command and agent documentation in the plugin's `commands/` and `agents/` directories.",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/sprint-workflow/SKILL.md"
    },
    {
      "slug": "testing-browser-compatibility",
      "name": "testing-browser-compatibility",
      "description": "Test across multiple browsers and devices for cross-browser compatibility. Use when ensuring cross-browser or device compatibility. Trigger with phrases like \"test browser compatibility\", \"check cross-browser\", or \"validate on browsers\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:browser-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:browser-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "browser-compatibility-tester",
        "category": "testing",
        "path": "plugins/testing/browser-compatibility-tester",
        "version": "1.0.0",
        "description": "Cross-browser testing with BrowserStack, Selenium Grid, and Playwright - test across Chrome, Firefox, Safari, Edge"
      },
      "filePath": "plugins/testing/browser-compatibility-tester/skills/browser-compatibility-tester/SKILL.md"
    },
    {
      "slug": "testing-load-balancers",
      "name": "testing-load-balancers",
      "description": "Validate load balancer behavior, failover, and traffic distribution. Use when performing specialized testing. Trigger with phrases like \"test load balancer\", \"validate failover\", or \"check traffic distribution\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:loadbalancer-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:loadbalancer-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "load-balancer-tester",
        "category": "testing",
        "path": "plugins/testing/load-balancer-tester",
        "version": "1.0.0",
        "description": "Test load balancing strategies with traffic distribution validation and failover testing"
      },
      "filePath": "plugins/testing/load-balancer-tester/skills/load-balancer-tester/SKILL.md"
    },
    {
      "slug": "testing-mobile-apps",
      "name": "testing-mobile-apps",
      "description": "Execute mobile app testing on iOS and Android devices/simulators. Use when performing specialized testing. Trigger with phrases like \"test mobile app\", \"run iOS tests\", or \"validate Android functionality\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:mobile-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:mobile-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "mobile-app-tester",
        "category": "testing",
        "path": "plugins/testing/mobile-app-tester",
        "version": "1.0.0",
        "description": "Mobile app test automation with Appium, Detox, XCUITest - test iOS and Android apps"
      },
      "filePath": "plugins/testing/mobile-app-tester/skills/mobile-app-tester/SKILL.md"
    },
    {
      "slug": "testing-visual-regression",
      "name": "testing-visual-regression",
      "description": "Detect visual changes in UI components using screenshot comparison. Use when detecting unintended UI changes or pixel differences. Trigger with phrases like \"test visual changes\", \"compare screenshots\", or \"detect UI regressions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:visual-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:visual-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "visual-regression-tester",
        "category": "testing",
        "path": "plugins/testing/visual-regression-tester",
        "version": "1.0.0",
        "description": "Visual diff testing with Percy, Chromatic, BackstopJS - catch unintended UI changes"
      },
      "filePath": "plugins/testing/visual-regression-tester/skills/visual-regression-tester/SKILL.md"
    },
    {
      "slug": "throttling-apis",
      "name": "throttling-apis",
      "description": "Manage API throttling policies to protect backend services from overload. Use when controlling API request rates. Trigger with phrases like \"throttle API\", \"control request rate\", or \"add throttling\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:throttle-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:throttle-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-throttling-manager",
        "category": "api-development",
        "path": "plugins/api-development/api-throttling-manager",
        "version": "1.0.0",
        "description": "Manage API throttling with dynamic rate limits and quota management"
      },
      "filePath": "plugins/api-development/api-throttling-manager/skills/api-throttling-manager/SKILL.md"
    },
    {
      "slug": "tracking-application-response-times",
      "name": "tracking-application-response-times",
      "description": "Track and optimize application response times across API endpoints, database queries, and service calls. Use when monitoring performance or identifying bottlenecks. Trigger with phrases like \"track response times\", \"monitor API performance\", or \"analyze latency\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(monitoring:*)",
        "Bash(metrics:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to proactively monitor and improve application performance by tracking response times across various layers. It provides detailed metrics and insights to identify and resolve performance bottlenecks.\n\n## How It Works\n\n1. **Initiate Tracking**: The user requests response time tracking.\n2. **Configure Monitoring**: The plugin automatically begins monitoring API endpoints, database queries, external service calls, frontend rendering, and background jobs.\n3. **Report Metrics**: The plugin generates reports including P50, P95, P99 percentiles, average, and maximum response times.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in your application.\n- Monitor service level objectives (SLOs) related to response times.\n- Receive alerts about performance degradation.\n\n## Examples\n\n### Example 1: Diagnosing Slow API Endpoint\n\nUser request: \"Track response times for the user authentication API endpoint.\"\n\nThe skill will:\n1. Activate the response-time-tracker plugin.\n2. Monitor the specified API endpoint and report response time metrics, highlighting potential bottlenecks.\n\n### Example 2: Monitoring Database Query Performance\n\nUser request: \"Monitor database query performance for the product catalog.\"\n\nThe skill will:\n1. Activate the response-time-tracker plugin.\n2. Track the execution time of database queries related to the product catalog and provide performance insights.\n\n## Best Practices\n\n- **Granularity**: Track response times at a granular level (e.g., individual API endpoints, specific database queries) for more precise insights.\n- **Alerting**: Configure alerts for significant deviations from baseline performance to proactively address potential issues.\n- **Contextualization**: Correlate response time data with other metrics (e.g., CPU usage, memory consumption) to identify root causes.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools to provide a comprehensive view of application performance. It can also be used in conjunction with optimization tools to automatically address identified bottlenecks.\n\n## Prerequisites\n\n- Access to application monitoring infrastructure\n- Response time data collection in {baseDir}/metrics/response-times/\n- APM tools or custom instrumentation\n- Performance SLO definitions\n\n## Instructions\n\n1. Configure monitoring for API endpoints and database queries\n2. Collect response time metrics (P50, P95, P99 percentiles)\n3. Analyze trends and identify performance degradation\n4. Compare against performance baselines and SLOs\n5. Identify bottlenecks and root causes\n6. Generate optimization recommendations\n\n## Output\n\n- Response time reports with percentile metrics\n- Performance trend visualizations\n- Bottleneck identification analysis\n- SLO compliance status\n- Optimization recommendations with priorities\n\n## Error Handling\n\nIf response time tracking fails:\n- Verify monitoring agent installation\n- Check instrumentation configuration\n- Validate metric export endpoints\n- Ensure data storage availability\n- Review sampling configuration\n\n## Resources\n\n- APM tool documentation\n- Response time monitoring best practices\n- Percentile-based SLO definitions\n- Performance optimization guides",
      "parentPlugin": {
        "name": "response-time-tracker",
        "category": "performance",
        "path": "plugins/performance/response-time-tracker",
        "version": "1.0.0",
        "description": "Track and optimize application response times"
      },
      "filePath": "plugins/performance/response-time-tracker/skills/response-time-tracker/SKILL.md"
    },
    {
      "slug": "tracking-crypto-derivatives",
      "name": "tracking-crypto-derivatives",
      "description": "Track futures, options, and perpetual swap positions with P&L calculations. Use when tracking futures and options positions. Trigger with phrases like \"track derivatives\", \"check futures positions\", or \"analyze perps\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:derivatives-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:derivatives-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "crypto-derivatives-tracker",
        "category": "crypto",
        "path": "plugins/crypto/crypto-derivatives-tracker",
        "version": "1.0.0",
        "description": "Track crypto futures, options, perpetual swaps with funding rates, open interest, and derivatives market analysis"
      },
      "filePath": "plugins/crypto/crypto-derivatives-tracker/skills/crypto-derivatives-tracker/SKILL.md"
    },
    {
      "slug": "tracking-crypto-portfolio",
      "name": "tracking-crypto-portfolio",
      "description": "Track multi-chain crypto portfolio with real-time valuations and performance metrics. Use when managing multi-chain crypto holdings. Trigger with phrases like \"track my portfolio\", \"check holdings\", or \"analyze positions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:portfolio-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:portfolio-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "crypto-portfolio-tracker",
        "category": "crypto",
        "path": "plugins/crypto/crypto-portfolio-tracker",
        "version": "1.0.0",
        "description": "Professional crypto portfolio tracking with real-time prices, PnL analysis, and risk metrics"
      },
      "filePath": "plugins/crypto/crypto-portfolio-tracker/skills/crypto-portfolio-tracker/SKILL.md"
    },
    {
      "slug": "tracking-crypto-prices",
      "name": "tracking-crypto-prices",
      "description": "Track real-time cryptocurrency prices across exchanges with historical data and alerts. Use when monitoring real-time cryptocurrency prices. Trigger with phrases like \"check price\", \"track prices\", or \"get price alert\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:price-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:price-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "market-price-tracker",
        "category": "crypto",
        "path": "plugins/crypto/market-price-tracker",
        "version": "1.0.0",
        "description": "Real-time market price tracking with multi-exchange feeds and advanced alerts"
      },
      "filePath": "plugins/crypto/market-price-tracker/skills/market-price-tracker/SKILL.md"
    },
    {
      "slug": "tracking-model-versions",
      "name": "tracking-model-versions",
      "description": "This skill enables claude to track and manage ai/ml model versions using",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to interact with the model-versioning-tracker plugin, providing a streamlined approach to managing and tracking AI/ML model versions. It ensures that model development and deployment are conducted with proper version control, logging, and performance monitoring.\n\n## How It Works\n\n1. **Analyze Request**: Claude analyzes the user's request to determine the specific model versioning task.\n2. **Generate Code**: Claude generates the necessary code to interact with the model-versioning-tracker plugin.\n3. **Execute Task**: The plugin executes the code, performing the requested model versioning operation, such as tracking a new version or retrieving performance metrics.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Track new versions of AI/ML models.\n- Retrieve performance metrics for specific model versions.\n- Implement automated workflows for model versioning.\n\n## Examples\n\n### Example 1: Tracking a New Model Version\n\nUser request: \"Track a new version of my image classification model.\"\n\nThe skill will:\n1. Generate code to log the new model version and its associated metadata using the model-versioning-tracker plugin.\n2. Execute the code, creating a new entry in the model registry.\n\n### Example 2: Retrieving Performance Metrics\n\nUser request: \"Get the performance metrics for version 3 of my sentiment analysis model.\"\n\nThe skill will:\n1. Generate code to query the model-versioning-tracker plugin for the performance metrics associated with the specified model version.\n2. Execute the code and return the metrics to the user.\n\n## Best Practices\n\n- **Data Validation**: Ensure input data is validated before logging model versions.\n- **Error Handling**: Implement robust error handling to manage unexpected issues during version tracking.\n- **Performance Monitoring**: Continuously monitor model performance to identify opportunities for optimization.\n\n## Integration\n\nThis skill integrates with other Claude Code plugins by providing a centralized location for managing AI/ML model versions. It can be used in conjunction with plugins that handle data processing, model training, and deployment to ensure a seamless AI/ML workflow.",
      "parentPlugin": {
        "name": "model-versioning-tracker",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-versioning-tracker",
        "version": "1.0.0",
        "description": "Track and manage model versions"
      },
      "filePath": "plugins/ai-ml/model-versioning-tracker/skills/model-versioning-tracker/SKILL.md"
    },
    {
      "slug": "tracking-regression-tests",
      "name": "tracking-regression-tests",
      "description": "Track and manage regression test suites across releases. Use when performing specialized testing. Trigger with phrases like \"track regressions\", \"manage regression suite\", or \"validate against baseline\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:regression-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:regression-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "regression-test-tracker",
        "category": "testing",
        "path": "plugins/testing/regression-test-tracker",
        "version": "1.0.0",
        "description": "Track and run regression tests to ensure new changes don't break existing functionality"
      },
      "filePath": "plugins/testing/regression-test-tracker/skills/regression-test-tracker/SKILL.md"
    },
    {
      "slug": "tracking-resource-usage",
      "name": "tracking-resource-usage",
      "description": "Track and optimize resource usage across application stack including CPU, memory, disk, and network I/O. Use when identifying bottlenecks or optimizing costs. Trigger with phrases like \"track resource usage\", \"monitor CPU and memory\", or \"optimize resource allocation\".",
      "allowedTools": [
        "Read",
        "Bash(top:*)",
        "Bash(ps:*)",
        "Bash(vmstat:*)",
        "Bash(iostat:*)",
        "Grep",
        "Glob"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill provides a comprehensive solution for monitoring and optimizing resource usage within an application. It leverages the resource-usage-tracker plugin to gather real-time metrics, identify performance bottlenecks, and suggest optimization strategies.\n\n## How It Works\n\n1. **Identify Resources**: The skill identifies the resources to be tracked based on the user's request and the application's configuration (CPU, memory, disk I/O, network I/O, etc.).\n2. **Collect Metrics**: The plugin collects real-time metrics for the identified resources, providing a snapshot of current resource consumption.\n3. **Analyze Data**: The skill analyzes the collected data to identify performance bottlenecks, resource imbalances, and potential optimization opportunities.\n4. **Provide Recommendations**: Based on the analysis, the skill provides specific recommendations for optimizing resource allocation, right-sizing instances, and reducing costs.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in an application.\n- Optimize resource allocation to improve efficiency.\n- Reduce cloud infrastructure costs by right-sizing instances.\n- Monitor resource usage in real-time to detect anomalies.\n- Track the impact of code changes on resource consumption.\n\n## Examples\n\n### Example 1: Identifying Memory Leaks\n\nUser request: \"Track memory usage and identify potential memory leaks.\"\n\nThe skill will:\n1. Activate the resource-usage-tracker plugin to monitor memory usage (heap, stack, RSS).\n2. Analyze the memory usage data over time to detect patterns indicative of memory leaks.\n3. Provide recommendations for identifying and resolving the memory leaks.\n\n### Example 2: Optimizing Database Connection Pool\n\nUser request: \"Optimize database connection pool utilization.\"\n\nThe skill will:\n1. Activate the resource-usage-tracker plugin to monitor database connection pool metrics.\n2. Analyze the connection pool utilization data to identify periods of high contention or underutilization.\n3. Provide recommendations for adjusting the connection pool size to optimize performance and resource consumption.\n\n## Best Practices\n\n- **Granularity**: Track resource usage at a granular level (e.g., process-level CPU usage) to identify specific bottlenecks.\n- **Historical Data**: Analyze historical resource usage data to identify trends and predict future resource needs.\n- **Alerting**: Configure alerts to notify you when resource usage exceeds predefined thresholds.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools to provide a comprehensive view of application performance. It can also be used in conjunction with deployment automation tools to automatically right-size instances based on resource usage patterns.\n\n## Prerequisites\n\n- Access to system monitoring tools (top, ps, vmstat, iostat)\n- Resource metrics collection infrastructure\n- Historical usage data in {baseDir}/metrics/resources/\n- Performance baseline definitions\n\n## Instructions\n\n1. Identify resources to track (CPU, memory, disk, network)\n2. Collect real-time metrics using system tools\n3. Analyze data for bottlenecks and patterns\n4. Compare against historical baselines\n5. Generate optimization recommendations\n6. Provide right-sizing and cost reduction strategies\n\n## Output\n\n- Resource usage reports with trends\n- Bottleneck identification and analysis\n- Right-sizing recommendations for instances\n- Cost optimization suggestions\n- Alert configurations for thresholds\n\n## Error Handling\n\nIf resource tracking fails:\n- Verify system monitoring tool permissions\n- Check metrics collection daemon status\n- Validate data storage availability\n- Ensure network access to monitoring endpoints\n- Review baseline data completeness\n\n## Resources\n\n- System performance monitoring guides\n- Cloud resource optimization best practices\n- CPU and memory profiling techniques\n- Infrastructure cost optimization strategies",
      "parentPlugin": {
        "name": "resource-usage-tracker",
        "category": "performance",
        "path": "plugins/performance/resource-usage-tracker",
        "version": "1.0.0",
        "description": "Track and optimize resource usage across the stack"
      },
      "filePath": "plugins/performance/resource-usage-tracker/skills/resource-usage-tracker/SKILL.md"
    },
    {
      "slug": "tracking-service-reliability",
      "name": "tracking-service-reliability",
      "description": "Define and track SLAs, SLIs, and SLOs for service reliability including availability, latency, and error rates. Use when establishing reliability targets or monitoring service health. Trigger with phrases like \"define SLOs\", \"track SLI metrics\", or \"calculate error budget\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(monitoring:*)",
        "Bash(metrics:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill provides a structured approach to defining and tracking SLAs, SLIs, and SLOs, which are essential for ensuring service reliability. It automates the process of setting performance targets and monitoring actual performance, enabling proactive identification and resolution of potential issues.\n\n## How It Works\n\n1. **SLI Definition**: The skill guides the user to define Service Level Indicators (SLIs) such as availability, latency, error rate, and throughput.\n2. **SLO Target Setting**: The skill assists in setting Service Level Objectives (SLOs) by establishing target values for the defined SLIs (e.g., 99.9% availability).\n3. **SLA Establishment**: The skill helps in formalizing Service Level Agreements (SLAs), which are customer-facing commitments based on the defined SLOs.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Define SLAs, SLIs, and SLOs for a service.\n- Track service performance against defined objectives.\n- Calculate error budgets based on SLOs.\n\n## Examples\n\n### Example 1: Defining SLOs for a New Service\n\nUser request: \"Create SLOs for our new payment processing service.\"\n\nThe skill will:\n1. Prompt the user to define SLIs (e.g., latency, error rate).\n2. Assist in setting target values for each SLI (e.g., p99 latency < 100ms, error rate < 0.01%).\n\n### Example 2: Tracking Availability\n\nUser request: \"Track the availability SLI for the database service.\"\n\nThe skill will:\n1. Guide the user in setting up the tracking of the availability SLI.\n2. Visualize availability performance against the defined SLO.\n\n## Best Practices\n\n- **Granularity**: Define SLIs that are specific and measurable.\n- **Realism**: Set SLOs that are challenging but achievable.\n- **Alignment**: Ensure SLAs align with the defined SLOs and business requirements.\n\n## Integration\n\nThis skill can be integrated with monitoring tools to automatically collect SLI data and track performance against SLOs. It can also be used in conjunction with alerting systems to trigger notifications when SLO violations occur.\n\n## Prerequisites\n\n- SLI definitions stored in {baseDir}/slos/sli-definitions.yaml\n- Access to monitoring and metrics systems\n- Historical performance data for baseline\n- Business requirements for service reliability\n\n## Instructions\n\n1. Define Service Level Indicators (availability, latency, error rate, throughput)\n2. Set Service Level Objectives with target values (e.g., 99.9% availability)\n3. Formalize Service Level Agreements with customer commitments\n4. Configure automated SLI data collection\n5. Calculate error budgets based on SLOs\n6. Track performance and alert on SLO violations\n\n## Output\n\n- SLI/SLO/SLA definition documents\n- Real-time SLI metric dashboards\n- Error budget calculations and burn rate\n- SLO compliance reports\n- Alerting configurations for violations\n\n## Error Handling\n\nIf SLI/SLO tracking fails:\n- Verify SLI definition completeness\n- Check metric collection infrastructure\n- Validate data accuracy and granularity\n- Ensure alerting system connectivity\n- Review error budget calculation logic\n\n## Resources\n\n- Google SRE book on SLIs and SLOs\n- Error budget implementation guides\n- Service reliability engineering practices\n- SLO definition templates and examples",
      "parentPlugin": {
        "name": "sla-sli-tracker",
        "category": "performance",
        "path": "plugins/performance/sla-sli-tracker",
        "version": "1.0.0",
        "description": "Track SLAs, SLIs, and SLOs for service reliability"
      },
      "filePath": "plugins/performance/sla-sli-tracker/skills/sla-sli-tracker/SKILL.md"
    },
    {
      "slug": "tracking-token-launches",
      "name": "tracking-token-launches",
      "description": "Monitor new token launches, IDOs, and fair launches with contract verification. Use when discovering new token launches. Trigger with phrases like \"track launches\", \"find new tokens\", or \"monitor IDOs\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:launch-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:launch-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks",
      "parentPlugin": {
        "name": "token-launch-tracker",
        "category": "crypto",
        "path": "plugins/crypto/token-launch-tracker",
        "version": "1.0.0",
        "description": "Track new token launches, detect rugpulls, and analyze contract security for early-stage crypto projects"
      },
      "filePath": "plugins/crypto/token-launch-tracker/skills/token-launch-tracker/SKILL.md"
    },
    {
      "slug": "training-machine-learning-models",
      "name": "training-machine-learning-models",
      "description": "Train machine learning models with automated workflows. Analyzes datasets,",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to automatically train and evaluate machine learning models. It streamlines the model development process by handling data analysis, model selection, training, and evaluation, ultimately providing a persisted model artifact.\n\n## How It Works\n\n1. **Data Analysis and Preparation**: The skill analyzes the provided dataset and identifies the target variable, determining the appropriate model type (classification, regression, etc.).\n2. **Model Selection and Training**: Based on the data analysis, the skill selects a suitable machine learning model and configures the training parameters. It then trains the model using cross-validation techniques.\n3. **Performance Evaluation and Persistence**: After training, the skill generates performance metrics to evaluate the model's effectiveness. Finally, it saves the trained model artifact for future use.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Train a machine learning model on a given dataset.\n- Evaluate the performance of a machine learning model.\n- Automate the machine learning model training process.\n\n## Examples\n\n### Example 1: Training a Classification Model\n\nUser request: \"Train a classification model on this dataset of customer churn data.\"\n\nThe skill will:\n1. Analyze the customer churn data, identify the churn status as the target variable, and determine that a classification model is appropriate.\n2. Select a suitable classification algorithm (e.g., Logistic Regression, Random Forest), train the model using cross-validation, and generate performance metrics such as accuracy, precision, and recall.\n\n### Example 2: Training a Regression Model\n\nUser request: \"Train a regression model to predict house prices based on features like size, location, and number of bedrooms.\"\n\nThe skill will:\n1. Analyze the house price data, identify the price as the target variable, and determine that a regression model is appropriate.\n2. Select a suitable regression algorithm (e.g., Linear Regression, Support Vector Regression), train the model using cross-validation, and generate performance metrics such as Mean Squared Error (MSE) and R-squared.\n\n## Best Practices\n\n- **Data Quality**: Ensure the dataset is clean and properly formatted before training the model.\n- **Feature Engineering**: Consider feature engineering techniques to improve model performance.\n- **Hyperparameter Tuning**: Experiment with different hyperparameter settings to optimize model performance.\n\n## Integration\n\nThis skill can be used in conjunction with other data analysis and manipulation tools to prepare data for training. It can also integrate with model deployment tools to deploy the trained model to production.",
      "parentPlugin": {
        "name": "ml-model-trainer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ml-model-trainer",
        "version": "1.0.0",
        "description": "Train and optimize machine learning models with automated workflows"
      },
      "filePath": "plugins/ai-ml/ml-model-trainer/skills/ml-model-trainer/SKILL.md"
    },
    {
      "slug": "tuning-hyperparameters",
      "name": "tuning-hyperparameters",
      "description": "Optimize machine learning model hyperparameters using grid search, random",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to fine-tune machine learning models by automatically searching for the optimal hyperparameter configurations. It leverages different search strategies (grid, random, Bayesian) to efficiently explore the hyperparameter space and identify settings that maximize model performance.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude analyzes the user's request to determine the model, the hyperparameters to tune, the search strategy, and the evaluation metric.\n2. **Generating Code**: Claude generates Python code using appropriate ML libraries (e.g., scikit-learn, Optuna) to implement the specified hyperparameter search. The code includes data loading, preprocessing, model training, and evaluation.\n3. **Executing Search**: The generated code is executed to perform the hyperparameter search. The plugin iterates through different hyperparameter combinations, trains the model with each combination, and evaluates its performance.\n4. **Reporting Results**: Claude reports the best hyperparameter configuration found during the search, along with the corresponding performance metrics. It also provides insights into the search process and potential areas for further optimization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Optimize the performance of a machine learning model.\n- Automatically search for the best hyperparameter settings.\n- Compare different hyperparameter search strategies.\n- Improve model accuracy, precision, recall, or other relevant metrics.\n\n## Examples\n\n### Example 1: Optimizing a Random Forest Model\n\nUser request: \"Tune hyperparameters of a Random Forest model using grid search to maximize accuracy on the iris dataset. Consider n_estimators and max_depth.\"\n\nThe skill will:\n1. Generate code to perform a grid search over the specified hyperparameters (n_estimators, max_depth) of a Random Forest model using the iris dataset.\n2. Execute the grid search and report the best hyperparameter combination and the corresponding accuracy score.\n\n### Example 2: Using Bayesian Optimization\n\nUser request: \"Optimize a Gradient Boosting model using Bayesian optimization with Optuna to minimize the root mean squared error on the Boston housing dataset.\"\n\nThe skill will:\n1. Generate code to perform Bayesian optimization using Optuna to find the best hyperparameters for a Gradient Boosting model on the Boston housing dataset.\n2. Execute the optimization and report the best hyperparameter combination and the corresponding RMSE.\n\n## Best Practices\n\n- **Define Search Space**: Clearly define the range and type of values for each hyperparameter to be tuned.\n- **Choose Appropriate Strategy**: Select the hyperparameter search strategy (grid, random, Bayesian) based on the complexity of the hyperparameter space and the available computational resources. Bayesian optimization is generally more efficient for complex spaces.\n- **Use Cross-Validation**: Implement cross-validation to ensure the robustness of the evaluation metric and prevent overfitting.\n\n## Integration\n\nThis skill integrates seamlessly with other Claude Code plugins that involve machine learning tasks, such as data analysis, model training, and deployment. It can be used in conjunction with data visualization tools to gain insights into the impact of different hyperparameter settings on model performance.",
      "parentPlugin": {
        "name": "hyperparameter-tuner",
        "category": "ai-ml",
        "path": "plugins/ai-ml/hyperparameter-tuner",
        "version": "1.0.0",
        "description": "Optimize hyperparameters using grid/random/bayesian search"
      },
      "filePath": "plugins/ai-ml/hyperparameter-tuner/skills/hyperparameter-tuner/SKILL.md"
    },
    {
      "slug": "validating-ai-ethics-and-fairness",
      "name": "validating-ai-ethics-and-fairness",
      "description": "Validate AI/ML models and datasets for bias, fairness, and ethical concerns. Use when auditing AI systems for ethical compliance, fairness assessment, or bias detection. Trigger with phrases like \"evaluate model fairness\", \"check for bias\", or \"validate AI ethics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(python:*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to the AI model or dataset requiring validation\n- Model predictions or training data available for analysis\n- Understanding of demographic attributes relevant to fairness evaluation\n- Python environment with fairness assessment libraries (e.g., Fairlearn, AIF360)\n- Appropriate permissions to analyze sensitive data attributes\n\n## Instructions\n\n### Step 1: Identify Validation Scope\nDetermine which aspects of the AI system require ethical validation:\n- Model predictions across demographic groups\n- Training dataset representation and balance\n- Feature selection and potential proxy variables\n- Output disparities and fairness metrics\n\n### Step 2: Analyze for Bias\nUse the skill to examine the AI system:\n1. Load model predictions or dataset using Read tool\n2. Identify sensitive attributes (age, gender, race, etc.)\n3. Calculate fairness metrics (demographic parity, equalized odds, etc.)\n4. Detect statistical disparities across groups\n\n### Step 3: Generate Validation Report\nThe skill produces a comprehensive report including:\n- Identified biases and their severity\n- Fairness metric calculations with thresholds\n- Representation analysis across demographic groups\n- Recommended mitigation strategies\n- Compliance assessment against ethical guidelines\n\n### Step 4: Implement Mitigations\nBased on findings, apply recommended strategies:\n- Rebalance training data using sampling techniques\n- Apply algorithmic fairness constraints during training\n- Adjust decision thresholds for specific groups\n- Document ethical considerations and trade-offs\n\n## Output\n\nThe skill generates structured reports containing:\n\n### Bias Detection Results\n- Statistical disparities identified across groups\n- Severity classification (low, medium, high, critical)\n- Affected demographic segments with quantified impact\n\n### Fairness Metrics\n- Demographic parity ratios\n- Equal opportunity differences\n- Predictive parity measurements\n- Calibration scores across groups\n\n### Mitigation Recommendations\n- Specific technical approaches to reduce bias\n- Data augmentation or resampling strategies\n- Model constraint adjustments\n- Monitoring and continuous evaluation plans\n\n### Compliance Assessment\n- Alignment with ethical AI guidelines\n- Regulatory compliance status\n- Documentation requirements for audit trails\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Insufficient Data**\n- Error: Cannot calculate fairness metrics with small sample sizes\n- Solution: Aggregate related groups or collect additional data for underrepresented segments\n\n**Missing Sensitive Attributes**\n- Error: Demographic information not available in dataset\n- Solution: Use proxy detection methods or request access to protected attributes under appropriate governance\n\n**Conflicting Fairness Criteria**\n- Error: Multiple fairness metrics show contradictory results\n- Solution: Document trade-offs and prioritize metrics based on use case context and stakeholder input\n\n**Data Quality Issues**\n- Error: Inconsistent or corrupted attribute values\n- Solution: Perform data cleaning, standardization, and validation before bias analysis\n\n## Resources\n\n### Fairness Assessment Frameworks\n- Fairlearn library for bias detection and mitigation\n- AI Fairness 360 (AIF360) toolkit for comprehensive fairness analysis\n- Google What-If Tool for interactive fairness exploration\n\n### Ethical AI Guidelines\n- IEEE Ethically Aligned Design principles\n- EU Ethics Guidelines for Trustworthy AI\n- ACM Code of Ethics for AI practitioners\n\n### Fairness Metrics Documentation\n- Demographic parity and statistical parity definitions\n- Equalized odds and equal opportunity metrics\n- Individual fairness and calibration measures\n\n### Best Practices\n- Involve diverse stakeholders in fairness criteria selection\n- Document all ethical decisions and trade-offs\n- Implement continuous monitoring for fairness drift\n- Maintain transparency in model limitations and biases",
      "parentPlugin": {
        "name": "ai-ethics-validator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ai-ethics-validator",
        "version": "1.0.0",
        "description": "AI ethics and fairness validation"
      },
      "filePath": "plugins/ai-ml/ai-ethics-validator/skills/ai-ethics-validator/SKILL.md"
    },
    {
      "slug": "validating-api-contracts",
      "name": "validating-api-contracts",
      "description": "Validate API contracts using consumer-driven contract testing (Pact, Spring Cloud Contract). Use when performing specialized testing. Trigger with phrases like \"validate API contract\", \"run contract tests\", or \"check consumer contracts\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:contract-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:contract-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines",
      "parentPlugin": {
        "name": "contract-test-validator",
        "category": "testing",
        "path": "plugins/testing/contract-test-validator",
        "version": "1.0.0",
        "description": "API contract testing with Pact, OpenAPI validation, and consumer-driven contract verification"
      },
      "filePath": "plugins/testing/contract-test-validator/skills/contract-test-validator/SKILL.md"
    },
    {
      "slug": "validating-api-responses",
      "name": "validating-api-responses",
      "description": "Validate API responses against schemas to ensure contract compliance and data integrity. Use when ensuring API response correctness. Trigger with phrases like \"validate responses\", \"check API responses\", or \"verify response format\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:validate-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:validate-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-response-validator",
        "category": "api-development",
        "path": "plugins/api-development/api-response-validator",
        "version": "1.0.0",
        "description": "Validate API responses against schemas and contracts"
      },
      "filePath": "plugins/api-development/api-response-validator/skills/api-response-validator/SKILL.md"
    },
    {
      "slug": "validating-api-schemas",
      "name": "validating-api-schemas",
      "description": "Validate API schemas against OpenAPI, JSON Schema, and GraphQL specifications. Use when validating API schemas and contracts. Trigger with phrases like \"validate API schema\", \"check OpenAPI spec\", or \"verify schema\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:schema-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:schema-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-schema-validator",
        "category": "api-development",
        "path": "plugins/api-development/api-schema-validator",
        "version": "1.0.0",
        "description": "Validate API schemas with JSON Schema, Joi, Yup, or Zod"
      },
      "filePath": "plugins/api-development/api-schema-validator/skills/api-schema-validator/SKILL.md"
    },
    {
      "slug": "validating-authentication-implementations",
      "name": "validating-authentication-implementations",
      "description": "Validate authentication mechanisms for security weaknesses and compliance. Use when reviewing login systems or auth flows. Trigger with 'validate authentication', 'check auth security', or 'review login'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill allows Claude to assess the security of authentication mechanisms in a system or application. It provides a detailed report highlighting potential vulnerabilities and offering recommendations for improvement based on established security principles.\n\n## How It Works\n\n1. **Initiate Validation**: Upon receiving a trigger phrase, the skill activates the `authentication-validator` plugin.\n2. **Analyze Authentication Methods**: The plugin examines the implemented authentication methods, such as JWT, OAuth, session-based, or API keys.\n3. **Generate Security Report**: The plugin generates a comprehensive report outlining potential vulnerabilities and recommended fixes related to password security, session management, token security (JWT), multi-factor authentication, and account security.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess the security of an application's authentication implementation.\n- Identify vulnerabilities in password policies and session management.\n- Evaluate the security of JWT tokens and MFA implementation.\n- Ensure compliance with security best practices and industry standards.\n\n## Examples\n\n### Example 1: Assessing JWT Security\n\nUser request: \"validate authentication for jwt implementation\"\n\nThe skill will:\n1. Activate the `authentication-validator` plugin.\n2. Analyze the JWT implementation, checking for strong signing algorithms, proper expiration claims, and audience/issuer validation.\n3. Generate a report highlighting any vulnerabilities and recommending best practices for JWT security.\n\n### Example 2: Checking Session Security\n\nUser request: \"authcheck session cookies\"\n\nThe skill will:\n1. Activate the `authentication-validator` plugin.\n2. Analyze the session cookie settings, including HttpOnly, Secure, and SameSite attributes.\n3. Generate a report outlining any potential session fixation or CSRF vulnerabilities and recommending appropriate countermeasures.\n\n## Best Practices\n\n- **Password Hashing**: Always use strong hashing algorithms like bcrypt or Argon2 with appropriate salt generation.\n- **Token Expiration**: Implement short-lived access tokens and refresh token rotation for enhanced security.\n- **Multi-Factor Authentication**: Encourage or enforce MFA to mitigate the risk of password compromise.\n\n## Integration\n\nThis skill can be used in conjunction with other security-related plugins to provide a comprehensive security assessment of an application. For example, it can be used alongside a code analysis plugin to identify potential code-level vulnerabilities related to authentication.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "authentication-validator",
        "category": "security",
        "path": "plugins/security/authentication-validator",
        "version": "1.0.0",
        "description": "Validate authentication implementations"
      },
      "filePath": "plugins/security/authentication-validator/skills/authentication-validator/SKILL.md"
    },
    {
      "slug": "validating-cors-policies",
      "name": "validating-cors-policies",
      "description": "Validate CORS policies for security issues and misconfigurations. Use when reviewing cross-origin resource sharing. Trigger with 'validate CORS', 'check CORS policy', or 'review cross-origin'.",
      "allowedTools": [
        "Read",
        "WebFetch",
        "WebSearch",
        "Grep"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to assess the security and correctness of CORS policies. By leveraging the cors-policy-validator plugin, it identifies misconfigurations and potential vulnerabilities in CORS settings, helping developers build more secure web applications.\n\n## How It Works\n\n1. **Analyze CORS Configuration**: The skill receives the CORS configuration details, such as headers or policy files.\n2. **Validate Policy**: It utilizes the cors-policy-validator plugin to analyze the provided configuration against established security best practices.\n3. **Report Findings**: The skill presents a detailed report outlining any identified vulnerabilities or misconfigurations in the CORS policy.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Validate a CORS policy for a web application.\n- Check the CORS configuration of an API endpoint.\n- Identify potential security vulnerabilities in existing CORS implementations.\n\n## Examples\n\n### Example 1: Validating a CORS Policy File\n\nUser request: \"Validate the CORS policy in `cors_policy.json`\"\n\nThe skill will:\n1. Read the `cors_policy.json` file.\n2. Use the cors-policy-validator plugin to analyze the CORS configuration.\n3. Output a report detailing any identified vulnerabilities or misconfigurations.\n\n### Example 2: Checking CORS Headers for an API Endpoint\n\nUser request: \"Check CORS headers for the API endpoint at `https://example.com/api`\"\n\nThe skill will:\n1. Fetch the CORS headers from the specified API endpoint.\n2. Use the cors-policy-validator plugin to analyze the headers.\n3. Output a report summarizing the CORS configuration and any potential issues.\n\n## Best Practices\n\n- **Configuration Source**: Always specify the source of the CORS configuration (e.g., file path, URL) for accurate validation.\n- **Regular Validation**: Regularly validate CORS policies, especially after making changes to the application or API.\n- **Heuristic Analysis**: Consider supplementing validation with manual review and heuristic analysis to catch subtle vulnerabilities.\n\n## Integration\n\nThis skill can be integrated with other security-related plugins to provide a more comprehensive security assessment. For example, it can be used in conjunction with vulnerability scanning tools to identify potential cross-site scripting (XSS) vulnerabilities related to CORS misconfigurations.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "cors-policy-validator",
        "category": "security",
        "path": "plugins/security/cors-policy-validator",
        "version": "1.0.0",
        "description": "Validate CORS policies"
      },
      "filePath": "plugins/security/cors-policy-validator/skills/cors-policy-validator/SKILL.md"
    },
    {
      "slug": "validating-csrf-protection",
      "name": "validating-csrf-protection",
      "description": "Validate CSRF protection implementations for security gaps. Use when reviewing form security or state-changing operations. Trigger with 'validate CSRF', 'check CSRF protection', or 'review token security'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill empowers Claude to analyze web applications for CSRF vulnerabilities. It assesses the effectiveness of implemented CSRF protection mechanisms, providing insights into potential weaknesses and recommendations for remediation.\n\n## How It Works\n\n1. **Analyze Endpoints**: The plugin examines application endpoints to identify those lacking CSRF protection.\n2. **Assess Protection Mechanisms**: It validates the implementation of CSRF protection mechanisms, including token validation, double-submit cookies, SameSite attributes, and origin validation.\n3. **Generate Report**: A detailed report is generated, highlighting vulnerable endpoints, potential attack scenarios, and recommended fixes.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Validate existing CSRF protection measures.\n- Identify CSRF vulnerabilities in a web application.\n- Assess the risk associated with unprotected endpoints.\n- Generate a report outlining CSRF vulnerabilities and recommended fixes.\n\n## Examples\n\n### Example 1: Identifying Unprotected API Endpoints\n\nUser request: \"validate csrf\"\n\nThe skill will:\n1. Analyze the application's API endpoints.\n2. Identify endpoints lacking CSRF protection, such as those handling sensitive data modifications.\n3. Generate a report outlining vulnerable endpoints and potential attack vectors.\n\n### Example 2: Checking SameSite Cookie Attributes\n\nUser request: \"Check for csrf vulnerabilities in my application\"\n\nThe skill will:\n1. Analyze the application's cookie settings.\n2. Verify that SameSite attributes are properly configured to mitigate CSRF attacks.\n3. Report any cookies lacking the SameSite attribute or using an insecure setting.\n\n## Best Practices\n\n- **Regular Validation**: Regularly validate CSRF protection mechanisms as part of the development lifecycle.\n- **Comprehensive Coverage**: Ensure all state-changing operations are protected against CSRF attacks.\n- **Secure Configuration**: Use secure configurations for CSRF protection mechanisms, such as strong token generation and proper SameSite attribute settings.\n\n## Integration\n\nThis skill can be used in conjunction with other security plugins to provide a comprehensive security assessment of web applications. For example, it can be combined with a vulnerability scanner to identify other potential vulnerabilities in addition to CSRF weaknesses.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "csrf-protection-validator",
        "category": "security",
        "path": "plugins/security/csrf-protection-validator",
        "version": "1.0.0",
        "description": "Validate CSRF protection"
      },
      "filePath": "plugins/security/csrf-protection-validator/skills/csrf-protection-validator/SKILL.md"
    },
    {
      "slug": "validating-database-integrity",
      "name": "validating-database-integrity",
      "description": "Use when you need to ensure database integrity through comprehensive data validation. This skill validates data types, ranges, formats, referential integrity, and business rules. Trigger with phrases like \"validate database data\", \"implement data validation rules\", \"enforce data integrity constraints\", or \"validate data formats\". allowed-tools: version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Database connection credentials are available\n- Appropriate database permissions for schema modifications\n- Backup of production databases before applying constraints\n- Understanding of existing data that may violate new constraints\n- Access to database documentation for column specifications\n\n## Instructions\n\n### Step 1: Analyze Validation Requirements\n1. Review database schema and identify columns requiring validation\n2. Determine validation types needed (data type, range, format, referential)\n3. Document existing data patterns that may conflict with new rules\n4. Prioritize validation rules by business criticality\n\n### Step 2: Define Validation Rules\n1. Create validation rule definitions for each column\n2. Specify data types, constraints, and acceptable ranges\n3. Define regular expressions for format validation\n4. Map foreign key relationships for referential integrity\n5. Document business rule logic for complex validations\n\n### Step 3: Implement Database Constraints\n1. Generate SQL constraints for data type validation\n2. Add CHECK constraints for range and format validation\n3. Create foreign key constraints for referential integrity\n4. Implement triggers for complex business rule validation\n5. Test constraints with valid and invalid sample data\n\n### Step 4: Validate Existing Data\n1. Query existing data to identify constraint violations\n2. Generate reports of data that would fail new constraints\n3. Create data cleanup scripts to fix violations\n4. Execute cleanup scripts in staging environment first\n5. Re-validate cleaned data before applying constraints\n\n### Step 5: Apply Validation Rules\n1. Apply constraints to staging database first\n2. Monitor for any application errors or failures\n3. Validate that legitimate operations still function\n4. Apply constraints to production database during maintenance window\n5. Monitor database logs for constraint violation attempts\n\n## Output\n\nThis skill produces:\n\n**Database Constraints**: SQL DDL statements with CHECK, FOREIGN KEY, and NOT NULL constraints\n\n**Validation Reports**: Analysis of existing data showing constraint violations with counts and examples\n\n**Data Cleanup Scripts**: SQL UPDATE/DELETE statements to fix existing data that violates new constraints\n\n**Test Results**: Documentation of constraint testing with valid/invalid data samples and outcomes\n\n**Implementation Log**: Timestamped record of constraint application with success/failure status\n\n## Error Handling\n\n**Constraint Violation Errors**:\n- Review existing data that violates the constraint\n- Create data cleanup scripts to fix violations\n- Re-run constraint application after cleanup\n- Document exceptions that require manual review\n\n**Permission Errors**:\n- Verify database user has ALTER TABLE privileges\n- Request elevated permissions from database administrator\n- Use separate admin connection for schema changes\n- Document permission requirements for future deployments\n\n**Circular Dependency Errors**:\n- Map all foreign key relationships before implementation\n- Apply constraints in dependency order (referenced tables first)\n- Use ALTER TABLE ADD CONSTRAINT for deferred constraint creation\n- Consider disabling foreign key checks temporarily during bulk operations\n\n**Performance Degradation**:\n- Analyze constraint checking overhead with EXPLAIN ANALYZE\n- Add appropriate indexes to support constraint validation\n- Consider batch validation for large data updates\n- Monitor query performance after constraint implementation\n\n## Resources\n\n**Database-Specific Constraint Syntax**:\n- PostgreSQL: `{baseDir}/docs/postgresql-constraints.md`\n- MySQL: `{baseDir}/docs/mysql-constraints.md`\n- SQL Server: `{baseDir}/docs/sqlserver-constraints.md`\n\n**Validation Rule Templates**: `{baseDir}/templates/validation-rules/`\n- Email format validation\n- Phone number validation\n- Date range validation\n- Numeric range validation\n- Custom business rules\n\n**Testing Guidelines**: `{baseDir}/docs/validation-testing.md`\n**Constraint Performance Analysis**: `{baseDir}/docs/constraint-performance.md`\n**Data Cleanup Procedures**: `{baseDir}/docs/data-cleanup-procedures.md`",
      "parentPlugin": {
        "name": "data-validation-engine",
        "category": "database",
        "path": "plugins/database/data-validation-engine",
        "version": "1.0.0",
        "description": "Database plugin for data-validation-engine"
      },
      "filePath": "plugins/database/data-validation-engine/skills/data-validation-engine/SKILL.md"
    },
    {
      "slug": "validating-pci-dss-compliance",
      "name": "validating-pci-dss-compliance",
      "description": "Validate PCI-DSS compliance for payment card data security. Use when auditing payment systems. Trigger with 'validate PCI-DSS', 'check payment security', or 'audit card data'.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill streamlines PCI DSS compliance checks by automatically analyzing code and configurations. It flags potential issues, allowing for proactive remediation and improved security posture. It is particularly useful for developers, security engineers, and compliance officers.\n\n## How It Works\n\n1. **Analyze the Target**: The skill identifies the codebase, configuration files, or infrastructure resources to be evaluated.\n2. **Run PCI DSS Validation**: The pci-dss-validator plugin scans the target for potential PCI DSS violations.\n3. **Generate Report**: The skill compiles a report detailing any identified vulnerabilities or non-compliant configurations, along with remediation recommendations.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Evaluate a new application or system for PCI DSS compliance before deployment.\n- Periodically assess existing systems to maintain PCI DSS compliance.\n- Investigate potential security vulnerabilities related to PCI DSS.\n\n## Examples\n\n### Example 1: Validating a Web Application\n\nUser request: \"Validate PCI compliance for my e-commerce web application.\"\n\nThe skill will:\n1. Identify the source code repository for the web application.\n2. Run the pci-dss-validator plugin against the codebase.\n3. Generate a report highlighting any PCI DSS violations found in the code.\n\n### Example 2: Checking Infrastructure Configuration\n\nUser request: \"Check PCI DSS compliance of my AWS infrastructure.\"\n\nThe skill will:\n1. Access the AWS configuration files (e.g., Terraform, CloudFormation).\n2. Execute the pci-dss-validator plugin against the infrastructure configuration.\n3. Produce a report outlining any non-compliant configurations in the AWS environment.\n\n## Best Practices\n\n- **Scope Definition**: Clearly define the scope of the PCI DSS assessment to ensure accurate and relevant results.\n- **Regular Assessments**: Conduct regular PCI DSS assessments to maintain continuous compliance.\n- **Remediation Tracking**: Track and document all remediation efforts to demonstrate ongoing commitment to security.\n\n## Integration\n\nThis skill can be integrated with other security tools and plugins to provide a comprehensive security assessment. For example, it can be used in conjunction with static analysis tools to identify vulnerabilities in code before it is deployed. It can also be integrated with infrastructure-as-code tools to ensure that infrastructure is compliant with PCI DSS from the start.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "pci-dss-validator",
        "category": "security",
        "path": "plugins/security/pci-dss-validator",
        "version": "1.0.0",
        "description": "Validate PCI DSS compliance"
      },
      "filePath": "plugins/security/pci-dss-validator/skills/pci-dss-validator/SKILL.md"
    },
    {
      "slug": "validating-performance-budgets",
      "name": "validating-performance-budgets",
      "description": "Validate application performance against defined budgets to identify regressions early. Use when checking page load times, bundle sizes, or API response times against thresholds. Trigger with phrases like \"validate performance budget\", \"check performance metrics\", or \"detect performance regression\".",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(lighthouse:*)",
        "Bash(webpack:*)",
        "Bash(performance:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Overview\n\nThis skill allows Claude to automatically validate your application's performance against predefined budgets. It helps identify performance regressions and ensures your application maintains optimal performance characteristics.\n\n## How It Works\n\n1. **Analyze Performance Metrics**: Claude analyzes current performance metrics, such as page load times, bundle sizes, and API response times.\n2. **Validate Against Budget**: The plugin validates these metrics against predefined performance budget thresholds.\n3. **Report Violations**: If any metrics exceed the defined budget, the skill reports violations and provides details on the exceeded thresholds.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Validate performance against predefined budgets.\n- Identify performance regressions in your application.\n- Integrate performance budget validation into your CI/CD pipeline.\n\n## Examples\n\n### Example 1: Preventing Performance Regressions\n\nUser request: \"Validate performance budget for the homepage.\"\n\nThe skill will:\n1. Analyze the homepage's performance metrics (load time, bundle size).\n2. Compare these metrics against the defined budget.\n3. Report any violations, such as exceeding the load time budget.\n\n### Example 2: Integrating with CI/CD\n\nUser request: \"Run performance budget validation as part of the build process.\"\n\nThe skill will:\n1. Execute the performance budget validation command.\n2. Check all defined performance metrics against their budgets.\n3. Report any violations that would cause the build to fail.\n\n## Best Practices\n\n- **Budget Definition**: Define realistic and achievable performance budgets based on current application performance and user expectations.\n- **Metric Selection**: Choose relevant performance metrics that directly impact user experience, such as page load times and API response times.\n- **CI/CD Integration**: Integrate performance budget validation into your CI/CD pipeline to automatically detect and prevent performance regressions.\n\n## Integration\n\nThis skill can be integrated with other plugins that provide performance metrics, such as website speed test tools or API monitoring services. It can also be used in conjunction with alerting plugins to notify developers of performance budget violations.\n\n## Prerequisites\n\n- Performance budget definitions in {baseDir}/performance-budgets.json\n- Access to performance testing tools (Lighthouse, WebPageTest)\n- Build output directory for bundle analysis\n- Historical performance metrics for comparison\n\n## Instructions\n\n1. Load performance budget configuration\n2. Collect current performance metrics (load time, bundle size, API latency)\n3. Compare metrics against defined budget thresholds\n4. Identify budget violations and severity\n5. Generate detailed violation report\n6. Provide remediation recommendations\n\n## Output\n\n- Performance budget validation report\n- List of metrics exceeding budget thresholds\n- Comparison with previous measurements\n- Detailed breakdown by metric category\n- Actionable recommendations for fixes\n\n## Error Handling\n\nIf budget validation fails:\n- Verify budget configuration file exists\n- Check performance testing tool availability\n- Validate metric collection permissions\n- Ensure network access to test endpoints\n- Review budget threshold definitions\n\n## Resources\n\n- Performance budget best practices\n- Lighthouse performance scoring guide\n- Bundle size optimization techniques\n- CI/CD integration patterns for performance testing",
      "parentPlugin": {
        "name": "performance-budget-validator",
        "category": "performance",
        "path": "plugins/performance/performance-budget-validator",
        "version": "1.0.0",
        "description": "Validate application against performance budgets"
      },
      "filePath": "plugins/performance/performance-budget-validator/skills/performance-budget-validator/SKILL.md"
    },
    {
      "slug": "validator-expert",
      "name": "validator-expert",
      "description": "Validate production readiness of Vertex AI Agent Engine deployments across",
      "allowedTools": [
        "Read",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## What This Skill Does\n\nProduction validator for Vertex AI deployments. Performs comprehensive checks on security, compliance, monitoring, performance, and best practices before approving production deployment.\n\n## When This Skill Activates\n\nTriggers: \"validate deployment\", \"production readiness\", \"security audit vertex ai\", \"check compliance\", \"validate adk agent\"\n\n## Validation Checklist\n\n### Security Validation\n- ‚úÖ IAM roles follow least privilege\n- ‚úÖ VPC Service Controls enabled\n- ‚úÖ Encryption at rest configured\n- ‚úÖ No hardcoded secrets\n- ‚úÖ Service accounts properly configured\n- ‚úÖ Model Armor enabled (for ADK)\n\n### Monitoring Validation\n- ‚úÖ Cloud Monitoring dashboards configured\n- ‚úÖ Alerting policies set\n- ‚úÖ Token usage tracking enabled\n- ‚úÖ Error rate monitoring active\n- ‚úÖ Latency SLOs defined\n\n### Performance Validation\n- ‚úÖ Auto-scaling configured\n- ‚úÖ Resource limits appropriate\n- ‚úÖ Caching strategy implemented\n- ‚úÖ Code Execution sandbox TTL set\n- ‚úÖ Memory Bank retention configured\n\n### Compliance Validation\n- ‚úÖ Audit logging enabled\n- ‚úÖ Data residency requirements met\n- ‚úÖ Privacy policies implemented\n- ‚úÖ Backup/disaster recovery configured\n\n## Tool Permissions\n\nRead, Grep, Glob, Bash - Read-only analysis for security\n\n## References\n\n- Vertex AI Security: https://cloud.google.com/vertex-ai/docs/security",
      "parentPlugin": {
        "name": "jeremy-vertex-validator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-vertex-validator",
        "version": "1.0.0",
        "description": "Production readiness validator for Vertex AI deployments and configurations"
      },
      "filePath": "plugins/ai-ml/jeremy-vertex-validator/skills/validator-expert/SKILL.md"
    },
    {
      "slug": "version-bumper",
      "name": "version-bumper",
      "description": "Automatically handles semantic version updates across plugin.json and",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Version Bumper\n\n## Purpose\nAutomatically manages semantic version updates for Claude Code plugins, ensuring consistency across plugin.json, marketplace catalog, and git tags - optimized for claude-code-plugins repository workflow.\n\n## Trigger Keywords\n- \"bump version\" or \"update version\"\n- \"release\" or \"new release\"\n- \"major version\" or \"minor version\" or \"patch version\"\n- \"increment version\"\n- \"version update\"\n\n## Semantic Versioning\n\n**Format:** MAJOR.MINOR.PATCH (e.g., 2.1.3)\n\n**Rules:**\n- **MAJOR (2.x.x)** - Breaking changes, incompatible API changes\n- **MINOR (x.1.x)** - New features, backward compatible\n- **PATCH (x.x.3)** - Bug fixes, backward compatible\n\n**Examples:**\n- `1.0.0` ‚Üí `1.0.1` (bug fix)\n- `1.0.0` ‚Üí `1.1.0` (new feature)\n- `1.0.0` ‚Üí `2.0.0` (breaking change)\n\n## Version Bump Process\n\nWhen activated, I will:\n\n1. **Identify Current Version**\n   ```bash\n   # Read plugin version\n   current=$(jq -r '.version' .claude-plugin/plugin.json)\n   echo \"Current version: $current\"\n   ```\n\n2. **Determine Bump Type**\n   - From user request (major/minor/patch)\n   - Or suggest based on changes\n   - Or ask user which type\n\n3. **Calculate New Version**\n   ```bash\n   # Example for patch bump: 1.2.3 ‚Üí 1.2.4\n   IFS='.' read -r major minor patch <<< \"$current\"\n   new_version=\"$major.$minor.$((patch + 1))\"\n   ```\n\n4. **Update Files**\n   - Update `.claude-plugin/plugin.json`\n   - Update `.claude-plugin/marketplace.extended.json`\n   - Sync to `marketplace.json`\n\n5. **Validate Consistency**\n   - Verify all files have same version\n   - Check no other plugins use this version\n   - Validate semver format\n\n6. **Create Git Tag (Optional)**\n   ```bash\n   git tag -a \"v$new_version\" -m \"Release v$new_version\"\n   ```\n\n## Update Locations\n\n### 1. Plugin JSON\n```json\n// .claude-plugin/plugin.json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"1.2.4\",  // ‚Üê Update here\n  ...\n}\n```\n\n### 2. Marketplace Extended\n```json\n// .claude-plugin/marketplace.extended.json\n{\n  \"plugins\": [\n    {\n      \"name\": \"plugin-name\",\n      \"version\": \"1.2.4\",  // ‚Üê Update here\n      ...\n    }\n  ]\n}\n```\n\n### 3. Sync CLI Catalog\n```bash\nnpm run sync-marketplace\n# Regenerates marketplace.json with new version\n```\n\n## Bump Types\n\n### Patch Bump (Bug Fix)\n**When to use:**\n- Bug fixes\n- Documentation updates\n- Minor improvements\n- No new features\n\n**Example:** 1.2.3 ‚Üí 1.2.4\n\n### Minor Bump (New Feature)\n**When to use:**\n- New features\n- New commands/agents/skills\n- Backward compatible changes\n- Enhanced functionality\n\n**Example:** 1.2.3 ‚Üí 1.3.0\n\n### Major Bump (Breaking Change)\n**When to use:**\n- Breaking API changes\n- Incompatible updates\n- Major refactor\n- Removed features\n\n**Example:** 1.2.3 ‚Üí 2.0.0\n\n## Validation Checks\n\nBefore bumping:\n- ‚úÖ Current version is valid semver\n- ‚úÖ New version is higher than current\n- ‚úÖ No other plugin uses new version\n- ‚úÖ All files have same current version\n- ‚úÖ Git working directory is clean (optional)\n\nAfter bumping:\n- ‚úÖ plugin.json updated\n- ‚úÖ marketplace.extended.json updated\n- ‚úÖ marketplace.json synced\n- ‚úÖ All versions consistent\n- ‚úÖ CHANGELOG.md updated (if exists)\n\n## Changelog Management\n\nIf CHANGELOG.md exists, I update it:\n\n```markdown\n# Changelog\n\n## [1.2.4] - 2025-10-16\n\n### Fixed\n- Bug fix description\n- Another fix\n\n## [1.2.3] - 2025-10-15\n...\n```\n\n## Git Integration\n\n### Option 1: Version Commit\n```bash\n# Update version files\ngit add .claude-plugin/plugin.json\ngit add .claude-plugin/marketplace.extended.json\ngit add .claude-plugin/marketplace.json\ngit add CHANGELOG.md  # if exists\n\n# Commit version bump\ngit commit -m \"chore: Bump plugin-name to v1.2.4\"\n```\n\n### Option 2: Version Tag\n```bash\n# Create annotated tag\ngit tag -a \"plugin-name-v1.2.4\" -m \"Release plugin-name v1.2.4\"\n\n# Or for monorepo\ngit tag -a \"v1.2.4\" -m \"Release v1.2.4\"\n\n# Push tag\ngit push origin plugin-name-v1.2.4\n```\n\n## Multi-Plugin Updates\n\nFor repository-wide version bump:\n\n```bash\n# Bump marketplace version\njq '.metadata.version = \"1.0.40\"' .claude-plugin/marketplace.extended.json\n\n# Update all plugins (if needed)\nfor plugin in plugins/*/; do\n  # Update plugin.json\n  # Update marketplace entry\ndone\n```\n\n## Version Consistency Check\n\nI verify:\n```bash\n# Plugin version\nplugin_v=$(jq -r '.version' plugins/category/plugin-name/.claude-plugin/plugin.json)\n\n# Marketplace version\nmarket_v=$(jq -r '.plugins[] | select(.name == \"plugin-name\") | .version' .claude-plugin/marketplace.extended.json)\n\n# Should match\nif [ \"$plugin_v\" != \"$market_v\" ]; then\n  echo \"‚ùå Version mismatch!\"\n  echo \"Plugin: $plugin_v\"\n  echo \"Marketplace: $market_v\"\nfi\n```\n\n## Release Workflow\n\nComplete release process:\n\n1. **Determine Bump Type**\n   - Review changes since last version\n   - Decide: patch/minor/major\n\n2. **Update Version**\n   - Bump plugin.json\n   - Update marketplace catalog\n   - Sync marketplace.json\n\n3. **Update Changelog**\n   - Add release notes\n   - List changes\n   - Include date\n\n4. **Commit Changes**\n   ```bash\n   git add .\n   git commit -m \"chore: Release v1.2.4\"\n   ```\n\n5. **Create Tag**\n   ```bash\n   git tag -a \"v1.2.4\" -m \"Release v1.2.4\"\n   ```\n\n6. **Push**\n   ```bash\n   git push origin main\n   git push origin v1.2.4\n   ```\n\n7. **Validate**\n   - Check GitHub release created\n   - Verify marketplace updated\n   - Test plugin installation\n\n## Output Format\n\n```\nüî¢ VERSION BUMP REPORT\n\nPlugin: plugin-name\nOld Version: 1.2.3\nNew Version: 1.2.4\nBump Type: PATCH\n\n‚úÖ UPDATES COMPLETED:\n1. Updated .claude-plugin/plugin.json ‚Üí v1.2.4\n2. Updated marketplace.extended.json ‚Üí v1.2.4\n3. Synced marketplace.json ‚Üí v1.2.4\n4. Updated CHANGELOG.md\n\nüìä CONSISTENCY CHECK:\n‚úÖ All files have version 1.2.4\n‚úÖ No version conflicts\n‚úÖ Semantic versioning valid\n\nüìù CHANGELOG ENTRY:\n## [1.2.4] - 2025-10-16\n### Fixed\n- Bug fix description\n\nüéØ NEXT STEPS:\n1. Review changes: git diff\n2. Commit: git add . && git commit -m \"chore: Bump to v1.2.4\"\n3. Tag: git tag -a \"v1.2.4\" -m \"Release v1.2.4\"\n4. Push: git push origin main && git push origin v1.2.4\n\n‚ú® Ready to release!\n```\n\n## Repository-Specific Features\n\n**For claude-code-plugins repo:**\n- Handles both plugin and marketplace versions\n- Updates marketplace metadata version\n- Manages plugin count in README\n- Syncs both catalog files\n- Creates proper release tags\n\n## Examples\n\n**User says:** \"Bump the security-scanner plugin to patch version\"\n\n**I automatically:**\n1. Read current version: 1.2.3\n2. Calculate patch bump: 1.2.4\n3. Update plugin.json\n4. Update marketplace.extended.json\n5. Sync marketplace.json\n6. Validate consistency\n7. Report success\n\n**User says:** \"Release version 2.0.0 of plugin-name\"\n\n**I automatically:**\n1. Recognize major version (breaking change)\n2. Update all version files\n3. Update CHANGELOG.md with major release notes\n4. Create git commit\n5. Create git tag v2.0.0\n6. Provide push commands\n\n**User says:** \"Increment version for new feature\"\n\n**I automatically:**\n1. Detect this is a minor bump\n2. Calculate new version (1.2.3 ‚Üí 1.3.0)\n3. Update all files\n4. Add changelog entry\n5. Report completion",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/version-bumper/SKILL.md"
    },
    {
      "slug": "versioning-apis",
      "name": "versioning-apis",
      "description": "Manage API versions with backward compatibility, deprecation notices, and migration paths. Use when managing API versions and backward compatibility. Trigger with phrases like \"version the API\", \"manage API versions\", or \"handle API versioning\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:version-*) license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:version-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques",
      "parentPlugin": {
        "name": "api-versioning-manager",
        "category": "api-development",
        "path": "plugins/api-development/api-versioning-manager",
        "version": "1.0.0",
        "description": "Manage API versions with migration strategies and backward compatibility"
      },
      "filePath": "plugins/api-development/api-versioning-manager/skills/api-versioning-manager/SKILL.md"
    },
    {
      "slug": "vertex-ai-media-master",
      "name": "Vertex AI Media Master",
      "description": "Automatic activation for all google vertex ai multimodal operations - Use when appropriate context detected. Trigger with relevant phrases based on skill purpose.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(general:*)",
        "Bash(util:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vertex AI Media Master - Comprehensive Multimodal AI Operations\n\nThis Agent Skill provides comprehensive mastery of Google Vertex AI multimodal capabilities for video, audio, image, and text processing with focus on marketing applications.\n\n## Core Capabilities\n\n### üé• Video Processing (Gemini 2.0/2.5)\n- **Video Understanding**: Process videos up to 6 hours at low resolution or 2 hours at default resolution\n- **2M Context Window**: Gemini 2.5 Pro handles massive video content\n- **Audio Track Processing**: Automatic audio transcription from video\n- **Multi-video Analysis**: Process multiple videos in single request\n- **Video Summarization**: Extract key moments, scenes, and insights\n- **Marketing Use Cases**:\n  - Analyze competitor video ads\n  - Extract highlights from long-form content\n  - Generate video summaries for social media\n  - Transcribe and caption video content\n  - Identify brand mentions and product placements\n\n### üéµ Audio Generation & Processing\n- **Lyria Model (2025)**: Native audio and music generation\n- **Speech-to-Text**: Transcribe audio with speaker diarization\n- **Text-to-Speech**: Generate natural voiceovers\n- **Music Composition**: Background music for campaigns\n- **Audio Enhancement**: Noise reduction and quality improvement\n- **Marketing Use Cases**:\n  - Generate podcast scripts and voiceovers\n  - Create audio ads and radio spots\n  - Produce background music for video campaigns\n  - Transcribe customer interviews\n  - Generate multilingual voiceovers\n\n### üñºÔ∏è Image Generation (Imagen 4 & Gemini 2.5 Flash Image)\n- **Imagen 4**: Highest quality text-to-image generation\n- **Gemini 2.5 Flash Image**: Interleaved image generation with text\n- **Style Transfer**: Apply brand styles to generated images\n- **Product Visualization**: Generate product mockups\n- **Campaign Assets**: Create ad creatives and social media graphics\n- **Marketing Use Cases**:\n  - Generate personalized ad images (Adios solution)\n  - Create social media graphics at scale\n  - Produce product lifestyle images\n  - Generate A/B test variations\n  - Create branded campaign visuals\n\n### üì¢ Marketing Campaign Automation\n- **ViGenAiR**: Convert long-form video ads to short formats automatically\n- **Adios**: Generate personalized ad images tailored to audience context\n- **Campaign Asset Generation**: Photos, soundtracks, voiceovers from prompts\n- **Content Pipeline**: Email copy, blog posts, social media, PMax assets\n- **Catalog Enrichment**: Multi-agent workflow for product onboarding\n- **Marketing Use Cases**:\n  - Automated campaign asset production\n  - Personalized content at scale\n  - Multi-channel content distribution\n  - Product catalog enhancement\n  - Visual merchandising automation\n\n### üîß Technical Implementation\n\n**API Integration:**\n```python\nfrom google.cloud import aiplatform\nfrom vertexai.preview.generative_models import GenerativeModel\n\n# Initialize Vertex AI\naiplatform.init(project=\"your-project\", location=\"us-central1\")\n\n# Gemini 2.5 Pro for video\nmodel = GenerativeModel(\"gemini-2.5-pro\")\n\n# Process video with audio\nresponse = model.generate_content([\n    \"Analyze this video and extract key marketing insights\",\n    video_file,  # Up to 6 hours\n])\n\n# Imagen 4 for image generation\nfrom vertexai.preview.vision_models import ImageGenerationModel\nimagen = ImageGenerationModel.from_pretrained(\"imagen-4\")\nimages = imagen.generate_images(\n    prompt=\"Professional product photo, studio lighting, white background\",\n    number_of_images=4\n)\n```\n\n**Gemini 2.5 Flash Image (Interleaved Generation):**\n```python\n# Generate images within text responses\nmodel = GenerativeModel(\"gemini-2.5-flash-image\")\nresponse = model.generate_content([\n    \"Create a 5-step recipe with images for each step\"\n])\n# Returns text + images interleaved\n```\n\n**Audio Generation (Lyria):**\n```python\nfrom vertexai.preview.audio_models import AudioGenerationModel\nlyria = AudioGenerationModel.from_pretrained(\"lyria\")\naudio = lyria.generate_audio(\n    prompt=\"Upbeat background music for product launch video, 30 seconds\",\n    duration=30\n)\n```\n\n### üìä Marketing Workflow Automation\n\n**1. Multi-Channel Campaign Creation:**\n```python\n# Single prompt generates all assets\ncampaign = model.generate_content([\n    \"\"\"Create a product launch campaign for [product]:\n    - Hero image (1920x1080)\n    - 3 social media graphics (1080x1080)\n    - 30-second video script\n    - Background music description\n    - Email marketing copy\n    - Instagram caption\"\"\"\n])\n```\n\n**2. Video Repurposing Pipeline:**\n```python\n# Long-form to short-form conversion (ViGenAiR approach)\nlong_video = \"gs://bucket/original-ad-60s.mp4\"\nresponse = model.generate_content([\n    f\"Extract 3 engaging 15-second clips from this video for TikTok/Reels\",\n    long_video\n])\n# Auto-generates format-specific versions\n```\n\n**3. Personalized Ad Generation:**\n```python\n# Context-aware image generation (Adios approach)\nfor audience in audiences:\n    ad_image = imagen.generate_images(\n        prompt=f\"Product ad for {product}, targeting {audience.demographics}, {audience.style_preference}\",\n        aspect_ratio=\"16:9\"\n    )\n```\n\n### üéØ Best Practices for Jeremy\n\n**1. Project Setup:**\n```bash\n# Set environment variables\nexport GOOGLE_CLOUD_PROJECT=\"your-project-id\"\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/service-account.json\"\n\n# Install SDK\npip install google-cloud-aiplatform[vision,audio] google-generativeai\n```\n\n**2. Rate Limits & Quotas:**\n- Gemini 2.5 Pro: 2M tokens/min (video processing)\n- Imagen 4: 100 images/min\n- Monitor usage in Cloud Console\n\n**3. Cost Optimization:**\n- Use Gemini 2.5 Flash for faster, cheaper operations\n- Batch image generation requests\n- Cache video embeddings for repeated analysis\n- Use low-resolution video setting when appropriate\n\n**4. Security & Compliance:**\n- Keep API keys in Secret Manager, never in code\n- Use service accounts with minimal permissions\n- Enable VPC Service Controls for data residency\n- Log all API calls for audit trails\n\n### üöÄ Advanced Marketing Use Cases\n\n**1. Campaign Performance Analysis:**\n```python\n# Analyze competitor campaigns\ncompetitor_videos = [\"gs://bucket/competitor1.mp4\", \"gs://bucket/competitor2.mp4\"]\nanalysis = model.generate_content([\n    \"Compare these competitor videos: themes, messaging, CTAs, production quality\",\n    *competitor_videos\n])\n```\n\n**2. Content Localization:**\n```python\n# Generate multilingual campaigns\nfor lang in [\"en\", \"es\", \"fr\", \"de\", \"ja\"]:\n    localized_content = model.generate_content([\n        f\"Translate and culturally adapt this campaign for {lang} market:\",\n        campaign_brief,\n        hero_image\n    ])\n```\n\n**3. A/B Test Generation:**\n```python\n# Generate variations automatically\nvariations = []\nfor style in [\"minimalist\", \"bold\", \"luxury\", \"playful\"]:\n    variation = imagen.generate_images(\n        prompt=f\"Product ad, {style} style, {brand_guidelines}\",\n        number_of_images=1\n    )\n    variations.append(variation)\n```\n\n### üìö Reference Documentation\n\n**Official Documentation:**\n- Vertex AI Multimodal: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview\n- Gemini 2.5 Pro: https://cloud.google.com/vertex-ai/generative-ai/docs/models\n- Imagen 4: https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview\n- Video Understanding: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding\n\n**Marketing Solutions:**\n- GenAI for Marketing: https://github.com/GoogleCloudPlatform/genai-for-marketing\n- ViGenAiR (video repurposing)\n- Adios (personalized ad images)\n\n**Pricing:**\n- Gemini 2.5 Pro: $3.50/1M input tokens, $10.50/1M output tokens\n- Imagen 4: $0.04/image\n- Video processing: Included in Gemini token pricing\n\n## When This Skill Activates\n\nThis skill automatically activates when you mention:\n- Video processing, analysis, or understanding\n- Audio generation, music composition, or voiceovers\n- Image generation, ad creatives, or visual content\n- Marketing campaigns, content automation, or asset production\n- Gemini multimodal capabilities\n- Vertex AI media operations\n- Social media content, email marketing, or PMax campaigns\n\n## Integration with Other Tools\n\n**Google Cloud Services:**\n- Cloud Storage for media asset management\n- BigQuery for campaign analytics\n- Cloud Functions for automation triggers\n- Vertex AI Pipelines for content workflows\n\n**Third-Party Integrations:**\n- Social media APIs (LinkedIn, Twitter, Instagram)\n- Marketing automation platforms (HubSpot, Marketo)\n- CMS integrations (WordPress, Contentful)\n- DAM systems (Bynder, Cloudinary)\n\n## Success Metrics\n\n**Track These KPIs:**\n- Asset generation speed (baseline: 5 images/min)\n- Content approval rate (target: >80%)\n- Campaign personalization scale (target: 1000+ variants)\n- Cost per asset (target: <$0.10/image)\n- Time saved vs manual production (target: 90% reduction)\n\n---\n\n**This skill makes Jeremy a Vertex AI multimodal expert with instant access to video processing, audio generation, image creation, and marketing automation capabilities.**\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nIf execution fails:\n- Verify prerequisites are met\n- Check input parameters and formats\n- Validate file paths and permissions\n- Review error messages for root cause\n- Consult documentation for troubleshooting\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "003-jeremy-vertex-ai-media-master",
        "category": "productivity",
        "path": "plugins/productivity/003-jeremy-vertex-ai-media-master",
        "version": "1.0.0",
        "description": "Comprehensive Google Vertex AI multimodal mastery for Jeremy - video processing (6+ hours), audio generation, image creation with Gemini 2.0/2.5 and Imagen 4. Marketing campaign automation, content generation, and media asset production."
      },
      "filePath": "plugins/productivity/003-jeremy-vertex-ai-media-master/skills/vertex-media-master/SKILL.md"
    },
    {
      "slug": "vertex-engine-inspector",
      "name": "vertex-engine-inspector",
      "description": "Inspect and validate Vertex AI Agent Engine deployments including Code",
      "allowedTools": [
        "Read",
        "Grep",
        "Glob",
        "Bash"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## What This Skill Does\n\nExpert inspector for the Vertex AI Agent Engine managed runtime. Performs comprehensive validation of deployed agents including runtime configuration, security posture, performance settings, A2A protocol compliance, and production readiness scoring.\n\n## When This Skill Activates\n\n### Trigger Phrases\n- \"Inspect Vertex AI Engine agent\"\n- \"Validate Agent Engine deployment\"\n- \"Check Code Execution Sandbox configuration\"\n- \"Verify Memory Bank settings\"\n- \"Monitor agent health\"\n- \"Agent Engine production readiness\"\n- \"A2A protocol compliance check\"\n- \"Agent Engine security audit\"\n\n### Use Cases\n- Pre-production deployment validation\n- Post-deployment health monitoring\n- Security compliance audits\n- Performance optimization reviews\n- Troubleshooting agent issues\n- Configuration drift detection\n\n## Inspection Categories\n\n### 1. Runtime Configuration ‚úÖ\n- Model selection (Gemini 2.5 Pro/Flash)\n- Tools enabled (Code Execution, Memory Bank, custom)\n- VPC configuration\n- Resource allocation\n- Scaling policies\n\n### 2. Code Execution Sandbox üîí\n- **Security**: Isolated environment, no external network access\n- **State Persistence**: TTL validation (1-14 days)\n- **IAM**: Least privilege permissions\n- **Performance**: Timeout and resource limits\n- **Concurrent Executions**: Max concurrent code runs\n\n**Critical Checks**:\n```\n‚úÖ State TTL between 7-14 days (optimal for production)\n‚úÖ Sandbox type is SECURE_ISOLATED\n‚úÖ IAM permissions limited to required GCP services only\n‚úÖ Timeout configured appropriately\n‚ö†Ô∏è State TTL < 7 days may cause premature session loss\n‚ùå State TTL > 14 days not allowed by Agent Engine\n```\n\n### 3. Memory Bank Configuration üß†\n- **Enabled Status**: Persistent memory active\n- **Retention Policy**: Max memories, retention days\n- **Storage Backend**: Firestore encryption & region\n- **Query Performance**: Indexing, caching, latency\n- **Auto-Cleanup**: Quota management\n\n**Critical Checks**:\n```\n‚úÖ Max memories >= 100 (prevents conversation truncation)\n‚úÖ Indexing enabled (fast query performance)\n‚úÖ Auto-cleanup enabled (prevents quota exhaustion)\n‚úÖ Encrypted at rest (Firestore default)\n‚ö†Ô∏è Low memory limit may truncate long conversations\n```\n\n### 4. A2A Protocol Compliance üîó\n- **AgentCard**: Available at `/.well-known/agent-card`\n- **Task API**: `POST /v1/tasks:send` responds correctly\n- **Status API**: `GET /v1/tasks/{task_id}` accessible\n- **Protocol Version**: 1.0 compliance\n- **Required Fields**: name, description, tools, version\n\n**Compliance Report**:\n```\n‚úÖ AgentCard accessible and valid\n‚úÖ Task submission API functional\n‚úÖ Status polling API functional\n‚úÖ Protocol version 1.0\n‚ùå Missing AgentCard fields: [...]\n‚ùå Task API not responding (check IAM/networking)\n```\n\n### 5. Security Posture üõ°Ô∏è\n- **IAM Roles**: Least privilege validation\n- **VPC Service Controls**: Perimeter protection\n- **Model Armor**: Prompt injection protection\n- **Encryption**: At-rest and in-transit\n- **Service Account**: Proper configuration\n- **Secret Management**: No hardcoded credentials\n\n**Security Score**:\n```\nüü¢ SECURE (90-100%): Production ready\nüü° NEEDS ATTENTION (70-89%): Address issues before prod\nüî¥ INSECURE (<70%): Do not deploy to production\n```\n\n### 6. Performance Metrics üìä\n- **Auto-Scaling**: Min/max instances configured\n- **Resource Limits**: CPU, memory appropriate\n- **Latency**: P50, P95, P99 within SLOs\n- **Throughput**: Requests per second\n- **Token Usage**: Cost tracking\n- **Error Rate**: < 5% target\n\n**Health Status**:\n```\nüü¢ HEALTHY: Error rate < 5%, latency < 3s (p95)\nüü° DEGRADED: Error rate 5-10% or latency 3-5s\nüî¥ UNHEALTHY: Error rate > 10% or latency > 5s\n```\n\n### 7. Monitoring & Observability üìà\n- **Cloud Monitoring**: Dashboards configured\n- **Alerting**: Policies for errors, latency, costs\n- **Logging**: Structured logs aggregated\n- **Tracing**: OpenTelemetry enabled\n- **Error Tracking**: Cloud Error Reporting\n\n**Observability Score**:\n```\n‚úÖ All 5 pillars configured: Metrics, Logs, Traces, Alerts, Dashboards\n‚ö†Ô∏è Missing alerts for critical scenarios\n‚ùå No monitoring configured (production blocker)\n```\n\n## Production Readiness Scoring\n\n### Scoring Matrix\n\n| Category | Weight | Checks |\n|----------|--------|--------|\n| Security | 30% | 6 checks (IAM, VPC-SC, encryption, etc.) |\n| Performance | 25% | 6 checks (scaling, limits, SLOs, etc.) |\n| Monitoring | 20% | 6 checks (dashboards, alerts, logs, etc.) |\n| Compliance | 15% | 5 checks (audit logs, DR, privacy, etc.) |\n| Reliability | 10% | 5 checks (multi-region, failover, etc.) |\n\n### Overall Readiness Status\n\n```\nüü¢ PRODUCTION READY (85-100%)\n   - All critical checks passed\n   - Minor optimizations recommended\n   - Safe to deploy\n\nüü° NEEDS IMPROVEMENT (70-84%)\n   - Some important checks failed\n   - Address issues before production\n   - Staging deployment acceptable\n\nüî¥ NOT READY (<70%)\n   - Critical failures present\n   - Do not deploy to production\n   - Fix blocking issues first\n```\n\n## Inspection Workflow\n\n### Phase 1: Configuration Analysis\n```\n1. Connect to Agent Engine\n2. Retrieve agent metadata\n3. Parse runtime configuration\n4. Extract Code Execution settings\n5. Extract Memory Bank settings\n6. Document VPC configuration\n```\n\n### Phase 2: Protocol Validation\n```\n1. Test AgentCard endpoint\n2. Validate AgentCard structure\n3. Test Task API (POST /v1/tasks:send)\n4. Test Status API (GET /v1/tasks/{id})\n5. Verify A2A protocol version\n```\n\n### Phase 3: Security Audit\n```\n1. Review IAM roles and permissions\n2. Check VPC Service Controls\n3. Validate encryption settings\n4. Scan for hardcoded secrets\n5. Verify Model Armor enabled\n6. Assess service account security\n```\n\n### Phase 4: Performance Analysis\n```\n1. Query Cloud Monitoring metrics\n2. Calculate error rate (last 24h)\n3. Analyze latency percentiles\n4. Review token usage and costs\n5. Check auto-scaling behavior\n6. Validate resource limits\n```\n\n### Phase 5: Production Readiness\n```\n1. Run all checklist items (28 checks)\n2. Calculate category scores\n3. Calculate overall score\n4. Determine readiness status\n5. Generate recommendations\n6. Create action plan\n```\n\n## Tool Permissions\n\n**Read-only inspection** - Cannot modify configurations:\n- **Read**: Analyze agent configuration files\n- **Grep**: Search for security issues\n- **Glob**: Find related configuration\n- **Bash**: Query GCP APIs (read-only)\n\n## Example Inspection Report\n\n```yaml\nAgent ID: gcp-deployer-agent\nDeployment Status: RUNNING\nInspection Date: 2025-12-09\n\nRuntime Configuration:\n  Model: gemini-2.5-flash\n  Code Execution: ‚úÖ Enabled (TTL: 14 days)\n  Memory Bank: ‚úÖ Enabled (retention: 90 days)\n  VPC: ‚úÖ Configured (private-vpc-prod)\n\nA2A Protocol Compliance:\n  AgentCard: ‚úÖ Valid\n  Task API: ‚úÖ Functional\n  Status API: ‚úÖ Functional\n  Protocol Version: 1.0\n\nSecurity Posture:\n  IAM: ‚úÖ Least privilege (score: 95%)\n  VPC-SC: ‚úÖ Enabled\n  Model Armor: ‚úÖ Enabled\n  Encryption: ‚úÖ At-rest & in-transit\n  Overall: üü¢ SECURE (92%)\n\nPerformance Metrics (24h):\n  Request Count: 12,450\n  Error Rate: 2.3% üü¢\n  Latency (p95): 1,850ms üü¢\n  Token Usage: 450K tokens\n  Cost Estimate: $12.50/day\n\nProduction Readiness:\n  Security: 92% (28/30 points)\n  Performance: 88% (22/25 points)\n  Monitoring: 95% (19/20 points)\n  Compliance: 80% (12/15 points)\n  Reliability: 70% (7/10 points)\n\n  Overall Score: 87% üü¢ PRODUCTION READY\n\nRecommendations:\n  1. Enable multi-region deployment (reliability +10%)\n  2. Configure automated backups (compliance +5%)\n  3. Add circuit breaker pattern (reliability +5%)\n  4. Optimize memory bank indexing (performance +3%)\n```\n\n## Integration with Other Plugins\n\n### Works with jeremy-adk-orchestrator\n- Orchestrator deploys agents\n- Inspector validates deployments\n- Feedback loop for optimization\n\n### Works with jeremy-vertex-validator\n- Validator checks code before deployment\n- Inspector validates runtime after deployment\n- Complementary pre/post checks\n\n### Works with jeremy-adk-terraform\n- Terraform provisions infrastructure\n- Inspector validates provisioned agents\n- Ensures IaC matches runtime\n\n## Troubleshooting Guide\n\n### Issue: Agent not responding\n**Inspector checks**:\n- VPC configuration allows traffic\n- IAM permissions correct\n- Agent Engine status is RUNNING\n- No quota limits exceeded\n\n### Issue: High error rate\n**Inspector checks**:\n- Model configuration appropriate\n- Resource limits not exceeded\n- Code Execution sandbox not timing out\n- Memory Bank not quota-exhausted\n\n### Issue: Slow response times\n**Inspector checks**:\n- Auto-scaling configured\n- Code Execution TTL appropriate\n- Memory Bank indexing enabled\n- Caching strategy implemented\n\n## Version History\n\n- **1.0.0** (2025): Initial release with Agent Engine GA support, Code Execution Sandbox, Memory Bank, A2A protocol validation\n\n## References\n\n- Agent Engine: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\n- Code Execution: https://cloud.google.com/agent-builder/agent-engine/code-execution/overview\n- Memory Bank: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/overview\n- A2A Protocol: https://google.github.io/adk-docs/a2a/",
      "parentPlugin": {
        "name": "jeremy-vertex-engine",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-vertex-engine",
        "version": "1.0.0",
        "description": "Vertex AI Agent Engine deployment inspector and runtime validator"
      },
      "filePath": "plugins/ai-ml/jeremy-vertex-engine/skills/vertex-engine-inspector/SKILL.md"
    },
    {
      "slug": "vertex-infra-expert",
      "name": "vertex-infra-expert",
      "description": "Use when provisioning Vertex AI infrastructure with Terraform. Trigger with phrases like \"vertex ai terraform\", \"deploy gemini terraform\", \"model garden infrastructure\", \"vertex ai endpoints terraform\", or \"vector search terraform\". Provisions Model Garden models, Gemini endpoints, vector search indices, ML pipelines, and production AI services with encryption and auto-scaling.",
      "allowedTools": [
        "Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(terraform:*)",
        "Bash(gcloud:*)"
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "## Prerequisites\n\nBefore using this skill, ensure:\n- Google Cloud project with Vertex AI API enabled\n- Terraform 1.0+ installed\n- gcloud CLI authenticated with appropriate permissions\n- Understanding of Vertex AI services and ML models\n- KMS keys created for encryption (if required)\n- GCS buckets for model artifacts and embeddings\n\n## Instructions\n\n1. **Define AI Services**: Identify required Vertex AI components (endpoints, vector search, pipelines)\n2. **Configure Terraform**: Set up backend and define project variables\n3. **Provision Endpoints**: Deploy Gemini or custom model endpoints with auto-scaling\n4. **Set Up Vector Search**: Create indices for embeddings with appropriate dimensions\n5. **Configure Encryption**: Apply KMS encryption to endpoints and data\n6. **Implement Monitoring**: Set up Cloud Monitoring for model performance\n7. **Apply IAM Policies**: Grant least privilege access to AI services\n8. **Validate Deployment**: Test endpoints and verify model availability\n\n## Output\n\n**Gemini Model Endpoint:**\n```hcl\n# {baseDir}/terraform/vertex-endpoints.tf\nresource \"google_vertex_ai_endpoint\" \"gemini_endpoint\" {\n  name         = \"gemini-25-flash-endpoint\"\n  display_name = \"Gemini 2.5 Flash Production\"\n  location     = var.region\n\n  encryption_spec {\n    kms_key_name = google_kms_crypto_key.vertex_key.id\n  }\n}\n\nresource \"google_vertex_ai_deployed_model\" \"gemini_deployment\" {\n  endpoint = google_vertex_ai_endpoint.gemini_endpoint.id\n  model    = \"publishers/google/models/gemini-2.5-flash\"\n\n  automatic_resources {\n    min_replica_count = 1\n    max_replica_count = 5\n  }\n}\n```\n\n**Vector Search Index:**\n```hcl\nresource \"google_vertex_ai_index\" \"embeddings_index\" {\n  display_name = \"production-embeddings\"\n  location     = var.region\n\n  metadata {\n    contents_delta_uri = \"gs://${google_storage_bucket.embeddings.name}/index\"\n    config {\n      dimensions = 768\n      approximate_neighbors_count = 150\n      distance_measure_type = \"DOT_PRODUCT_DISTANCE\"\n\n      algorithm_config {\n        tree_ah_config {\n          leaf_node_embedding_count = 1000\n          leaf_nodes_to_search_percent = 10\n        }\n      }\n    }\n  }\n}\n```\n\n## Error Handling\n\n**API Not Enabled**\n- Error: \"Vertex AI API has not been used in project\"\n- Solution: Enable with `gcloud services enable aiplatform.googleapis.com`\n\n**Model Not Found**\n- Error: \"Model publishers/google/models/... not found\"\n- Solution: Verify model ID and region availability\n\n**Quota Exceeded**\n- Error: \"Quota exceeded for resource\"\n- Solution: Request quota increase or reduce replica count\n\n**KMS Key Access Denied**\n- Error: \"Permission denied on KMS key\"\n- Solution: Grant cloudkms.cryptoKeyEncrypterDecrypter role to Vertex AI service account\n\n**Vector Search Build Failed**\n- Error: \"Index build failed\"\n- Solution: Check GCS bucket permissions and embedding format\n\n## Resources\n\n- Vertex AI Terraform: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/vertex_ai_endpoint\n- Vertex AI documentation: https://cloud.google.com/vertex-ai/docs\n- Model Garden: https://cloud.google.com/model-garden\n- Vector Search guide: https://cloud.google.com/vertex-ai/docs/vector-search\n- Terraform examples in {baseDir}/vertex-examples/",
      "parentPlugin": {
        "name": "jeremy-vertex-terraform",
        "category": "devops",
        "path": "plugins/devops/jeremy-vertex-terraform",
        "version": "1.0.0",
        "description": "Terraform configurations for Vertex AI platform and Agent Engine"
      },
      "filePath": "plugins/devops/jeremy-vertex-terraform/skills/vertex-infra-expert/SKILL.md"
    },
    {
      "slug": "yaml-master",
      "name": "yaml-master",
      "description": "Proactive YAML intelligence: automatically activates when working with YAML files. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. version: 1.0.0 allowed-tools: license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# YAML Master Agent\n\n**‚ö° This skill activates AUTOMATICALLY when you work with YAML files!**\n\n## Automatic Trigger Conditions\n\nThis skill proactively activates when Claude detects:\n\n1. **File Operations**: Reading, writing, or editing `.yaml` or `.yml` files\n2. **Configuration Management**: Working with Ansible, Kubernetes, Docker Compose, GitHub Actions\n3. **CI/CD Workflows**: GitLab CI, CircleCI, Travis CI, Azure Pipelines configurations\n4. **Schema Validation**: Validating configuration files against schemas\n5. **Format Conversion**: Converting between YAML, JSON, TOML, XML formats\n6. **User Requests**: Explicit mentions of \"yaml\", \"validate yaml\", \"fix yaml syntax\", \"convert yaml\"\n\n**No commands needed!** Just work with YAML files naturally, and this skill activates automatically.\n\n---\n\n## Core Capabilities\n\n### 1. Intelligent YAML Validation\n\n**What It Does**:\n- Detects syntax errors (indentation, duplicate keys, invalid scalars)\n- Validates against YAML 1.2 specification\n- Identifies common anti-patterns (tabs vs spaces, anchors/aliases issues)\n- Provides detailed error messages with line numbers and fix suggestions\n\n**Example**:\n```yaml\n# ‚ùå INVALID YAML\nservices:\n  web:\n    image: nginx\n\t  ports:  # Mixed tabs and spaces - ERROR!\n      - \"80:80\"\n```\n\n**Agent Action**: Automatically detects mixed indentation, suggests fix:\n```yaml\n# ‚úÖ FIXED YAML\nservices:\n  web:\n    image: nginx\n    ports:  # Consistent 2-space indentation\n      - \"80:80\"\n```\n\n### 2. Schema Inference & Generation\n\n**What It Does**:\n- Analyzes YAML structure and infers JSON Schema\n- Generates OpenAPI/Swagger schemas from YAML\n- Creates type definitions for TypeScript/Python from YAML configs\n- Validates instances against inferred or provided schemas\n\n**Example**:\n```yaml\n# Input YAML\nuser:\n  name: Jeremy\n  age: 35\n  roles:\n    - admin\n    - developer\n```\n\n**Agent Action**: Infers schema:\n```json\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"user\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"age\": { \"type\": \"integer\" },\n        \"roles\": {\n          \"type\": \"array\",\n          \"items\": { \"type\": \"string\" }\n        }\n      },\n      \"required\": [\"name\", \"age\", \"roles\"]\n    }\n  }\n}\n```\n\n### 3. Format Conversion (YAML ‚Üî JSON ‚Üî TOML ‚Üî XML)\n\n**What It Does**:\n- Bidirectional conversion between YAML, JSON, TOML, XML\n- Preserves comments when possible (YAML ‚Üî TOML)\n- Handles complex nested structures, arrays, anchors/aliases\n- Validates converted output maintains semantic equivalence\n\n**Example - YAML to JSON**:\n```yaml\n# config.yaml\ndatabase:\n  host: localhost\n  port: 5432\n  credentials: &creds\n    user: admin\n    pass: secret\n```\n\n**Agent Action**: Converts to JSON:\n```json\n{\n  \"database\": {\n    \"host\": \"localhost\",\n    \"port\": 5432,\n    \"credentials\": {\n      \"user\": \"admin\",\n      \"pass\": \"secret\"\n    }\n  }\n}\n```\n\n### 4. Kubernetes Manifest Expertise\n\n**What It Does**:\n- Validates K8s manifests against API versions\n- Suggests best practices (resource limits, labels, health checks)\n- Detects security issues (privileged containers, root users)\n- Generates complete manifests from minimal specs\n\n**Example**:\n```yaml\n# Minimal input\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n```\n\n**Agent Action**: Expands with best practices:\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    version: \"1.0\"\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: \"1.0\"\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25-alpine\n        ports:\n        - containerPort: 80\n          name: http\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1000\n          allowPrivilegeEscalation: false\n```\n\n### 5. Docker Compose Optimization\n\n**What It Does**:\n- Validates Docker Compose syntax (v2.x, v3.x)\n- Suggests networking best practices\n- Optimizes volume mounts and environment variables\n- Detects security misconfigurations\n\n**Example**:\n```yaml\n# ‚ùå SUBOPTIMAL\nversion: '3'\nservices:\n  db:\n    image: postgres\n    environment:\n      POSTGRES_PASSWORD: admin123  # Hardcoded secret!\n```\n\n**Agent Action**: Suggests secure alternative:\n```yaml\n# ‚úÖ OPTIMIZED\nversion: '3.8'\nservices:\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_PASSWORD_FILE: /run/secrets/db_password\n    secrets:\n      - db_password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U postgres\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    restart: unless-stopped\n\nsecrets:\n  db_password:\n    file: ./secrets/db_password.txt\n\nvolumes:\n  db_data:\n    driver: local\n```\n\n### 6. CI/CD Pipeline Intelligence\n\n**What It Does**:\n- Validates GitHub Actions, GitLab CI, CircleCI workflows\n- Suggests caching strategies for faster builds\n- Detects matrix build inefficiencies\n- Optimizes job dependencies and parallelization\n\n**Example - GitHub Actions**:\n```yaml\n# ‚ùå INEFFICIENT\nname: CI\non: [push]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: npm install  # No caching!\n      - run: npm test\n```\n\n**Agent Action**: Optimizes with caching:\n```yaml\n# ‚úÖ OPTIMIZED\nname: CI\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [18, 20, 22]\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci  # Faster than npm install\n\n      - name: Run tests\n        run: npm test\n\n      - name: Upload coverage\n        if: matrix.node-version == 20\n        uses: codecov/codecov-action@v4\n```\n\n### 7. YAML Linting & Style Enforcement\n\n**What It Does**:\n- Enforces consistent indentation (2 spaces, 4 spaces, tabs)\n- Validates key ordering (alphabetical, custom)\n- Detects trailing whitespace, missing newlines\n- Suggests canonical YAML representations\n\n**Linting Rules**:\n```yaml\n# Rule 1: Consistent 2-space indentation\n# Rule 2: No duplicate keys\n# Rule 3: Quoted strings for special characters\n# Rule 4: Explicit document markers (---, ...)\n# Rule 5: No tabs, only spaces\n# Rule 6: Max line length 120 characters\n# Rule 7: Comments aligned at column 40\n```\n\n### 8. Anchors & Aliases Mastery\n\n**What It Does**:\n- Manages complex YAML anchors and aliases\n- Suggests reusable configurations with merge keys\n- Validates anchor references\n- Refactors duplicate blocks into anchors\n\n**Example**:\n```yaml\n# ‚ùå REPETITIVE\nservices:\n  web:\n    image: nginx\n    restart: always\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n  api:\n    image: node:20\n    restart: always\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n```\n\n**Agent Action**: Refactors with anchors:\n```yaml\n# ‚úÖ DRY (Don't Repeat Yourself)\nx-common-config: &common-config\n  restart: always\n  logging:\n    driver: json-file\n    options:\n      max-size: \"10m\"\n\nservices:\n  web:\n    <<: *common-config\n    image: nginx\n\n  api:\n    <<: *common-config\n    image: node:20\n```\n\n---\n\n## Advanced Features\n\n### Multi-Document YAML Handling\n\nWorks with YAML files containing multiple documents:\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n---\n```\n\n**Agent Action**: Validates each document independently, ensures consistency across documents.\n\n### Environment-Specific Configurations\n\nManages environment overrides and templates:\n\n```yaml\n# base.yaml\ndatabase: &db\n  host: localhost\n  port: 5432\n\n# production.yaml (inherits from base)\ndatabase:\n  <<: *db\n  host: prod-db.example.com\n  ssl: true\n```\n\n### Complex Data Type Handling\n\nSupports advanced YAML data types:\n\n```yaml\n# Timestamps\ncreated_at: 2025-10-24T23:00:00Z\n\n# Binary data (base64)\nssl_cert: !!binary |\n  R0lGODlhDAAMAIQAAP//9/X\n  17unp5WZmZgAAAOfn515eXv\n\n# Null values\noptional_field: null\nanother_null: ~\n\n# Custom tags\ncolor: !rgb [255, 128, 0]\n```\n\n---\n\n## Common Use Cases\n\n### 1. Fixing Broken YAML Files\n\n**User**: \"My Kubernetes manifest won't apply, fix it\"\n\n**Agent Action**:\n1. Reads the YAML file\n2. Identifies syntax errors (indentation, missing fields)\n3. Validates against Kubernetes API schema\n4. Provides corrected version with explanations\n\n### 2. Converting JSON API Response to YAML Config\n\n**User**: \"Convert this JSON to YAML for my config file\"\n\n**Agent Action**:\n1. Parses JSON input\n2. Converts to idiomatic YAML (multi-line strings, minimal quotes)\n3. Adds helpful comments\n4. Validates output\n\n### 3. Generating Docker Compose from Requirements\n\n**User**: \"Create docker-compose.yaml for nginx + postgres + redis\"\n\n**Agent Action**:\n1. Generates complete docker-compose.yaml\n2. Adds healthchecks, volumes, networks\n3. Includes environment variable templates\n4. Suggests .env file structure\n\n### 4. Optimizing CI/CD Pipeline\n\n**User**: \"My GitHub Actions workflow is slow, optimize it\"\n\n**Agent Action**:\n1. Analyzes workflow YAML\n2. Identifies bottlenecks (no caching, sequential jobs)\n3. Suggests parallelization, caching strategies\n4. Provides optimized workflow\n\n---\n\n## Integration with Other Tools\n\n### Works Seamlessly With:\n\n- **yamllint**: Validates against yamllint rules\n- **Kustomize**: Handles Kustomization files\n- **Helm**: Works with Helm chart values.yaml\n- **Ansible**: Validates playbooks and roles\n- **OpenAPI/Swagger**: Converts to/from OpenAPI specs\n- **JSON Schema**: Validates against schemas\n- **Terraform**: Converts YAML to HCL (experimental)\n\n---\n\n## Error Handling & Troubleshooting\n\n### Common YAML Errors This Skill Fixes:\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `mapping values are not allowed here` | Incorrect indentation | Align keys properly |\n| `found duplicate key` | Same key defined twice | Remove or rename duplicate |\n| `expected <block end>, but found` | Tab instead of spaces | Replace tabs with spaces |\n| `found undefined tag handle` | Custom tag without definition | Define tag or remove |\n| `could not find expected ':'` | Missing colon after key | Add colon |\n\n---\n\n## Best Practices Enforced\n\n1. **Indentation**: Consistent 2-space indentation (configurable)\n2. **Quotes**: Minimal quoting (only when necessary)\n3. **Comments**: Descriptive comments for complex sections\n4. **Security**: No hardcoded secrets, use secrets managers\n5. **Validation**: Always validate against schemas\n6. **Documentation**: Inline documentation for anchors/aliases\n7. **Versioning**: Explicit version tags (Docker Compose, K8s API)\n\n---\n\n## Performance Considerations\n\n- **Large Files**: Streams YAML instead of loading entire file into memory\n- **Validation**: Incremental validation for real-time feedback\n- **Conversion**: Optimized parsers for fast format conversion\n- **Caching**: Caches schema validation results\n\n---\n\n## Compliance & Standards\n\n‚úÖ **YAML 1.2 Specification**: Fully compliant\n‚úÖ **YAML 1.1**: Backward compatible where possible\n‚úÖ **JSON Schema Draft 7**: Supports schema validation\n‚úÖ **OpenAPI 3.1**: Compatible with OpenAPI specs\n‚úÖ **Kubernetes API**: Validates against all stable APIs\n‚úÖ **Docker Compose v3.8**: Full support for latest spec\n\n---\n\n## Examples by Complexity\n\n### Beginner: Simple Config File\n\n```yaml\n# app-config.yaml\napp:\n  name: MyApp\n  version: 1.0.0\n  environment: production\n\nserver:\n  host: 0.0.0.0\n  port: 8080\n\ndatabase:\n  url: postgres://localhost:5432/mydb\n```\n\n### Intermediate: Multi-Service Docker Compose\n\n```yaml\nversion: '3.8'\n\nservices:\n  web:\n    build: ./web\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - api\n      - redis\n\n  api:\n    build: ./api\n    environment:\n      DATABASE_URL: postgres://db:5432/app\n    depends_on:\n      db:\n        condition: service_healthy\n\n  db:\n    image: postgres:15-alpine\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\"]\n      interval: 5s\n\n  redis:\n    image: redis:7-alpine\n    command: redis-server --appendonly yes\n\nvolumes:\n  db_data:\n```\n\n### Advanced: Kubernetes Deployment with Secrets\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\nstringData:\n  DATABASE_URL: postgres://user:pass@db:5432/app\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: myapp:latest\n        envFrom:\n        - secretRef:\n            name: app-secrets\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\nspec:\n  selector:\n    app: web\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: LoadBalancer\n```\n\n---\n\n## Troubleshooting Guide\n\n### Issue: \"YAML won't parse\"\n\n**Diagnosis**:\n1. Check indentation (tabs vs spaces)\n2. Verify key-value separator (`:` with space after)\n3. Look for duplicate keys\n\n### Issue: \"Kubernetes apply fails\"\n\n**Diagnosis**:\n1. Validate API version matches cluster version\n2. Check required fields are present\n3. Verify resource names are DNS-compliant\n\n### Issue: \"Docker Compose won't start\"\n\n**Diagnosis**:\n1. Check version compatibility\n2. Validate service dependencies\n3. Verify volume mount paths exist\n\n---\n\n## Version History\n\n- **v1.0.0** (2025-10-24): Initial release with comprehensive YAML capabilities\n\n---\n\n## License\n\nMIT License - See LICENSE file\n\n---\n\n## Support\n\n- **Issues**: Report issues with YAML handling\n- **Documentation**: This SKILL.md + plugin README\n- **Community**: Share YAML tips and tricks\n\n---\n\n## Credits\n\n**Author**: Jeremy Longshore\n**Plugin**: 002-jeremy-yaml-master-agent\n**Spec Compliance**: Anthropic Agent Skills Spec v1.0\n\nü§ñ Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nIf execution fails:\n- Verify prerequisites are met\n- Check input parameters and formats\n- Validate file paths and permissions\n- Review error messages for root cause\n- Consult documentation for troubleshooting\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "002-jeremy-yaml-master-agent",
        "category": "productivity",
        "path": "plugins/productivity/002-jeremy-yaml-master-agent",
        "version": "1.0.0",
        "description": "Intelligent YAML validation, generation, and transformation agent with schema inference, linting, and format conversion capabilities"
      },
      "filePath": "plugins/productivity/002-jeremy-yaml-master-agent/skills/yaml-master/SKILL.md"
    }
  ],
  "count": 239,
  "generatedAt": "2025-12-26T00:00:37.561Z",
  "categories": [
    "ai-ml",
    "api-development",
    "business-tools",
    "community",
    "crypto",
    "database",
    "devops",
    "examples",
    "packages",
    "performance",
    "productivity",
    "security",
    "skill-enhancers",
    "testing"
  ],
  "allowedToolsUsed": [
    "Bash",
    "Bash(analysis:*)",
    "Bash(artillery:*)",
    "Bash(audit:*)",
    "Bash(awk:*)",
    "Bash(aws:*)",
    "Bash(az:*)",
    "Bash(ci:*)",
    "Bash(curl:*)",
    "Bash(diff:*)",
    "Bash(docker:*)",
    "Bash(gcloud:*)",
    "Bash(general:*)",
    "Bash(git:*)",
    "Bash(grep:*)",
    "Bash(helm:*)",
    "Bash(iostat:*)",
    "Bash(jmeter:*)",
    "Bash(k6:*)",
    "Bash(kubectl:*)",
    "Bash(lighthouse:*)",
    "Bash(logs:*)",
    "Bash(memory:*)",
    "Bash(metrics:*)",
    "Bash(monitoring:*)",
    "Bash(nginx:*)",
    "Bash(npm:*)",
    "Bash(performance:*)",
    "Bash(ping:*)",
    "Bash(profiling:*)",
    "Bash(prometheus:*)",
    "Bash(ps:*)",
    "Bash(rum:*)",
    "Bash(scan:*)",
    "Bash(security:*)",
    "Bash(system:*)",
    "Bash(terraform:*)",
    "Bash(testing:*)",
    "Bash(top:*)",
    "Bash(traceroute:*)",
    "Bash(util:*)",
    "Bash(vmstat:*)",
    "Bash(webpack:*)",
    "Edit",
    "Glob",
    "Grep",
    "Read",
    "WebFetch",
    "WebSearch",
    "Write"
  ]
}