{
  "skills": [
    {
      "slug": "000-jeremy-content-consistency-validator",
      "name": "000-jeremy-content-consistency-validator",
      "description": "Validate messaging consistency across website, GitHub repos, and local documentation generating read-only discrepancy reports. Use when checking content alignment or finding mixed messaging. Trigger with phrases like \"check consistency\", \"validate documentation\", or \"audit messaging\". allowed-tools: Read, WebFetch, WebSearch, Grep, Bash(diff:*), Bash(grep:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# 000 Jeremy Content Consistency Validator\n\nThis skill provides automated assistance for 000 jeremy content consistency validator tasks.\n\n**CRITICAL OPERATING PARAMETERS:**\n- **Temperature: 0.0** - ZERO creativity. Pure factual analysis only.\n- **Read-only** - Report discrepancies, never suggest creative fixes\n- **Exact matching** - Report differences precisely as found\n- **No interpretation** - Facts only, no opinions\n\n**WORKFLOW MANDATE:**\n- Website = OFFICIAL source of truth\n- Local docs (SOPs, standards, principles, beliefs) MUST match website\n- Report what internal docs are missing compared to published website\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## What This Skill Does\n\nThis skill performs comprehensive **read-only validation** of messaging consistency across three critical content sources:\n\n1. **Website Content** (ANY HTML site: WordPress, Hugo, Astro, Next.js, static HTML, etc.) - **OFFICIAL SOURCE OF TRUTH**\n2. **GitHub Repositories** (README files, technical documentation)\n3. **Local Documentation** (SOPs, standards, principles, beliefs, training materials, internal docs, procedures)\n\n**CRITICAL: This skill NEVER makes changes.** It only generates detailed discrepancy reports for human review.\n\n## When This Skill Activates\n\nTrigger this skill when you mention:\n- \"Check consistency between website and GitHub\"\n- \"Validate documentation consistency\"\n- \"Audit messaging across platforms\"\n- \"Find mixed messaging\"\n- \"Before I update internal docs, check website first\"\n- \"Ensure website matches GitHub\"\n- \"Generate consistency report\"\n\n## How It Works\n\n### Phase 1: Source Discovery\n\n1. **Identify Website Sources**\n   - Detect and analyze ANY HTML-based website:\n     - Static HTML sites (index.html, about.html)\n     - Hugo/Astro static site generators\n     - Jekyll/GitHub Pages sites\n     - WordPress sites (wp-content/)\n     - Next.js/React sites (build/, out/, .next/)\n     - Vue/Nuxt sites (dist/, .nuxt/)\n     - Gatsby sites (public/)\n     - 11ty/Eleventy sites (_site/)\n     - Docusaurus sites (build/)\n     - Any other HTML-based website structure\n   - Find marketing pages, landing pages, product descriptions\n   - Extract key messaging: taglines, value propositions, feature lists\n\n2. **Identify GitHub Sources**\n   - Locate relevant repositories\n   - Find README.md, CONTRIBUTING.md, documentation folders\n   - Extract: project descriptions, feature claims, installation instructions\n\n3. **Identify Local Documentation**\n   - Find internal docs, training materials, SOPs\n   - Locate claudes-docs/, docs/, internal/ directories\n   - Extract: procedures, guidelines, technical specifications\n\n### Phase 2: Content Extraction\n\nFor each source, extract:\n- **Core messaging** (mission statements, value propositions)\n- **Feature descriptions** (what the product/service does)\n- **Version numbers** (software versions, release dates)\n- **URLs and links** (external references, documentation links)\n- **Contact information** (emails, support channels)\n- **Technical specifications** (requirements, dependencies)\n- **Terminology** (consistent use of product names, technical terms)\n\n### Phase 3: Consistency Analysis\n\nCompare content across sources and identify:\n\n**üî¥ Critical Discrepancies:**\n- Conflicting version numbers\n- Different feature lists\n- Contradictory technical requirements\n- Mismatched contact information\n- Broken cross-references\n\n**üü° Warning-Level Issues:**\n- Inconsistent terminology (e.g., \"plugin\" vs \"extension\")\n- Different phrasing of same concept\n- Missing information in one source\n- Outdated timestamps or dates\n\n**üü¢ Informational Notes:**\n- Stylistic differences (acceptable)\n- Platform-specific variations (expected)\n- Different levels of detail (appropriate)\n\n### Phase 4: Generate Discrepancy Report\n\nCreate a comprehensive Markdown report with:\n\n```markdown\n# Content Consistency Validation Report\nGenerated: [timestamp]\n\n## Executive Summary\n- Total sources analyzed: X\n- Critical discrepancies: X\n- Warnings: X\n- Informational notes: X\n\n## 1. Website vs GitHub Discrepancies\n\n### üî¥ CRITICAL: Version Mismatch\n**Website says:** v1.2.0\n**GitHub says:** v1.2.1\n**Location:**\n- Website: /about/index.html:45\n- GitHub: README.md:12\n**Recommendation:** Update website to reflect v1.2.1\n\n### üü° WARNING: Feature Description Inconsistency\n**Website says:** \"Supports 236 plugins\"\n**GitHub says:** \"Over 230 plugins available\"\n**Impact:** Potential customer confusion\n**Recommendation:** Standardize on exact number\n\n## 2. Website vs Local Docs Discrepancies\n\n### üî¥ CRITICAL: Contact Email Mismatch\n**Website says:** support@example.com\n**Local docs say:** help@example.com\n**Training materials:** Support email is support@example.com\n**Recommendation:** Update local docs to support@example.com\n\n## 3. GitHub vs Local Docs Discrepancies\n\n### üü° WARNING: Installation Instructions Differ\n**GitHub:** \"Run npm install\"\n**Local docs:** \"Use pnpm install\"\n**Impact:** Training may teach wrong commands\n**Recommendation:** Synchronize to pnpm install\n\n## 4. Terminology Consistency Issues\n\n| Term Used | Website | GitHub | Local Docs | Recommendation |\n|-----------|---------|--------|------------|----------------|\n| Plugin/Extension | Plugin | Extension | Plugin | Standardize on \"Plugin\" |\n| Marketplace/Repository | Marketplace | Repository | Marketplace | Standardize on \"Marketplace\" |\n\n## 5. Action Items (Priority Order)\n\n1. üî¥ Update website version to v1.2.1\n2. üî¥ Fix contact email in local docs\n3. üü° Standardize plugin count messaging\n4. üü° Align installation instructions\n5. üü¢ Standardize terminology usage\n```\n\n## Validation Workflow Example\n\n**User:** \"Before I update my internal training materials, check if my website matches GitHub\"\n\n**Skill Actions:**\n1. Scans website for core messaging, features, version\n2. Scans GitHub README, docs for same information\n3. Extracts current training materials content\n4. Compares all three sources\n5. Generates detailed discrepancy report\n6. Highlights critical issues that must be fixed first\n7. Provides specific file locations and line numbers\n\n**Output:** Comprehensive report showing exactly what's inconsistent and where to fix it\n\n## Best Practices\n\n### Source Priority (Use This When Conflicts Exist)\n\n**Trust Priority Order:**\n1. **Website** - Public-facing, most authoritative\n2. **GitHub** - Developer-facing, technical accuracy\n3. **Local Docs** - Internal-use, lowest priority for public messaging\n\n**Update Flow:**\nWebsite ‚Üí GitHub ‚Üí Local Docs\n\n### When to Run Validation\n\n‚úÖ **Run validation BEFORE:**\n- Updating internal documentation\n- Creating training materials\n- Writing new marketing content\n- Publishing blog posts\n- Releasing new versions\n\n‚úÖ **Run validation AFTER:**\n- Website updates\n- GitHub README changes\n- Major feature releases\n- Rebranding efforts\n\n### What This Skill Does NOT Do\n\n‚ùå Does NOT automatically fix issues\n‚ùå Does NOT modify any files\n‚ùå Does NOT make content decisions\n‚ùå Does NOT prioritize which version is \"correct\"\n‚úÖ ONLY generates read-only reports for human review\n\n## Integration with Your Workflow\n\n### Scenario: Pre-Update Validation\n\n**You:** \"I need to update our internal SOPs. First, validate consistency with the website.\"\n\n**Skill Response:**\n1. Reads current website content\n2. Reads current GitHub documentation\n3. Reads existing internal SOPs\n4. Generates comparison report\n5. Shows you exactly what needs updating in SOPs\n6. Identifies messaging that website uses but SOPs don't\n\n**Result:** You update SOPs with confidence, knowing they match public messaging\n\n### Scenario: Post-Website Update\n\n**You:** \"I just updated the website pricing page. Check if GitHub and docs are now inconsistent.\"\n\n**Skill Response:**\n1. Reads NEW website pricing information\n2. Compares to GitHub repository pricing docs\n3. Compares to internal sales training materials\n4. Flags any discrepancies created by website update\n5. Provides checklist of what to update next\n\n**Result:** Prevents mixed messaging cascade\n\n## Technical Implementation\n\n### Read-Only Tools Used\n\n- `Read` - Reads local files (website, docs, SOPs)\n- `Glob` - Finds relevant files by pattern\n- `Grep` - Searches for specific terms across files\n- `WebFetch` - Reads deployed website pages (if needed)\n- `Bash` (read-only) - Uses `cat`, `grep`, `find` for analysis\n\n### NO Write Operations\n\nThis skill NEVER uses:\n- ‚ùå `Write` tool\n- ‚ùå `Edit` tool\n- ‚ùå `git commit` commands\n- ‚ùå File modification operations\n\n### Output Format\n\n- Markdown report saved to `consistency-reports/YYYY-MM-DD-HH-MM-SS.md`\n- Terminal-friendly summary\n- Export to JSON for automation (optional)\n\n## Example Use Cases\n\n### Use Case 1: Version Consistency Check\n\n**Trigger:** \"Check if all docs mention the same version number\"\n\n**Result:**\n```\nVersion Analysis Report\nWebsite: v1.2.1 (5 mentions)\nGitHub: v1.2.1 (3 mentions), v1.2.0 (2 mentions) ‚ö†Ô∏è\nLocal Docs: v1.2.0 (8 mentions) üî¥\n\nAction: Update Local Docs to v1.2.1\n```\n\n### Use Case 2: Feature Claim Validation\n\n**Trigger:** \"Validate that all platforms claim the same features\"\n\n**Result:**\n```\nFeature Consistency Analysis\n\"236 plugins\": Website ‚úÖ, GitHub ‚úÖ, Docs ‚ùå (says \"230+\")\n\"Agent Skills\": Website ‚úÖ, GitHub ‚úÖ, Docs ‚úÖ\n\"MCP Support\": Website ‚úÖ, GitHub ‚úÖ, Docs ‚ö†Ô∏è (unclear mention)\n\nAction: Update Docs to specify \"236 plugins\" and clarify MCP support\n```\n\n### Use Case 3: Pre-Training Update\n\n**Trigger:** \"Before I update training materials, what's changed on the website?\"\n\n**Result:**\n```\nWebsite Changes Since Last Training Update (Oct 15)\n- New feature added: \"Skill Enhancers\" (not in training)\n- Pricing updated: $39/mo ‚Üí $49/mo (not in training)\n- Contact form URL changed (broken link in training)\n\nSuggested Training Updates:\n1. Add Skill Enhancers section\n2. Update pricing screenshots\n3. Fix contact form URL\n```\n\n## Integration Points\n\nWorks seamlessly with:\n- **All HTML-based websites**: Static HTML, Hugo, Astro, Jekyll, WordPress, Next.js, React, Vue, Nuxt, Gatsby, 11ty, Docusaurus, and more\n- **GitHub repositories**: README files, documentation, code comments\n- **Local markdown documentation**: Internal docs, training materials\n- **Internal wikis and knowledge bases**: Confluence, Notion exports, custom wikis\n- **Content management systems**: WordPress, Drupal, custom CMS\n- **Static site generators**: Hugo, Jekyll, 11ty, Gatsby, Astro, Docusaurus\n- **Modern web frameworks**: Next.js, Nuxt, SvelteKit build outputs\n\n## Report Storage\n\nReports saved to:\n```\nconsistency-reports/\n‚îú‚îÄ‚îÄ 2025-10-23-10-30-45-full-audit.md\n‚îú‚îÄ‚îÄ 2025-10-22-15-20-12-website-github.md\n‚îî‚îÄ‚îÄ 2025-10-20-09-15-33-docs-sync.md\n```\n\n## Expected Activation Patterns\n\n**Natural Language Triggers:**\n- \"Check consistency\"\n- \"Validate documentation\"\n- \"Audit messaging\"\n- \"Find discrepancies\"\n- \"Compare website to GitHub\"\n- \"Before I update X, check Y\"\n- \"What's out of sync?\"\n\n**Context-Aware Activation:**\n- When user is about to update documentation\n- When user asks about version consistency\n- When user mentions \"mixed messaging\"\n- When user is preparing training materials\n\n## Prerequisites\n\n- Access to website content (local build or deployed site)\n- Access to GitHub repositories\n- Local documentation in {baseDir}/docs/ or claudes-docs/\n- WebFetch permissions for remote content\n\n## Instructions\n\n1. Identify and discover all content sources (website, GitHub, local docs)\n2. Extract key messaging, features, versions from each source\n3. Compare content systematically across sources\n4. Identify critical discrepancies, warnings, and informational notes\n5. Generate comprehensive Markdown report\n6. Provide prioritized action items for consistency fixes\n\n## Output\n\n- Comprehensive consistency validation report in Markdown format\n- Executive summary with discrepancy counts by severity\n- Detailed comparison by source pairs (website vs GitHub, etc.)\n- Terminology consistency matrix\n- Prioritized action items with file locations and line numbers\n- Reports saved to consistency-reports/YYYY-MM-DD-HH-MM-SS.md\n\n## Error Handling\n\nIf validation fails:\n- Verify website accessibility (local or deployed)\n- Check GitHub repository permissions\n- Validate local documentation paths\n- Ensure WebFetch permissions configured\n- Review content extraction patterns\n\n## Resources\n\n- Content consistency best practices\n- Documentation style guides\n- Version control strategies for content\n- Multi-platform content management approaches",
      "parentPlugin": {
        "name": "000-jeremy-content-consistency-validator",
        "category": "productivity",
        "path": "plugins/productivity/000-jeremy-content-consistency-validator",
        "version": "1.0.0",
        "description": "Read-only validator that generates comprehensive discrepancy reports comparing messaging consistency across ANY HTML-based website (WordPress, Hugo, Next.js, React, Vue, static HTML, etc.), GitHub repositories, and local documentation. Detects mixed messaging without making changes."
      },
      "filePath": "plugins/productivity/000-jeremy-content-consistency-validator/skills/000-jeremy-content-consistency-validator/SKILL.md"
    },
    {
      "slug": "adapting-transfer-learning-models",
      "name": "adapting-transfer-learning-models",
      "description": "This skill automates the adaptation of pre-trained machine learning models using transfer learning techniques. it is triggered when the user requests assistance with fine-tuning a model, adapting a pre-trained model to a new dataset, or performing... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Transfer Learning Adapter\n\nThis skill provides automated assistance for transfer learning adapter tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for transfer learning adapter tasks.\nThis skill streamlines the process of adapting pre-trained machine learning models via transfer learning. It enables you to quickly fine-tune models for specific tasks, saving time and resources compared to training from scratch. It handles the complexities of model adaptation, data validation, and performance optimization.\n\n## How It Works\n\n1. **Analyze Requirements**: Examines the user's request to understand the target task, dataset characteristics, and desired performance metrics.\n2. **Generate Adaptation Code**: Creates Python code using appropriate ML frameworks (e.g., TensorFlow, PyTorch) to fine-tune the pre-trained model on the new dataset. This includes data preprocessing steps and model architecture modifications if needed.\n3. **Implement Validation and Error Handling**: Adds code to validate the data, monitor the training process, and handle potential errors gracefully.\n4. **Provide Performance Metrics**: Calculates and reports key performance indicators (KPIs) such as accuracy, precision, recall, and F1-score to assess the model's effectiveness.\n5. **Save Artifacts and Documentation**: Saves the adapted model, training logs, performance metrics, and automatically generates documentation outlining the adaptation process and results.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Fine-tune a pre-trained model for a specific task.\n- Adapt a pre-trained model to a new dataset.\n- Perform transfer learning to improve model performance.\n- Optimize an existing model for a particular application.\n\n## Examples\n\n### Example 1: Adapting a Vision Model for Image Classification\n\nUser request: \"Fine-tune a ResNet50 model to classify images of different types of flowers.\"\n\nThe skill will:\n1. Download the ResNet50 model and load a flower image dataset.\n2. Generate code to fine-tune the model on the flower dataset, including data augmentation and optimization techniques.\n\n### Example 2: Adapting a Language Model for Sentiment Analysis\n\nUser request: \"Adapt a BERT model to perform sentiment analysis on customer reviews.\"\n\nThe skill will:\n1. Download the BERT model and load a dataset of customer reviews with sentiment labels.\n2. Generate code to fine-tune the model on the review dataset, including tokenization, padding, and attention mechanisms.\n\n## Best Practices\n\n- **Data Preprocessing**: Ensure data is properly preprocessed and formatted to match the input requirements of the pre-trained model.\n- **Hyperparameter Tuning**: Experiment with different hyperparameters (e.g., learning rate, batch size) to optimize model performance.\n- **Regularization**: Apply regularization techniques (e.g., dropout, weight decay) to prevent overfitting.\n\n## Integration\n\nThis skill can be integrated with other plugins for data loading, model evaluation, and deployment. For example, it can work with a data loading plugin to fetch datasets and a model deployment plugin to deploy the adapted model to a serving infrastructure.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "transfer-learning-adapter",
        "category": "ai-ml",
        "path": "plugins/ai-ml/transfer-learning-adapter",
        "version": "1.0.0",
        "description": "Transfer learning adaptation"
      },
      "filePath": "plugins/ai-ml/transfer-learning-adapter/skills/adapting-transfer-learning-models/SKILL.md"
    },
    {
      "slug": "adk-agent-builder",
      "name": "adk-agent-builder",
      "description": "Build production-ready AI agents using Google's Agent Development Kit with AI assistant integration, React patterns, multi-agent orchestration, and comprehensive tool libraries. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# ADK Agent Builder\n\nBuild production-ready agents with Google‚Äôs Agent Development Kit (ADK): scaffolding, tool wiring, orchestration patterns, testing, and optional deployment to Vertex AI Agent Engine.\n\n## Overview\n\n- Creates a minimal, production-oriented ADK scaffold (agent entrypoint, tool registry, config, and tests).\n- Supports single-agent ReAct-style workflows and multi-agent orchestration (Sequential/Parallel/Loop).\n- Produces a validation checklist suitable for CI (lint/tests/smoke prompts) and optional Agent Engine deployment verification.\n\n## Prerequisites\n\n- Python runtime compatible with your project (often Python 3.10+)\n- `google-adk` installed and importable\n- If deploying: access to a Google Cloud project with Vertex AI enabled and permissions to deploy Agent Engine runtimes\n- Secrets available via environment variables or a secret manager (never hardcoded)\n\n## Instructions\n\n1. Confirm scope: local-only agent scaffold vs Vertex AI Agent Engine deployment.\n2. Choose an architecture:\n   - Single agent (ReAct) for adaptive tool-driven tasks\n   - Multi-agent system (specialists + orchestrator) for complex, multi-step workflows\n3. Define the tool surface (built-in ADK tools + any custom tools you need) and required credentials.\n4. Scaffold the project:\n   - `src/agents/`, `src/tools/`, `tests/`, and a dependency file (`pyproject.toml` or `requirements.txt`)\n5. Implement the minimum viable agent and a smoke test prompt; add regression tests for tool failures.\n6. If deploying, produce an `adk deploy ...` command and a post-deploy validation checklist (AgentCard/task endpoints, permissions, logs).\n\n## Output\n\n- A repo-ready ADK scaffold (files and directories) plus starter agent code\n- Tool stubs and wiring points (where to add new tools safely)\n- A test + validation plan (unit tests and a minimal smoke prompt)\n- Optional: deployment commands and verification steps for Agent Engine\n\n## Error Handling\n\n- Dependency/runtime issues: provide pinned install commands and validate imports.\n- Auth/permission failures: identify the missing role/API and propose least-privilege fixes.\n- Tool failures/rate limits: add retries/backoff guidance and a regression test to prevent recurrence.\n\n## Examples\n\n**Example: Scaffold a single ReAct agent**\n- Request: ‚ÄúCreate an ADK agent that summarizes PRs and proposes test updates.‚Äù\n- Result: agent entrypoint + tool registry + a smoke test command for local verification.\n\n**Example: Multi-agent orchestrator**\n- Request: ‚ÄúBuild a supervisor + deployer + verifier team and deploy to Agent Engine.‚Äù\n- Result: orchestrator skeleton, per-agent responsibilities, and `adk deploy ...` + post-deploy health checks.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Repo standards (source of truth):\n  - `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`\n  - `000-docs/6767-b-SPEC-DR-STND-claude-skills-standard.md`\n- ADK / Agent Engine docs: https://cloud.google.com/vertex-ai/docs/agent-engine",
      "parentPlugin": {
        "name": "jeremy-google-adk",
        "category": "jeremy-google-adk",
        "path": "plugins/jeremy-google-adk",
        "version": "1.0.0",
        "description": "Google Agent Development Kit (ADK) SDK starter kit for building Claude-powered AI agents with React patterns, multi-agent orchestration, and tool integration"
      },
      "filePath": "plugins/jeremy-google-adk/skills/adk-agent-builder/SKILL.md"
    },
    {
      "slug": "adk-deployment-specialist",
      "name": "adk-deployment-specialist",
      "description": "Deploy and orchestrate Vertex AI ADK agents using A2A protocol. Manages AgentCard discovery, task submission, Code Execution Sandbox, and Memory Bank. Use when asked to \"deploy ADK agent\" or \"orchestrate agents\". Trigger with phrases like 'deploy', 'infrastructure', or 'CI/CD'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Adk Deployment Specialist\n\nThis skill provides automated assistance for adk deployment specialist tasks.\n\n## Overview\n\nExpert in building and deploying production multi-agent systems using Google's Agent Development Kit (ADK). Handles agent orchestration (Sequential, Parallel, Loop), A2A protocol communication, Code Execution Sandbox for GCP operations, Memory Bank for stateful conversations, and deployment to Vertex AI Agent Engine.\n\n### Core Capabilities\n\n1. **ADK Agent Creation**: Build agents in Python (stable), Java (0.3.0), or Go (Nov 2025)\n2. **Multi-Agent Orchestration**: Sequential/Parallel/Loop agent patterns\n3. **A2A Protocol Management**: Agent-to-Agent communication and task delegation\n4. **Code Execution**: Secure sandbox for running gcloud commands and Python/Go code\n5. **Memory Bank**: Persistent conversation memory across sessions (14-day TTL)\n6. **Production Deployment**: One-command deployment with `adk deploy`\n7. **Observability**: Agent Engine UI dashboards, token tracking, error monitoring\n\n## Prerequisites\n\n- A Google Cloud project with Vertex AI enabled (and permissions to deploy Agent Engine runtimes)\n- ADK installed (and pinned to the project‚Äôs supported version)\n- A clear agent contract: tools required, orchestration pattern, and deployment target (local vs Agent Engine)\n- A plan for secrets/credentials (OIDC/WIF where possible; never commit long-lived keys)\n\n## Instructions\n\n1. Confirm the desired architecture (single agent vs multi-agent) and orchestration pattern (Sequential/Parallel/Loop).\n2. Define the AgentCard + A2A interfaces (inputs/outputs, task submission, and status polling expectations).\n3. Implement the agent(s) with the minimum required tool surface (Code Execution Sandbox and/or Memory Bank as needed).\n4. Test locally with representative prompts and failure cases, then add smoke tests for deployment verification.\n5. Deploy to Vertex AI Agent Engine and validate the generated endpoints (`/.well-known/agent-card`, task send/status APIs).\n6. Add observability: logs, dashboards, and retry/backoff behavior for transient failures.\n\n## Output\n\n- Agent source files (or patches) ready for deployment\n- Deployment commands/config (e.g., `adk deploy` invocation + required flags)\n- A verification checklist for Agent Engine endpoints (AgentCard + task APIs) and security posture\n\n## Error Handling\n\n- Deployment failures: provide the exact failing command, logs to inspect, and the minimal change to unblock\n- Auth/permission issues: identify the missing role/API and suggest least-privilege remediation\n- Orchestration failures: isolate which agent step failed and provide a retry-safe plan\n\n## Examples\n\n- ‚ÄúDeploy this ADK multi-agent supervisor to Agent Engine with Code Execution and Memory Bank enabled.‚Äù\n- ‚ÄúExpose an A2A AgentCard and implement `/v1/tasks:send` + polling for long-running tasks.‚Äù\n\n## Resources\n\n- ADK docs: https://cloud.google.com/vertex-ai/docs/agent-engine\n- Workload Identity (CI/CD): https://cloud.google.com/iam/docs/workload-identity-federation\n- A2A / AgentCard patterns: see `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`\n\n## When This Skill Activates\n\n### Trigger Phrases\n- \"Deploy ADK agent to Agent Engine\"\n- \"Create multi-agent system with ADK\"\n- \"Implement A2A protocol\"\n- \"Use Code Execution Sandbox\"\n- \"Set up Memory Bank for agent\"\n- \"Orchestrate multiple agents\"\n- \"Build ADK agent in Python/Java/Go\"\n- \"Deploy to Vertex AI Agent Engine\"\n\n### Use Case Patterns\n- Building GCP deployment automation agents\n- Creating RAG agents with LangChain integration\n- Orchestrating Genkit flows with ADK supervisors\n- Implementing stateful conversational agents\n- Deploying secure code execution environments\n\n## How It Works\n\n### Phase 1: Agent Architecture Design\n\n```\nUser Request ‚Üí Analyze:\n- Single agent vs multi-agent system?\n- Tools needed (Code Exec, Memory Bank, custom tools)?\n- Orchestration pattern (Sequential, Parallel, Loop)?\n- Integration with LangChain/Genkit?\n- Deployment target (local, Agent Engine, Cloud Run)?\n```\n\n### Phase 2: ADK Agent Implementation\n\n**Simple Agent (Python)**:\n```python\nfrom google import adk\n\n# Define agent with tools\nagent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[\n        adk.tools.CodeExecution(),  # Secure sandbox\n        adk.tools.MemoryBank(),     # Persistent memory\n    ],\n    system_instruction=\"\"\"\nYou are a GCP deployment specialist.\nHelp users deploy resources securely using gcloud commands.\n    \"\"\"\n)\n\n# Run agent\nresponse = agent.run(\"Deploy a GKE cluster named prod in us-central1\")\nprint(response)\n```\n\n**Multi-Agent Orchestrator (Python)**:\n```python\nfrom google import adk\n\n# Define specialized sub-agents\nvalidator_agent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    system_instruction=\"Validate GCP configurations\"\n)\n\ndeployer_agent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[adk.tools.CodeExecution()],\n    system_instruction=\"Deploy validated GCP resources\"\n)\n\nmonitor_agent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    system_instruction=\"Monitor deployment status\"\n)\n\n# Orchestrate with Sequential pattern\norchestrator = adk.SequentialAgent(\n    agents=[validator_agent, deployer_agent, monitor_agent],\n    system_instruction=\"Coordinate validation ‚Üí deployment ‚Üí monitoring\"\n)\n\nresult = orchestrator.run(\"Deploy a production GKE cluster\")\n```\n\n### Phase 3: Code Execution Integration\n\nThe Code Execution Sandbox provides:\n- **Security**: Isolated environment, no access to your system\n- **State Persistence**: 14-day memory, configurable TTL\n- **Stateful Sessions**: Builds on previous executions\n\n```python\n# Agent with Code Execution\nagent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[adk.tools.CodeExecution()],\n    system_instruction=\"\"\"\nExecute gcloud commands in the secure sandbox.\nRemember previous operations in this session.\n    \"\"\"\n)\n\n# Turn 1: Create cluster\nagent.run(\"Create GKE cluster named dev-cluster with 3 nodes\")\n# Sandbox executes: gcloud container clusters create dev-cluster --num-nodes=3\n\n# Turn 2: Deploy to cluster (remembers cluster from Turn 1)\nagent.run(\"Deploy my-app:latest to that cluster\")\n# Sandbox remembers dev-cluster, executes kubectl commands\n```\n\n### Phase 4: Memory Bank Integration\n\nPersistent conversation memory across sessions:\n\n```python\nagent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[adk.tools.MemoryBank()],\n    system_instruction=\"Remember user preferences and project context\"\n)\n\n# Session 1 (Monday)\nagent.run(\"I prefer deploying to us-central1 region\", session_id=\"user-123\")\n\n# Session 2 (Wednesday) - same session_id\nagent.run(\"Deploy a Cloud Run service\", session_id=\"user-123\")\n# Agent remembers: uses us-central1 automatically\n```\n\n### Phase 5: A2A Protocol Deployment\n\nDeploy agent to Agent Engine with A2A endpoint:\n\n```bash\n# Install ADK\npip install google-adk\n\n# Deploy with one command\nadk deploy \\\n  --agent-file agent.py \\\n  --project-id my-project \\\n  --region us-central1 \\\n  --service-name gcp-deployer-agent\n```\n\nAgent Engine creates:\n- **A2A Endpoint**: `https://gcp-deployer-agent-{hash}.run.app`\n- **AgentCard**: `/.well-known/agent-card` metadata\n- **Task API**: `/v1/tasks:send` for task submission\n- **Status API**: `/v1/tasks/{task_id}` for polling\n\n### Phase 6: Calling from Claude\n\nOnce deployed, Claude can invoke via A2A protocol:\n\n```python\n# In Claude Code plugin / external script\nimport requests\n\ndef invoke_adk_agent(message, session_id=None):\n    \"\"\"\n    Call deployed ADK agent via A2A protocol.\n    \"\"\"\n    response = requests.post(\n        \"https://gcp-deployer-agent-xyz.run.app/v1/tasks:send\",\n        json={\n            \"message\": message,\n            \"session_id\": session_id or \"claude-session-123\",\n            \"config\": {\n                \"enable_code_execution\": True,\n                \"enable_memory_bank\": True,\n            }\n        },\n        headers={\"Authorization\": f\"Bearer {get_token()}\"}\n    )\n\n    return response.json()\n\n# Use from Claude\nresult = invoke_adk_agent(\"Deploy GKE cluster named prod-api\")\n```\n\n## Workflow Examples\n\n### Example 1: GCP Deployment Agent\n\n**User**: \"Create an ADK agent that deploys GCP resources\"\n\n**Implementation**:\n```python\nfrom google import adk\n\ndeployment_agent = adk.Agent(\n    model=\"gemini-2.5-flash\",\n    tools=[\n        adk.tools.CodeExecution(),\n        adk.tools.MemoryBank(),\n    ],\n    system_instruction=\"\"\"\nYou are a GCP deployment specialist.\n\nCAPABILITIES:\n- Deploy GKE clusters\n- Deploy Cloud Run services\n- Deploy Vertex AI Pipelines\n- Manage IAM permissions\n- Monitor deployments\n\nSECURITY:\n- Validate all configurations before deployment\n- Use least-privilege IAM\n- Log all operations\n- Never expose credentials\n    \"\"\"\n)\n\n# Deploy to Agent Engine\n# $ adk deploy --agent-file deployment_agent.py --service-name gcp-deployer\n```\n\n### Example 2: Multi-Agent RAG System\n\n**User**: \"Build a RAG system with ADK orchestrating a LangChain retriever\"\n\n**Implementation**:\n```python\nfrom google import adk\nfrom langchain.retrievers import VertexAISearchRetriever\n\n# Sub-Agent 1: LangChain RAG\nclass RAGAgent(adk.Agent):\n    def __init__(self):\n        self.retriever = VertexAISearchRetriever(...)\n        super().__init__(model=\"gemini-2.5-flash\")\n\n    def retrieve_docs(self, query):\n        return self.retriever.get_relevant_documents(query)\n\n# Sub-Agent 2: ADK Answer Generator\nanswer_agent = adk.Agent(\n    model=\"gemini-2.5-pro\",  # More powerful for final answer\n    system_instruction=\"Generate comprehensive answers from retrieved docs\"\n)\n\n# Orchestrator\norchestrator = adk.SequentialAgent(\n    agents=[RAGAgent(), answer_agent],\n    system_instruction=\"First retrieve docs, then generate answer\"\n)\n```\n\n### Example 3: Async Deployment with Status Polling\n\n**User**: \"Deploy a GKE cluster and monitor progress\"\n\n**Implementation**:\n```python\n# Submit async task\ntask_response = invoke_adk_agent(\n    \"Deploy GKE cluster named prod-api with 5 nodes in us-central1\"\n)\n\ntask_id = task_response[\"task_id\"]\nprint(f\"‚úÖ Task submitted: {task_id}\")\n\n# Poll for status\nimport time\nwhile True:\n    status = requests.get(\n        f\"https://gcp-deployer-agent-xyz.run.app/v1/tasks/{task_id}\",\n        headers={\"Authorization\": f\"Bearer {get_token()}\"}\n    ).json()\n\n    if status[\"status\"] == \"SUCCESS\":\n        print(f\"‚úÖ Cluster deployed!\")\n        break\n    elif status[\"status\"] == \"FAILURE\":\n        print(f\"‚ùå Deployment failed: {status['error']}\")\n        break\n    else:\n        print(f\"‚è≥ Status: {status['status']} ({status.get('progress', 0)*100}%)\")\n        time.sleep(10)\n```\n\n## Production Best Practices\n\n1. **Agent Identities**: ADK agents get Native Agent Identities (IAM principals)\n2. **Least Privilege**: Grant minimum required permissions\n3. **VPC Service Controls**: Enable for enterprise security\n4. **Model Armor**: Protects against prompt injection\n5. **Session Management**: Use consistent session_ids for Memory Bank\n6. **Error Handling**: Implement retries with exponential backoff\n7. **Observability**: Monitor via Agent Engine UI dashboard\n\n## Tool Permissions\n\n- **Read**: Analyze existing agent code\n- **Write**: Create new agent files\n- **Edit**: Modify agent configurations\n- **Grep**: Find integration points\n- **Glob**: Locate related files\n- **Bash**: Install ADK, deploy agents, run tests\n\n## Integration Patterns\n\n### ADK + Genkit\n```python\n# Use Genkit for flows, ADK for orchestration\ngenkit_flow_agent = create_genkit_flow()\norchestrator = adk.SequentialAgent(\n    agents=[validator, genkit_flow_agent, monitor]\n)\n```\n\n### ADK + LangChain\n```python\n# LangChain for RAG, ADK for multi-agent coordination\nlangchain_rag = create_langchain_retriever()\norchestrator = adk.ParallelAgent(\n    agents=[langchain_rag, fact_checker, answer_generator]\n)\n```\n\n## Deployment Commands\n\n```bash\n# Install ADK\npip install google-adk  # Python\ngo get google.golang.org/adk  # Go\n\n# Deploy to Agent Engine\nadk deploy \\\n  --agent-file my_agent.py \\\n  --project-id my-project \\\n  --region us-central1 \\\n  --service-name my-agent\n\n# Deploy to Cloud Run (custom)\ngcloud run deploy my-agent \\\n  --source . \\\n  --region us-central1\n\n# Deploy locally for testing\nadk run --agent-file my_agent.py\n```\n\n## Version History\n\n- **1.0.0** (2025): ADK Preview with Python/Java/Go support, Agent Engine GA, Code Execution Sandbox, Memory Bank\n\n## References\n\n- ADK Docs: https://google.github.io/adk-docs/\n- A2A Protocol: https://google.github.io/adk-docs/a2a/\n- Agent Engine: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\n- Code Execution: https://cloud.google.com/agent-builder/agent-engine/code-execution/overview\n- Memory Bank: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/overview",
      "parentPlugin": {
        "name": "jeremy-adk-orchestrator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-adk-orchestrator",
        "version": "1.0.0",
        "description": "Production ADK orchestrator for A2A protocol and multi-agent coordination on Vertex AI"
      },
      "filePath": "plugins/ai-ml/jeremy-adk-orchestrator/skills/adk-deployment-specialist/SKILL.md"
    },
    {
      "slug": "adk-engineer",
      "name": "adk-engineer",
      "description": "Software engineer specializing in creating production-ready ADK agents with best practices, code structure, testing, and deployment automation. Use when asked to \"build ADK agent\", \"create agent code\", or \"engineer ADK application\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# ADK Engineer\n\nEngineer production-ready Agent Development Kit (ADK) agents and multi-agent systems: clean structure, testability, safe tool usage, and deployment automation.\n\n## Overview\n\nUse this skill to design and implement ADK agent code that is maintainable and shippable: clear module boundaries, structured tool interfaces, regression tests, and a deployment checklist (local or Agent Engine).\n\n## Prerequisites\n\n- A target runtime (Python/Java/Go) consistent with the project‚Äôs pinned versions\n- ADK installed (and any required model/provider SDKs configured)\n- A test runner available in the repo (unit tests at minimum)\n- If deploying: access to a Google Cloud project and permissions for the chosen deployment target\n\n## Instructions\n\n1. Clarify requirements: agent goals, tool surface, latency/cost constraints, and deployment target.\n2. Propose architecture: single agent vs multi-agent, orchestration pattern, state strategy (Memory Bank / external store).\n3. Scaffold structure: agent entrypoint(s), tool modules, config, and tests.\n4. Implement incrementally:\n   - add one tool at a time with input validation and structured outputs\n   - add regression tests for each tool and critical prompt flows\n5. Add operational guardrails: retries/backoff, timeouts, logging, and safe error messages.\n6. Validate locally (tests + smoke prompts) and provide a deployment plan (when requested).\n\n## Output\n\n- A concrete architecture plan and file layout\n- Agent and tool implementations (or patches) with tests\n- A validation checklist (commands to run, expected outputs, and failure triage)\n- Optional: deployment instructions and post-deploy health checks\n\n## Error Handling\n\n- Build/test failures: isolate the failing module, minimize the repro, fix, and add a regression test.\n- Tool/runtime errors: enforce structured error responses and safe retries where appropriate.\n- Deployment failures: provide the exact failing command, logs to inspect, and least-privilege IAM fixes.\n\n## Examples\n\n**Example: Productionizing an existing ADK agent**\n- Request: ‚ÄúRefactor this agent into a clean module structure and add tests before we deploy.‚Äù\n- Result: reorganized `src/` layout, tool boundaries, a test suite, and a deployment checklist.\n\n**Example: Multi-agent workflow**\n- Request: ‚ÄúBuild a validator + deployer + monitor agent team with a sequential orchestrator.‚Äù\n- Result: orchestrator skeleton, per-agent responsibilities, and smoke tests for each step.\n\n## Resources\n\n- Full detailed playbook (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Repo standards (source of truth):\n  - `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`\n  - `000-docs/6767-b-SPEC-DR-STND-claude-skills-standard.md`\n- ADK / Agent Engine docs: https://cloud.google.com/vertex-ai/docs/agent-engine",
      "parentPlugin": {
        "name": "jeremy-adk-software-engineer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-adk-software-engineer",
        "version": "1.0.0",
        "description": "ADK software engineer for creating production-ready agents (placeholder - to be implemented)"
      },
      "filePath": "plugins/ai-ml/jeremy-adk-software-engineer/skills/adk-engineer/SKILL.md"
    },
    {
      "slug": "adk-infra-expert",
      "name": "adk-infra-expert",
      "description": "Use when provisioning Vertex AI ADK infrastructure with Terraform. Trigger with phrases like \"deploy ADK terraform\", \"agent engine infrastructure\", \"provision ADK agent\", \"vertex AI agent terraform\", or \"code execution sandbox terraform\". Provisions Agent Engine runtime, 14-day code execution sandbox, Memory Bank, VPC Service Controls, IAM roles, and secure multi-agent infrastructure. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Adk Infra Expert\n\nThis skill provides automated assistance for adk infra expert tasks.\n\n## Overview\n\nProvision production-grade Vertex AI ADK infrastructure with Terraform: secure networking, least-privilege IAM, Agent Engine runtime, Code Execution sandbox defaults, and Memory Bank configuration. Use this skill to generate/validate Terraform modules and a deployment checklist that matches enterprise security constraints (including VPC Service Controls when required).\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Google Cloud project with billing enabled\n- Terraform 1.0+ installed\n- gcloud CLI authenticated with appropriate permissions\n- Vertex AI API enabled in target project\n- VPC Service Controls access policy created (for enterprise)\n- Understanding of Agent Engine architecture and requirements\n\n## Instructions\n\n1. **Initialize Terraform**: Set up backend for remote state storage\n2. **Configure Variables**: Define project_id, region, agent configuration\n3. **Provision VPC**: Create network infrastructure with Private Service Connect\n4. **Set Up IAM**: Create service accounts with least privilege roles\n5. **Deploy Agent Engine**: Configure runtime with code execution and memory bank\n6. **Enable VPC-SC**: Apply service perimeter for data exfiltration protection\n7. **Configure Monitoring**: Set up Cloud Monitoring dashboards and alerts\n8. **Validate Deployment**: Test agent endpoint and verify all components\n\n## Examples\n\n**Example: Stand up an ADK Agent Engine runtime**\n- Inputs: `project_id`, `region`, desired model, and whether Code Execution + Memory Bank are enabled.\n- Outputs: Terraform resources for the runtime + IAM + VPC/PSC, plus validation commands to confirm the agent is reachable and permissions are correct.\n\n## Output\n\n**Agent Engine Deployment:**\n```hcl\n# {baseDir}/terraform/main.tf\nresource \"google_vertex_ai_agent_runtime\" \"adk_agent\" {\n  project  = var.project_id\n  location = var.region\n  display_name = \"adk-production-agent\"\n\n  agent_config {\n    model = \"gemini-2.5-flash\"\n    code_execution {\n      enabled = true\n      state_ttl_days = 14\n      sandbox_type = \"SECURE_ISOLATED\"\n    }\n    memory_bank {\n      enabled = true\n    }\n  }\n\n  vpc_config {\n    vpc_network = google_compute_network.agent_vpc.id\n    private_service_connect {\n      enabled = true\n    }\n  }\n}\n```\n\n**VPC Service Controls:**\n```hcl\nresource \"google_access_context_manager_service_perimeter\" \"adk_perimeter\" {\n  parent = \"accessPolicies/${var.access_policy_id}\"\n  title  = \"ADK Agent Engine Perimeter\"\n\n  status {\n    restricted_services = [\n      \"aiplatform.googleapis.com\",\n      \"run.googleapis.com\"\n    ]\n  }\n}\n```\n\n**IAM Configuration:**\n```hcl\nresource \"google_service_account\" \"adk_agent\" {\n  account_id   = \"adk-agent-sa\"\n  display_name = \"ADK Agent Service Account\"\n}\n\nresource \"google_project_iam_member\" \"agent_identity\" {\n  project = var.project_id\n  role    = \"roles/aiplatform.agentUser\"\n  member  = \"serviceAccount:${google_service_account.adk_agent.email}\"\n}\n```\n\n## Error Handling\n\n**Terraform State Lock**\n- Error: \"Error acquiring the state lock\"\n- Solution: Use `terraform force-unlock <lock-id>` or wait for lock expiry\n\n**API Not Enabled**\n- Error: \"Vertex AI API has not been used\"\n- Solution: Enable with `gcloud services enable aiplatform.googleapis.com`\n\n**VPC-SC Configuration**\n- Error: \"Access denied by VPC Service Controls\"\n- Solution: Add project to service perimeter or adjust ingress/egress policies\n\n**IAM Permission Denied**\n- Error: \"does not have required permission\"\n- Solution: Grant roles/owner temporarily to service account running Terraform\n\n**Resource Already Exists**\n- Error: \"Resource already exists\"\n- Solution: Import existing resource or use data source instead\n\n## Resources\n\n- Agent Engine: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\n- VPC-SC: https://cloud.google.com/vpc-service-controls/docs\n- Terraform Google Provider: https://registry.terraform.io/providers/hashicorp/google/latest\n- ADK Terraform examples in {baseDir}/examples/",
      "parentPlugin": {
        "name": "jeremy-adk-terraform",
        "category": "devops",
        "path": "plugins/devops/jeremy-adk-terraform",
        "version": "1.0.0",
        "description": "Terraform infrastructure as code for ADK and Vertex AI Agent Engine deployments"
      },
      "filePath": "plugins/devops/jeremy-adk-terraform/skills/adk-infra-expert/SKILL.md"
    },
    {
      "slug": "agent-context-loader",
      "name": "agent-context-loader",
      "description": "Proactive auto-loading: automatically detects and loads agents.md files. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(general:*), Bash(util:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore",
      "license": "MIT",
      "content": "# Agent Context Auto-Loader\n\nThis skill provides automated assistance for agent context loader tasks.\n\n**‚ö° This skill activates AUTOMATICALLY - no user action required!**\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## Purpose\n\nThis skill makes Claude Code recognize and load `AGENTS.md` files with the same priority as `CLAUDE.md` files, enabling specialized agent-specific instructions for your projects.\n\n## How It Works\n\n### Automatic Trigger Conditions\n\nThis skill automatically activates when:\n1. **Starting a new Claude Code session** in any directory\n2. **Changing directories** during a session (via `cd` or file operations)\n3. **Any other agent skill is invoked** (ensures agent context is loaded first)\n4. **User explicitly requests**: \"load agent context\", \"check for AGENTS.md\", or \"read agent rules\"\n\n### Execution Flow\n\nWhen triggered, Claude Code will:\n\n1. **Check for AGENTS.md**: Look for `./AGENTS.md` in the current working directory\n2. **Read the file** (if it exists): Use the Read tool to load full content\n3. **Incorporate into context**: Treat AGENTS.md rules as session-level instructions\n4. **Announce loading**: Confirm with user: \"üìã Loaded agent-specific context from AGENTS.md\"\n5. **Apply for session**: Follow these rules for all subsequent operations\n\n### Priority and Conflict Resolution\n\n- **AGENTS.md supplements CLAUDE.md**: Both are active simultaneously\n- **In case of conflicts**: AGENTS.md takes precedence for agent-specific behaviors\n- **Scope**: AGENTS.md applies to agent workflows; CLAUDE.md applies to general project context\n\n## Expected Behavior\n\n### If AGENTS.md exists:\n```\nüìã Loaded agent-specific context from AGENTS.md\n\nFollowing specialized agent rules for this session:\n- [rule 1 from AGENTS.md]\n- [rule 2 from AGENTS.md]\n...\n```\n\n### If AGENTS.md doesn't exist:\n```\nNo AGENTS.md found - using standard CLAUDE.md context only\n```\n\n## User Experience\n\n**Fully Automatic** (preferred):\n- Install plugin ‚Üí AGENTS.md loads automatically ‚Üí Agent rules active ‚Üí No user action needed\n\n**Manual Invocation** (fallback):\n```bash\n# If auto-loading doesn't trigger, user can say:\n\"load agent context\"\n\"check for AGENTS.md\"\n\"read agent rules from AGENTS.md\"\n```\n\n## Implementation Details\n\n### Step 1: Check for File\n```bash\n# Claude executes internally:\nif [ -f \"./AGENTS.md\" ]; then\n    echo \"üìã AGENTS.md detected\"\nfi\n```\n\n### Step 2: Read Content\n```markdown\nUse Read tool:\nfile_path: ./AGENTS.md\n\nLoad full content into session context\n```\n\n### Step 3: Apply Rules\n```\nTreat AGENTS.md content as:\n- Session-level instructions (like CLAUDE.md)\n- Agent-specific behavioral rules\n- Overrides for agent workflows\n```\n\n## Example AGENTS.md Structure\n\n```markdown\n# AGENTS.md - Agent-Specific Instructions\n\n## Agent Behavior Rules\n\nWhen working with Agent Skills in this project:\n\n1. **Always use TypeScript strict mode** for all generated code\n2. **Never create files** without explicit user permission\n3. **Follow naming convention**: use kebab-case for all file names\n4. **Auto-commit after changes**: Create git commits automatically when tasks complete\n\n## Specialized Workflows\n\n### Code Generation\n- Use templates from `./templates/` directory\n- Run ESLint after generating any .ts/.js files\n- Add comprehensive JSDoc comments\n\n### Testing\n- Generate tests alongside implementation files\n- Use Jest for all test files\n- Achieve 80%+ code coverage\n\n## Priority Overrides\n\nThese rules override CLAUDE.md when agent skills are active:\n- AGENTS.md ‚Üí agent-specific strict rules\n- CLAUDE.md ‚Üí general project context\n```\n\n## Integration with Other Skills\n\nThis skill runs **before** other agent skills to ensure agent context is loaded first. When any other skill is invoked, this skill checks if AGENTS.md has been loaded for the current directory and loads it if not already present.\n\n## Troubleshooting\n\n**If AGENTS.md isn't loading automatically:**\n\n1. **Manual invoke**: Say \"load agent context\"\n2. **Check file location**: Ensure `AGENTS.md` is in current working directory (`pwd`)\n3. **Check file permissions**: Ensure `AGENTS.md` is readable\n4. **Use slash command**: Run `/sync-agent-context` to merge AGENTS.md into CLAUDE.md permanently\n\n## Related Features\n\n- **Slash Command**: `/sync-agent-context` - Permanently merges AGENTS.md into CLAUDE.md\n- **Hook Script**: Runs on directory change to remind Claude to load context\n- **Manual Loading**: Can always explicitly request \"load AGENTS.md\"\n\n## Benefits\n\n- **Zero configuration**: Just create `AGENTS.md` and it works\n- **Project-specific rules**: Different agent behaviors per project\n- **No CLAUDE.md pollution**: Keep agent-specific rules separate\n- **Automatic synchronization**: Always up-to-date with current directory\n\n---\n\n**Status**: Proactive Auto-Loading Enabled\n**Requires User Action**: No (automatic)\n**Fallback**: Manual invocation if auto-loading fails\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nIf execution fails:\n- Verify prerequisites are met\n- Check input parameters and formats\n- Validate file paths and permissions\n- Review error messages for root cause\n- Consult documentation for troubleshooting\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "agent-context-manager",
        "category": "productivity",
        "path": "plugins/productivity/agent-context-manager",
        "version": "1.0.0",
        "description": "Automatically detects and loads AGENTS.md files to provide agent-specific instructions alongside CLAUDE.md. Enables specialized agent behaviors without manual intervention."
      },
      "filePath": "plugins/productivity/agent-context-manager/skills/agent-context-loader/SKILL.md"
    },
    {
      "slug": "agent-patterns",
      "name": "agent-patterns",
      "description": "This skill should be used when the user asks about \"SPAWN REQUEST format\", \"agent reports\", \"agent coordination\", \"parallel agents\", \"report format\", \"agent communication\", or needs to understand how agents coordinate within the sprint system. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read version: 1.0.0 author: Damien Laine <damien.laine@gmail.com> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Agent Patterns\n\nSprint coordinates multiple specialized agents through structured communication patterns. This skill covers the SPAWN REQUEST format, report structure, parallel execution, and inter-agent coordination.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## Agent Hierarchy\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         Sprint Orchestrator         ‚îÇ\n‚îÇ    (parses requests, spawns agents) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                  ‚îÇ\n                  ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ        Project Architect            ‚îÇ\n‚îÇ  (plans, creates specs, coordinates)‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                  ‚îÇ SPAWN REQUEST\n                  ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ      Implementation & Test Agents   ‚îÇ\n‚îÇ   (execute, report, never spawn)    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Key rule**: Only the orchestrator spawns agents. Agents communicate by returning structured messages.\n\n## SPAWN REQUEST Format\n\nThe architect requests agent spawning using a specific format:\n\n```markdown\n## SPAWN REQUEST\n\n- python-dev\n- nextjs-dev\n- cicd-agent\n```\n\n### Parsing Rules\n\nThe orchestrator parses SPAWN REQUEST blocks:\n- Section must start with `## SPAWN REQUEST` on its own line\n- Each agent listed as a bullet point\n- Multiple agents spawn in parallel\n\n### Implementation Agents\n\nRequest during Phase 2:\n```markdown\n## SPAWN REQUEST\n\n- python-dev\n- nextjs-dev\n```\n\nNever include test agents in implementation requests.\n\n### Testing Agents\n\nRequest during Phase 3:\n```markdown\n## SPAWN REQUEST\n\n- qa-test-agent\n- ui-test-agent\n```\n\nThe ui-test-agent runs in AUTOMATED or MANUAL mode based on `specs.md`:\n- `UI Testing Mode: automated` (default) ‚Üí runs test scenarios automatically\n- `UI Testing Mode: manual` ‚Üí opens browser for user to test, waits for tab close\n\n## Agent Report Format\n\nAll agents return structured reports. The orchestrator saves these as files.\n\n### Standard Report Sections\n\nEvery report includes:\n\n| Section | Purpose |\n|---------|---------|\n| CONFORMITY STATUS | YES/NO - did agent follow specs? |\n| SUMMARY | Brief outcome description |\n| DEVIATIONS | Justified departures from specs |\n| FILES CHANGED | List of modified files |\n| ISSUES | Problems encountered |\n| NOTES FOR ARCHITECT | Suggestions, observations |\n\n### Example: Backend Report\n\n```markdown\n## BACKEND REPORT\n\n### CONFORMITY STATUS: YES\n\n### SUMMARY\nImplemented user authentication endpoints per api-contract.md.\n\n### FILES CHANGED\n- backend/app/routers/auth.py (new)\n- backend/app/models/user.py (modified)\n- backend/app/schemas/auth.py (new)\n- backend/alembic/versions/001_add_users.py (new)\n\n### DEVIATIONS\nNone.\n\n### ISSUES\nNone.\n\n### NOTES FOR ARCHITECT\n- Consider adding rate limiting middleware in next sprint\n- Password hashing uses bcrypt (industry standard)\n```\n\n### Example: QA Report\n\n```markdown\n## QA REPORT\n\n### SUITE STATUS\n- New test files: 2\n- Updated test files: 1\n- Test framework(s): pytest\n- Test command(s): pytest tests/api/\n\n### API CONFORMITY STATUS: YES\n\n### SUMMARY\n- Total endpoints in contract: 5\n- Endpoints covered by automated tests: 5\n- Endpoints with failing tests: 0\n\n### FAILURES AND DEVIATIONS\nNone.\n\n### TEST EXECUTION\n- Tests run: YES\n- Result: ALL PASS\n- Notes: 23 tests passed in 1.4s\n\n### NOTES FOR ARCHITECT\n- Added edge case tests for password validation\n```\n\n### Example: UI Test Report\n\n```markdown\n## UI TEST REPORT\n\n### MODE\nAUTOMATED\n\n### SUMMARY\n- Total tests run: 8\n- Passed: 7\n- Failed: 1\n- Session duration: 45s\n\n### COVERAGE\n- Scenarios covered:\n  - Login with valid credentials\n  - Login with invalid password\n  - Registration flow\n  - Password reset request\n- Not covered (yet):\n  - Email verification flow (requires email testing setup)\n\n### FAILURES\n- Scenario: Registration validation\n  - Path/URL: /register\n  - Symptom: Error message not displayed\n  - Expected: \"Email already exists\" message\n  - Actual: Form submits without feedback\n\n### CONSOLE ERRORS\nNone.\n\n### NOTES FOR ARCHITECT\n- Registration error handling needs frontend fix\n```\n\n## Parallel Execution\n\n### Implementation Agents\n\nSpawn all implementation agents simultaneously:\n\n```\nTask: python-dev     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Backend Report\nTask: nextjs-dev     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Frontend Report\nTask: cicd-agent     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ CICD Report\n```\n\nUse a single message with multiple Task tool calls.\n\n### Testing Agents\n\nQA runs first, then UI tests:\n\n```\nTask: qa-test-agent  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ QA Report\n                                    ‚îÇ\n                                    ‚ñº\nTask: ui-test-agent  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ UI Test Report\nTask: diagnostics    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Diagnostics Report\n     (parallel with ui-test)\n```\n\nUI test and diagnostics agents run in parallel with each other.\n\n## Agent Constraints\n\n### What Agents Can Do\n\n- Read specification files\n- Read project-map.md (read-only)\n- Implement code in their domain\n- Run tests in their domain\n- Return structured reports\n\n### What Agents Cannot Do\n\n- Spawn other agents\n- Modify `status.md` (architect only)\n- Modify `project-map.md` (architect only)\n- Create methodology documentation\n- Push to git repositories\n\n## File-Based Coordination\n\nWhen agents need to coordinate beyond reports:\n\n### Report Files\n\nOrchestrator saves reports as:\n```\n.claude/sprint/[N]/[slug]-report-[iteration].md\n```\n\nSlugs:\n- `python-dev` ‚Üí `backend`\n- `nextjs-dev` ‚Üí `frontend`\n- `qa-test-agent` ‚Üí `qa`\n- `ui-test-agent` ‚Üí `ui-test`\n\n## Specialized Agent Roles\n\n### Project Architect\n\nThe decision-maker:\n- Creates specification files\n- Maintains project-map.md and status.md\n- Analyzes agent reports\n- Decides next steps (implement, test, finalize)\n\n### Implementation Agents\n\nTechnology-specific builders:\n- `python-dev`: Python/FastAPI backend\n- `nextjs-dev`: Next.js frontend\n- `cicd-agent`: CI/CD pipelines\n- `allpurpose-agent`: Any other technology\n\n### Testing Agents\n\nQuality validators:\n- `qa-test-agent`: API and unit tests\n- `ui-test-agent`: Browser-based E2E tests\n- Framework-specific diagnostics agents\n\n## FINALIZE Signal\n\nThe architect signals sprint completion:\n\n```markdown\nFINALIZE\nPhase 5 complete. Sprint [N] is finalized.\n```\n\nThe orchestrator detects this and exits the iteration loop.\n\n## Best Practices\n\n### Report Quality\n\n- Keep reports concise\n- Focus on actionable information\n- No verbose logs or stack traces\n- Summarize, don't dump\n\n### Parallel Efficiency\n\n- Spawn independent agents together\n- Don't wait unnecessarily\n\n### Spec Adherence\n\n- Always read specs before implementing\n- Report deviations with justification\n- Follow API contracts exactly\n\n## Additional Resources\n\nFor detailed agent definitions and report formats, see the agent files in the plugin's `agents/` directory.",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/agent-patterns/SKILL.md"
    },
    {
      "slug": "aggregating-crypto-news",
      "name": "aggregating-crypto-news",
      "description": "Aggregate breaking crypto news, announcements, and market-moving events in real-time. Use when staying updated on crypto market events. Trigger with phrases like \"get crypto news\", \"check latest announcements\", or \"scan for updates\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:news-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Crypto News Aggregator\n\nThis skill provides automated assistance for crypto news aggregator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:news-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "crypto-news-aggregator",
        "category": "crypto",
        "path": "plugins/crypto/crypto-news-aggregator",
        "version": "1.0.0",
        "description": "Aggregate and analyze crypto news from multiple sources with sentiment analysis"
      },
      "filePath": "plugins/crypto/crypto-news-aggregator/skills/aggregating-crypto-news/SKILL.md"
    },
    {
      "slug": "aggregating-performance-metrics",
      "name": "aggregating-performance-metrics",
      "description": "Aggregate and centralize performance metrics from applications, systems, databases, caches, and services. Use when consolidating monitoring data from multiple sources. Trigger with phrases like \"aggregate metrics\", \"centralize monitoring\", or \"collect performance data\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Bash(prometheus:*)",
        "Bash(metrics:*)",
        "Bash(monitoring:*)",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Metrics Aggregator\n\nThis skill provides automated assistance for metrics aggregator tasks.\n\n## Overview\n\nThis skill empowers Claude to streamline performance monitoring by aggregating metrics from diverse systems into a unified view. It simplifies the process of collecting, centralizing, and analyzing performance data, leading to improved insights and faster issue resolution.\n\n## How It Works\n\n1. **Metrics Taxonomy Design**: Claude assists in defining a clear and consistent naming convention for metrics across all systems.\n2. **Aggregation Tool Selection**: Claude helps select the appropriate metrics aggregation tool (e.g., Prometheus, StatsD, CloudWatch) based on the user's environment and requirements.\n3. **Configuration and Integration**: Claude guides the configuration of the chosen aggregation tool and its integration with various data sources.\n4. **Dashboard and Alert Setup**: Claude helps set up dashboards for visualizing metrics and defining alerts for critical performance indicators.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Centralize performance metrics from multiple applications and systems.\n- Design a consistent metrics naming convention.\n- Choose the right metrics aggregation tool for your needs.\n- Set up dashboards and alerts for performance monitoring.\n\n## Examples\n\n### Example 1: Centralizing Application and System Metrics\n\nUser request: \"Aggregate application and system metrics into Prometheus.\"\n\nThe skill will:\n1. Guide the user in defining metrics for applications (e.g., request latency, error rates) and systems (e.g., CPU usage, memory utilization).\n2. Help configure Prometheus to scrape metrics from the application and system endpoints.\n\n### Example 2: Setting Up Alerts for Database Performance\n\nUser request: \"Centralize database metrics and set up alerts for slow queries.\"\n\nThe skill will:\n1. Help the user define metrics for database performance (e.g., query execution time, connection pool usage).\n2. Guide the user in configuring the aggregation tool to collect these metrics from the database.\n3. Assist in setting up alerts in the aggregation tool to notify the user when query execution time exceeds a defined threshold.\n\n## Best Practices\n\n- **Naming Conventions**: Use a consistent and well-defined naming convention for all metrics to ensure clarity and ease of analysis.\n- **Granularity**: Choose an appropriate level of granularity for metrics to balance detail and storage requirements.\n- **Retention Policies**: Define retention policies for metrics to manage storage space and ensure data is available for historical analysis.\n\n## Integration\n\nThis skill integrates with other plugins that manage infrastructure, deploy applications, and monitor system health. For example, it can be used in conjunction with a deployment plugin to automatically configure metrics collection after a new application deployment.\n\n## Prerequisites\n\n- Access to metrics collection tools (Prometheus, StatsD, CloudWatch)\n- Network connectivity to metric sources\n- Metrics storage configuration in {baseDir}/metrics/\n- Understanding of metrics taxonomy\n\n## Instructions\n\n1. Design consistent metrics naming convention\n2. Select appropriate aggregation tool for environment\n3. Configure metric collection from all sources\n4. Set up centralized storage and retention policies\n5. Create dashboards for visualization\n6. Define alerts for critical metrics\n\n## Output\n\n- Metrics aggregation configuration files\n- Unified naming convention documentation\n- Dashboard definitions for key metrics\n- Alert rules for performance thresholds\n- Integration guides for metric sources\n\n## Error Handling\n\nIf metrics aggregation fails:\n- Verify network connectivity to sources\n- Check authentication credentials\n- Validate metrics format compatibility\n- Review storage capacity and retention\n- Ensure aggregation tool configuration\n\n## Resources\n\n- Prometheus aggregation documentation\n- StatsD protocol specifications\n- CloudWatch metrics API reference\n- Metrics naming best practices",
      "parentPlugin": {
        "name": "metrics-aggregator",
        "category": "performance",
        "path": "plugins/performance/metrics-aggregator",
        "version": "1.0.0",
        "description": "Aggregate and centralize performance metrics"
      },
      "filePath": "plugins/performance/metrics-aggregator/skills/aggregating-performance-metrics/SKILL.md"
    },
    {
      "slug": "analyzing-capacity-planning",
      "name": "analyzing-capacity-planning",
      "description": "This skill enables AI assistant to analyze capacity requirements and plan for future growth. it uses the capacity-planning-analyzer plugin to assess current utilization, forecast growth trends, and recommend scaling strategies. use this skill when the u... Use when analyzing code or data. Trigger with phrases like 'analyze', 'review', or 'examine'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Capacity Planning Analyzer\n\nThis skill provides automated assistance for capacity planning analyzer tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze current resource utilization, predict future capacity needs, and provide actionable recommendations for scaling infrastructure. It generates insights into growth trends, identifies potential bottlenecks, and estimates costs associated with capacity expansion.\n\n## How It Works\n\n1. **Analyze Utilization**: The plugin analyzes current CPU, memory, database storage, network bandwidth, and request rate utilization.\n2. **Forecast Growth**: Based on historical data, the plugin forecasts future growth trends for key capacity metrics.\n3. **Generate Recommendations**: The plugin recommends scaling strategies, including vertical and horizontal scaling options, and estimates associated costs.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze current infrastructure capacity and identify potential bottlenecks.\n- Forecast future resource requirements based on projected growth.\n- Develop a capacity roadmap to ensure optimal performance and availability.\n\n## Examples\n\n### Example 1: Planning for Database Growth\n\nUser request: \"Analyze database capacity and plan for future growth.\"\n\nThe skill will:\n1. Analyze current database storage utilization and growth rate.\n2. Forecast future storage requirements based on historical trends.\n3. Recommend scaling options, such as adding storage or migrating to a larger instance.\n\n### Example 2: Identifying CPU Bottlenecks\n\nUser request: \"Analyze CPU utilization and identify potential bottlenecks.\"\n\nThe skill will:\n1. Analyze CPU utilization trends across different servers and applications.\n2. Identify periods of high CPU usage and potential bottlenecks.\n3. Recommend scaling options, such as adding more CPU cores or optimizing application code.\n\n## Best Practices\n\n- **Data Accuracy**: Ensure that the data used for analysis is accurate and up-to-date.\n- **Metric Selection**: Choose the right capacity metrics to monitor based on your specific application requirements.\n- **Regular Monitoring**: Regularly monitor capacity metrics to identify potential issues before they impact performance.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools to provide proactive capacity management. It can also be used in conjunction with infrastructure-as-code tools to automate scaling operations.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "capacity-planning-analyzer",
        "category": "performance",
        "path": "plugins/performance/capacity-planning-analyzer",
        "version": "1.0.0",
        "description": "Analyze and plan for capacity requirements"
      },
      "filePath": "plugins/performance/capacity-planning-analyzer/skills/analyzing-capacity-planning/SKILL.md"
    },
    {
      "slug": "analyzing-database-indexes",
      "name": "analyzing-database-indexes",
      "description": "Use when you need to work with database indexing. This skill provides index design and optimization with comprehensive guidance and automation. Trigger with phrases like \"create indexes\", \"optimize indexes\", or \"improve query performance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Index Advisor\n\nThis skill provides automated assistance for database index advisor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-index-advisor/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-index-advisor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-index-advisor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-index-advisor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-index-advisor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-index-advisor-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-index-advisor",
        "category": "database",
        "path": "plugins/database/database-index-advisor",
        "version": "1.0.0",
        "description": "Analyze query patterns and recommend optimal database indexes with impact analysis"
      },
      "filePath": "plugins/database/database-index-advisor/skills/analyzing-database-indexes/SKILL.md"
    },
    {
      "slug": "analyzing-dependencies",
      "name": "analyzing-dependencies",
      "description": "Analyze dependencies for known security vulnerabilities and outdated versions. Use when auditing third-party libraries. Trigger with 'check dependencies', 'scan for vulnerabilities', or 'audit packages'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Dependency Checker\n\nThis skill provides automated assistance for dependency checker tasks.\n\n## Overview\n\nThis skill empowers Claude to automatically analyze your project's dependencies for security vulnerabilities, outdated packages, and license compliance issues. It uses the dependency-checker plugin to identify potential risks and provides insights for remediation.\n\n## How It Works\n\n1. **Detecting Package Manager**: The skill identifies the relevant package manager (npm, pip, composer, gem, go modules) based on the presence of manifest files (e.g., package.json, requirements.txt, composer.json).\n2. **Scanning Dependencies**: The skill utilizes the dependency-checker plugin to scan the identified dependencies against known vulnerability databases (CVEs), outdated package lists, and license information.\n3. **Generating Report**: The skill presents a comprehensive report summarizing the findings, including vulnerability summaries, detailed vulnerability information, outdated packages with recommended updates, and license compliance issues.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Check a project for known security vulnerabilities in its dependencies.\n- Identify outdated packages that may contain security flaws or performance issues.\n- Ensure that the project's dependencies comply with licensing requirements.\n\n## Examples\n\n### Example 1: Identifying Vulnerabilities Before Deployment\n\nUser request: \"Check dependencies for vulnerabilities before deploying to production.\"\n\nThe skill will:\n1. Detect the relevant package manager (e.g., npm).\n2. Scan the project's dependencies for known vulnerabilities using the dependency-checker plugin.\n3. Generate a report highlighting any identified vulnerabilities, their severity, and recommended fixes.\n\n### Example 2: Updating Outdated Packages\n\nUser request: \"Scan for outdated packages and suggest updates.\"\n\nThe skill will:\n1. Detect the relevant package manager (e.g., pip).\n2. Scan the project's dependencies for outdated packages.\n3. Generate a report listing the outdated packages and their available updates, including major, minor, and patch releases.\n\n## Best Practices\n\n- **Regular Scanning**: Schedule dependency checks regularly (e.g., weekly or monthly) to stay informed about new vulnerabilities and updates.\n- **Pre-Deployment Checks**: Always run a dependency check before deploying any code to production to prevent introducing vulnerable dependencies.\n- **Review and Remediation**: Carefully review the generated reports and take appropriate action to remediate identified vulnerabilities and update outdated packages.\n\n## Integration\n\nThis skill seamlessly integrates with other Claude Code tools, allowing you to use the identified vulnerabilities to guide further actions, such as automatically creating pull requests to update dependencies or generating security reports for compliance purposes.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "dependency-checker",
        "category": "security",
        "path": "plugins/security/dependency-checker",
        "version": "1.0.0",
        "description": "Check dependencies for known vulnerabilities, outdated packages, and license compliance"
      },
      "filePath": "plugins/security/dependency-checker/skills/analyzing-dependencies/SKILL.md"
    },
    {
      "slug": "analyzing-liquidity-pools",
      "name": "analyzing-liquidity-pools",
      "description": "Analyze liquidity pool metrics including TVL, volume, fees, and impermanent loss. Use when analyzing DEX liquidity pools. Trigger with phrases like \"analyze pool\", \"check TVL\", or \"calculate impermanent loss\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:liquidity-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Liquidity Pool Analyzer\n\nThis skill provides automated assistance for liquidity pool analyzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:liquidity-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "liquidity-pool-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/liquidity-pool-analyzer",
        "version": "1.0.0",
        "description": "Analyze DeFi liquidity pools for impermanent loss, APY, and optimization opportunities"
      },
      "filePath": "plugins/crypto/liquidity-pool-analyzer/skills/analyzing-liquidity-pools/SKILL.md"
    },
    {
      "slug": "analyzing-logs",
      "name": "analyzing-logs",
      "description": "Analyze application logs for performance insights and issue detection including slow requests, error patterns, and resource usage. Use when troubleshooting performance issues or debugging errors. Trigger with phrases like \"analyze logs\", \"find slow requests\", or \"detect error patterns\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Bash(logs:*)",
        "Bash(grep:*)",
        "Bash(awk:*)",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Log Analysis Tool\n\nThis skill provides automated assistance for log analysis tool tasks.\n\n## Overview\n\nThis skill empowers Claude to automatically analyze application logs, pinpoint performance bottlenecks, and identify recurring errors. It streamlines the debugging process and helps optimize application performance by extracting key insights from log data.\n\n## How It Works\n\n1. **Initiate Analysis**: Claude activates the log analysis tool upon detecting relevant trigger phrases.\n2. **Log Data Extraction**: The tool extracts relevant data, including timestamps, request durations, error messages, and resource usage metrics.\n3. **Pattern Identification**: The tool identifies patterns such as slow requests, frequent errors, and resource exhaustion warnings.\n4. **Report Generation**: Claude presents a summary of findings, highlighting potential performance issues and optimization opportunities.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in an application.\n- Debug recurring errors and exceptions.\n- Analyze log data for trends and anomalies.\n- Set up structured logging or log aggregation.\n\n## Examples\n\n### Example 1: Identifying Slow Requests\n\nUser request: \"Analyze logs for slow requests.\"\n\nThe skill will:\n1. Activate the log analysis tool.\n2. Identify requests exceeding predefined latency thresholds.\n3. Present a list of slow requests with corresponding timestamps and durations.\n\n### Example 2: Detecting Error Patterns\n\nUser request: \"Find error patterns in the application logs.\"\n\nThe skill will:\n1. Activate the log analysis tool.\n2. Scan logs for recurring error messages and exceptions.\n3. Group similar errors and present a summary of error frequencies.\n\n## Best Practices\n\n- **Log Level**: Ensure appropriate log levels (e.g., INFO, WARN, ERROR) are used to capture relevant information.\n- **Structured Logging**: Implement structured logging (e.g., JSON format) to facilitate efficient analysis.\n- **Log Rotation**: Configure log rotation policies to prevent log files from growing excessively.\n\n## Integration\n\nThis skill can be integrated with other tools for monitoring and alerting. For example, it can be used in conjunction with a monitoring plugin to automatically trigger alerts based on log analysis results. It can also work with deployment tools to rollback deployments when critical errors are detected in the logs.\n\n## Prerequisites\n\n- Access to application log files in {baseDir}/logs/\n- Log parsing tools (grep, awk, sed)\n- Understanding of application log format and structure\n- Read permissions for log directories\n\n## Instructions\n\n1. Identify log files to analyze based on timeframe and application\n2. Extract relevant data (timestamps, durations, error messages)\n3. Apply pattern matching to identify slow requests and errors\n4. Aggregate and group similar issues\n5. Generate analysis report with findings and recommendations\n6. Suggest optimization opportunities based on patterns\n\n## Output\n\n- Summary of slow requests with response times\n- Error frequency reports grouped by type\n- Resource usage patterns and anomalies\n- Performance bottleneck identification\n- Recommendations for log improvements and optimizations\n\n## Error Handling\n\nIf log analysis fails:\n- Verify log file paths and permissions\n- Check log format compatibility\n- Validate timestamp parsing\n- Ensure sufficient disk space for analysis\n- Review log rotation configuration\n\n## Resources\n\n- Application logging best practices\n- Structured logging format guides\n- Log aggregation tools documentation\n- Performance analysis methodologies",
      "parentPlugin": {
        "name": "log-analysis-tool",
        "category": "performance",
        "path": "plugins/performance/log-analysis-tool",
        "version": "1.0.0",
        "description": "Analyze logs for performance insights and issues"
      },
      "filePath": "plugins/performance/log-analysis-tool/skills/analyzing-logs/SKILL.md"
    },
    {
      "slug": "analyzing-market-sentiment",
      "name": "analyzing-market-sentiment",
      "description": "Analyze crypto market sentiment from social media, news, and on-chain metrics. Use when gauging market sentiment and social trends. Trigger with phrases like \"analyze sentiment\", \"check market mood\", or \"gauge social trends\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:sentiment-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Market Sentiment Analyzer\n\nThis skill provides automated assistance for market sentiment analyzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:sentiment-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "market-sentiment-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/market-sentiment-analyzer",
        "version": "1.0.0",
        "description": "Analyze market sentiment from social media, news, and on-chain data"
      },
      "filePath": "plugins/crypto/market-sentiment-analyzer/skills/analyzing-market-sentiment/SKILL.md"
    },
    {
      "slug": "analyzing-mempool",
      "name": "analyzing-mempool",
      "description": "Monitor blockchain mempools for pending transactions, front-running, and MEV opportunities. Use when monitoring pending blockchain transactions. Trigger with phrases like \"check mempool\", \"scan pending txs\", or \"find MEV\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:mempool-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Mempool Analyzer\n\nThis skill provides automated assistance for mempool analyzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:mempool-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "mempool-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/mempool-analyzer",
        "version": "1.0.0",
        "description": "Advanced mempool analysis for MEV opportunities, pending transaction monitoring, and gas price optimization"
      },
      "filePath": "plugins/crypto/mempool-analyzer/skills/analyzing-mempool/SKILL.md"
    },
    {
      "slug": "analyzing-network-latency",
      "name": "analyzing-network-latency",
      "description": "Analyze network latency and optimize request patterns for faster communication. Use when diagnosing slow network performance or optimizing API calls. Trigger with phrases like \"analyze network latency\", \"optimize API calls\", or \"reduce network delays\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Bash(curl:*)",
        "Bash(ping:*)",
        "Bash(traceroute:*)",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Network Latency Analyzer\n\nThis skill provides automated assistance for network latency analyzer tasks.\n\n## Overview\n\nThis skill empowers Claude to diagnose network latency issues and propose optimizations to improve application performance. It analyzes request patterns, identifies potential bottlenecks, and recommends solutions for faster and more efficient network communication.\n\n## How It Works\n\n1. **Request Pattern Identification**: Claude identifies all network requests made by the application.\n2. **Latency Analysis**: Claude analyzes the latency associated with each request, looking for patterns and anomalies.\n3. **Optimization Recommendations**: Claude suggests optimizations such as parallelization, request batching, connection pooling, and timeout adjustments.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze network latency in an application.\n- Optimize network request patterns for improved performance.\n- Identify bottlenecks in network communication.\n\n## Examples\n\n### Example 1: Optimizing API Calls\n\nUser request: \"Analyze network latency and suggest improvements for our API calls.\"\n\nThe skill will:\n1. Identify all API calls made by the application.\n2. Analyze the latency of each API call.\n3. Suggest parallelizing certain API calls and implementing connection pooling.\n\n### Example 2: Reducing Page Load Time\n\nUser request: \"Optimize network request patterns to reduce page load time.\"\n\nThe skill will:\n1. Identify all network requests made during page load.\n2. Analyze the latency of each request.\n3. Suggest batching multiple requests into a single request and optimizing timeout configurations.\n\n## Best Practices\n\n- **Parallelization**: Identify serial requests that can be executed in parallel to reduce overall latency.\n- **Request Batching**: Batch multiple small requests into a single larger request to reduce overhead.\n- **Connection Pooling**: Reuse existing HTTP connections to avoid the overhead of establishing new connections for each request.\n\n## Integration\n\nThis skill can be used in conjunction with other plugins that manage infrastructure or application code, allowing for automated implementation of the suggested optimizations. For instance, it can work with a code modification plugin to automatically apply connection pooling or adjust timeout values.\n\n## Prerequisites\n\n- Access to application network configuration\n- Network monitoring tools (curl, ping, traceroute)\n- Request pattern documentation\n- Performance baseline metrics\n\n## Instructions\n\n1. Identify all network requests in the application\n2. Measure latency for each request type\n3. Analyze patterns for serial vs parallel execution\n4. Identify opportunities for batching and pooling\n5. Recommend timeout and retry configurations\n6. Provide optimization implementation plan\n\n## Output\n\n- Network latency analysis report\n- Request pattern visualizations\n- Optimization recommendations with priorities\n- Implementation examples for suggested changes\n- Expected performance improvements\n\n## Error Handling\n\nIf latency analysis fails:\n- Verify network connectivity to endpoints\n- Check DNS resolution and routing\n- Validate request authentication\n- Review firewall and security rules\n- Ensure monitoring tools are installed\n\n## Resources\n\n- HTTP connection pooling guides\n- Request batching best practices\n- Network performance optimization references\n- API design patterns for latency reduction",
      "parentPlugin": {
        "name": "network-latency-analyzer",
        "category": "performance",
        "path": "plugins/performance/network-latency-analyzer",
        "version": "1.0.0",
        "description": "Analyze network latency and optimize request patterns"
      },
      "filePath": "plugins/performance/network-latency-analyzer/skills/analyzing-network-latency/SKILL.md"
    },
    {
      "slug": "analyzing-nft-rarity",
      "name": "analyzing-nft-rarity",
      "description": "Calculate NFT rarity scores and floor prices across collections and marketplaces. Use when analyzing NFT collections and rarity. Trigger with phrases like \"check NFT rarity\", \"analyze collection\", or \"calculate floor price\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:nft-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Nft Rarity Analyzer\n\nThis skill provides automated assistance for nft rarity analyzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:nft-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "nft-rarity-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/nft-rarity-analyzer",
        "version": "1.0.0",
        "description": "Analyze NFT rarity scores and valuations across collections"
      },
      "filePath": "plugins/crypto/nft-rarity-analyzer/skills/analyzing-nft-rarity/SKILL.md"
    },
    {
      "slug": "analyzing-on-chain-data",
      "name": "analyzing-on-chain-data",
      "description": "Perform on-chain analysis including whale tracking, token flows, and network activity. Use when performing crypto analysis. Trigger with phrases like \"analyze crypto\", \"check blockchain\", or \"monitor market\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:onchain-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# On Chain Analytics\n\nThis skill provides automated assistance for on chain analytics tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:onchain-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "on-chain-analytics",
        "category": "crypto",
        "path": "plugins/crypto/on-chain-analytics",
        "version": "1.0.0",
        "description": "Analyze on-chain metrics including whale movements, network activity, and holder distribution"
      },
      "filePath": "plugins/crypto/on-chain-analytics/skills/analyzing-on-chain-data/SKILL.md"
    },
    {
      "slug": "analyzing-options-flow",
      "name": "analyzing-options-flow",
      "description": "Track crypto options flow to identify institutional positioning and market sentiment. Use when tracking institutional options flow. Trigger with phrases like \"track options flow\", \"analyze derivatives\", or \"check institutional\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:options-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Options Flow Analyzer\n\nThis skill provides automated assistance for options flow analyzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:options-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "options-flow-analyzer",
        "category": "crypto",
        "path": "plugins/crypto/options-flow-analyzer",
        "version": "1.0.0",
        "description": "Track institutional options flow, unusual activity, and smart money movements"
      },
      "filePath": "plugins/crypto/options-flow-analyzer/skills/analyzing-options-flow/SKILL.md"
    },
    {
      "slug": "analyzing-query-performance",
      "name": "analyzing-query-performance",
      "description": "Use when you need to work with query optimization. This skill provides query performance analysis with comprehensive guidance and automation. Trigger with phrases like \"optimize queries\", \"analyze performance\", or \"improve query speed\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Query Performance Analyzer\n\nThis skill provides automated assistance for query performance analyzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/query-performance-analyzer/`\n\n**Documentation and Guides**: `{baseDir}/docs/query-performance-analyzer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/query-performance-analyzer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/query-performance-analyzer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/query-performance-analyzer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/query-performance-analyzer-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "query-performance-analyzer",
        "category": "database",
        "path": "plugins/database/query-performance-analyzer",
        "version": "1.0.0",
        "description": "Analyze query performance with EXPLAIN plan interpretation, bottleneck identification, and optimization recommendations"
      },
      "filePath": "plugins/database/query-performance-analyzer/skills/analyzing-query-performance/SKILL.md"
    },
    {
      "slug": "analyzing-security-headers",
      "name": "analyzing-security-headers",
      "description": "Analyze HTTP security headers of web domains to identify vulnerabilities and misconfigurations. Use when you need to audit website security headers, assess header compliance, or get security recommendations for web applications. Trigger with phrases like \"analyze security headers\", \"check HTTP headers\", \"audit website security headers\", or \"evaluate CSP and HSTS configuration\". allowed-tools: Read, WebFetch, WebSearch, Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Security Headers Analyzer\n\nThis skill provides automated assistance for security headers analyzer tasks.\n\n## Overview\n\nAnalyzes HTTP response headers (HSTS, CSP, etc.) for a target domain, scores gaps, and recommends concrete remediations.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Target URL or domain name is accessible\n- Network connectivity for HTTP requests\n- Permission to scan the target domain\n- Optional: Save results to {baseDir}/security-reports/\n\n## Instructions\n\n1. Collect the target URL/domain and environment context (CDN/proxy, redirects).\n2. Fetch response headers (HTTP/HTTPS) and capture redirects/cookies.\n3. Compare headers to recommended baselines and score gaps.\n4. Provide concrete remediation steps and verify fixes.\n\n### 1. Domain Input Phase\n\nAccept domain specification:\n- Full URL with protocol (https://example.com)\n- Domain name only (example.com - will test HTTPS first)\n- Multiple domains for batch analysis\n- Specific paths for header variation testing\n\n### 2. Header Fetching Phase\n\nRetrieve HTTP response headers:\n- Make HEAD or GET request to target\n- Capture all security-relevant headers\n- Test both HTTP and HTTPS responses\n- Record redirect chains and final destination\n\n### 3. Analysis Phase\n\nEvaluate each security header against best practices:\n\n**Critical Headers**:\n- Strict-Transport-Security (HSTS)\n- Content-Security-Policy (CSP)\n- X-Frame-Options\n- X-Content-Type-Options\n- Permissions-Policy\n\n**Important Headers**:\n- Referrer-Policy\n- Cross-Origin-Embedder-Policy (COEP)\n- Cross-Origin-Opener-Policy (COOP)\n- Cross-Origin-Resource-Policy (CORP)\n\n**Additional Checks**:\n- Server header information disclosure\n- X-Powered-By header exposure\n- Cookie security attributes (Secure, HttpOnly, SameSite)\n\n### 4. Grading Phase\n\nCalculate security score:\n- A+ (95-100): All critical headers properly configured\n- A (85-94): Critical headers present, minor issues\n- B (75-84): Most headers present, some weaknesses\n- C (65-74): Missing critical headers\n- D (50-64): Significant security gaps\n- F (<50): Multiple critical vulnerabilities\n\n### 5. Report Generation Phase\n\nCreate comprehensive report with:\n- Overall security grade and numeric score\n- Missing headers with impact assessment\n- Misconfigured headers with specific issues\n- Remediation recommendations with examples\n- Priority ranking for fixes\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Security headers analysis report\n\n**Report Structure**:\n```\n# Security Headers Analysis - example.com\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## Overall Grade: B (82/100)\n\n## Critical Headers Status\n‚úÖ Strict-Transport-Security: Present (max-age=31536000; includeSubDomains)\n‚ùå Content-Security-Policy: Missing\n‚úÖ X-Frame-Options: Present (DENY)\n‚úÖ X-Content-Type-Options: Present (nosniff)\n‚ö†Ô∏è  Permissions-Policy: Misconfigured\n\n## Detailed Findings\n\n### Missing Headers (High Priority)\n1. Content-Security-Policy\n   - Risk: XSS vulnerability exposure\n   - Recommendation: Implement strict CSP\n   - Example: Content-Security-Policy: default-src 'self'; script-src 'self' 'unsafe-inline'\n\n### Misconfigured Headers\n1. Permissions-Policy\n   - Current: geolocation=*\n   - Issue: Allows all origins\n   - Fix: geolocation=(self)\n\n## Priority Actions\n1. Add Content-Security-Policy (Critical)\n2. Fix Permissions-Policy wildcard (High)\n3. Add Referrer-Policy (Medium)\n```\n\n**Optional Outputs**:\n- JSON format for automation: {baseDir}/security-reports/headers-DOMAIN-YYYYMMDD.json\n- CSV for spreadsheet analysis\n- Comparison report for multiple domains\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Domain Unreachable**\n   - Error: \"Failed to connect to example.com\"\n   - Resolution: Check domain spelling, network connectivity, firewall rules\n   - Fallback: Test alternate protocols (HTTP vs HTTPS)\n\n2. **SSL/TLS Errors**\n   - Error: \"SSL certificate verification failed\"\n   - Resolution: Note in report, test with certificate validation disabled\n   - Impact: Indicates HSTS not properly enforced\n\n3. **Redirect Loops**\n   - Error: \"Too many redirects\"\n   - Resolution: Report redirect chain, analyze headers at each hop\n   - Note: Headers may differ across redirect chain\n\n4. **Rate Limiting**\n   - Error: \"HTTP 429 Too Many Requests\"\n   - Resolution: Implement exponential backoff, reduce request frequency\n   - Fallback: Queue domain for later analysis\n\n5. **Mixed Content Issues**\n   - Error: \"Headers differ between HTTP and HTTPS\"\n   - Resolution: Report both sets, highlight critical differences\n   - Recommendation: Ensure HSTS enforces HTTPS-only\n\n## Examples\n\n- \"Analyze security headers for https://claudecodeplugins.io and explain any CSP/HSTS issues.\"\n- \"Check headers for example.com and provide a remediation checklist.\"\n\n## Resources\n\n**Security Header References**:\n- OWASP Secure Headers Project: https://owasp.org/www-project-secure-headers/\n- MDN Security Headers Guide: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers#security\n- Security Headers Scanner: https://securityheaders.com/\n\n**Header-Specific Documentation**:\n- CSP Reference: https://content-security-policy.com/\n- HSTS Preload: https://hstspreload.org/\n- Permissions Policy: https://www.w3.org/TR/permissions-policy/\n\n**Best Practice Guides**:\n- NIST Web Security Guidelines: https://pages.nist.gov/800-63-3/\n- Mozilla Observatory: https://observatory.mozilla.org/\n\n**Testing Tools**:\n- Online header checker: https://securityheaders.com/\n- Browser DevTools Network tab for manual inspection\n- curl command for command-line testing: `curl -I https://example.com`\n\n**Integration Examples**:\n- Automated header checks in CI/CD pipelines\n- Periodic scanning with alerting on grade degradation\n- Compliance reporting for security audits",
      "parentPlugin": {
        "name": "security-headers-analyzer",
        "category": "security",
        "path": "plugins/security/security-headers-analyzer",
        "version": "1.0.0",
        "description": "Analyze HTTP security headers"
      },
      "filePath": "plugins/security/security-headers-analyzer/skills/analyzing-security-headers/SKILL.md"
    },
    {
      "slug": "analyzing-system-throughput",
      "name": "analyzing-system-throughput",
      "description": "Analyze and optimize system throughput including request handling, data processing, and resource utilization. Use when identifying capacity limits or evaluating scaling strategies. Trigger with phrases like \"analyze throughput\", \"optimize capacity\", or \"identify bottlenecks\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Bash(performance:*)",
        "Bash(monitoring:*)",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Throughput Analyzer\n\nThis skill provides automated assistance for throughput analyzer tasks.\n\n## Overview\n\nThis skill allows Claude to analyze system performance and identify areas for throughput optimization. It uses the `throughput-analyzer` plugin to provide insights into request handling, data processing, and resource utilization.\n\n## How It Works\n\n1. **Identify Critical Components**: Determines which system components are most relevant to throughput.\n2. **Analyze Throughput Metrics**: Gathers and analyzes current throughput metrics for the identified components.\n3. **Identify Limiting Factors**: Pinpoints the bottlenecks and constraints that are hindering optimal throughput.\n4. **Evaluate Scaling Strategies**: Explores potential scaling strategies and their impact on overall throughput.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze system throughput to identify performance bottlenecks.\n- Optimize system performance for increased capacity.\n- Evaluate scaling strategies to improve throughput.\n\n## Examples\n\n### Example 1: Analyzing Web Server Throughput\n\nUser request: \"Analyze the throughput of my web server and identify any bottlenecks.\"\n\nThe skill will:\n1. Activate the `throughput-analyzer` plugin.\n2. Analyze request throughput, data throughput, and resource saturation of the web server.\n3. Provide a report identifying potential bottlenecks and optimization opportunities.\n\n### Example 2: Optimizing Data Processing Pipeline\n\nUser request: \"Optimize the throughput of my data processing pipeline.\"\n\nThe skill will:\n1. Activate the `throughput-analyzer` plugin.\n2. Analyze data throughput, queue processing, and concurrency limits of the data processing pipeline.\n3. Suggest improvements to increase data processing rates and overall throughput.\n\n## Best Practices\n\n- **Component Selection**: Focus the analysis on the most throughput-critical components to avoid unnecessary overhead.\n- **Metric Interpretation**: Carefully interpret throughput metrics to accurately identify limiting factors.\n- **Scaling Evaluation**: Thoroughly evaluate the potential impact of scaling strategies before implementation.\n\n## Integration\n\nThis skill can be used in conjunction with other monitoring and performance analysis tools to gain a more comprehensive understanding of system behavior. It provides a starting point for further investigation and optimization efforts.\n\n## Prerequisites\n\n- Access to throughput metrics in {baseDir}/metrics/throughput/\n- System performance monitoring tools\n- Historical throughput baselines\n- Current capacity and scaling limits\n\n## Instructions\n\n1. Identify critical system components for throughput analysis\n2. Collect request and data throughput metrics\n3. Analyze resource saturation and queue depths\n4. Identify bottlenecks and limiting factors\n5. Evaluate horizontal and vertical scaling strategies\n6. Generate capacity planning recommendations\n\n## Output\n\n- Throughput analysis reports with current capacity\n- Bottleneck identification and root cause analysis\n- Resource saturation metrics\n- Scaling strategy recommendations\n- Capacity planning projections\n\n## Error Handling\n\nIf throughput analysis fails:\n- Verify metrics collection infrastructure\n- Check system monitoring tool access\n- Validate historical baseline data\n- Ensure performance testing environment\n- Review component identification logic\n\n## Resources\n\n- Throughput optimization best practices\n- Capacity planning methodologies\n- Scaling strategy comparison guides\n- Performance bottleneck detection techniques",
      "parentPlugin": {
        "name": "throughput-analyzer",
        "category": "performance",
        "path": "plugins/performance/throughput-analyzer",
        "version": "1.0.0",
        "description": "Analyze and optimize system throughput"
      },
      "filePath": "plugins/performance/throughput-analyzer/skills/analyzing-system-throughput/SKILL.md"
    },
    {
      "slug": "analyzing-test-coverage",
      "name": "analyzing-test-coverage",
      "description": "Analyze code coverage metrics and identify untested code paths. Use when analyzing untested code or coverage gaps. Trigger with phrases like \"analyze coverage\", \"check test coverage\", or \"find untested code\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:coverage-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Test Coverage Analyzer\n\nThis skill provides automated assistance for test coverage analyzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:coverage-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test coverage analyzer tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-coverage-analyzer",
        "category": "testing",
        "path": "plugins/testing/test-coverage-analyzer",
        "version": "1.0.0",
        "description": "Analyze code coverage metrics, identify untested code, and generate comprehensive coverage reports"
      },
      "filePath": "plugins/testing/test-coverage-analyzer/skills/analyzing-test-coverage/SKILL.md"
    },
    {
      "slug": "analyzing-text-sentiment",
      "name": "analyzing-text-sentiment",
      "description": "This skill enables AI assistant to analyze the sentiment of text data. it identifies the emotional tone expressed in text, classifying it as positive, negative, or neutral. use this skill when a user requests sentiment analysis, opinion mining, or emoti... Use when analyzing code or data. Trigger with phrases like 'analyze', 'review', or 'examine'. allowed-tools: Read, Write, Bash(cmd:*), Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Sentiment Analysis Tool\n\nThis skill provides automated assistance for sentiment analysis tool tasks.\n\n## Overview\n\nThis skill empowers Claude to perform sentiment analysis on text, providing insights into the emotional content and polarity of the provided data. By leveraging AI/ML techniques, it helps understand public opinion, customer feedback, and overall emotional tone in written communication.\n\n## How It Works\n\n1. **Text Input**: The skill receives text data as input from the user.\n2. **Sentiment Analysis**: The skill processes the text using a pre-trained sentiment analysis model to determine the sentiment polarity (positive, negative, or neutral).\n3. **Result Output**: The skill provides a sentiment score and classification, indicating the overall sentiment expressed in the text.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Determine the overall sentiment of customer reviews.\n- Analyze the emotional tone of social media posts.\n- Gauge public opinion on a particular topic.\n- Identify positive and negative feedback in survey responses.\n\n## Examples\n\n### Example 1: Analyzing Customer Reviews\n\nUser request: \"Analyze the sentiment of these customer reviews: 'The product is amazing!', 'The service was terrible.', 'It was okay.'\"\n\nThe skill will:\n1. Process the provided customer reviews.\n2. Classify each review as positive, negative, or neutral and provide sentiment scores.\n\n### Example 2: Monitoring Social Media Sentiment\n\nUser request: \"Perform sentiment analysis on the following tweet: 'I love this new feature!'\"\n\nThe skill will:\n1. Analyze the provided tweet.\n2. Identify the sentiment as positive and provide a corresponding sentiment score.\n\n## Best Practices\n\n- **Data Quality**: Ensure the input text is clear and free from ambiguous language for accurate sentiment analysis.\n- **Context Awareness**: Consider the context of the text when interpreting sentiment scores, as sarcasm or irony can affect results.\n- **Model Selection**: Use appropriate sentiment analysis models based on the type of text being analyzed (e.g., social media, customer reviews).\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins to automate workflows, such as summarizing feedback alongside sentiment scores or triggering actions based on sentiment polarity (e.g., escalating negative feedback).\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "sentiment-analysis-tool",
        "category": "ai-ml",
        "path": "plugins/ai-ml/sentiment-analysis-tool",
        "version": "1.0.0",
        "description": "Sentiment analysis on text data"
      },
      "filePath": "plugins/ai-ml/sentiment-analysis-tool/skills/analyzing-text-sentiment/SKILL.md"
    },
    {
      "slug": "analyzing-text-with-nlp",
      "name": "analyzing-text-with-nlp",
      "description": "This skill enables AI assistant to perform natural language processing and text analysis using the nlp-text-analyzer plugin. it should be used when the user requests analysis of text, including sentiment analysis, keyword extraction, topic modeling, or ... Use when analyzing code or data. Trigger with phrases like 'analyze', 'review', or 'examine'. allowed-tools: Read, Bash(cmd:*), Grep, Glob version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Nlp Text Analyzer\n\nThis skill provides automated assistance for nlp text analyzer tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze text using the nlp-text-analyzer plugin, extracting meaningful information and insights. It facilitates tasks such as sentiment analysis, keyword extraction, and topic modeling, enabling a deeper understanding of textual data.\n\n## How It Works\n\n1. **Request Analysis**: Claude receives a user request to analyze text.\n2. **Text Processing**: The nlp-text-analyzer plugin processes the text using NLP techniques.\n3. **Insight Extraction**: The plugin extracts insights such as sentiment, keywords, and topics.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform sentiment analysis on a piece of text.\n- Extract keywords from a document.\n- Identify the main topics discussed in a text.\n\n## Examples\n\n### Example 1: Sentiment Analysis\n\nUser request: \"Analyze the sentiment of this product review: 'I loved the product! It exceeded my expectations.'\"\n\nThe skill will:\n1. Process the review text using the nlp-text-analyzer plugin.\n2. Determine the sentiment as positive and provide a confidence score.\n\n### Example 2: Keyword Extraction\n\nUser request: \"Extract the keywords from this news article about the latest AI advancements.\"\n\nThe skill will:\n1. Process the article text using the nlp-text-analyzer plugin.\n2. Identify and return a list of relevant keywords, such as \"AI\", \"advancements\", \"machine learning\", and \"neural networks\".\n\n## Best Practices\n\n- **Clarity**: Be specific in your requests to ensure accurate and relevant analysis.\n- **Context**: Provide sufficient context to improve the quality of the analysis.\n- **Iteration**: Refine your requests based on the initial results to achieve the desired outcome.\n\n## Integration\n\nThis skill can be integrated with other tools to provide a comprehensive workflow, such as using the extracted keywords to perform further research or using sentiment analysis to categorize customer feedback.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "nlp-text-analyzer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/nlp-text-analyzer",
        "version": "1.0.0",
        "description": "Natural language processing and text analysis"
      },
      "filePath": "plugins/ai-ml/nlp-text-analyzer/skills/analyzing-text-with-nlp/SKILL.md"
    },
    {
      "slug": "api-contract",
      "name": "api-contract",
      "description": "This skill should be used when the user asks about \"API contract\", \"api-contract.md\", \"shared interface\", \"TypeScript interfaces\", \"request response schemas\", \"endpoint design\", or needs guidance on designing contracts that coordinate backend and frontend agents. Use when building or modifying API endpoints. Trigger with phrases like 'create API', 'design endpoint', or 'API scaffold'. allowed-tools: Read version: 1.0.0 author: Damien Laine <damien.laine@gmail.com> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# API Contract Design\n\nThe API contract (`api-contract.md`) is the shared interface between implementation agents. It defines what the backend provides and what the frontend consumes, ensuring both sides build compatible systems without direct communication.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## Purpose\n\nThe contract serves as:\n- **Single source of truth** for API shape\n- **Coordination mechanism** between backend and frontend agents\n- **Validation reference** for QA testing\n- **Documentation** for the implemented API\n\n## Contract Structure\n\nA complete api-contract.md contains:\n\n```markdown\n# API Contract: [Feature Name]\n\n## Base URL\n[API base path, e.g., /api/v1]\n\n## Authentication\n[Auth requirements for endpoints]\n\n## Endpoints\n\n### [Endpoint Group]\n\n#### [METHOD] [Route]\n[Description]\n\n**Request:**\n[Request schema]\n\n**Response:**\n[Response schema]\n\n**Errors:**\n[Error codes and meanings]\n\n## TypeScript Interfaces\n[Shared type definitions]\n\n## Validation Rules\n[Input validation requirements]\n```\n\n## Writing Endpoints\n\n### Endpoint Definition\n\n```markdown\n#### POST /auth/register\n\nCreate a new user account.\n\n**Request:**\n```json\n{\n  \"email\": \"string (required, valid email)\",\n  \"password\": \"string (required, min 8 chars)\",\n  \"name\": \"string (optional)\"\n}\n```\n\n**Response (201):**\n```json\n{\n  \"id\": \"uuid\",\n  \"email\": \"string\",\n  \"name\": \"string | null\",\n  \"createdAt\": \"ISO 8601 datetime\"\n}\n```\n\n**Errors:**\n- 400: Invalid request body\n- 409: Email already exists\n- 422: Validation failed\n```\n\n### Key Elements\n\n| Element | Purpose |\n|---------|---------|\n| Method + Route | HTTP verb and path |\n| Description | What the endpoint does |\n| Request | Input schema with types and constraints |\n| Response | Output schema with status code |\n| Errors | Possible error responses |\n\n## TypeScript Interfaces\n\nDefine shared types for type safety across agents:\n\n```markdown\n## TypeScript Interfaces\n\n```typescript\n// User types\ninterface User {\n  id: string;\n  email: string;\n  name: string | null;\n  createdAt: string;\n}\n\ninterface CreateUserRequest {\n  email: string;\n  password: string;\n  name?: string;\n}\n\ninterface LoginRequest {\n  email: string;\n  password: string;\n}\n\ninterface AuthResponse {\n  user: User;\n  token: string;\n  expiresAt: string;\n}\n\n// Error response\ninterface ApiError {\n  code: string;\n  message: string;\n  details?: Record<string, string[]>;\n}\n```\n```\n\n### Type Guidelines\n\n- Use explicit types, not `any`\n- Mark optional fields with `?`\n- Use union types for nullable: `string | null`\n- Include all possible response shapes\n- Match types to JSON serialization\n\n## Validation Rules\n\nDocument validation requirements explicitly:\n\n```markdown\n## Validation Rules\n\n### User Registration\n| Field | Rules |\n|-------|-------|\n| email | Required, valid email format, unique |\n| password | Required, min 8 chars, 1 uppercase, 1 number |\n| name | Optional, max 100 chars |\n\n### Product Creation\n| Field | Rules |\n|-------|-------|\n| title | Required, 3-200 chars |\n| price | Required, positive number, max 2 decimals |\n| category | Required, must exist in categories table |\n```\n\n### Why Document Validation\n\n- Backend implements the rules\n- Frontend can pre-validate before submit\n- QA tests edge cases\n- All agents have same understanding\n\n## Error Handling\n\n### Standard Error Format\n\n```markdown\n## Error Response Format\n\nAll errors return:\n```json\n{\n  \"code\": \"ERROR_CODE\",\n  \"message\": \"Human-readable message\",\n  \"details\": {\n    \"field\": [\"error1\", \"error2\"]\n  }\n}\n```\n\n### Error Codes\n| Code | HTTP Status | Meaning |\n|------|-------------|---------|\n| VALIDATION_ERROR | 422 | Input validation failed |\n| NOT_FOUND | 404 | Resource doesn't exist |\n| UNAUTHORIZED | 401 | Authentication required |\n| FORBIDDEN | 403 | Permission denied |\n| CONFLICT | 409 | Resource conflict |\n```\n\n### Error Consistency\n\nUse consistent error codes across all endpoints. This helps:\n- Frontend implement unified error handling\n- QA test all error scenarios\n- Users get predictable feedback\n\n## Pagination\n\nFor list endpoints:\n\n```markdown\n#### GET /products\n\nList products with pagination.\n\n**Query Parameters:**\n| Param | Type | Default | Description |\n|-------|------|---------|-------------|\n| page | integer | 1 | Page number |\n| limit | integer | 20 | Items per page (max 100) |\n| sort | string | createdAt | Sort field |\n| order | string | desc | Sort order (asc/desc) |\n\n**Response (200):**\n```json\n{\n  \"data\": [Product],\n  \"pagination\": {\n    \"page\": 1,\n    \"limit\": 20,\n    \"total\": 150,\n    \"totalPages\": 8\n  }\n}\n```\n```\n\n### Pagination Interface\n\n```typescript\ninterface PaginatedResponse<T> {\n  data: T[];\n  pagination: {\n    page: number;\n    limit: number;\n    total: number;\n    totalPages: number;\n  };\n}\n```\n\n## Contract Evolution\n\n### During Sprint\n\n1. **Initial**: Architect creates contract based on specs\n2. **Implementation**: Agents follow contract exactly\n3. **Deviations**: If agent must deviate, document in report\n4. **Update**: Architect updates contract if deviation justified\n5. **Final**: Contract reflects implemented reality\n\n### Contract Changes\n\nWhen changing an existing contract:\n- Note breaking changes\n- Consider versioning for major changes\n- Update both request and response if affected\n- Notify dependent agents via specs\n\n## Best Practices\n\n### Be Specific\n\n```markdown\n// Good\n\"email\": \"string (required, valid email format)\"\n\n// Bad\n\"email\": \"string\"\n```\n\n### Include Examples\n\n```markdown\n**Request Example:**\n```json\n{\n  \"email\": \"user@example.com\",\n  \"password\": \"SecurePass123\"\n}\n```\n```\n\n### Document All States\n\nInclude responses for:\n- Success (200, 201, 204)\n- Client errors (400, 401, 403, 404, 422)\n- Empty states (empty arrays, null values)\n\n### Keep It DRY\n\nReference shared types instead of duplicating:\n\n```markdown\n**Response:** `User` (see TypeScript Interfaces)\n```\n\n### No Implementation Details\n\nThe contract defines WHAT, not HOW:\n- Don't mention database columns\n- Don't specify frameworks\n- Don't include file paths\n\n## Common Patterns\n\n### CRUD Endpoints\n\n```markdown\n### Products\n\n| Method | Route | Description |\n|--------|-------|-------------|\n| GET | /products | List all products |\n| GET | /products/:id | Get single product |\n| POST | /products | Create product |\n| PUT | /products/:id | Update product |\n| DELETE | /products/:id | Delete product |\n```\n\n### Authentication Endpoints\n\n```markdown\n### Authentication\n\n| Method | Route | Auth | Description |\n|--------|-------|------|-------------|\n| POST | /auth/register | No | Create account |\n| POST | /auth/login | No | Get token |\n| POST | /auth/logout | Yes | Invalidate token |\n| GET | /auth/me | Yes | Current user |\n| POST | /auth/refresh | Yes | Refresh token |\n```\n\n## Additional Resources\n\nFor complete examples, examine the `api-contract.md` files generated in sprint directories after running `/sprint`.",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/api-contract/SKILL.md"
    },
    {
      "slug": "archiving-databases",
      "name": "archiving-databases",
      "description": "Use when you need to archive historical database records to reduce primary database size. This skill automates moving old data to archive tables or cold storage (S3, Azure Blob, GCS). Trigger with phrases like \"archive old database records\", \"implement data retention policy\", \"move historical data to cold storage\", or \"reduce database size with archival\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(aws:s3:*), Bash(az:storage:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Archival System\n\nThis skill provides automated assistance for database archival system tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Database credentials with SELECT and DELETE permissions on source tables\n- Access to destination storage (archive table or cloud storage credentials)\n- Network connectivity to cloud storage services if using S3/Azure/GCS\n- Backup of database before first archival run\n- Understanding of data retention requirements and compliance policies\n- Monitoring tools configured to track archival job success\n\n## Instructions\n\n### Step 1: Define Archival Criteria\n1. Identify tables containing historical data for archival\n2. Define age threshold for archival (e.g., records older than 1 year)\n3. Determine additional criteria (status flags, record size, access frequency)\n4. Calculate expected data volume to be archived\n5. Document business requirements and compliance policies\n\n### Step 2: Choose Archival Destination\n1. Evaluate options: archive table in same database, separate archive database, or cold storage\n2. For cloud storage: select S3, Azure Blob, or GCS based on infrastructure\n3. Configure destination storage with appropriate security and access controls\n4. Set up compression settings for storage efficiency\n5. Define data format for archived records (CSV, Parquet, JSON)\n\n### Step 3: Create Archive Schema\n1. Design archive table schema matching source table structure\n2. Add metadata columns (archived_at, source_table, archive_reason)\n3. Create indexes on commonly queried archive columns\n4. For cloud storage: define bucket structure and naming conventions\n5. Test archive schema with sample data\n\n### Step 4: Implement Archival Logic\n1. Write SQL query to identify records meeting archival criteria\n2. Create extraction script to export records from source tables\n3. Implement transformation logic if archive format differs from source\n4. Build verification queries to confirm data integrity after archival\n5. Add transaction handling to ensure atomicity (delete only if archive succeeds)\n\n### Step 5: Execute Archival Process\n1. Run archival in staging environment first with subset of data\n2. Verify archived data integrity and completeness\n3. Execute archival in production during low-traffic window\n4. Monitor database performance during archival operation\n5. Generate archival report with record counts and storage savings\n\n### Step 6: Automate Retention Policy\n1. Schedule periodic archival jobs (weekly, monthly)\n2. Configure automated monitoring and alerting for job failures\n3. Implement cleanup of successfully archived records from source tables\n4. Set up expiration policies on archived data per compliance requirements\n5. Document archival schedule and retention periods\n\n## Output\n\nThis skill produces:\n\n**Archival Scripts**: SQL and shell scripts to extract, transform, and load data to archive destination\n\n**Archive Tables/Files**: Structured storage containing historical records with metadata and timestamps\n\n**Verification Reports**: Row counts, data checksums, and integrity checks confirming successful archival\n\n**Storage Metrics**: Database size reduction, archive storage utilization, and cost savings estimates\n\n**Archival Logs**: Detailed logs of each archival run with timestamps, record counts, and any errors\n\n## Error Handling\n\n**Insufficient Storage Space**:\n- Check available disk space on archive destination before execution\n- Implement storage monitoring and alerting\n- Use compression to reduce archive size\n- Clean up old archives per retention policy before new archival\n\n**Data Integrity Issues**:\n- Run checksums on source data before and after archival\n- Implement row count verification between source and archive\n- Keep source data until archive verification completes\n- Rollback archive transaction if verification fails\n\n**Permission Denied Errors**:\n- Verify database user has SELECT on source tables and INSERT on archive tables\n- Confirm cloud storage credentials have write permissions\n- Check network security groups allow connections to cloud storage\n- Document required permissions for archival automation\n\n**Timeout During Large Archival**:\n- Split archival into smaller batches by date ranges\n- Run archival incrementally over multiple days\n- Increase database timeout settings for archival sessions\n- Schedule archival during maintenance windows with extended timeouts\n\n## Resources\n\n**Archival Configuration Templates**:\n- PostgreSQL archival: `{baseDir}/templates/postgresql-archive-config.yaml`\n- MySQL archival: `{baseDir}/templates/mysql-archive-config.yaml`\n- S3 cold storage: `{baseDir}/templates/s3-archive-config.yaml`\n- Azure Blob storage: `{baseDir}/templates/azure-archive-config.yaml`\n\n**Retention Policy Definitions**: `{baseDir}/policies/retention-policies.yaml`\n\n**Archival Scripts Library**: `{baseDir}/scripts/archival/`\n- Extract to CSV script\n- Extract to Parquet script\n- S3 upload with compression\n- Archive verification queries\n\n**Monitoring Dashboards**: `{baseDir}/monitoring/archival-dashboard.json`\n**Cost Analysis Tools**: `{baseDir}/tools/storage-cost-calculator.py`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-archival-system",
        "category": "database",
        "path": "plugins/database/database-archival-system",
        "version": "1.0.0",
        "description": "Database plugin for database-archival-system"
      },
      "filePath": "plugins/database/database-archival-system/skills/archiving-databases/SKILL.md"
    },
    {
      "slug": "assisting-with-soc2-audit-preparation",
      "name": "assisting-with-soc2-audit-preparation",
      "description": "Automate SOC 2 audit preparation including evidence gathering, control assessment, and compliance gap identification. Use when you need to prepare for SOC 2 audits, assess Trust Service Criteria compliance, document security controls, or generate readiness reports. Trigger with phrases like \"SOC 2 audit preparation\", \"SOC 2 readiness assessment\", \"collect SOC 2 evidence\", or \"Trust Service Criteria compliance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(audit-collect:*), Bash(compliance-check:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Soc2 Audit Helper\n\nThis skill provides automated assistance for soc2 audit helper tasks.\n\n## Overview\n\nGuides SOC 2 readiness by mapping controls to Trust Services Criteria, planning evidence collection, identifying gaps, and producing audit-ready checklists.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Documentation directory accessible in {baseDir}/docs/\n- Infrastructure-as-code and configuration files available\n- Access to cloud provider logs (AWS CloudTrail, Azure Activity Log, GCP Audit Logs)\n- Security policies and procedures documented\n- Employee training records available\n- Incident response documentation accessible\n- Write permissions for audit reports in {baseDir}/soc2-audit/\n\n## Instructions\n\n1. Confirm scope (services, systems, period) and applicable SOC 2 criteria.\n2. Gather existing controls, policies, and evidence sources.\n3. Identify gaps and draft an evidence collection plan.\n4. Produce an audit-ready checklist and remediation backlog.\n\n### 1. Trust Service Criteria Assessment\n\nEvaluate controls across five categories:\n\n**Security (Common Criteria)** - Required for all SOC 2 audits:\n- CC1: Control Environment\n- CC2: Communication and Information\n- CC3: Risk Assessment\n- CC4: Monitoring Activities\n- CC5: Control Activities\n- CC6: Logical and Physical Access Controls\n- CC7: System Operations\n- CC8: Change Management\n- CC9: Risk Mitigation\n\n**Additional Criteria** (Optional):\n- Availability\n- Processing Integrity\n- Confidentiality\n- Privacy\n\n### 2. Evidence Collection Phase\n\n**Security Controls Evidence**:\n- Access control policies and configurations\n- Multi-factor authentication implementation\n- Password policy documentation\n- Firewall rules and network segmentation\n- Encryption at rest and in transit\n- Security monitoring and alerting configs\n\n**Operational Evidence**:\n- Change management logs\n- Backup and recovery procedures\n- Disaster recovery testing results\n- System monitoring dashboards\n- Capacity planning documentation\n- Performance metrics\n\n**Policy and Procedure Evidence**:\n- Information security policy\n- Incident response plan\n- Business continuity plan\n- Vendor management procedures\n- Employee onboarding/offboarding\n- Security awareness training records\n\n**System Evidence**:\n- System architecture diagrams\n- Data flow diagrams\n- Asset inventory\n- Software bill of materials (SBOM)\n- Configuration management database\n\n### 3. Control Effectiveness Testing\n\nFor each control point:\n- Verify control design (is it properly designed?)\n- Test operating effectiveness (is it working as intended?)\n- Document test results with screenshots/logs\n- Identify gaps or weaknesses\n- Recommend remediation actions\n\n### 4. Compliance Gap Analysis\n\nCompare current state against SOC 2 requirements:\n- Missing controls (critical gaps)\n- Partially implemented controls (needs improvement)\n- Improperly documented controls (evidence gaps)\n- Ineffective controls (design or operating failures)\n\n### 5. Evidence Documentation\n\nOrganize evidence by Trust Service Criteria:\n```\n{baseDir}/soc2-audit/\n‚îú‚îÄ‚îÄ CC1-control-environment/\n‚îÇ   ‚îú‚îÄ‚îÄ org-chart.pdf\n‚îÇ   ‚îú‚îÄ‚îÄ security-policy.md\n‚îÇ   ‚îî‚îÄ‚îÄ training-records.xlsx\n‚îú‚îÄ‚îÄ CC6-access-controls/\n‚îÇ   ‚îú‚îÄ‚îÄ iam-policies.json\n‚îÇ   ‚îú‚îÄ‚îÄ mfa-config.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ access-review-logs.csv\n‚îú‚îÄ‚îÄ CC7-system-operations/\n‚îÇ   ‚îú‚îÄ‚îÄ monitoring-configs/\n‚îÇ   ‚îú‚îÄ‚îÄ backup-procedures.md\n‚îÇ   ‚îî‚îÄ‚îÄ incident-logs/\n‚îî‚îÄ‚îÄ readiness-report.md\n```\n\n### 6. Generate Readiness Report\n\nCreate comprehensive SOC 2 readiness assessment with:\n- Executive summary with readiness score\n- Control-by-control assessment\n- Gap analysis with severity ratings\n- Remediation roadmap with timelines\n- Evidence collection checklist\n- Auditor interview preparation guide\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: SOC 2 readiness report saved to {baseDir}/soc2-audit/readiness-report-YYYYMMDD.md\n\n**Report Structure**:\n```\n# SOC 2 Readiness Assessment\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\nAssessment Date: 2024-01-15\nOrganization: TechCorp Inc.\nAudit Type: SOC 2 Type II (Security + Availability)\n\n## Executive Summary\n- Overall Readiness: 75% (Needs Improvement)\n- Controls Implemented: 28/40 (70%)\n- Critical Gaps: 3\n- High Priority Items: 8\n- Estimated Remediation Time: 8-12 weeks\n\n## Readiness by Trust Service Category\n\n### CC1: Control Environment (80%)\n‚úÖ Implemented (4):\n- Organizational structure documented\n- Security policy established\n- Risk assessment framework in place\n- Board oversight of security\n\n‚ö†Ô∏è Gaps (1):\n- Security role and responsibility matrix incomplete\n\n### CC6: Logical and Physical Access Controls (60%)\n‚úÖ Implemented (5):\n- Multi-factor authentication enabled\n- Role-based access control (RBAC) implemented\n- Password policy enforced\n- Access review process established\n- Visitor access controls in place\n\n‚ùå Critical Gaps (2):\n- No automated user deprovisioning\n- Privileged access not logged/monitored\n\n‚ö†Ô∏è High Priority (3):\n- Access logs retention < 1 year\n- No formal access request workflow\n- Physical security cameras not monitored 24/7\n\n### CC7: System Operations (70%)\n[Similar breakdown...]\n\n## Critical Gaps Requiring Immediate Action\n\n### 1. Automated User Deprovisioning (CC6.2)\n**Current State**: Manual offboarding process\n**Risk**: Terminated employees retain system access\n**Evidence**: {baseDir}/hr/offboarding-checklist.pdf\n**Remediation**:\n- Implement automated deprovisioning tied to HR system\n- Set up alerts for access not removed within 24 hours\n- Estimated effort: 2-3 weeks\n**Priority**: CRITICAL\n\n### 2. Privileged Access Monitoring (CC6.7)\n**Current State**: No logging of admin actions\n**Risk**: Insider threats undetected\n**Remediation**:\n- Enable audit logging for all admin accounts\n- Set up SIEM alerts for privileged actions\n- Implement session recording for production access\n**Priority**: CRITICAL\n\n### 3. Disaster Recovery Testing (CC7.4)\n**Current State**: DR plan exists but never tested\n**Risk**: Recovery time objectives may not be achievable\n**Remediation**:\n- Schedule quarterly DR tests\n- Document test results\n- Update plan based on test findings\n**Priority**: CRITICAL\n\n## Evidence Collection Status\n\n| Control | Evidence Type | Status | Location |\n|---------|--------------|--------|----------|\n| CC1.1 | Org Chart | ‚úÖ Complete | {baseDir}/soc2-audit/CC1/ |\n| CC6.1 | MFA Config | ‚úÖ Complete | {baseDir}/soc2-audit/CC6/ |\n| CC6.2 | Offboarding Logs | ‚ùå Missing | N/A |\n| CC7.1 | Monitoring Dashboards | ‚ö†Ô∏è Partial | Need 90-day history |\n\n## Remediation Roadmap\n\n**Weeks 1-2 (Critical Fixes)**:\n- Implement automated deprovisioning\n- Enable privileged access monitoring\n- Begin DR test planning\n\n**Weeks 3-6 (High Priority)**:\n- Extend log retention to 1 year\n- Implement access request workflow\n- Complete initial DR test\n\n**Weeks 7-12 (Medium Priority)**:\n- Enhance physical security monitoring\n- Improve change management documentation\n- Complete second DR test cycle\n\n## Auditor Preparation\n\n**Key Interview Topics**:\n- Control environment and tone from the top\n- Incident response capabilities\n- Change management process\n- Access control procedures\n- Monitoring and alerting effectiveness\n\n**Suggested Interviewees**:\n- CTO/CISO (control environment)\n- Security Engineer (technical controls)\n- HR Manager (employee lifecycle)\n- Operations Lead (monitoring, DR)\n\n## Next Steps\n\n1. Review and approve remediation roadmap\n2. Assign owners to each gap remediation\n3. Begin evidence collection for completed controls\n4. Schedule monthly progress reviews\n5. Engage SOC 2 auditor for scoping discussion\n```\n\n**Secondary Outputs**:\n- Evidence collection checklist (Excel/CSV)\n- Control testing templates\n- Auditor questionnaire responses\n- Gap tracking dashboard (JSON for dashboarding tools)\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Missing Evidence Files**\n   - Error: \"Cannot locate security policy in {baseDir}/docs/\"\n   - Resolution: Request document locations from user\n   - Fallback: Mark as evidence gap in report\n\n2. **Incomplete Access Logs**\n   - Error: \"Log retention < SOC 2 requirement (1 year)\"\n   - Resolution: Note current retention period, flag as gap\n   - Remediation: Extend retention, backfill if possible\n\n3. **Undocumented Procedures**\n   - Error: \"No incident response playbook found\"\n   - Resolution: Flag as critical gap requiring documentation\n   - Assistance: Provide template for creating procedure\n\n4. **Cloud Provider Access Required**\n   - Error: \"Cannot assess AWS controls without API access\"\n   - Resolution: Request CloudTrail exports or console screenshots\n   - Alternative: Provide manual checklist for cloud controls\n\n5. **Multiple Environments Not Distinguished**\n   - Error: \"Production and dev configs mixed in {baseDir}/\"\n   - Resolution: Request environment separation or clear labeling\n   - Risk: May audit wrong environment\n\n## Examples\n\n- \"Prepare a SOC 2 evidence checklist for Security + Availability for our production systems.\"\n- \"Generate a readiness gap analysis and remediation backlog for SOC 2.\"\n\n## Resources\n\n**SOC 2 Framework References**:\n- AICPA Trust Service Criteria: https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/trustdataintegritytaskforce.html\n- SOC 2 Compliance Checklist: https://secureframe.com/hub/soc-2/checklist\n\n**Control Implementation Guides**:\n- CIS Controls: https://www.cisecurity.org/controls/\n- NIST Cybersecurity Framework: https://www.nist.gov/cyberframework\n\n**Compliance Automation Tools**:\n- Drata: SOC 2 compliance automation\n- Vanta: Continuous compliance monitoring\n- Secureframe: Evidence collection platform\n\n**Template Documents**:\n- Information Security Policy: {baseDir}/templates/security-policy-template.md\n- Incident Response Plan: {baseDir}/templates/incident-response-template.md\n- Risk Assessment Template: {baseDir}/templates/risk-assessment-template.xlsx\n\n**Auditor Resources**:\n- Find SOC 2 auditors: https://www.aicpa.org/\n- SOC 2 vs ISO 27001 comparison\n- Type I vs Type II audit differences\n\n**Evidence Examples**:\n- Sample SOC 2 control matrix\n- Evidence collection best practices\n- Auditor interview preparation guide",
      "parentPlugin": {
        "name": "soc2-audit-helper",
        "category": "security",
        "path": "plugins/security/soc2-audit-helper",
        "version": "1.0.0",
        "description": "Assist with SOC2 audit preparation"
      },
      "filePath": "plugins/security/soc2-audit-helper/skills/assisting-with-soc2-audit-preparation/SKILL.md"
    },
    {
      "slug": "auditing-access-control",
      "name": "auditing-access-control",
      "description": "Audit access control implementations for security vulnerabilities and misconfigurations. Use when reviewing authentication and authorization. Trigger with 'audit access control', 'check permissions', or 'validate authorization'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Access Control Auditor\n\nThis skill provides automated assistance for access control auditor tasks.\n\n## Overview\n\nThis skill leverages the access-control-auditor plugin to perform comprehensive audits of access control configurations. It helps identify potential security risks associated with overly permissive access, misconfigured permissions, and non-compliance with security policies.\n\n## How It Works\n\n1. **Analyze Request**: Claude identifies the user's intent to audit access control.\n2. **Invoke Plugin**: The access-control-auditor plugin is activated.\n3. **Execute Audit**: The plugin analyzes the specified access control configuration (e.g., IAM policies, ACLs).\n4. **Report Findings**: The plugin generates a report highlighting potential vulnerabilities and misconfigurations.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Audit IAM policies in a cloud environment.\n- Review access control lists (ACLs) for network resources.\n- Assess user permissions in an application.\n- Identify potential privilege escalation paths.\n- Ensure compliance with access control security policies.\n\n## Examples\n\n### Example 1: Auditing AWS IAM Policies\n\nUser request: \"Audit the AWS IAM policies in my account for overly permissive access.\"\n\nThe skill will:\n1. Invoke the access-control-auditor plugin, specifying the AWS account and IAM policies as the target.\n2. Generate a report identifying IAM policies that grant overly broad permissions or violate security best practices.\n\n### Example 2: Reviewing Network ACLs\n\nUser request: \"Review the network ACLs for my VPC to identify any potential security vulnerabilities.\"\n\nThe skill will:\n1. Activate the access-control-auditor plugin, specifying the VPC and network ACLs as the target.\n2. Produce a report highlighting ACL rules that allow unauthorized access or expose the VPC to unnecessary risks.\n\n## Best Practices\n\n- **Scope Definition**: Clearly define the scope of the audit (e.g., specific IAM roles, network segments, applications).\n- **Contextual Information**: Provide contextual information about the environment being audited (e.g., security policies, compliance requirements).\n- **Remediation Guidance**: Use the audit findings to develop and implement remediation strategies to address identified vulnerabilities.\n\n## Integration\n\nThis skill can be integrated with other security plugins to provide a more comprehensive security assessment. For example, it can be combined with a vulnerability scanner to identify vulnerabilities that could be exploited due to access control misconfigurations. It can also be integrated with compliance tools to ensure adherence to regulatory requirements.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "access-control-auditor",
        "category": "security",
        "path": "plugins/security/access-control-auditor",
        "version": "1.0.0",
        "description": "Audit access control implementations"
      },
      "filePath": "plugins/security/access-control-auditor/skills/auditing-access-control/SKILL.md"
    },
    {
      "slug": "auditing-wallet-security",
      "name": "auditing-wallet-security",
      "description": "Review crypto wallet security including private key management and transaction signing. Use when auditing wallet security practices. Trigger with phrases like \"audit wallet\", \"check security\", or \"verify signatures\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:wallet-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Wallet Security Auditor\n\nThis skill provides automated assistance for wallet security auditor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:wallet-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "wallet-security-auditor",
        "category": "crypto",
        "path": "plugins/crypto/wallet-security-auditor",
        "version": "1.0.0",
        "description": "Crypto wallet security auditor for reviewing wallet implementations, key management, signing flows, and common vulnerability patterns."
      },
      "filePath": "plugins/crypto/wallet-security-auditor/skills/auditing-wallet-security/SKILL.md"
    },
    {
      "slug": "automating-api-testing",
      "name": "automating-api-testing",
      "description": "Automate API endpoint testing including request generation, validation, and comprehensive test coverage for REST and GraphQL APIs. Use when testing API contracts, validating OpenAPI specifications, or ensuring endpoint reliability. Trigger with phrases like \"test the API\", \"generate API tests\", or \"validate API contracts\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:api-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Api Test Automation\n\nThis skill provides automated assistance for api test automation tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API definition files (OpenAPI/Swagger, GraphQL schema, or endpoint documentation)\n- Base URL for the API service (development, staging, or test environment)\n- Authentication credentials or API keys if endpoints require authorization\n- Testing framework installed (Jest, Mocha, Supertest, or equivalent)\n- Network connectivity to the target API service\n\n## Instructions\n\n### Step 1: Analyze API Definition\nExamine the API structure and endpoints:\n1. Use Read tool to load OpenAPI/Swagger specifications from {baseDir}/api-specs/\n2. Identify all available endpoints, HTTP methods, and request/response schemas\n3. Document authentication requirements and rate limiting constraints\n4. Note any deprecated endpoints or breaking changes\n\n### Step 2: Generate Test Cases\nCreate comprehensive test coverage:\n1. Generate CRUD operation tests (Create, Read, Update, Delete)\n2. Add authentication flow tests (login, token refresh, logout)\n3. Include edge case tests (invalid inputs, boundary conditions, malformed requests)\n4. Create contract validation tests against OpenAPI schemas\n5. Add performance tests for critical endpoints\n\n### Step 3: Execute Test Suite\nRun automated API tests:\n1. Use Bash(test:api-*) to execute test framework with generated test files\n2. Validate HTTP status codes match expected responses (200, 201, 400, 401, 404, 500)\n3. Verify response headers (Content-Type, Cache-Control, CORS headers)\n4. Validate response body structure against schemas using JSON Schema validation\n5. Test authentication token expiration and renewal flows\n\n### Step 4: Generate Test Report\nDocument results in {baseDir}/test-reports/api/:\n- Test execution summary with pass/fail counts\n- Coverage metrics by endpoint and HTTP method\n- Failed test details with request/response payloads\n- Performance benchmarks (response times, throughput)\n- Contract violation details if schema mismatches detected\n\n## Output\n\nThe skill generates structured API test artifacts:\n\n### Test Suite Files\nGenerated test files organized by resource:\n- `{baseDir}/tests/api/users.test.js` - User endpoint tests\n- `{baseDir}/tests/api/products.test.js` - Product endpoint tests\n- `{baseDir}/tests/api/auth.test.js` - Authentication flow tests\n\n### Test Coverage Report\n- Endpoint coverage percentage (target: 100% for critical paths)\n- HTTP method coverage per endpoint (GET, POST, PUT, PATCH, DELETE)\n- Authentication scenario coverage (authenticated vs. unauthenticated)\n- Error condition coverage (4xx and 5xx responses)\n\n### Contract Validation Results\n- OpenAPI schema compliance status for each endpoint\n- Breaking changes detected between specification versions\n- Undocumented endpoints or parameters found in implementation\n- Response schema violations with diff details\n\n### Performance Metrics\n- Average response time per endpoint\n- 95th and 99th percentile latencies\n- Requests per second throughput measurements\n- Timeout occurrences and slow endpoint identification\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Connection Refused**\n- Error: Cannot connect to API service at specified base URL\n- Solution: Verify service is running using Bash(test:api-healthcheck); check network connectivity and firewall rules\n\n**Authentication Failures**\n- Error: 401 Unauthorized or 403 Forbidden on protected endpoints\n- Solution: Verify API keys are valid and not expired; ensure bearer token format is correct; check scope permissions\n\n**Schema Validation Errors**\n- Error: Response does not match OpenAPI schema definition\n- Solution: Update OpenAPI specification to match actual API behavior; file bug if API implementation is incorrect\n\n**Timeout Errors**\n- Error: Request exceeded configured timeout threshold\n- Solution: Increase timeout for slow endpoints; investigate performance issues on API server; add retry logic for transient failures\n\n## Resources\n\n### API Testing Frameworks\n- Supertest for Node.js HTTP assertion testing\n- REST-assured for Java API testing\n- Postman/Newman for collection-based API testing\n- Pact for contract testing and consumer-driven contracts\n\n### Validation Libraries\n- Ajv for JSON Schema validation\n- OpenAPI Schema Validator for spec compliance\n- Joi for Node.js schema validation\n- GraphQL Schema validation tools\n\n### Best Practices\n- Test against non-production environments to avoid data corruption\n- Use test data factories to create consistent test fixtures\n- Implement proper test isolation with database cleanup between tests\n- Version control test suites alongside API specifications\n- Run tests in CI/CD pipeline for continuous validation\n\n## Overview\n\n\nThis skill provides automated assistance for api test automation tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-test-automation",
        "category": "testing",
        "path": "plugins/testing/api-test-automation",
        "version": "1.0.0",
        "description": "Automated API endpoint testing with request generation, validation, and comprehensive test coverage"
      },
      "filePath": "plugins/testing/api-test-automation/skills/automating-api-testing/SKILL.md"
    },
    {
      "slug": "automating-database-backups",
      "name": "automating-database-backups",
      "description": "Use when you need to automate database backup processes with scheduling and encryption. This skill creates backup scripts for PostgreSQL, MySQL, MongoDB, and SQLite with compression. Trigger with phrases like \"automate database backups\", \"schedule database dumps\", \"create backup scripts\", or \"implement disaster recovery for database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(pg_dump:*), Bash(mysqldump:*), Bash(mongodump:*), Bash(cron:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Backup Automator\n\nThis skill provides automated assistance for database backup automator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Database credentials with backup permissions (SELECT on all tables)\n- Sufficient disk space for backup files (estimate 2-3x database size with compression)\n- Cron or task scheduler access for automated scheduling\n- Backup destination storage (local disk, NFS, S3, GCS, Azure Blob)\n- Encryption tools installed (gpg, openssl) for secure backups\n- Test database available for restore validation\n\n## Instructions\n\n### Step 1: Assess Backup Requirements\n1. Identify database type (PostgreSQL, MySQL, MongoDB, SQLite)\n2. Determine backup frequency (hourly, daily, weekly, monthly)\n3. Define retention policy (how long to keep backups)\n4. Calculate expected backup size and storage needs\n5. Document RTO (Recovery Time Objective) and RPO (Recovery Point Objective)\n\n### Step 2: Design Backup Strategy\n1. Choose backup type: full, incremental, or differential\n2. Select backup destination (local, network storage, cloud)\n3. Plan backup scheduling to avoid peak usage times\n4. Define backup naming convention with timestamps\n5. Determine compression and encryption requirements\n\n### Step 3: Generate Backup Scripts\n1. Create database-specific backup command (pg_dump, mysqldump, mongodump)\n2. Add compression using gzip or zstd for storage efficiency\n3. Implement encryption using gpg or openssl for security\n4. Add error handling and logging to backup script\n5. Include backup verification (checksum, test restore)\n\n### Step 4: Configure Backup Schedule\n1. Create cron job entry for automated execution\n2. Set appropriate schedule based on backup frequency\n3. Configure environment variables for credentials\n4. Set up log rotation for backup logs\n5. Test manual execution before enabling automation\n\n### Step 5: Implement Retention Policy\n1. Create cleanup script to remove old backups\n2. Implement tiered retention (daily 7 days, weekly 4 weeks, monthly 12 months)\n3. Schedule retention cleanup after backup completion\n4. Add safeguards to prevent accidental deletion of recent backups\n5. Log all backup deletions for audit trail\n\n### Step 6: Create Restore Procedures\n1. Document step-by-step restore process\n2. Create restore scripts for each database type\n3. Include procedures for point-in-time recovery\n4. Test restore process on non-production environment\n5. Document restore time estimates and validation steps\n\n## Output\n\nThis skill produces:\n\n**Backup Scripts**: Shell scripts for database dumps with compression and encryption\n\n**Cron Configurations**: Crontab entries for automated backup scheduling\n\n**Retention Scripts**: Automated cleanup scripts implementing retention policies\n\n**Restore Procedures**: Step-by-step documentation and scripts for database restoration\n\n**Monitoring Configuration**: Log file locations and success/failure notification setup\n\n## Error Handling\n\n**Backup Failures**:\n- Check database connectivity and credentials\n- Verify sufficient disk space for backup files\n- Review database logs for lock or permission issues\n- Implement retry logic with exponential backoff\n- Send alerts on backup failures\n\n**Insufficient Disk Space**:\n- Monitor disk usage before backup execution\n- Implement pre-backup cleanup of old backups\n- Use incremental backups to reduce space requirements\n- Compress backups more aggressively\n- Move backups to remote storage immediately after creation\n\n**Encryption Errors**:\n- Verify encryption tools (gpg, openssl) are installed\n- Check encryption key availability and permissions\n- Test encryption/decryption process manually\n- Document key management procedures\n- Store encryption keys securely separate from backups\n\n**Schedule Conflicts**:\n- Ensure only one backup runs at a time (use lock files)\n- Adjust backup schedule to avoid peak database usage\n- Implement backup queuing for multiple databases\n- Monitor backup duration and adjust schedule if needed\n- Alert if backup duration exceeds acceptable window\n\n## Resources\n\n**Backup Script Templates**:\n- PostgreSQL: `{baseDir}/templates/backup-scripts/postgresql-backup.sh`\n- MySQL: `{baseDir}/templates/backup-scripts/mysql-backup.sh`\n- MongoDB: `{baseDir}/templates/backup-scripts/mongodb-backup.sh`\n- SQLite: `{baseDir}/templates/backup-scripts/sqlite-backup.sh`\n\n**Restore Procedures**: `{baseDir}/docs/restore-procedures/`\n- Point-in-time recovery\n- Full database restore\n- Selective table restore\n- Cross-server migration\n\n**Retention Policy Templates**: `{baseDir}/templates/retention-policies.yaml`\n**Cron Job Examples**: `{baseDir}/examples/crontab-entries.txt`\n**Monitoring Scripts**: `{baseDir}/scripts/backup-monitoring.sh`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-backup-automator",
        "category": "database",
        "path": "plugins/database/database-backup-automator",
        "version": "1.0.0",
        "description": "Automate database backups with scheduling, compression, encryption, and restore procedures"
      },
      "filePath": "plugins/database/database-backup-automator/skills/automating-database-backups/SKILL.md"
    },
    {
      "slug": "backtesting-trading-strategies",
      "name": "backtesting-trading-strategies",
      "description": "Backtest crypto trading strategies against historical data with performance metrics. Use when validating trading strategies with historical data. Trigger with phrases like \"backtest strategy\", \"test trading signals\", or \"validate approach\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:backtest-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Trading Strategy Backtester\n\nThis skill provides automated assistance for trading strategy backtester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:backtest-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "trading-strategy-backtester",
        "category": "crypto",
        "path": "plugins/crypto/trading-strategy-backtester",
        "version": "1.0.0",
        "description": "Backtest trading strategies with historical data, performance metrics, and risk analysis"
      },
      "filePath": "plugins/crypto/trading-strategy-backtester/skills/backtesting-trading-strategies/SKILL.md"
    },
    {
      "slug": "building-api-authentication",
      "name": "building-api-authentication",
      "description": "Build secure API authentication systems with OAuth2, JWT, API keys, and session management. Use when implementing secure authentication flows. Trigger with phrases like \"build authentication\", \"add API auth\", or \"secure the API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:auth-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Authentication Builder\n\nThis skill provides automated assistance for api authentication builder tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:auth-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api authentication builder tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-authentication-builder",
        "category": "api-development",
        "path": "plugins/api-development/api-authentication-builder",
        "version": "1.0.0",
        "description": "Build authentication systems with JWT, OAuth2, and API keys"
      },
      "filePath": "plugins/api-development/api-authentication-builder/skills/building-api-authentication/SKILL.md"
    },
    {
      "slug": "building-api-gateway",
      "name": "building-api-gateway",
      "description": "Create API gateways with routing, load balancing, rate limiting, and authentication. Use when routing and managing multiple API services. Trigger with phrases like \"build API gateway\", \"create API router\", or \"setup API gateway\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:gateway-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Gateway Builder\n\nThis skill provides automated assistance for api gateway builder tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:gateway-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api gateway builder tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-gateway-builder",
        "category": "api-development",
        "path": "plugins/api-development/api-gateway-builder",
        "version": "1.0.0",
        "description": "Build API gateway with routing, authentication, and rate limiting"
      },
      "filePath": "plugins/api-development/api-gateway-builder/skills/building-api-gateway/SKILL.md"
    },
    {
      "slug": "building-automl-pipelines",
      "name": "building-automl-pipelines",
      "description": "Build automated machine learning pipelines with feature engineering, model selection, and hyperparameter tuning. Use when automating ML workflows from data preparation through model deployment. Trigger with phrases like \"build automl pipeline\", \"automate ml workflow\", or \"create automated training pipeline\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(python:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Automl Pipeline Builder\n\nThis skill provides automated assistance for automl pipeline builder tasks.\n\n## Overview\n\nBuild an end-to-end AutoML pipeline: data checks, feature preprocessing, model search/tuning, evaluation, and exportable deployment artifacts. Use this when you want repeatable training runs with a clear budget (time/compute) and a structured output (configs, reports, and a runnable pipeline).\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Python environment with AutoML libraries (Auto-sklearn, TPOT, H2O AutoML, or PyCaret)\n- Training dataset in accessible format (CSV, Parquet, or database)\n- Understanding of problem type (classification, regression, time-series)\n- Sufficient computational resources for automated search\n- Knowledge of evaluation metrics appropriate for task\n- Target variable and feature columns clearly defined\n\n## Instructions\n\n### Step 1: Define Pipeline Requirements\nSpecify the machine learning task and constraints:\n1. Identify problem type (binary/multi-class classification, regression, etc.)\n2. Define evaluation metrics (accuracy, F1, RMSE, etc.)\n3. Set time and resource budgets for AutoML search\n4. Specify feature types and preprocessing needs\n5. Determine model interpretability requirements\n\n### Step 2: Prepare Data Infrastructure\nSet up data access and preprocessing:\n1. Load training data using Read tool\n2. Perform initial data quality assessment\n3. Configure train/validation/test split strategy\n4. Define feature engineering transformations\n5. Set up data validation checks\n\n### Step 3: Configure AutoML Pipeline\nBuild the automated pipeline configuration:\n- Select AutoML framework based on requirements\n- Define search space for algorithms (random forest, XGBoost, neural networks, etc.)\n- Configure feature preprocessing steps (scaling, encoding, imputation)\n- Set hyperparameter tuning strategy (Bayesian optimization, random search, grid search)\n- Establish early stopping criteria and timeout limits\n\n### Step 4: Execute Pipeline Training\nRun the automated training process:\n1. Initialize AutoML pipeline with configuration\n2. Execute automated feature engineering\n3. Perform model selection across algorithm families\n4. Conduct hyperparameter optimization for top models\n5. Evaluate models using cross-validation\n\n### Step 5: Analyze and Export Results\nEvaluate pipeline performance and prepare for deployment:\n- Compare model performances across metrics\n- Extract best model and configuration\n- Generate feature importance analysis\n- Create model performance visualizations\n- Export trained pipeline for deployment\n\n## Examples\n\n**Example: Tabular classification with a 1-hour budget**\n```json\n{\n  \"task_type\": \"classification\",\n  \"time_budget_seconds\": 3600,\n  \"algorithms\": [\"rf\", \"xgboost\", \"catboost\"],\n  \"preprocessing\": [\"scaling\", \"encoding\"],\n  \"tuning_strategy\": \"bayesian\",\n  \"cv_folds\": 5\n}\n```\n\n## Output\n\nThe skill generates comprehensive AutoML pipeline artifacts:\n\n### Pipeline Configuration Files\nExample `automl_config.py`:\n```python\nTASK_TYPE = \"classification\"\nTIME_BUDGET_SECONDS = 3600\nALGORITHMS = [\"rf\", \"xgboost\", \"catboost\"]\nPREPROCESSING = [\"scaling\", \"encoding\"]\nTUNING_STRATEGY = \"bayesian\"\nCV_FOLDS = 5\n```\n\n### Pipeline Code\n- Complete Python implementation of AutoML pipeline\n- Data loading and preprocessing functions\n- Feature engineering transformations\n- Model training and evaluation logic\n- Hyperparameter search configuration\n\n### Model Performance Report\n- Best model architecture and hyperparameters\n- Cross-validation scores with confidence intervals\n- Feature importance rankings\n- Confusion matrix or residual plots\n- ROC curves and precision-recall curves (for classification)\n\n### Training Artifacts\n- Serialized best model file (pickle, joblib, or ONNX)\n- Feature preprocessing pipeline\n- Training history and search logs\n- Model performance metrics on test set\n- Documentation for model deployment\n\n### Deployment Package\n- Prediction API code for serving model\n- Input validation and preprocessing scripts\n- Model loading and inference functions\n- Example usage documentation\n- Requirements file with dependencies\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Insufficient Training Time**\n- Error: AutoML search terminated before finding good model\n- Solution: Increase time budget, reduce search space, or use faster algorithms\n\n**Memory Exhaustion**\n- Error: Out of memory during pipeline training\n- Solution: Reduce dataset size through sampling, use incremental learning, or simplify feature engineering\n\n**Poor Model Performance**\n- Error: Best model accuracy below acceptable threshold\n- Solution: Collect more data, engineer better features, expand algorithm search space, or adjust evaluation metrics\n\n**Feature Engineering Failures**\n- Error: Automated feature transformations produce invalid values\n- Solution: Add data validation checks, handle missing values explicitly, restrict transformation types\n\n**Model Convergence Issues**\n- Error: Optimization fails to converge for certain algorithms\n- Solution: Adjust hyperparameter ranges, increase iteration limits, or exclude problematic algorithms\n\n## Resources\n\n### AutoML Frameworks\n- **Auto-sklearn**: Automated scikit-learn pipeline construction with metalearning\n- **TPOT**: Genetic programming for pipeline optimization\n- **H2O AutoML**: Scalable AutoML with ensemble methods\n- **PyCaret**: Low-code ML library with automated workflows\n\n### Feature Engineering\n- Automated feature selection techniques\n- Categorical encoding strategies (one-hot, target, ordinal)\n- Numerical transformation methods (scaling, binning, polynomial features)\n- Time-series feature extraction\n\n### Hyperparameter Optimization\n- Bayesian optimization with Gaussian processes\n- Random search and grid search strategies\n- Hyperband and successive halving algorithms\n- Multi-objective optimization for multiple metrics\n\n### Evaluation Strategies\n- Cross-validation techniques (k-fold, stratified, time-series)\n- Evaluation metrics selection guide\n- Model ensembling and stacking approaches\n- Bias-variance tradeoff analysis\n\n### Best Practices\n- Start with baseline models before AutoML\n- Balance automation with domain knowledge\n- Monitor resource consumption during search\n- Validate model performance on holdout data\n- Document pipeline decisions for reproducibility",
      "parentPlugin": {
        "name": "automl-pipeline-builder",
        "category": "ai-ml",
        "path": "plugins/ai-ml/automl-pipeline-builder",
        "version": "1.0.0",
        "description": "Build AutoML pipelines"
      },
      "filePath": "plugins/ai-ml/automl-pipeline-builder/skills/building-automl-pipelines/SKILL.md"
    },
    {
      "slug": "building-cicd-pipelines",
      "name": "building-cicd-pipelines",
      "description": "Use when you need to work with deployment and CI/CD. This skill provides deployment automation and pipeline orchestration with comprehensive guidance and automation. Trigger with phrases like \"deploy application\", \"create pipeline\", or \"automate deployment\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*), Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Ci Cd Pipeline Builder\n\nThis skill provides automated assistance for ci cd pipeline builder tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/ci-cd-pipeline-builder/`\n\n**Documentation and Guides**: `{baseDir}/docs/ci-cd-pipeline-builder/`\n\n**Example Scripts and Code**: `{baseDir}/examples/ci-cd-pipeline-builder/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/ci-cd-pipeline-builder-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/ci-cd-pipeline-builder-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/ci-cd-pipeline-builder-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "ci-cd-pipeline-builder",
        "category": "devops",
        "path": "plugins/devops/ci-cd-pipeline-builder",
        "version": "1.0.0",
        "description": "Build CI/CD pipelines for GitHub Actions, GitLab CI, Jenkins, and more"
      },
      "filePath": "plugins/devops/ci-cd-pipeline-builder/skills/building-cicd-pipelines/SKILL.md"
    },
    {
      "slug": "building-classification-models",
      "name": "building-classification-models",
      "description": "Build and evaluate classification models for supervised learning tasks with labeled data. Use when requesting \"build a classifier\", \"create classification model\", or \"train classifier\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Classification Model Builder\n\nThis skill provides automated assistance for classification model builder tasks.\n\n## Overview\n\nThis skill empowers Claude to efficiently build and deploy classification models. It automates the process of model selection, training, and evaluation, providing users with a robust and reliable classification solution. The skill also provides insights into model performance and suggests potential improvements.\n\n## How It Works\n\n1. **Context Analysis**: Claude analyzes the user's request, identifying the dataset, target variable, and any specific requirements for the classification model.\n2. **Model Generation**: The skill utilizes the classification-model-builder plugin to generate code for training a classification model based on the identified dataset and requirements. This includes data preprocessing, feature selection, model selection, and hyperparameter tuning.\n3. **Evaluation and Reporting**: The generated model is trained and evaluated using appropriate metrics (e.g., accuracy, precision, recall, F1-score). Performance metrics and insights are then provided to the user.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Build a classification model from a given dataset.\n- Train a classifier to predict categorical outcomes.\n- Evaluate the performance of a classification model.\n\n## Examples\n\n### Example 1: Building a Spam Classifier\n\nUser request: \"Build a classifier to detect spam emails using this dataset.\"\n\nThe skill will:\n1. Analyze the provided email dataset to identify features and the target variable (spam/not spam).\n2. Generate Python code using the classification-model-builder plugin to train a spam classification model, including data cleaning, feature extraction, and model selection.\n\n### Example 2: Predicting Customer Churn\n\nUser request: \"Create a classification model to predict customer churn using customer data.\"\n\nThe skill will:\n1. Analyze the customer data to identify relevant features and the churn status.\n2. Generate code to build a classification model for churn prediction, including data validation, model training, and performance reporting.\n\n## Best Practices\n\n- **Data Quality**: Ensure the input data is clean and preprocessed before training the model.\n- **Model Selection**: Choose the appropriate classification algorithm based on the characteristics of the data and the specific requirements of the task.\n- **Hyperparameter Tuning**: Optimize the model's hyperparameters to achieve the best possible performance.\n\n## Integration\n\nThis skill integrates with the classification-model-builder plugin to automate the model building process. It can also be used in conjunction with other plugins for data analysis and visualization.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "classification-model-builder",
        "category": "ai-ml",
        "path": "plugins/ai-ml/classification-model-builder",
        "version": "1.0.0",
        "description": "Build classification models"
      },
      "filePath": "plugins/ai-ml/classification-model-builder/skills/building-classification-models/SKILL.md"
    },
    {
      "slug": "building-gitops-workflows",
      "name": "building-gitops-workflows",
      "description": "Use when constructing GitOps workflows using ArgoCD or Flux. Trigger with phrases like \"create GitOps workflow\", \"setup ArgoCD\", \"configure Flux\", or \"automate Kubernetes deployments\". Generates production-ready configurations, implements best practices, and ensures security-first approach for continuous deployment. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(kubectl:*), Bash(git:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Gitops Workflow Builder\n\nThis skill provides automated assistance for gitops workflow builder tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Kubernetes cluster is accessible and kubectl is configured\n- Git repository is available for GitOps source\n- ArgoCD or Flux is installed on the cluster (or ready to install)\n- Appropriate RBAC permissions for GitOps operator\n- Network connectivity between cluster and Git repository\n\n## Instructions\n\n1. **Select GitOps Tool**: Determine whether to use ArgoCD or Flux based on requirements\n2. **Define Application Structure**: Establish repository layout with environment separation (dev/staging/prod)\n3. **Generate Manifests**: Create Application/Kustomization files pointing to Git sources\n4. **Configure Sync Policy**: Set automated or manual sync with self-heal and prune options\n5. **Implement RBAC**: Define service accounts and role bindings for GitOps operator\n6. **Set Up Monitoring**: Configure notifications and health checks for deployments\n7. **Validate Configuration**: Test sync behavior and verify reconciliation loops\n\n## Output\n\nGenerates GitOps workflow configurations including:\n\n**ArgoCD Application Manifest:**\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-name\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/repo\n    path: manifests/prod\n    targetRevision: main\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n```\n\n**Flux Kustomization:**\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: app-name\n  namespace: flux-system\nspec:\n  interval: 5m\n  path: ./manifests/prod\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: app-repo\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Sync Failures**\n- Error: \"ComparisonError: Failed to load target state\"\n- Solution: Verify Git repository URL, credentials, and target path exist\n\n**RBAC Permissions**\n- Error: \"User cannot create resource in API group\"\n- Solution: Grant GitOps service account appropriate cluster roles\n\n**Out of Sync State**\n- Warning: \"Application is OutOfSync\"\n- Solution: Enable automated sync or manually sync via UI/CLI\n\n**Git Authentication**\n- Error: \"Authentication failed for repository\"\n- Solution: Configure SSH keys or access tokens in {baseDir}/.git/config\n\n**Resource Conflicts**\n- Error: \"Resource already exists and is not managed by GitOps\"\n- Solution: Import existing resources or remove conflicting manual deployments\n\n## Resources\n\n- ArgoCD documentation: https://argo-cd.readthedocs.io/\n- Flux documentation: https://fluxcd.io/docs/\n- GitOps principles and patterns guide\n- Kubernetes manifest best practices\n- Repository structure templates in {baseDir}/gitops-examples/\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "gitops-workflow-builder",
        "category": "devops",
        "path": "plugins/devops/gitops-workflow-builder",
        "version": "1.0.0",
        "description": "Build GitOps workflows with ArgoCD and Flux"
      },
      "filePath": "plugins/devops/gitops-workflow-builder/skills/building-gitops-workflows/SKILL.md"
    },
    {
      "slug": "building-graphql-server",
      "name": "building-graphql-server",
      "description": "Build production-ready GraphQL servers with schema design, resolvers, and subscriptions. Use when building GraphQL APIs with schemas and resolvers. Trigger with phrases like \"build GraphQL API\", \"create GraphQL server\", or \"setup GraphQL\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:graphql-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Graphql Server Builder\n\nThis skill provides automated assistance for graphql server builder tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:graphql-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for graphql server builder tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "graphql-server-builder",
        "category": "api-development",
        "path": "plugins/api-development/graphql-server-builder",
        "version": "1.0.0",
        "description": "Build GraphQL servers with schema-first design, resolvers, and subscriptions"
      },
      "filePath": "plugins/api-development/graphql-server-builder/skills/building-graphql-server/SKILL.md"
    },
    {
      "slug": "building-neural-networks",
      "name": "building-neural-networks",
      "description": "This skill allows AI assistant to construct and configure neural network architectures using the neural-network-builder plugin. it should be used when the user requests the creation of a new neural network, modification of an existing one, or assistance... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Neural Network Builder\n\nThis skill provides automated assistance for neural network builder tasks.\n\n## Overview\n\nThis skill empowers Claude to design and implement neural networks tailored to specific tasks. It leverages the neural-network-builder plugin to automate the process of defining network architectures, configuring layers, and setting training parameters. This ensures efficient and accurate creation of neural network models.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude analyzes the user's request to understand the desired neural network architecture, task, and performance goals.\n2. **Generating Configuration**: Based on the analysis, Claude generates the appropriate configuration for the neural-network-builder plugin, specifying the layers, activation functions, and other relevant parameters.\n3. **Executing Build**: Claude executes the `build-nn` command, triggering the neural-network-builder plugin to construct the neural network based on the generated configuration.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new neural network architecture for a specific machine learning task.\n- Modify an existing neural network's layers, parameters, or training process.\n- Design a neural network using specific layer types, such as convolutional, recurrent, or transformer layers.\n\n## Examples\n\n### Example 1: Image Classification\n\nUser request: \"Build a convolutional neural network for image classification with three convolutional layers and two fully connected layers.\"\n\nThe skill will:\n1. Analyze the request and determine the required CNN architecture.\n2. Generate the configuration for the `build-nn` command, specifying the layer types, filter sizes, and activation functions.\n\n### Example 2: Text Generation\n\nUser request: \"Define an RNN architecture for text generation with LSTM cells and an embedding layer.\"\n\nThe skill will:\n1. Analyze the request and determine the required RNN architecture.\n2. Generate the configuration for the `build-nn` command, specifying the LSTM cell parameters, embedding dimension, and output layer.\n\n## Best Practices\n\n- **Layer Selection**: Choose appropriate layer types (e.g., convolutional, recurrent, transformer) based on the task and data characteristics.\n- **Parameter Tuning**: Experiment with different parameter values (e.g., learning rate, batch size, number of layers) to optimize performance.\n- **Regularization**: Implement regularization techniques (e.g., dropout, L1/L2 regularization) to prevent overfitting.\n\n## Integration\n\nThis skill integrates with the core Claude Code environment by utilizing the `build-nn` command provided by the neural-network-builder plugin. It can be combined with other skills for data preprocessing, model evaluation, and deployment.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "neural-network-builder",
        "category": "ai-ml",
        "path": "plugins/ai-ml/neural-network-builder",
        "version": "1.0.0",
        "description": "Build and configure neural network architectures"
      },
      "filePath": "plugins/ai-ml/neural-network-builder/skills/building-neural-networks/SKILL.md"
    },
    {
      "slug": "building-recommendation-systems",
      "name": "building-recommendation-systems",
      "description": "This skill empowers AI assistant to construct recommendation systems using collaborative filtering, content-based filtering, or hybrid approaches. it analyzes user preferences, item features, and interaction data to generate personalized recommendations... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Recommendation Engine\n\nThis skill provides automated assistance for recommendation engine tasks.\n\n## Overview\n\nThis skill enables Claude to design and implement recommendation systems tailored to specific datasets and use cases. It automates the process of selecting appropriate algorithms, preprocessing data, training models, and evaluating performance, ultimately providing users with a functional recommendation engine.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude identifies the type of recommendation needed (collaborative, content-based, hybrid), data availability, and performance goals.\n2. **Generating Code**: Claude generates Python code using relevant libraries (e.g., scikit-learn, TensorFlow, PyTorch) to build the recommendation model. This includes data loading, preprocessing, model training, and evaluation.\n3. **Implementing Best Practices**: The code incorporates best practices for recommendation system development, such as handling cold starts, addressing scalability, and mitigating bias.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Build a personalized movie recommendation system.\n- Create a product recommendation engine for an e-commerce platform.\n- Implement a content recommendation system for a news website.\n\n## Examples\n\n### Example 1: Personalized Movie Recommendations\n\nUser request: \"Build a movie recommendation system using collaborative filtering.\"\n\nThe skill will:\n1. Generate code to load and preprocess movie rating data.\n2. Implement a collaborative filtering algorithm (e.g., matrix factorization) to predict user preferences.\n\n### Example 2: E-commerce Product Recommendations\n\nUser request: \"Create a product recommendation engine for an online store, using content-based filtering.\"\n\nThe skill will:\n1. Generate code to extract features from product descriptions and user purchase history.\n2. Implement a content-based filtering algorithm to recommend similar products.\n\n## Best Practices\n\n- **Data Preprocessing**: Ensure data is properly cleaned and formatted before training the recommendation model.\n- **Model Evaluation**: Use appropriate metrics (e.g., precision, recall, NDCG) to evaluate the performance of the recommendation system.\n- **Scalability**: Design the recommendation system to handle large datasets and user bases efficiently.\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins to access data sources, deploy models, and monitor performance. For example, it can use data analysis plugins to extract features from raw data and deployment plugins to deploy the recommendation system to a production environment.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "recommendation-engine",
        "category": "ai-ml",
        "path": "plugins/ai-ml/recommendation-engine",
        "version": "1.0.0",
        "description": "Build recommendation systems and engines"
      },
      "filePath": "plugins/ai-ml/recommendation-engine/skills/building-recommendation-systems/SKILL.md"
    },
    {
      "slug": "building-terraform-modules",
      "name": "building-terraform-modules",
      "description": "This skill empowers AI assistant to build reusable terraform modules based on user specifications. it leverages the terraform-module-builder plugin to generate production-ready, well-documented terraform module code, incorporating best practices for sec... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Terraform Module Builder\n\nThis skill provides automated assistance for terraform module builder tasks.\n\n## Overview\n\nThis skill allows Claude to efficiently generate Terraform modules, streamlining infrastructure-as-code development. By utilizing the terraform-module-builder plugin, it ensures modules are production-ready, well-documented, and incorporate best practices.\n\n## How It Works\n\n1. **Receiving User Request**: Claude receives a request to create a Terraform module, including details about the module's purpose and desired features.\n2. **Generating Module Structure**: Claude invokes the terraform-module-builder plugin to create the basic file structure and configuration files for the module.\n3. **Customizing Module Content**: Claude uses the user's specifications to populate the module with variables, outputs, and resource definitions, ensuring best practices are followed.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new Terraform module from scratch.\n- Generate production-ready Terraform configuration files.\n- Implement infrastructure as code using Terraform modules.\n\n## Examples\n\n### Example 1: Creating a VPC Module\n\nUser request: \"Create a Terraform module for a VPC with public and private subnets, a NAT gateway, and appropriate security groups.\"\n\nThe skill will:\n1. Invoke the terraform-module-builder plugin to generate a basic VPC module structure.\n2. Populate the module with Terraform code to define the VPC, subnets, NAT gateway, and security groups based on best practices.\n\n### Example 2: Generating an S3 Bucket Module\n\nUser request: \"Generate a Terraform module for an S3 bucket with versioning enabled, encryption at rest, and a lifecycle policy for deleting objects after 30 days.\"\n\nThe skill will:\n1. Invoke the terraform-module-builder plugin to create a basic S3 bucket module structure.\n2. Populate the module with Terraform code to define the S3 bucket with the requested features (versioning, encryption, lifecycle policy).\n\n## Best Practices\n\n- **Documentation**: Ensure the generated Terraform module includes comprehensive documentation, explaining the module's purpose, inputs, and outputs.\n- **Security**: Implement security best practices, such as using least privilege principles and encrypting sensitive data.\n- **Modularity**: Design the Terraform module to be reusable and configurable, allowing it to be easily adapted to different environments.\n\n## Integration\n\nThis skill integrates seamlessly with other Claude Code plugins by providing a foundation for infrastructure provisioning. The generated Terraform modules can be used by other plugins to deploy and manage resources in various cloud environments.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "terraform-module-builder",
        "category": "devops",
        "path": "plugins/devops/terraform-module-builder",
        "version": "1.0.0",
        "description": "Build reusable Terraform modules"
      },
      "filePath": "plugins/devops/terraform-module-builder/skills/building-terraform-modules/SKILL.md"
    },
    {
      "slug": "building-websocket-server",
      "name": "building-websocket-server",
      "description": "Build scalable WebSocket servers for real-time bidirectional communication. Use when enabling real-time bidirectional communication. Trigger with phrases like \"build WebSocket server\", \"add real-time API\", or \"implement WebSocket\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:websocket-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Websocket Server Builder\n\nThis skill provides automated assistance for websocket server builder tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:websocket-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for websocket server builder tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "websocket-server-builder",
        "category": "api-development",
        "path": "plugins/api-development/websocket-server-builder",
        "version": "1.0.0",
        "description": "Build WebSocket servers for real-time bidirectional communication"
      },
      "filePath": "plugins/api-development/websocket-server-builder/skills/building-websocket-server/SKILL.md"
    },
    {
      "slug": "calculating-crypto-taxes",
      "name": "calculating-crypto-taxes",
      "description": "Calculate cryptocurrency tax obligations with cost basis tracking and jurisdiction rules. Use when calculating crypto tax obligations. Trigger with phrases like \"calculate crypto taxes\", \"compute tax liability\", or \"generate tax report\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:tax-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Crypto Tax Calculator\n\nThis skill provides automated assistance for crypto tax calculator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:tax-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "crypto-tax-calculator",
        "category": "crypto",
        "path": "plugins/crypto/crypto-tax-calculator",
        "version": "1.0.0",
        "description": "Calculate crypto taxes with FIFO/LIFO methods and generate tax reports"
      },
      "filePath": "plugins/crypto/crypto-tax-calculator/skills/calculating-crypto-taxes/SKILL.md"
    },
    {
      "slug": "changelog-orchestrator",
      "name": "changelog-orchestrator",
      "description": "Draft changelog PRs by collecting GitHub/Slack/Git changes, formatting with templates, running quality gates, and preparing a branch/PR. Use when generating weekly/monthly release notes or when the user asks to create a changelog from recent merges. Trigger with \"changelog weekly\", \"generate release notes\", \"draft changelog\", \"create changelog PR\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(git:*)",
        "Bash(gh:*)",
        "Bash(python:*)",
        "Bash(date:*)\""
      ],
      "version": "\"0.1.0\"",
      "author": "\"Mattyp <mattyp@claudecodeplugins.io>\"",
      "license": "\"MIT\"",
      "content": "# Changelog Orchestrator\n\nAutomate changelog generation with a predictable 6‚Äëphase workflow: fetch ‚Üí synthesize ‚Üí format ‚Üí quality gate ‚Üí PR ‚Üí handoff.\n\n## Overview\n\nThis skill turns raw repo activity (merged PRs, issues, commits, optional Slack updates) into a publishable changelog draft and prepares a branch/PR for review.\n\n## Prerequisites\n\n- A project config file at `.changelog-config.json` in the target repo.\n- Required environment variables set (at minimum `GITHUB_TOKEN` for GitHub source).\n- Git available in PATH; `gh` optional (used for PR creation if configured).\n\n## Instructions\n\n### Phase 1: Initialize & Load Config\n\n1. Read `.changelog-config.json` from the repo root.\n2. Validate it with `{baseDir}/scripts/validate_config.py`.\n3. Decide date range:\n   - Weekly mode: today minus 7 days ‚Üí today\n   - Custom mode: use provided `start_date`/`end_date`\n\n### Phase 2: Fetch Changelog Inputs\n\nCollect items from configured sources:\n- GitHub: merged PRs + closed issues in date range (labels filtering if configured)\n- Slack (optional): messages from configured channels\n- Git: commit log summary (conventional commits if enabled)\n\nIf a live API isn‚Äôt available, still proceed with Git-only changes and record gaps in the final draft.\n\n### Phase 3: AI Synthesis (Narrative Draft)\n\nCreate a first draft that:\n- Groups changes into **Highlights**, **Features**, **Fixes**, **Breaking Changes**, **Internal/Infra**\n- Uses a user-facing tone (clear outcomes, minimal jargon)\n- Links back to PRs/issues when URLs are present\n\n### Phase 4: Template Formatting + Frontmatter\n\n1. Load the configured markdown template (or fall back to `{baseDir}/assets/weekly-template.md`).\n2. Render the final markdown using `{baseDir}/scripts/render_template.py`.\n3. Ensure frontmatter contains at least `date` (ISO) and `version` (SemVer if known; otherwise `0.0.0`).\n\n### Phase 5: Quality Gate (Deterministic + Editorial)\n\n1. Run deterministic checks using `{baseDir}/scripts/quality_score.py`.\n2. If score is below threshold:\n   - Fix structural issues first (missing sections, broken links, invalid frontmatter)\n   - Rewrite only the weakest sections (max 2 iterations)\n\n### Phase 6: PR Creation + User Handoff\n\n1. Write the changelog file to the configured `output_path`.\n2. Create a branch `changelog-YYYY-MM-DD`, commit with `docs: add changelog for YYYY-MM-DD`.\n3. If `gh` is configured, open a PR; otherwise, print the exact commands the user should run.\n\n## Output\n\n- A markdown changelog draft (usually `CHANGELOG.md`), plus an optional PR URL.\n- A quality report (score + findings) from `{baseDir}/scripts/quality_score.py`.\n\n## Error Handling\n\n- Missing config: instruct the user to copy `${CLAUDE_PLUGIN_ROOT}/config/changelog-config.example.json` to `.changelog-config.json`.\n- Missing token env var: show which `token_env` is required and how to export it.\n- Missing template: fall back to `{baseDir}/assets/default-changelog.md` and note it in output.\n- No changes found: produce an empty changelog with a short ‚ÄúNo user-visible changes‚Äù note.\n\n## Examples\n\n### Weekly changelog\n\nUser: ‚ÄúGenerate the weekly changelog and open a PR‚Äù\n\nExpected behavior: compute last 7 days, draft changelog, pass quality gate, create branch and PR.\n\n### Custom range changelog\n\nUser: ‚ÄúGenerate changelog from 2025-12-01 to 2025-12-15‚Äù\n\nExpected behavior: use that range, draft changelog, and prepare PR.\n\n## Resources\n\n- Validate config: `{baseDir}/scripts/validate_config.py`\n- Render template: `{baseDir}/scripts/render_template.py`\n- Quality scoring: `{baseDir}/scripts/quality_score.py`\n- Default templates:\n  - `{baseDir}/assets/default-changelog.md`\n  - `{baseDir}/assets/weekly-template.md`\n  - `{baseDir}/assets/release-template.md`",
      "parentPlugin": {
        "name": "mattyp-changelog",
        "category": "automation",
        "path": "plugins/automation/mattyp-changelog",
        "version": "0.1.0",
        "description": "Automate changelog generation: fetch recent changes, synthesize release notes, run quality gates, and prepare a PR."
      },
      "filePath": "plugins/automation/mattyp-changelog/skills/changelog-orchestrator/SKILL.md"
    },
    {
      "slug": "checking-hipaa-compliance",
      "name": "checking-hipaa-compliance",
      "description": "Check HIPAA compliance for healthcare data security requirements. Use when auditing healthcare applications. Trigger with 'check HIPAA compliance', 'validate health data security', or 'audit PHI protection'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Hipaa Compliance Checker\n\nThis skill provides automated assistance for hipaa compliance checker tasks.\n\n## Overview\n\nThis skill automates the process of identifying potential HIPAA compliance issues within a software project. By using the hipaa-compliance-checker plugin, it helps developers and security professionals proactively address vulnerabilities and ensure adherence to HIPAA guidelines.\n\n## How It Works\n\n1. **Analyze Request**: Claude identifies the user's intent to check for HIPAA compliance.\n2. **Initiate Plugin**: Claude activates the hipaa-compliance-checker plugin.\n3. **Execute Checks**: The plugin scans the specified codebase, configuration files, or documentation for potential HIPAA violations.\n4. **Generate Report**: The plugin generates a detailed report outlining identified issues and their potential impact on HIPAA compliance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Evaluate a codebase for HIPAA compliance before deployment.\n- Identify potential HIPAA violations in existing systems.\n- Assess the HIPAA readiness of infrastructure configurations.\n- Verify that documentation adheres to HIPAA guidelines.\n\n## Examples\n\n### Example 1: Checking a codebase for HIPAA compliance\n\nUser request: \"Check HIPAA compliance of the patient data API codebase.\"\n\nThe skill will:\n1. Activate the hipaa-compliance-checker plugin.\n2. Scan the specified API codebase for potential HIPAA violations.\n3. Generate a report listing any identified issues, such as insecure data storage or insufficient access controls.\n\n### Example 2: Assessing infrastructure configuration for HIPAA readiness\n\nUser request: \"Assess the HIPAA readiness of our AWS infrastructure configuration.\"\n\nThe skill will:\n1. Activate the hipaa-compliance-checker plugin.\n2. Analyze the AWS infrastructure configuration files for potential HIPAA violations, such as misconfigured security groups or inadequate encryption.\n3. Generate a report outlining any identified issues and recommendations for remediation.\n\n## Best Practices\n\n- **Specify Target**: Always clearly specify the target (e.g., codebase, configuration file, documentation) for the HIPAA compliance check.\n- **Review Reports**: Carefully review the generated reports to understand the identified issues and their potential impact.\n- **Prioritize Remediation**: Prioritize the remediation of identified issues based on their severity and potential impact on HIPAA compliance.\n\n## Integration\n\nThis skill can be integrated with other security and compliance tools to provide a comprehensive view of a system's security posture. The generated reports can be used as input for vulnerability management systems and security information and event management (SIEM) platforms.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "hipaa-compliance-checker",
        "category": "security",
        "path": "plugins/security/hipaa-compliance-checker",
        "version": "1.0.0",
        "description": "Check HIPAA compliance"
      },
      "filePath": "plugins/security/hipaa-compliance-checker/skills/checking-hipaa-compliance/SKILL.md"
    },
    {
      "slug": "checking-infrastructure-compliance",
      "name": "checking-infrastructure-compliance",
      "description": "Use when you need to work with compliance checking. This skill provides compliance monitoring and validation with comprehensive guidance and automation. Trigger with phrases like \"check compliance\", \"validate policies\", or \"audit compliance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Compliance Checker\n\nThis skill provides automated assistance for compliance checker tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/compliance-checker/`\n\n**Documentation and Guides**: `{baseDir}/docs/compliance-checker/`\n\n**Example Scripts and Code**: `{baseDir}/examples/compliance-checker/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/compliance-checker-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/compliance-checker-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/compliance-checker-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "compliance-checker",
        "category": "devops",
        "path": "plugins/devops/compliance-checker",
        "version": "1.0.0",
        "description": "Check infrastructure compliance (SOC2, HIPAA, PCI-DSS)"
      },
      "filePath": "plugins/devops/compliance-checker/skills/checking-infrastructure-compliance/SKILL.md"
    },
    {
      "slug": "checking-owasp-compliance",
      "name": "checking-owasp-compliance",
      "description": "Check compliance with OWASP Top 10 security risks and best practices. Use when performing comprehensive security audits. Trigger with 'check OWASP compliance', 'audit web security', or 'validate OWASP'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Owasp Compliance Checker\n\nThis skill provides automated assistance for owasp compliance checker tasks.\n\n## Overview\n\nThis skill empowers Claude to assess your project's adherence to the OWASP Top 10 (2021) security guidelines. It automates the process of identifying potential vulnerabilities related to common web application security risks, providing actionable insights to improve your application's security posture.\n\n## How It Works\n\n1. **Initiate Scan**: The skill activates the owasp-compliance-checker plugin upon request.\n2. **Analyze Codebase**: The plugin scans the codebase for potential vulnerabilities related to each OWASP Top 10 category.\n3. **Generate Report**: A detailed report is generated, highlighting compliance gaps and providing specific remediation guidance for each identified issue.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Evaluate your application's security posture against the OWASP Top 10 (2021).\n- Identify potential vulnerabilities related to common web application security risks.\n- Obtain actionable remediation guidance to address identified vulnerabilities.\n- Generate a compliance report for auditing or reporting purposes.\n\n## Examples\n\n### Example 1: Identifying SQL Injection Vulnerabilities\n\nUser request: \"Check OWASP compliance for SQL injection vulnerabilities.\"\n\nThe skill will:\n1. Activate the owasp-compliance-checker plugin.\n2. Scan the codebase for potential SQL injection vulnerabilities.\n3. Generate a report highlighting any identified SQL injection vulnerabilities and providing remediation guidance.\n\n### Example 2: Assessing Overall OWASP Compliance\n\nUser request: \"/owasp\"\n\nThe skill will:\n1. Activate the owasp-compliance-checker plugin.\n2. Scan the entire codebase for vulnerabilities across all OWASP Top 10 categories.\n3. Generate a comprehensive report detailing compliance gaps and remediation steps for each category.\n\n## Best Practices\n\n- **Regular Scanning**: Integrate OWASP compliance checks into your development workflow for continuous security monitoring.\n- **Prioritize Remediation**: Address identified vulnerabilities based on their severity and potential impact.\n- **Stay Updated**: Keep your OWASP compliance checker plugin updated to benefit from the latest vulnerability detection rules and remediation guidance.\n\n## Integration\n\nThis skill can be integrated with other plugins to automate vulnerability remediation or generate comprehensive security reports. For example, it can be used in conjunction with a code modification plugin to automatically apply recommended fixes for identified vulnerabilities.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "owasp-compliance-checker",
        "category": "security",
        "path": "plugins/security/owasp-compliance-checker",
        "version": "1.0.0",
        "description": "Check OWASP Top 10 compliance"
      },
      "filePath": "plugins/security/owasp-compliance-checker/skills/checking-owasp-compliance/SKILL.md"
    },
    {
      "slug": "checking-session-security",
      "name": "checking-session-security",
      "description": "Analyze session management implementations to identify security vulnerabilities in web applications. Use when you need to audit session handling, check for session fixation risks, review session timeout configurations, or validate session ID generation security. Trigger with phrases like \"check session security\", \"audit session management\", \"review session handling\", or \"session fixation vulnerability\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(code-scan:*), Bash(security-check:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Session Security Checker\n\nThis skill provides automated assistance for session security checker tasks.\n\n## Overview\n\nReviews session management implementations for common weaknesses (cookie flags, rotation, expiration, invalidation) and recommends fixes to reduce takeover risk.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Source code accessible in {baseDir}/\n- Session management code locations known (auth modules, middleware)\n- Framework information (Express, Django, Spring, etc.)\n- Configuration files for session settings\n- Write permissions for security report in {baseDir}/security-reports/\n\n## Instructions\n\n1. Review session creation, storage, and transport security controls.\n2. Validate cookie flags, rotation, expiration, and invalidation behavior.\n3. Identify common attack paths (fixation, CSRF, replay) and mitigations.\n4. Provide prioritized fixes with configuration/code examples.\n\n### 1. Code Discovery Phase\n\nLocate session management code:\n- Authentication/login handlers\n- Session middleware configuration\n- Cookie handling code\n- Session storage implementations\n- Logout/session termination code\n\n**Common file patterns**:\n- `**/auth/**`, `**/session/**`, `**/middleware/**`\n- `session.config.*`, `auth.config.*`\n- Framework-specific: `settings.py`, `application.yml`, `web.config`\n\n### 2. Session ID Security Analysis\n\n**Generation Strength**:\n- Check for cryptographically secure random generators\n- Verify sufficient entropy (at least 128 bits)\n- Ensure unpredictable session ID patterns\n- No sequential or timestamp-based IDs\n\n**Bad Patterns to Detect**:\n```javascript\n// INSECURE: Predictable\nsessionId = Date.now() + userId;\nsessionId = Math.random().toString();\n\n// SECURE: Cryptographically random\nsessionId = crypto.randomBytes(32).toString('hex');\n```\n\n### 3. Session Fixation Vulnerability Check\n\nVerify session ID regeneration:\n- New session ID generated after login\n- Session ID changes on privilege escalation\n- Old session ID invalidated after regeneration\n\n**Vulnerable Pattern**:\n```python\n# INSECURE: Reuses existing session ID\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\ndef login(username, password):\n    if authenticate(username, password):\n        session['authenticated'] = True  # Session ID not regenerated\n```\n\n**Secure Pattern**:\n```python\n# SECURE: Regenerates session ID\ndef login(username, password):\n    if authenticate(username, password):\n        session.regenerate()  # New session ID\n        session['authenticated'] = True\n```\n\n### 4. Session Timeout Analysis\n\nCheck timeout configurations:\n- Idle timeout (session inactivity limit)\n- Absolute timeout (maximum session lifetime)\n- Remember-me token expiration\n- Sensitive operation re-authentication\n\n**Configuration Review**:\n- Idle timeout: 15-30 minutes recommended\n- Absolute timeout: 8-12 hours maximum\n- Financial apps: Shorter timeouts (5-10 minutes)\n- Sensitive operations: Force re-authentication\n\n### 5. Cookie Security Attributes\n\nVerify secure cookie flags:\n- `HttpOnly`: Prevents JavaScript access (XSS protection)\n- `Secure`: Ensures HTTPS-only transmission\n- `SameSite`: Prevents CSRF attacks (Strict or Lax)\n- `Domain` and `Path`: Properly scoped\n\n**Insecure Cookie**:\n```javascript\nres.cookie('sessionId', sessionId);  // No security flags\n```\n\n**Secure Cookie**:\n```javascript\nres.cookie('sessionId', sessionId, {\n  httpOnly: true,\n  secure: true,\n  sameSite: 'strict',\n  maxAge: 3600000  // 1 hour\n});\n```\n\n### 6. Session Storage Security\n\nEvaluate storage mechanisms:\n- Server-side storage (recommended): Redis, Memcached, database\n- Encrypted storage for sensitive data\n- No sensitive data in client-side cookies\n- Proper session cleanup on logout\n\n**Check for**:\n- Sessions persisting after logout\n- Lack of session invalidation\n- Sensitive data stored in cookies\n- Unencrypted session stores\n\n### 7. Session Hijacking Protections\n\nVerify anti-hijacking measures:\n- IP address binding (with caution for mobile users)\n- User-Agent validation\n- Token-based CSRF protection\n- Automatic logout on suspicious activity\n\n### 8. Concurrent Session Management\n\nCheck concurrent session handling:\n- Limit concurrent sessions per user\n- Session conflict detection\n- Force logout previous sessions option\n- Session monitoring and alerting\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Session security report saved to {baseDir}/security-reports/session-security-YYYYMMDD.md\n\n**Report Structure**:\n```\n# Session Security Analysis Report\nAnalysis Date: 2024-01-15\nApplication: Web Portal\nFramework: Express.js\n\n## Executive Summary\n- Overall Security Rating: MEDIUM RISK\n- Critical Issues: 2\n- High Priority Issues: 4\n- Recommendations: 8\n\n## Critical Findings\n\n### 1. Session Fixation Vulnerability\n**File**: {baseDir}/src/auth/login.js\n**Line**: 45\n**Issue**: Session ID not regenerated after authentication\n**Risk**: Attacker can hijack authenticated session\n**Code**:\n```javascript\nfunction handleLogin(req, res) {\n  if (validateCredentials(req.body)) {\n    req.session.authenticated = true;  // VULNERABLE\n    res.redirect('/dashboard');\n  }\n}\n```\n**Remediation**:\n```javascript\nfunction handleLogin(req, res) {\n  if (validateCredentials(req.body)) {\n    req.session.regenerate((err) => {  // SECURE\n      req.session.authenticated = true;\n      res.redirect('/dashboard');\n    });\n  }\n}\n```\n\n### 2. Missing HttpOnly Flag\n**File**: {baseDir}/config/session.js\n**Line**: 12\n**Issue**: Session cookies accessible to JavaScript\n**Risk**: XSS attacks can steal session tokens\n**Remediation**: Set `httpOnly: true` in cookie configuration\n\n## High Priority Findings\n\n### 3. Weak Session Timeout\n**File**: {baseDir}/config/session.js\n**Line**: 15\n**Issue**: Session timeout set to 24 hours\n**Risk**: Extended exposure window for compromised sessions\n**Current**: `maxAge: 86400000  // 24 hours`\n**Recommendation**: `maxAge: 1800000  // 30 minutes`\n\n### 4. Insecure Session ID Generation\n**File**: {baseDir}/src/auth/session-manager.js\n**Line**: 28\n**Issue**: Using Math.random() for session IDs\n**Risk**: Predictable session IDs enable brute-force attacks\n**Remediation**: Use crypto.randomBytes()\n\n[Additional findings...]\n\n## Session Configuration Summary\n- Session Store: Redis (Good)\n- Cookie Secure Flag: Missing (Critical)\n- Cookie HttpOnly Flag: Missing (Critical)\n- Cookie SameSite: None (High Risk)\n- Idle Timeout: 24 hours (Too Long)\n- Session Regeneration: Not Implemented (Critical)\n\n## Compliance Check\n- OWASP Session Management: 4/10 controls implemented\n- PCI-DSS 8.1.8: Non-compliant (timeout too long)\n- NIST 800-63B: Partial compliance\n\n## Remediation Priority\n1. [CRITICAL] Implement session regeneration on login\n2. [CRITICAL] Enable HttpOnly and Secure cookie flags\n3. [HIGH] Reduce session timeout to 30 minutes\n4. [HIGH] Implement cryptographically secure session IDs\n5. [MEDIUM] Add SameSite=Strict to cookies\n6. [MEDIUM] Implement concurrent session limits\n```\n\n**Secondary Outputs**:\n- Vulnerable code snippets with line numbers\n- Remediation code examples\n- Framework-specific configuration guide\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Cannot Locate Session Management Code**\n   - Error: \"No session handling code found in {baseDir}/\"\n   - Resolution: Search for framework-specific patterns\n   - Fallback: Request explicit file paths from user\n\n2. **Framework Not Recognized**\n   - Error: \"Unknown session framework\"\n   - Resolution: Apply generic session security checks\n   - Note: Framework-specific recommendations unavailable\n\n3. **Encrypted or Obfuscated Code**\n   - Error: \"Cannot analyze minified/compiled code\"\n   - Resolution: Request source code or unminified version\n   - Limitation: Document inability to fully audit\n\n4. **Custom Session Implementation**\n   - Error: \"Non-standard session management detected\"\n   - Resolution: Apply fundamental security principles\n   - Extra Scrutiny: Custom implementations often have flaws\n\n5. **Configuration in Environment Variables**\n   - Error: \"Session config in environment, not code\"\n   - Resolution: Request .env.example or config documentation\n   - Fallback: Provide general configuration recommendations\n\n## Examples\n\n- \"Audit our session cookie flags and rotation logic for fixation/CSRF risks.\"\n- \"Review logout and password reset flows to confirm sessions are invalidated correctly.\"\n\n## Resources\n\n**OWASP Guidelines**:\n- Session Management Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html\n- OWASP Top 10 - Broken Authentication: https://owasp.org/www-project-top-ten/\n\n**Standards and Best Practices**:\n- NIST 800-63B Authentication: https://pages.nist.gov/800-63-3/sp800-63b.html\n- PCI-DSS Session Requirements: https://www.pcisecuritystandards.org/\n\n**Framework-Specific Guides**:\n- Express.js Session Security: https://expressjs.com/en/advanced/best-practice-security.html\n- Django Session Framework: https://docs.djangoproject.com/en/stable/topics/http/sessions/\n- Spring Session: https://spring.io/projects/spring-session\n\n**Security Tools**:\n- Burp Suite for session testing\n- OWASP ZAP session analysis\n- Browser DevTools for cookie inspection\n\n**Common Vulnerabilities**:\n- CWE-384: Session Fixation\n- CWE-613: Insufficient Session Expiration\n- CWE-539: Information Exposure Through Persistent Cookies\n- CWE-5 52: Insufficiently Protected Credentials",
      "parentPlugin": {
        "name": "session-security-checker",
        "category": "security",
        "path": "plugins/security/session-security-checker",
        "version": "1.0.0",
        "description": "Check session security implementation"
      },
      "filePath": "plugins/security/session-security-checker/skills/checking-session-security/SKILL.md"
    },
    {
      "slug": "code-formatter",
      "name": "code-formatter",
      "description": "Automatically formats and validates code files using Prettier and other formatting tools. Use when users mention \"format my code\", \"fix formatting\", \"apply code style\", \"check formatting\", \"make code consistent\", or \"clean up code formatting\". Handles JavaScript, TypeScript, JSON, CSS, Markdown, and many other file types. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Code Formatter Skill\n\nThis skill provides comprehensive code formatting and validation capabilities using industry-standard tools like Prettier.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## Prerequisites\n\n- Node.js and npm/npx installed\n- Prettier available globally or locally\n- Write permissions for target files\n- Supported file types in the project\n\n## Instructions\n\n1. Analyze current formatting (`prettier --check`) and identify files to update.\n2. Configure formatting rules (`.prettierrc`, `.editorconfig`) for the project.\n3. Apply formatting (`prettier --write`) to the target files/directories.\n4. Add ignore patterns (`.prettierignore`) for generated/vendor outputs.\n5. Optionally enforce formatting via git hooks (husky/lint-staged).\n\n### 1. Analyze Current Formatting\n\nFirst, check the current formatting state of files:\n\n```bash\n# Check if prettier is available\nnpx prettier --version || npm install -g prettier\n\n# Find all formattable files\nfind . -type f \\( -name \"*.js\" -o -name \"*.jsx\" -o -name \"*.ts\" -o -name \"*.tsx\" -o -name \"*.json\" -o -name \"*.css\" -o -name \"*.md\" \\) -not -path \"*/node_modules/*\" -not -path \"*/dist/*\"\n\n# Check which files need formatting\nnpx prettier --check \"**/*.{js,jsx,ts,tsx,json,css,md}\" --ignore-path .prettierignore\n```\n\n### 2. Configure Formatting Rules\n\nCreate or check for existing Prettier configuration:\n\n```javascript\n// .prettierrc\n{\n  \"semi\": true,\n  \"singleQuote\": true,\n  \"tabWidth\": 2,\n  \"trailingComma\": \"es5\",\n  \"printWidth\": 100,\n  \"bracketSpacing\": true,\n  \"arrowParens\": \"avoid\"\n}\n```\n\n### 3. Apply Formatting\n\nFormat individual files or entire directories:\n\n```bash\n# Format a single file\nnpx prettier --write src/app.js\n\n# Format all JavaScript files\nnpx prettier --write \"**/*.js\"\n\n# Format with specific config\nnpx prettier --write --config .prettierrc \"src/**/*.{js,jsx,ts,tsx}\"\n\n# Dry run to see what would change\nnpx prettier --check src/\n```\n\n### 4. Set Up Ignore Patterns\n\nCreate .prettierignore for files to skip:\n\n```\n# Dependencies\nnode_modules/\nvendor/\n\n# Build outputs\ndist/\nbuild/\n*.min.js\n*.min.css\n\n# Generated files\ncoverage/\n*.lock\n```\n\n### 5. Integrate with Git Hooks (Optional)\n\nSet up pre-commit formatting:\n\n```bash\n# Install husky and lint-staged\nnpm install --save-dev husky lint-staged\n\n# Configure in package.json\n{\n  \"lint-staged\": {\n    \"*.{js,jsx,ts,tsx,json,css,md}\": [\n      \"prettier --write\"\n    ]\n  }\n}\n```\n\n## Output\n\nThe formatter provides detailed feedback:\n\n### Success Output\n```\n‚úì Formatted: src/app.js\n‚úì Formatted: src/components/Button.jsx\n‚úì Formatted: package.json\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚úÖ Successfully formatted 3 files\n```\n\n### Validation Output\n```\nChecking formatting...\n[warn] src/app.js\n[warn] src/utils/helper.ts\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚ö† Found 2 files that need formatting\n```\n\n### Error Output\n```\n‚úó Error: Cannot format src/broken.js\n  SyntaxError: Unexpected token (line 15:8)\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚ùå Failed to format 1 file\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n### 1. Prettier Not Found\n```bash\n# Install globally\nnpm install -g prettier\n\n# Or use npx (no installation needed)\nnpx prettier --version\n```\n\n### 2. Syntax Errors\n```bash\n# Validate JavaScript syntax first\nnpx eslint src/app.js --fix-dry-run\n\n# Check for parsing errors\nnpx prettier --debug-check src/app.js\n```\n\n### 3. Configuration Conflicts\n```bash\n# Find all config files\nfind . -name \".prettier*\" -o -name \"prettier.config.js\"\n\n# Use specific config\nnpx prettier --config ./custom-prettier.json --write src/\n```\n\n### 4. Permission Issues\n```bash\n# Check file permissions\nls -la src/app.js\n\n# Fix permissions if needed\nchmod u+w src/app.js\n```\n\n## Resources\n\n### File Type Support\n\n| Extension | Language | Notes |\n|-----------|----------|-------|\n| .js, .jsx | JavaScript | ES6+ supported |\n| .ts, .tsx | TypeScript | Type annotations preserved |\n| .json | JSON | Sorts keys optionally |\n| .css, .scss | CSS/SASS | Vendor prefixes handled |\n| .md, .mdx | Markdown | Table formatting |\n| .yaml, .yml | YAML | Preserves comments |\n| .html | HTML | Attribute formatting |\n| .vue | Vue | Template + script + style |\n| .svelte | Svelte | Full component support |\n\n### Configuration Options\n\n```javascript\n{\n  // Semicolons\n  \"semi\": true,              // Add semicolons\n\n  // Quotes\n  \"singleQuote\": true,       // Use single quotes\n  \"jsxSingleQuote\": false,   // JSX double quotes\n\n  // Indentation\n  \"tabWidth\": 2,             // Spaces per tab\n  \"useTabs\": false,          // Use spaces\n\n  // Line Length\n  \"printWidth\": 100,         // Line wrap length\n  \"proseWrap\": \"preserve\",   // Markdown wrapping\n\n  // Trailing Commas\n  \"trailingComma\": \"es5\",    // Valid in ES5\n\n  // Brackets\n  \"bracketSpacing\": true,    // { foo: bar }\n  \"bracketSameLine\": false,  // JSX brackets\n\n  // Arrow Functions\n  \"arrowParens\": \"avoid\",    // x => x vs (x) => x\n\n  // Formatting\n  \"htmlWhitespaceSensitivity\": \"css\",\n  \"endOfLine\": \"lf\",\n  \"embeddedLanguageFormatting\": \"auto\"\n}\n```\n\n### Command Reference\n\n```bash\n# Basic formatting\nprettier --write <file>           # Format file\nprettier --check <file>           # Check if formatted\nprettier --debug-check <file>     # Debug parse errors\n\n# Multiple files\nprettier --write \"src/**/*.js\"    # Glob pattern\nprettier --write . --ignore-path .gitignore\n\n# Configuration\nprettier --config <path>          # Use specific config\nprettier --no-config              # Ignore config files\nprettier --find-config-path <file> # Show config used\n\n# Output options\nprettier --list-different         # List unformatted files\nprettier --log-level debug        # Verbose output\nprettier --no-color              # Disable colors\n\n# Special options\nprettier --require-pragma         # Only format with @format\nprettier --insert-pragma          # Add @format comment\nprettier --range-start 10 --range-end 50  # Partial format\n```\n\n### Integration Examples\n\n**VS Code Integration:**\n```json\n{\n  \"editor.defaultFormatter\": \"esbenp.prettier-vscode\",\n  \"editor.formatOnSave\": true,\n  \"[javascript]\": {\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n  }\n}\n```\n\n**CI/CD Pipeline:**\n```yaml\n# GitHub Actions\n- name: Check formatting\n  run: npx prettier --check .\n\n# Fail on unformatted code\n- name: Enforce formatting\n  run: |\n    npx prettier --check . || {\n      echo \"Code is not formatted!\"\n      exit 1\n    }\n```\n\n**Pre-commit Hook:**\n```bash\n#!/bin/sh\n# .git/hooks/pre-commit\n\nfiles=$(git diff --cached --name-only --diff-filter=ACMR | grep -E '\\.(js|jsx|ts|tsx|json|css|md)$')\n\nif [ -n \"$files\" ]; then\n  npx prettier --write $files\n  git add $files\nfi\n```\n\n### Performance Tips\n\n1. **Use .prettierignore** to skip large/generated files\n2. **Cache node_modules** in CI environments\n3. **Run in parallel** for multiple files:\n   ```bash\n   find . -name \"*.js\" | xargs -P 4 -I {} npx prettier --write {}\n   ```\n4. **Use --cache** flag for incremental formatting:\n   ```bash\n   npx prettier --write --cache src/\n   ```\n\n### Related Tools\n\n- **ESLint** - Linting and code quality\n- **Stylelint** - CSS/SCSS linting\n- **Markdownlint** - Markdown style checking\n- **Black** - Python formatting\n- **rustfmt** - Rust formatting\n- **gofmt** - Go formatting\n\n---\n\n**Version**: 1.0.0\n**Last Updated**: 2025-12-12\n**Compatibility**: Claude Code v1.5.0+",
      "parentPlugin": {
        "name": "formatter",
        "category": "examples",
        "path": "plugins/examples/formatter",
        "version": "2.0.1",
        "description": "Comprehensive code formatting plugin with Prettier integration. Use when you need to format code, validate formatting, or maintain consistent code style. Activates with phrases like 'format my code', 'check formatting', or 'apply code style'. Supports JavaScript, TypeScript, JSON, CSS, Markdown, and many other file types with automatic formatting on file operations."
      },
      "filePath": "plugins/examples/formatter/skills/code-formatter/SKILL.md"
    },
    {
      "slug": "collecting-infrastructure-metrics",
      "name": "collecting-infrastructure-metrics",
      "description": "Collect comprehensive infrastructure performance metrics across compute, storage, network, containers, load balancers, and databases. Use when monitoring system performance or troubleshooting infrastructure issues. Trigger with phrases like \"collect infrastructure metrics\", \"monitor server performance\", or \"track system resources\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(metrics:*)",
        "Bash(monitoring:*)",
        "Bash(system:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Infrastructure Metrics Collector\n\nThis skill provides automated assistance for infrastructure metrics collector tasks.\n\n## Overview\n\nThis skill automates the process of setting up infrastructure metrics collection. It identifies key performance indicators (KPIs) across various infrastructure layers, configures agents to collect these metrics, and assists in setting up central aggregation and visualization.\n\n## How It Works\n\n1. **Identify Infrastructure Layers**: Determines the infrastructure layers to monitor (compute, storage, network, containers, load balancers, databases).\n2. **Configure Metrics Collection**: Sets up agents (Prometheus, Datadog, CloudWatch) to collect metrics from the identified layers.\n3. **Aggregate Metrics**: Configures central aggregation of the collected metrics for analysis and visualization.\n4. **Create Dashboards**: Generates infrastructure dashboards for health monitoring, performance analysis, and capacity tracking.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Monitor the performance of your infrastructure.\n- Identify bottlenecks in your system.\n- Set up dashboards for real-time monitoring.\n\n## Examples\n\n### Example 1: Setting up basic monitoring\n\nUser request: \"Collect infrastructure metrics for my web server.\"\n\nThe skill will:\n1. Identify compute, storage, and network layers relevant to the web server.\n2. Configure Prometheus to collect CPU, memory, disk I/O, and network bandwidth metrics.\n\n### Example 2: Troubleshooting database performance\n\nUser request: \"I'm seeing slow database queries. Can you help me monitor the database performance?\"\n\nThe skill will:\n1. Identify the database layer and relevant metrics such as connection pool usage, replication lag, and cache hit rates.\n2. Configure Datadog to collect these metrics and create a dashboard to visualize performance trends.\n\n## Best Practices\n\n- **Agent Selection**: Choose the appropriate agent (Prometheus, Datadog, CloudWatch) based on your existing infrastructure and monitoring tools.\n- **Metric Granularity**: Balance the granularity of metrics collection with the storage and processing overhead. Collect only the essential metrics for your use case.\n- **Alerting**: Configure alerts based on thresholds for key metrics to proactively identify and address performance issues.\n\n## Integration\n\nThis skill can be integrated with other plugins for deployment, configuration management, and alerting to provide a comprehensive infrastructure management solution. For example, it can be used with a deployment plugin to automatically configure metrics collection after deploying new infrastructure.\n\n## Prerequisites\n\n- Access to infrastructure monitoring systems (Prometheus, Datadog, CloudWatch)\n- System permissions for metrics agent installation\n- Network access to monitored infrastructure components\n- Storage for metrics data in {baseDir}/metrics/\n\n## Instructions\n\n1. Identify infrastructure layers to monitor (compute, storage, network, databases)\n2. Select appropriate metrics collection agent based on environment\n3. Configure agent with target endpoints and metric types\n4. Set up central aggregation for collected metrics\n5. Create dashboards for visualization\n6. Configure alerts for critical metrics thresholds\n\n## Output\n\n- Metrics collection configuration files\n- Agent installation and setup scripts\n- Dashboard definitions for infrastructure monitoring\n- Metric export configurations\n- Alert rules for critical thresholds\n\n## Error Handling\n\nIf metrics collection fails:\n- Verify agent installation and permissions\n- Check network connectivity to targets\n- Validate authentication credentials\n- Review firewall and security group rules\n- Confirm metric endpoint availability\n\n## Resources\n\n- Prometheus documentation for metric collection\n- Datadog agent configuration guides\n- AWS CloudWatch metrics reference\n- Infrastructure monitoring best practices",
      "parentPlugin": {
        "name": "infrastructure-metrics-collector",
        "category": "performance",
        "path": "plugins/performance/infrastructure-metrics-collector",
        "version": "1.0.0",
        "description": "Collect comprehensive infrastructure performance metrics"
      },
      "filePath": "plugins/performance/infrastructure-metrics-collector/skills/collecting-infrastructure-metrics/SKILL.md"
    },
    {
      "slug": "comparing-database-schemas",
      "name": "comparing-database-schemas",
      "description": "Use when you need to work with schema comparison. This skill provides database schema diff and sync with comprehensive guidance and automation. Trigger with phrases like \"compare schemas\", \"diff databases\", or \"sync database schemas\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Diff Tool\n\nThis skill provides automated assistance for database diff tool tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-diff-tool/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-diff-tool/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-diff-tool/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-diff-tool-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-diff-tool-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-diff-tool-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-diff-tool",
        "category": "database",
        "path": "plugins/database/database-diff-tool",
        "version": "1.0.0",
        "description": "Database plugin for database-diff-tool"
      },
      "filePath": "plugins/database/database-diff-tool/skills/comparing-database-schemas/SKILL.md"
    },
    {
      "slug": "configuring-auto-scaling-policies",
      "name": "configuring-auto-scaling-policies",
      "description": "Use when you need to work with auto-scaling. This skill provides auto-scaling configuration with comprehensive guidance and automation. Trigger with phrases like \"configure auto-scaling\", \"set up elastic scaling\", or \"implement scaling\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Auto Scaling Configurator\n\nThis skill provides automated assistance for auto scaling configurator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/auto-scaling-configurator/`\n\n**Documentation and Guides**: `{baseDir}/docs/auto-scaling-configurator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/auto-scaling-configurator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/auto-scaling-configurator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/auto-scaling-configurator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/auto-scaling-configurator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "auto-scaling-configurator",
        "category": "devops",
        "path": "plugins/devops/auto-scaling-configurator",
        "version": "1.0.0",
        "description": "Configure auto-scaling policies for applications and infrastructure"
      },
      "filePath": "plugins/devops/auto-scaling-configurator/skills/configuring-auto-scaling-policies/SKILL.md"
    },
    {
      "slug": "configuring-load-balancers",
      "name": "configuring-load-balancers",
      "description": "Use when configuring load balancers including ALB, NLB, Nginx, and HAProxy. Trigger with phrases like \"configure load balancer\", \"create ALB\", \"setup nginx load balancing\", or \"haproxy configuration\". Generates production-ready configurations with health checks, SSL termination, sticky sessions, and traffic distribution rules. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(aws:*), Bash(gcloud:*), Bash(nginx:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Load Balancer Configurator\n\nThis skill provides automated assistance for load balancer configurator tasks.\n\n## Overview\n\nCreates and hardens load balancer configurations (cloud or self-managed) including health checks, TLS termination, routing rules, and session persistence.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Backend servers are identified with IPs or DNS names\n- Load balancer type is determined (ALB, NLB, Nginx, HAProxy)\n- SSL certificates are available if using HTTPS\n- Health check endpoints are defined\n- Understanding of traffic distribution requirements (round-robin, least-connections)\n- Cloud provider CLI installed (if using cloud load balancers)\n\n## Instructions\n\n1. **Select Load Balancer Type**: Choose based on requirements (L4 vs L7, cloud vs on-prem)\n2. **Define Backend Pool**: List backend servers with ports and weights\n3. **Configure Health Checks**: Set check interval, timeout, and healthy threshold\n4. **Set Up SSL/TLS**: Configure certificates and cipher suites\n5. **Define Routing Rules**: Create path-based or host-based routing\n6. **Enable Session Persistence**: Configure sticky sessions if needed\n7. **Add Monitoring**: Set up logging and metrics collection\n8. **Test Configuration**: Validate syntax and test traffic distribution\n\n## Output\n\n**Nginx Configuration:**\n```nginx\n# {baseDir}/nginx/load-balancer.conf\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\nupstream backend_servers {\n    least_conn;\n    server 10.0.1.10:8080 weight=3;\n    server 10.0.1.11:8080 weight=2;\n    server 10.0.1.12:8080 backup;\n}\n\nserver {\n    listen 80;\n    server_name app.example.com;\n\n    location / {\n        proxy_pass http://backend_servers;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        proxy_connect_timeout 5s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n    }\n\n    location /health {\n        access_log off;\n        return 200 \"healthy\\n\";\n    }\n}\n```\n\n**AWS ALB (Terraform):**\n```hcl\n# {baseDir}/terraform/alb.tf\nresource \"aws_lb\" \"app_alb\" {\n  name               = \"app-load-balancer\"\n  internal           = false\n  load_balancer_type = \"application\"\n  security_groups    = [aws_security_group.alb_sg.id]\n  subnets            = aws_subnet.public[*].id\n}\n\nresource \"aws_lb_target_group\" \"app_tg\" {\n  name     = \"app-target-group\"\n  port     = 80\n  protocol = \"HTTP\"\n  vpc_id   = aws_vpc.main.id\n\n  health_check {\n    path                = \"/health\"\n    interval            = 30\n    timeout             = 5\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n  }\n}\n\nresource \"aws_lb_listener\" \"app_listener\" {\n  load_balancer_arn = aws_lb.app_alb.arn\n  port              = \"443\"\n  protocol          = \"HTTPS\"\n  ssl_policy        = \"ELBSecurityPolicy-TLS-1-2-2017-01\"\n  certificate_arn   = aws_acm_certificate.cert.arn\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.app_tg.arn\n  }\n}\n```\n\n## Error Handling\n\n**Backend Server Unreachable**\n- Error: \"502 Bad Gateway\" or connection refused\n- Solution: Verify backend server IPs, ports, and firewall rules\n\n**SSL Certificate Error**\n- Error: \"certificate verify failed\"\n- Solution: Check certificate validity, chain, and private key match\n\n**Health Check Failures**\n- Error: \"Target is unhealthy\"\n- Solution: Verify health check path returns 200 status and backends are running\n\n**Configuration Syntax Error**\n- Error: \"nginx: configuration file test failed\"\n- Solution: Run `nginx -t` to validate syntax and fix errors\n\n**Session Persistence Not Working**\n- Issue: Users losing session on subsequent requests\n- Solution: Enable sticky sessions using cookie-based or IP-based persistence\n\n## Examples\n\n- \"Configure an AWS ALB with HTTPS + path-based routing to two target groups.\"\n- \"Generate an Nginx upstream + server block with health checks and sticky sessions.\"\n\n## Resources\n\n- Nginx documentation: https://nginx.org/en/docs/\n- HAProxy configuration guide: https://www.haproxy.org/\n- AWS ALB documentation: https://docs.aws.amazon.com/elasticloadbalancing/\n- GCP Load Balancing: https://cloud.google.com/load-balancing/docs\n- Example configurations in {baseDir}/lb-examples/",
      "parentPlugin": {
        "name": "load-balancer-configurator",
        "category": "devops",
        "path": "plugins/devops/load-balancer-configurator",
        "version": "1.0.0",
        "description": "Configure load balancers (ALB, NLB, Nginx, HAProxy)"
      },
      "filePath": "plugins/devops/load-balancer-configurator/skills/configuring-load-balancers/SKILL.md"
    },
    {
      "slug": "configuring-service-meshes",
      "name": "configuring-service-meshes",
      "description": "This skill configures service meshes like istio and linkerd for microservices. it generates production-ready configurations, implements best practices, and ensures a security-first approach. use this skill when the user asks to \"configure service ... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Service Mesh Configurator\n\nThis skill provides automated assistance for service mesh configurator tasks.\n\n## Overview\n\nThis skill enables Claude to generate configurations and setup code for service meshes like Istio and Linkerd. It simplifies the process of deploying and managing microservices by automating the configuration of essential service mesh components.\n\n## How It Works\n\n1. **Requirement Gathering**: Claude identifies the specific service mesh (Istio or Linkerd) and infrastructure requirements from the user's request.\n2. **Configuration Generation**: Based on the requirements, Claude generates the necessary configuration files, including YAML manifests and setup scripts.\n3. **Code Delivery**: Claude provides the generated configurations and setup code to the user, ready for deployment.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Configure Istio for a microservices application.\n- Configure Linkerd for a microservices application.\n- Generate service mesh configurations based on specific infrastructure requirements.\n\n## Examples\n\n### Example 1: Setting up Istio\n\nUser request: \"Configure Istio for my Kubernetes microservices deployment with mTLS enabled.\"\n\nThe skill will:\n1. Generate Istio configuration files with mTLS enabled.\n2. Provide the generated YAML manifests and setup instructions.\n\n### Example 2: Configuring Linkerd\n\nUser request: \"Setup Linkerd on my existing microservices cluster, focusing on traffic splitting and observability.\"\n\nThe skill will:\n1. Generate Linkerd configuration files for traffic splitting and observability.\n2. Provide the generated YAML manifests and setup instructions.\n\n## Best Practices\n\n- **Security**: Always prioritize security configurations, such as mTLS, when configuring service meshes.\n- **Observability**: Ensure that the service mesh is configured for comprehensive observability, including metrics, tracing, and logging.\n- **Traffic Management**: Use traffic management features like traffic splitting and canary deployments to manage application updates safely.\n\n## Integration\n\nThis skill can be integrated with other DevOps tools and plugins in the Claude Code ecosystem to automate the deployment and management of microservices applications. For example, it can work with a Kubernetes deployment plugin to automatically deploy the generated configurations.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "service-mesh-configurator",
        "category": "devops",
        "path": "plugins/devops/service-mesh-configurator",
        "version": "1.0.0",
        "description": "Configure service mesh (Istio, Linkerd) for microservices"
      },
      "filePath": "plugins/devops/service-mesh-configurator/skills/configuring-service-meshes/SKILL.md"
    },
    {
      "slug": "creating-alerting-rules",
      "name": "creating-alerting-rules",
      "description": "This skill enables AI assistant to create intelligent alerting rules for proactive performance monitoring. it is triggered when the user requests to \"create alerts\", \"define monitoring rules\", or \"set up alerting\". the skill helps define thresholds, rou... Use when generating or creating new content. Trigger with phrases like 'generate', 'create', or 'scaffold'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Alerting Rule Creator\n\nThis skill provides automated assistance for alerting rule creator tasks.\n\n## Overview\n\nThis skill automates the creation of comprehensive alerting rules, reducing the manual effort required for performance monitoring. It guides you through defining alert categories, setting intelligent thresholds, and configuring routing and escalation policies. The skill also helps generate runbooks and establish alert testing procedures.\n\n## How It Works\n\n1. **Identify Alert Category**: Determines the type of alert to create (e.g., latency, error rate, resource utilization).\n2. **Define Thresholds**: Sets appropriate thresholds to avoid alert fatigue and ensure timely notification of performance issues.\n3. **Configure Routing and Escalation**: Establishes routing policies to direct alerts to the appropriate teams and escalation policies for timely response.\n4. **Generate Runbook**: Creates a basic runbook with steps to diagnose and resolve the alerted issue.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement performance monitoring for a new service.\n- Refine existing alerting rules to reduce false positives.\n- Create alerts for specific performance metrics, such as latency or error rate.\n\n## Examples\n\n### Example 1: Setting up Latency Alerts\n\nUser request: \"create latency alerts for the payment service\"\n\nThe skill will:\n1. Prompt for latency thresholds (e.g., warning and critical).\n2. Configure alerts to trigger when latency exceeds defined thresholds.\n\n### Example 2: Creating Error Rate Alerts\n\nUser request: \"set up alerting for error rate increases in the API gateway\"\n\nThe skill will:\n1. Request the baseline error rate and acceptable deviation.\n2. Configure alerts to trigger when the error rate exceeds the defined deviation from the baseline.\n\n## Best Practices\n\n- **Threshold Selection**: Use historical data and statistical analysis to determine appropriate thresholds that minimize false positives and negatives.\n- **Alert Routing**: Route alerts to the appropriate teams or individuals based on the alert category and severity.\n- **Runbook Creation**: Generate or link to detailed runbooks that provide clear instructions for diagnosing and resolving the alerted issue.\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins to automate incident response workflows. For example, it can trigger automated remediation actions or create tickets in an issue tracking system.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "alerting-rule-creator",
        "category": "performance",
        "path": "plugins/performance/alerting-rule-creator",
        "version": "1.0.0",
        "description": "Create intelligent alerting rules for performance monitoring"
      },
      "filePath": "plugins/performance/alerting-rule-creator/skills/creating-alerting-rules/SKILL.md"
    },
    {
      "slug": "creating-ansible-playbooks",
      "name": "creating-ansible-playbooks",
      "description": "Use when you need to work with Ansible automation. This skill provides Ansible playbook creation with comprehensive guidance and automation. Trigger with phrases like \"create Ansible playbook\", \"automate with Ansible\", or \"configure with Ansible\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(ansible:*), Bash(terraform:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Ansible Playbook Creator\n\nThis skill provides automated assistance for ansible playbook creator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/ansible-playbook-creator/`\n\n**Documentation and Guides**: `{baseDir}/docs/ansible-playbook-creator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/ansible-playbook-creator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/ansible-playbook-creator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/ansible-playbook-creator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/ansible-playbook-creator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "ansible-playbook-creator",
        "category": "devops",
        "path": "plugins/devops/ansible-playbook-creator",
        "version": "1.0.0",
        "description": "Create Ansible playbooks for configuration management"
      },
      "filePath": "plugins/devops/ansible-playbook-creator/skills/creating-ansible-playbooks/SKILL.md"
    },
    {
      "slug": "creating-apm-dashboards",
      "name": "creating-apm-dashboards",
      "description": "This skill enables AI assistant to create application performance monitoring (apm) dashboards. it is triggered when the user requests the creation of a new apm dashboard, monitoring dashboard, or a dashboard for application performance. the skill helps ... Use when generating or creating new content. Trigger with phrases like 'generate', 'create', or 'scaffold'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Apm Dashboard Creator\n\nThis skill provides automated assistance for apm dashboard creator tasks.\n\n## Overview\n\nThis skill automates the creation of Application Performance Monitoring (APM) dashboards, providing a structured approach to visualizing critical application metrics. By defining key performance indicators and generating dashboard configurations, this skill simplifies the process of monitoring application health and performance.\n\n## How It Works\n\n1. **Identify Requirements**: Determine the specific metrics and visualizations needed for the APM dashboard based on the user's request.\n2. **Define Dashboard Components**: Select relevant components such as golden signals (latency, traffic, errors, saturation), request metrics, resource utilization, database metrics, cache metrics, business metrics, and error tracking.\n3. **Generate Configuration**: Create the dashboard configuration file based on the selected components and user preferences.\n4. **Deploy Dashboard**: Deploy the generated configuration to the target monitoring platform (e.g., Grafana, Datadog).\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new APM dashboard for an application.\n- Define key metrics and visualizations for monitoring application performance.\n- Generate dashboard configurations for Grafana, Datadog, or other monitoring platforms.\n\n## Examples\n\n### Example 1: Creating a Grafana Dashboard\n\nUser request: \"Create a Grafana dashboard for monitoring my web application's performance.\"\n\nThe skill will:\n1. Identify the need for a Grafana dashboard focused on web application performance.\n2. Define dashboard components including request rate, response times, error rates, and resource utilization (CPU, memory).\n3. Generate a Grafana dashboard configuration file with pre-defined visualizations for these metrics.\n\n### Example 2: Setting up a Datadog Dashboard\n\nUser request: \"Set up a Datadog dashboard to track the golden signals for my microservice.\"\n\nThe skill will:\n1. Identify the need for a Datadog dashboard focused on golden signals.\n2. Define dashboard components including latency, traffic, errors, and saturation metrics.\n3. Generate a Datadog dashboard configuration file with pre-defined visualizations for these metrics.\n\n## Best Practices\n\n- **Specificity**: Provide detailed information about the application and metrics to be monitored.\n- **Platform Selection**: Clearly specify the target monitoring platform (Grafana, Datadog, etc.) to ensure compatibility.\n- **Iteration**: Review and refine the generated dashboard configuration to meet specific monitoring needs.\n\n## Integration\n\nThis skill can be integrated with other plugins that manage infrastructure or application deployment to automatically create APM dashboards as part of the deployment process. It can also work with alerting plugins to define alert rules based on the metrics displayed in the generated dashboards.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "apm-dashboard-creator",
        "category": "performance",
        "path": "plugins/performance/apm-dashboard-creator",
        "version": "1.0.0",
        "description": "Create Application Performance Monitoring dashboards"
      },
      "filePath": "plugins/performance/apm-dashboard-creator/skills/creating-apm-dashboards/SKILL.md"
    },
    {
      "slug": "creating-data-visualizations",
      "name": "creating-data-visualizations",
      "description": "Generate plots, charts, and graphs from data with automatic visualization type selection. Use when requesting \"visualization\", \"plot\", \"chart\", or \"graph\". Trigger with phrases like 'generate', 'create', or 'scaffold'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Data Visualization Creator\n\nThis skill provides automated assistance for data visualization creator tasks.\n\n## Overview\n\nThis skill empowers Claude to transform raw data into compelling visual representations. It leverages intelligent automation to select optimal visualization types and generate informative plots, charts, and graphs. This skill helps users understand complex data more easily.\n\n## How It Works\n\n1. **Data Analysis**: Claude analyzes the provided data to understand its structure, type, and distribution.\n2. **Visualization Selection**: Based on the data analysis, Claude selects the most appropriate visualization type (e.g., bar chart, scatter plot, line graph).\n3. **Visualization Generation**: Claude generates the visualization using appropriate libraries and best practices for visual clarity and accuracy.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a visual representation of data.\n- Generate a specific type of plot, chart, or graph (e.g., \"create a bar chart\").\n- Explore data patterns and relationships through visualization.\n\n## Examples\n\n### Example 1: Visualizing Sales Data\n\nUser request: \"Create a bar chart showing sales by region.\"\n\nThe skill will:\n1. Analyze the sales data, identifying regions and corresponding sales figures.\n2. Generate a bar chart with regions on the x-axis and sales on the y-axis.\n\n### Example 2: Plotting Stock Prices\n\nUser request: \"Plot the stock price of AAPL over the last year.\"\n\nThe skill will:\n1. Retrieve historical stock price data for AAPL.\n2. Generate a line graph showing the stock price over time.\n\n## Best Practices\n\n- **Data Clarity**: Ensure the data is clean and well-formatted before requesting a visualization.\n- **Specific Requests**: Be specific about the desired visualization type and any relevant data filters.\n- **Contextual Information**: Provide context about the data and the purpose of the visualization.\n\n## Integration\n\nThis skill can be integrated with other data processing and analysis tools within the Claude Code environment. It can receive data from other skills and provide visualizations for further analysis or reporting.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "data-visualization-creator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/data-visualization-creator",
        "version": "1.0.0",
        "description": "Create data visualizations and plots"
      },
      "filePath": "plugins/ai-ml/data-visualization-creator/skills/creating-data-visualizations/SKILL.md"
    },
    {
      "slug": "creating-github-issues-from-web-research",
      "name": "creating-github-issues-from-web-research",
      "description": "This skill enhances AI assistant's ability to conduct web research and translate findings into actionable github issues. it automates the process of extracting key information from web search results and formatting it into a well-structured issue, ready... Use when managing version control. Trigger with phrases like 'commit', 'branch', or 'git'. allowed-tools: Read, WebFetch, WebSearch, Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins Team <hello@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Web To Github Issue\n\nThis skill provides automated assistance for web to github issue tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for web to github issue tasks.\nThis skill empowers Claude to streamline the research-to-implementation workflow. By integrating web search with GitHub issue creation, Claude can efficiently convert research findings into trackable tasks for development teams.\n\n## How It Works\n\n1. **Web Search**: Claude utilizes its web search capabilities to gather information on the specified topic.\n2. **Information Extraction**: The plugin extracts relevant details, key findings, and supporting evidence from the search results.\n3. **GitHub Issue Creation**: A new GitHub issue is created with a clear title, a summary of the research, key recommendations, and links to the original sources.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Investigate a technical topic and create an implementation ticket.\n- Track security vulnerabilities and generate a security issue with remediation steps.\n- Research competitor features and create a feature request ticket.\n\n## Examples\n\n### Example 1: Researching Security Best Practices\n\nUser request: \"research Docker security best practices and create a ticket in myorg/backend\"\n\nThe skill will:\n1. Search the web for Docker security best practices.\n2. Extract key recommendations, security vulnerabilities, and mitigation strategies.\n3. Create a GitHub issue in the specified repository with a summary of the findings, a checklist of best practices, and links to relevant resources.\n\n### Example 2: Investigating API Rate Limiting\n\nUser request: \"find articles about API rate limiting, create issue with label performance\"\n\nThe skill will:\n1. Search the web for articles and documentation on API rate limiting.\n2. Extract different rate limiting techniques, their pros and cons, and implementation examples.\n3. Create a GitHub issue with the \"performance\" label, summarizing the findings and providing links to the source articles.\n\n## Best Practices\n\n- **Specify Repository**: When creating issues for a specific project, explicitly mention the repository name to ensure the issue is created in the correct location.\n- **Use Labels**: Add relevant labels to the issue to categorize it appropriately and facilitate issue tracking.\n- **Provide Context**: Include sufficient context in your request to guide the web search and ensure the generated issue contains the most relevant information.\n\n## Integration\n\nThis skill seamlessly integrates with Claude's web search Skill and requires authentication with a GitHub account. It can be used in conjunction with other skills to further automate development workflows.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "web-to-github-issue",
        "category": "skill-enhancers",
        "path": "plugins/skill-enhancers/web-to-github-issue",
        "version": "1.0.0",
        "description": "Enhances web_search Skill by automatically creating GitHub issues from research findings"
      },
      "filePath": "plugins/skill-enhancers/web-to-github-issue/skills/creating-github-issues-from-web-research/SKILL.md"
    },
    {
      "slug": "creating-kubernetes-deployments",
      "name": "creating-kubernetes-deployments",
      "description": "Use when generating Kubernetes deployment manifests and services. Trigger with phrases like \"create kubernetes deployment\", \"generate k8s manifest\", \"deploy app to kubernetes\", or \"create service and ingress\". Produces production-ready YAML with health checks, auto-scaling, resource limits, ingress configuration, and TLS termination. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Kubernetes Deployment Creator\n\nThis skill provides automated assistance for kubernetes deployment creator tasks.\n\n## Overview\n\nGenerates Kubernetes manifests (Deployment, Service, Ingress, HPA) with health checks, resource limits, and environment configuration for production-ready deployments.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Kubernetes cluster is accessible and kubectl is configured\n- Container image is built and pushed to registry\n- Understanding of application resource requirements\n- Namespace exists or will be created\n- Ingress controller is installed (if using ingress)\n- TLS certificates are available (if using HTTPS)\n\n## Instructions\n\n1. **Gather Requirements**: Application name, image, replicas, ports, environment\n2. **Create Deployment**: Generate YAML with container spec and resource limits\n3. **Add Health Checks**: Configure liveness and readiness probes\n4. **Define Service**: Create ClusterIP, NodePort, or LoadBalancer service\n5. **Configure Ingress**: Set up routing rules and TLS termination\n6. **Add ConfigMaps/Secrets**: Externalize configuration and sensitive data\n7. **Enable Auto-scaling**: Create HorizontalPodAutoscaler if needed\n8. **Apply Manifests**: Use kubectl apply to deploy resources\n\n## Output\n\n**Deployment Manifest:**\n```yaml\n# {baseDir}/k8s/deployment.yaml\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-web-app\n  namespace: production\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-web-app\n  template:\n    metadata:\n      labels:\n        app: my-web-app\n    spec:\n      containers:\n      - name: app\n        image: registry/my-web-app:v1.0.0\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 5\n```\n\n**Service and Ingress:**\n```yaml\n# {baseDir}/k8s/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-web-app\n  namespace: production\nspec:\n  type: ClusterIP\n  selector:\n    app: my-web-app\n  ports:\n  - port: 80\n    targetPort: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-web-app\n  namespace: production\nspec:\n  tls:\n  - hosts:\n    - app.example.com\n    secretName: tls-cert\n  rules:\n  - host: app.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-web-app\n            port:\n              number: 80\n```\n\n## Error Handling\n\n**Image Pull Error**\n- Error: \"ErrImagePull\" or \"ImagePullBackOff\"\n- Solution: Verify image name, tag, and registry credentials in imagePullSecrets\n\n**CrashLoopBackOff**\n- Error: Pod repeatedly crashes and restarts\n- Solution: Check application logs with `kubectl logs` and review container health\n\n**Resource Quota Exceeded**\n- Error: \"forbidden: exceeded quota\"\n- Solution: Reduce resource requests or increase namespace quota\n\n**Ingress Not Working**\n- Error: 404 or 503 on ingress domain\n- Solution: Verify ingress controller is running and service endpoints are ready\n\n**TLS Certificate Error**\n- Error: \"certificate signed by unknown authority\"\n- Solution: Create or update TLS secret with valid certificate\n\n## Examples\n\n- \"Create a Kubernetes Deployment + Service + Ingress for my API on port 8080 with HPA.\"\n- \"Generate manifests for a worker deployment with CPU/memory limits and liveness/readiness probes.\"\n\n## Resources\n\n- Kubernetes documentation: https://kubernetes.io/docs/\n- kubectl reference: https://kubernetes.io/docs/reference/kubectl/\n- Deployment best practices: https://kubernetes.io/docs/concepts/workloads/\n- Example manifests in {baseDir}/k8s-examples/",
      "parentPlugin": {
        "name": "kubernetes-deployment-creator",
        "category": "devops",
        "path": "plugins/devops/kubernetes-deployment-creator",
        "version": "1.0.0",
        "description": "Create Kubernetes deployments, services, and configurations with best practices"
      },
      "filePath": "plugins/devops/kubernetes-deployment-creator/skills/creating-kubernetes-deployments/SKILL.md"
    },
    {
      "slug": "creating-webhook-handlers",
      "name": "creating-webhook-handlers",
      "description": "Create webhook endpoints with signature verification, retry logic, and payload validation. Use when receiving and processing webhook events. Trigger with phrases like \"create webhook\", \"handle webhook events\", or \"setup webhook handler\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:webhook-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Webhook Handler Creator\n\nThis skill provides automated assistance for webhook handler creator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:webhook-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for webhook handler creator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "webhook-handler-creator",
        "category": "api-development",
        "path": "plugins/api-development/webhook-handler-creator",
        "version": "1.0.0",
        "description": "Create secure webhook endpoints with signature verification and retry logic"
      },
      "filePath": "plugins/api-development/webhook-handler-creator/skills/creating-webhook-handlers/SKILL.md"
    },
    {
      "slug": "database-documentation-gen",
      "name": "database-documentation-gen",
      "description": "Use when you need to work with database documentation. This skill provides automated documentation generation with comprehensive guidance and automation. Trigger with phrases like \"generate docs\", \"document schema\", or \"create database documentation\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Documentation Gen\n\nThis skill provides automated assistance for database documentation gen tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-documentation-gen/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-documentation-gen/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-documentation-gen/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-documentation-gen-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-documentation-gen-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-documentation-gen-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-documentation-gen",
        "category": "database",
        "path": "plugins/database/database-documentation-gen",
        "version": "1.0.0",
        "description": "Database plugin for database-documentation-gen"
      },
      "filePath": "plugins/database/database-documentation-gen/skills/database-documentation-gen/SKILL.md"
    },
    {
      "slug": "deploying-machine-learning-models",
      "name": "deploying-machine-learning-models",
      "description": "This skill enables AI assistant to deploy machine learning models to production environments. it automates the deployment workflow, implements best practices for serving models, optimizes performance, and handles potential errors. use this skill when th... Use when deploying or managing infrastructure. Trigger with phrases like 'deploy', 'infrastructure', or 'CI/CD'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Model Deployment Helper\n\nThis skill provides automated assistance for model deployment helper tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for model deployment helper tasks.\nThis skill streamlines the process of deploying machine learning models to production, ensuring efficient and reliable model serving. It leverages automated workflows and best practices to simplify the deployment process and optimize performance.\n\n## How It Works\n\n1. **Analyze Requirements**: The skill analyzes the context and user requirements to determine the appropriate deployment strategy.\n2. **Generate Code**: It generates the necessary code for deploying the model, including API endpoints, data validation, and error handling.\n3. **Deploy Model**: The skill deploys the model to the specified production environment.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Deploy a trained machine learning model to a production environment.\n- Serve a model via an API endpoint for real-time predictions.\n- Automate the model deployment process.\n\n## Examples\n\n### Example 1: Deploying a Regression Model\n\nUser request: \"Deploy my regression model trained on the housing dataset.\"\n\nThe skill will:\n1. Analyze the model and data format.\n2. Generate code for a REST API endpoint to serve the model.\n3. Deploy the model to a cloud-based serving platform.\n\n### Example 2: Productionizing a Classification Model\n\nUser request: \"Productionize the classification model I just trained.\"\n\nThe skill will:\n1. Create a Docker container for the model.\n2. Implement data validation and error handling.\n3. Deploy the container to a Kubernetes cluster.\n\n## Best Practices\n\n- **Data Validation**: Implement thorough data validation to ensure the model receives correct inputs.\n- **Error Handling**: Include robust error handling to gracefully manage unexpected issues.\n- **Performance Monitoring**: Set up performance monitoring to track model latency and throughput.\n\n## Integration\n\nThis skill can be integrated with other tools for model training, data preprocessing, and monitoring.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "model-deployment-helper",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-deployment-helper",
        "version": "1.0.0",
        "description": "Deploy ML models to production"
      },
      "filePath": "plugins/ai-ml/model-deployment-helper/skills/deploying-machine-learning-models/SKILL.md"
    },
    {
      "slug": "deploying-monitoring-stacks",
      "name": "deploying-monitoring-stacks",
      "description": "Use when deploying monitoring stacks including Prometheus, Grafana, and Datadog. Trigger with phrases like \"deploy monitoring stack\", \"setup prometheus\", \"configure grafana\", or \"install datadog agent\". Generates production-ready configurations with metric collection, visualization dashboards, and alerting rules. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Monitoring Stack Deployer\n\nThis skill provides automated assistance for monitoring stack deployer tasks.\n\n## Overview\n\nDeploys monitoring stacks (Prometheus/Grafana/Datadog) including collectors, scraping config, dashboards, and alerting rules for production systems.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Target infrastructure is identified (Kubernetes, Docker, bare metal)\n- Metric endpoints are accessible from monitoring platform\n- Storage backend is configured for time-series data\n- Alert notification channels are defined (email, Slack, PagerDuty)\n- Resource requirements are calculated based on scale\n\n## Instructions\n\n1. **Select Platform**: Choose Prometheus/Grafana, Datadog, or hybrid approach\n2. **Deploy Collectors**: Install exporters and agents on monitored systems\n3. **Configure Scraping**: Define metric collection endpoints and intervals\n4. **Set Up Storage**: Configure retention policies and data compaction\n5. **Create Dashboards**: Build visualization panels for key metrics\n6. **Define Alerts**: Create alerting rules with appropriate thresholds\n7. **Test Monitoring**: Verify metrics flow and alert triggering\n\n## Output\n\n**Prometheus + Grafana (Kubernetes):**\n```yaml\n# {baseDir}/monitoring/prometheus.yaml\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n      evaluation_interval: 15s\n    scrape_configs:\n      - job_name: 'kubernetes-pods'\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus:latest\n        args:\n          - '--config.file=/etc/prometheus/prometheus.yml'\n          - '--storage.tsdb.retention.time=30d'\n        ports:\n        - containerPort: 9090\n```\n\n**Grafana Dashboard Configuration:**\n```json\n{\n  \"dashboard\": {\n    \"title\": \"Application Metrics\",\n    \"panels\": [\n      {\n        \"title\": \"CPU Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(container_cpu_usage_seconds_total[5m])\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Error Handling\n\n**Metrics Not Appearing**\n- Error: \"No data points\"\n- Solution: Verify scrape targets are accessible and returning metrics\n\n**High Cardinality**\n- Error: \"Too many time series\"\n- Solution: Reduce label combinations or increase Prometheus resources\n\n**Alert Not Firing**\n- Error: \"Alert condition met but no notification\"\n- Solution: Check Alertmanager configuration and notification channels\n\n**Dashboard Load Failure**\n- Error: \"Failed to load dashboard\"\n- Solution: Verify Grafana datasource configuration and permissions\n\n## Examples\n\n- \"Deploy Prometheus + Grafana on Kubernetes and add alerts for high error rate and latency.\"\n- \"Install Datadog agents across hosts and configure a dashboard for CPU/memory saturation.\"\n\n## Resources\n\n- Prometheus documentation: https://prometheus.io/docs/\n- Grafana documentation: https://grafana.com/docs/\n- Example dashboards in {baseDir}/monitoring-examples/",
      "parentPlugin": {
        "name": "monitoring-stack-deployer",
        "category": "devops",
        "path": "plugins/devops/monitoring-stack-deployer",
        "version": "1.0.0",
        "description": "Deploy monitoring stacks (Prometheus, Grafana, Datadog)"
      },
      "filePath": "plugins/devops/monitoring-stack-deployer/skills/deploying-monitoring-stacks/SKILL.md"
    },
    {
      "slug": "designing-database-schemas",
      "name": "designing-database-schemas",
      "description": "Use when you need to work with database schema design. This skill provides schema design and migrations with comprehensive guidance and automation. Trigger with phrases like \"design schema\", \"create migration\", or \"model database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Schema Designer\n\nThis skill provides automated assistance for database schema designer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-schema-designer/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-schema-designer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-schema-designer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-schema-designer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-schema-designer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-schema-designer-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-schema-designer",
        "category": "database",
        "path": "plugins/database/database-schema-designer",
        "version": "1.0.0",
        "description": "Design and visualize database schemas with normalization guidance, relationship mapping, and ERD generation"
      },
      "filePath": "plugins/database/database-schema-designer/skills/designing-database-schemas/SKILL.md"
    },
    {
      "slug": "detecting-data-anomalies",
      "name": "detecting-data-anomalies",
      "description": "Identify anomalies and outliers in datasets using machine learning algorithms. Use when analyzing data for unusual patterns, outliers, or unexpected deviations from normal behavior. Trigger with phrases like \"detect anomalies\", \"find outliers\", or \"identify unusual patterns\". allowed-tools: Read, Bash(python:*), Grep, Glob version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Anomaly Detection System\n\nThis skill provides automated assistance for anomaly detection system tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Dataset in accessible format (CSV, JSON, or database)\n- Python environment with scikit-learn or similar ML libraries\n- Understanding of data distribution and expected patterns\n- Sufficient data volume for statistical significance\n- Knowledge of domain-specific normal behavior\n- Data preprocessing capabilities for cleaning and scaling\n\n## Instructions\n\n### Step 1: Prepare Data for Analysis\nSet up the dataset for anomaly detection:\n1. Load dataset using Read tool\n2. Inspect data structure and identify relevant features\n3. Clean data by handling missing values and inconsistencies\n4. Normalize or scale features as appropriate for algorithm\n5. Split temporal data if time-series analysis is needed\n\n### Step 2: Select Detection Algorithm\nChoose appropriate anomaly detection method based on data characteristics:\n- **Isolation Forest**: For high-dimensional data with complex anomalies\n- **One-Class SVM**: For clearly defined normal behavior patterns\n- **Local Outlier Factor (LOF)**: For density-based anomaly detection\n- **Statistical Methods**: For simple univariate or multivariate analysis\n- **Autoencoders**: For complex patterns in large datasets\n\n### Step 3: Configure Detection Parameters\nSet algorithm parameters to balance sensitivity:\n- Define contamination rate (expected proportion of anomalies)\n- Set distance metrics appropriate for feature types\n- Configure threshold values for anomaly scoring\n- Establish validation strategy for parameter tuning\n\n### Step 4: Execute Anomaly Detection\nRun the detection algorithm on prepared data:\n1. Apply selected algorithm using Bash tool\n2. Generate anomaly scores for each data point\n3. Classify points as normal or anomalous based on threshold\n4. Extract characteristics of identified anomalies\n\n### Step 5: Analyze and Report Results\nInterpret detection results and provide insights:\n- Summarize number and distribution of anomalies\n- Highlight most significant outliers with context\n- Identify patterns or clusters among anomalies\n- Generate visualizations showing anomaly distribution\n- Provide recommendations for further investigation\n\n## Output\n\nThe skill produces comprehensive anomaly detection results:\n\n### Anomaly Summary Report\n- Total data points analyzed\n- Number of anomalies detected\n- Contamination rate (percentage of anomalies)\n- Algorithm used and configuration parameters\n- Confidence scores for detected anomalies\n\n### Detailed Anomaly List\nFor each detected anomaly:\n- Record identifier and timestamp (if applicable)\n- Anomaly score and confidence level\n- Feature values showing deviation from normal\n- Contextual information about the outlier\n- Severity classification (low, medium, high, critical)\n\n### Statistical Analysis\n- Distribution of anomaly scores across dataset\n- Feature importance for anomaly classification\n- Comparison with normal data patterns\n- Temporal distribution of anomalies (if time-series)\n- Clustering analysis of anomaly types\n\n### Visualizations\n- Scatter plots highlighting anomalies in feature space\n- Time-series plots with anomaly markers\n- Distribution histograms comparing normal vs anomalous data\n- Heatmaps showing feature correlations for anomalies\n\n### Recommendations\n- Suggested follow-up investigations for critical anomalies\n- Data quality improvements to reduce false positives\n- Monitoring strategies for real-time detection\n- Algorithm refinements based on domain knowledge\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Insufficient Data Volume**\n- Error: Not enough data points for statistical significance\n- Solution: Collect more data, adjust contamination rate, or use simpler statistical methods\n\n**High False Positive Rate**\n- Error: Too many normal points classified as anomalies\n- Solution: Adjust detection threshold, refine feature selection, or use domain-specific constraints\n\n**Algorithm Performance Issues**\n- Error: Detection algorithm too slow for large datasets\n- Solution: Use sampling techniques, optimize parameters, or switch to faster algorithms like Isolation Forest\n\n**Feature Scaling Problems**\n- Error: Anomalies dominated by high-magnitude features\n- Solution: Apply appropriate normalization or standardization to all features before detection\n\n**Missing Ground Truth**\n- Error: Unable to validate detection accuracy without labels\n- Solution: Use domain expertise for manual validation, implement feedback loop for model improvement\n\n## Resources\n\n### Anomaly Detection Algorithms\n- Isolation Forest documentation and implementation examples\n- One-Class SVM for novelty detection\n- Local Outlier Factor (LOF) for density-based detection\n- Autoencoder-based anomaly detection for deep learning approaches\n\n### Python Libraries\n- scikit-learn anomaly detection module\n- PyOD (Python Outlier Detection) comprehensive library\n- TensorFlow/PyTorch for deep learning-based detection\n- statsmodels for statistical anomaly detection\n\n### Domain-Specific Applications\n- Fraud detection in financial transactions\n- Network intrusion detection and security monitoring\n- Manufacturing quality control and defect detection\n- Healthcare anomaly detection for patient monitoring\n- IoT sensor data anomaly identification\n\n### Best Practices\n- Balance sensitivity to avoid excessive false positives\n- Validate results with domain experts\n- Monitor detection performance over time\n- Update models as normal behavior evolves\n- Document anomaly investigation procedures\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "anomaly-detection-system",
        "category": "ai-ml",
        "path": "plugins/ai-ml/anomaly-detection-system",
        "version": "1.0.0",
        "description": "Detect anomalies and outliers in data"
      },
      "filePath": "plugins/ai-ml/anomaly-detection-system/skills/detecting-data-anomalies/SKILL.md"
    },
    {
      "slug": "detecting-database-deadlocks",
      "name": "detecting-database-deadlocks",
      "description": "Use when you need to work with deadlock detection. This skill provides deadlock detection and resolution with comprehensive guidance and automation. Trigger with phrases like \"detect deadlocks\", \"resolve deadlocks\", or \"prevent deadlocks\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Deadlock Detector\n\nThis skill provides automated assistance for database deadlock detector tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-deadlock-detector/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-deadlock-detector/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-deadlock-detector/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-deadlock-detector-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-deadlock-detector-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-deadlock-detector-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-deadlock-detector",
        "category": "database",
        "path": "plugins/database/database-deadlock-detector",
        "version": "1.0.0",
        "description": "Database plugin for database-deadlock-detector"
      },
      "filePath": "plugins/database/database-deadlock-detector/skills/detecting-database-deadlocks/SKILL.md"
    },
    {
      "slug": "detecting-infrastructure-drift",
      "name": "detecting-infrastructure-drift",
      "description": "Use when detecting infrastructure drift from desired state. Trigger with phrases like \"check for drift\", \"infrastructure drift detection\", \"compare actual vs desired state\", or \"detect configuration changes\". Identifies discrepancies between current infrastructure and IaC definitions using terraform plan, cloudformation drift detection, or manual comparison. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(aws:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Infrastructure Drift Detector\n\nThis skill provides automated assistance for infrastructure drift detector tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Infrastructure as Code (IaC) files are up to date in {baseDir}\n- Cloud provider CLI is installed and authenticated\n- IaC tool (Terraform/CloudFormation/Pulumi) is installed\n- Remote state storage is configured and accessible\n- Appropriate read permissions for infrastructure resources\n\n## Instructions\n\n1. **Identify IaC Tool**: Determine if using Terraform, CloudFormation, Pulumi, or ARM\n2. **Fetch Current State**: Retrieve actual infrastructure state from cloud provider\n3. **Load Desired State**: Read IaC configuration from {baseDir}/terraform or equivalent\n4. **Compare States**: Execute drift detection command for the IaC platform\n5. **Analyze Differences**: Identify added, modified, or removed resources\n6. **Generate Report**: Create detailed report of drift with affected resources\n7. **Suggest Remediation**: Provide commands to resolve drift (apply or import)\n8. **Document Findings**: Save drift report to {baseDir}/drift-reports/\n\n## Output\n\nGenerates drift detection reports:\n\n**Terraform Drift Report:**\n```\nDrift Detection Report - 2025-12-10 10:30:00\n==============================================\n\nResources with Drift: 3\n\n1. aws_instance.web_server\n   Status: Modified\n   Drift: instance_type changed from \"t3.micro\" to \"t3.small\"\n   Action: Update IaC to match or revert instance type\n\n2. aws_s3_bucket.assets\n   Status: Modified\n   Drift: versioning_enabled changed from true to false\n   Action: Re-enable versioning or update IaC\n\n3. aws_iam_role.lambda_exec\n   Status: Deleted\n   Drift: Role no longer exists in AWS\n   Action: terraform apply to recreate\n\nRemediation Command:\nterraform plan -out=drift-fix.tfplan\nterraform apply drift-fix.tfplan\n```\n\n**CloudFormation Drift:**\n```yaml\nStackName: production-vpc\nDriftStatus: DRIFTED\nResources:\n  - LogicalResourceId: VPC\n    ResourceType: AWS::EC2::VPC\n    DriftStatus: IN_SYNC\n  - LogicalResourceId: PublicSubnet\n    ResourceType: AWS::EC2::Subnet\n    DriftStatus: MODIFIED\n    PropertyDifferences:\n      - PropertyPath: /Tags\n        ExpectedValue: [{Key: Env, Value: prod}]\n        ActualValue: [{Key: Env, Value: production}]\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**State Lock Error**\n- Error: \"Error acquiring state lock\"\n- Solution: Ensure no other terraform process is running, or force-unlock if safe\n\n**Authentication Failure**\n- Error: \"Unable to authenticate to cloud provider\"\n- Solution: Refresh credentials with `aws configure` or `gcloud auth login`\n\n**Missing State File**\n- Error: \"No state file found\"\n- Solution: Initialize terraform with `terraform init` or specify remote backend\n\n**Permission Denied**\n- Error: \"Access denied reading resource\"\n- Solution: Grant read-only IAM permissions to service account\n\n**State Version Mismatch**\n- Error: \"State file version too new\"\n- Solution: Upgrade Terraform version or use compatible state version\n\n## Resources\n\n- Terraform drift documentation: https://www.terraform.io/docs/cli/state/\n- AWS CloudFormation drift detection: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detect-drift-stack.html\n- Drift remediation best practices in {baseDir}/docs/drift-remediation.md\n- Automated drift detection scripts in {baseDir}/scripts/drift-check.sh\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "infrastructure-drift-detector",
        "category": "devops",
        "path": "plugins/devops/infrastructure-drift-detector",
        "version": "1.0.0",
        "description": "Detect infrastructure drift from desired state"
      },
      "filePath": "plugins/devops/infrastructure-drift-detector/skills/detecting-infrastructure-drift/SKILL.md"
    },
    {
      "slug": "detecting-memory-leaks",
      "name": "detecting-memory-leaks",
      "description": "Detect potential memory leaks and analyze memory usage patterns in code. Use when troubleshooting performance issues related to memory growth or identifying leak sources. Trigger with phrases like \"detect memory leaks\", \"analyze memory usage\", or \"find memory issues\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(profiling:*)",
        "Bash(memory:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Memory Leak Detector\n\nThis skill provides automated assistance for memory leak detector tasks.\n\n## Overview\n\nThis skill helps you identify and resolve memory leaks in your code. By analyzing your code for common memory leak patterns, it can help you improve the performance and stability of your application.\n\n## How It Works\n\n1. **Initiate Analysis**: The user requests memory leak detection.\n2. **Code Analysis**: The plugin analyzes the codebase for potential memory leak patterns.\n3. **Report Generation**: The plugin generates a report detailing potential memory leaks and recommended fixes.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Detect potential memory leaks in your application.\n- Analyze memory usage patterns to identify performance bottlenecks.\n- Troubleshoot performance issues related to memory leaks.\n\n## Examples\n\n### Example 1: Identifying Event Listener Leaks\n\nUser request: \"detect memory leaks in my event handling code\"\n\nThe skill will:\n1. Analyze the code for unremoved event listeners.\n2. Generate a report highlighting potential event listener leaks and suggesting how to properly remove them.\n\n### Example 2: Analyzing Cache Growth\n\nUser request: \"analyze memory usage to find excessive cache growth\"\n\nThe skill will:\n1. Analyze cache implementations for unbounded growth.\n2. Identify caches that are not properly managed and recommend strategies for limiting their size.\n\n## Best Practices\n\n- **Code Review**: Always review the reported potential leaks to ensure they are genuine issues.\n- **Regular Analysis**: Incorporate memory leak detection into your regular development workflow.\n- **Targeted Analysis**: Focus your analysis on specific areas of your code that are known to be memory-intensive.\n\n## Integration\n\nThis skill can be used in conjunction with other performance analysis tools to provide a comprehensive view of application performance.\n\n## Prerequisites\n\n- Access to application source code in {baseDir}/\n- Memory profiling tools (valgrind, heapdump, etc.)\n- Understanding of application memory architecture\n- Runtime environment for testing\n\n## Instructions\n\n1. Analyze code for common memory leak patterns\n2. Identify unremoved event listeners and callbacks\n3. Check for unbounded cache growth\n4. Review closure usage and retained references\n5. Generate report with leak locations and severity\n6. Provide remediation recommendations\n\n## Output\n\n- Memory leak detection report with file locations\n- Pattern analysis for event listeners and caches\n- Memory usage trends and growth patterns\n- Code snippets highlighting potential leaks\n- Recommended fixes with code examples\n\n## Error Handling\n\nIf memory leak detection fails:\n- Verify code file access permissions\n- Check profiling tool installation\n- Validate code syntax and structure\n- Ensure sufficient memory for analysis\n- Review runtime environment configuration\n\n## Resources\n\n- Memory profiling tool documentation\n- Memory leak detection best practices\n- JavaScript/Node.js memory management guides\n- Performance optimization resources",
      "parentPlugin": {
        "name": "memory-leak-detector",
        "category": "performance",
        "path": "plugins/performance/memory-leak-detector",
        "version": "1.0.0",
        "description": "Detect memory leaks and analyze memory usage patterns"
      },
      "filePath": "plugins/performance/memory-leak-detector/skills/detecting-memory-leaks/SKILL.md"
    },
    {
      "slug": "detecting-performance-bottlenecks",
      "name": "detecting-performance-bottlenecks",
      "description": "This skill enables AI assistant to detect and resolve performance bottlenecks in applications. it analyzes cpu, memory, i/o, and database performance to identify areas of concern. use this skill when you need to diagnose slow application performance, op... Use when optimizing performance. Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Bash(cmd:*), Grep, Glob version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Bottleneck Detector\n\nThis skill provides automated assistance for bottleneck detector tasks.\n\n## Overview\n\nThis skill empowers Claude to identify and address performance bottlenecks across different layers of an application. By pinpointing performance issues in CPU, memory, I/O, and database operations, it assists in optimizing resource utilization and improving overall application speed and responsiveness.\n\n## How It Works\n\n1. **Architecture Analysis**: Claude analyzes the application's architecture and data flow to understand potential bottlenecks.\n2. **Bottleneck Identification**: The plugin identifies bottlenecks across CPU, memory, I/O, database, lock contention, and resource exhaustion.\n3. **Remediation Suggestions**: Claude provides remediation strategies with code examples to resolve the identified bottlenecks.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Diagnose slow application performance.\n- Optimize resource usage (CPU, memory, I/O, database).\n- Proactively prevent performance issues.\n\n## Examples\n\n### Example 1: Diagnosing Slow Database Queries\n\nUser request: \"detect bottlenecks in my database queries\"\n\nThe skill will:\n1. Analyze database query performance and identify slow-running queries.\n2. Suggest optimizations like indexing or query rewriting to improve database performance.\n\n### Example 2: Identifying Memory Leaks\n\nUser request: \"analyze performance and find memory leaks\"\n\nThe skill will:\n1. Profile memory usage patterns to identify potential memory leaks.\n2. Provide code examples and recommendations to fix the memory leaks.\n\n## Best Practices\n\n- **Comprehensive Analysis**: Always analyze all potential bottleneck areas (CPU, memory, I/O, database) for a complete picture.\n- **Prioritize by Severity**: Focus on addressing the most severe bottlenecks first for maximum impact.\n- **Test Thoroughly**: After implementing remediation strategies, thoroughly test the application to ensure the bottlenecks are resolved and no new issues are introduced.\n\n## Integration\n\nThis skill can be used in conjunction with code generation plugins to automatically implement the suggested remediation strategies. It also integrates with monitoring and logging tools to provide real-time performance data.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "bottleneck-detector",
        "category": "performance",
        "path": "plugins/performance/bottleneck-detector",
        "version": "1.0.0",
        "description": "Detect and resolve performance bottlenecks"
      },
      "filePath": "plugins/performance/bottleneck-detector/skills/detecting-performance-bottlenecks/SKILL.md"
    },
    {
      "slug": "detecting-performance-regressions",
      "name": "detecting-performance-regressions",
      "description": "Automatically detect performance regressions in CI/CD pipelines by comparing metrics against baselines. Use when validating builds or analyzing performance trends. Trigger with phrases like \"detect performance regression\", \"compare performance metrics\", or \"analyze performance degradation\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(ci:*)",
        "Bash(metrics:*)",
        "Bash(testing:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Performance Regression Detector\n\nThis skill provides automated assistance for performance regression detector tasks.\n\n## Overview\n\nThis skill automates the detection of performance regressions within a CI/CD pipeline. It utilizes various methods, including baseline comparison, statistical analysis, and threshold violation checks, to identify performance degradation. The skill provides insights into potential performance bottlenecks and helps maintain application performance.\n\n## How It Works\n\n1. **Analyze Performance Data**: The plugin gathers performance metrics from the CI/CD environment.\n2. **Detect Regressions**: It employs methods like baseline comparison, statistical analysis, and threshold checks to detect regressions.\n3. **Report Findings**: The plugin generates a report summarizing the detected performance regressions and their potential impact.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance regressions in a CI/CD pipeline.\n- Analyze performance metrics for potential degradation.\n- Compare current performance against historical baselines.\n\n## Examples\n\n### Example 1: Identifying a Response Time Regression\n\nUser request: \"Detect performance regressions in the latest build. Specifically, check for increases in response time.\"\n\nThe skill will:\n1. Analyze response time metrics from the latest build.\n2. Compare the response times against a historical baseline.\n3. Report any statistically significant increases in response time that exceed a defined threshold.\n\n### Example 2: Detecting Throughput Degradation\n\nUser request: \"Analyze throughput for performance regressions after the recent code merge.\"\n\nThe skill will:\n1. Gather throughput data (requests per second) from the post-merge CI/CD run.\n2. Compare the throughput to pre-merge values, looking for statistically significant drops.\n3. Generate a report highlighting any throughput degradation, indicating a potential performance regression.\n\n## Best Practices\n\n- **Define Baselines**: Establish clear and representative performance baselines for accurate comparison.\n- **Set Thresholds**: Configure appropriate thresholds for identifying significant performance regressions.\n- **Monitor Key Metrics**: Focus on monitoring critical performance metrics relevant to the application's behavior.\n\n## Integration\n\nThis skill can be integrated with other CI/CD tools to automatically trigger regression detection upon new builds or code merges. It can also be combined with reporting plugins to generate detailed performance reports.\n\n## Prerequisites\n\n- Historical performance baselines in {baseDir}/performance/baselines/\n- Access to CI/CD performance metrics\n- Statistical analysis tools\n- Defined regression thresholds\n\n## Instructions\n\n1. Collect performance metrics from current build\n2. Load historical baseline data\n3. Apply statistical analysis to detect significant changes\n4. Check for threshold violations\n5. Identify specific regressed metrics\n6. Generate regression report with root cause analysis\n\n## Output\n\n- Performance regression detection report\n- Statistical comparison with baselines\n- List of regressed metrics with severity\n- Visualization of performance trends\n- Recommendations for investigation\n\n## Error Handling\n\nIf regression detection fails:\n- Verify baseline data availability\n- Check metrics collection configuration\n- Validate statistical analysis parameters\n- Ensure threshold definitions are valid\n- Review CI/CD integration setup\n\n## Resources\n\n- Statistical process control for performance testing\n- CI/CD performance testing best practices\n- Regression detection algorithms\n- Performance monitoring strategies",
      "parentPlugin": {
        "name": "performance-regression-detector",
        "category": "performance",
        "path": "plugins/performance/performance-regression-detector",
        "version": "1.0.0",
        "description": "Detect performance regressions in CI/CD pipeline"
      },
      "filePath": "plugins/performance/performance-regression-detector/skills/detecting-performance-regressions/SKILL.md"
    },
    {
      "slug": "detecting-sql-injection-vulnerabilities",
      "name": "detecting-sql-injection-vulnerabilities",
      "description": "Detect and analyze SQL injection vulnerabilities in application code and database queries. Use when you need to scan code for SQL injection risks, review query construction, validate input sanitization, or implement secure query patterns. Trigger with phrases like \"detect SQL injection\", \"scan for SQLi vulnerabilities\", \"review database queries\", or \"check SQL security\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(code-scan:*), Bash(security-test:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Sql Injection Detector\n\nThis skill provides automated assistance for sql injection detector tasks.\n\n## Overview\n\nReviews database query construction and input handling to detect SQL injection risks and recommend secure query patterns and tests.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Application source code accessible in {baseDir}/\n- Database query files and ORM configurations available\n- Framework information (Django, Rails, Express, Spring, etc.)\n- Write permissions for security reports in {baseDir}/security-reports/\n\n## Instructions\n\n1. Identify input surfaces and data flows into database queries.\n2. Review query construction and parameterization patterns.\n3. Flag injection vectors and document impact.\n4. Recommend fixes (parameterized queries, ORM patterns, validation) and tests.\n\n### 1. Code Discovery Phase\n\nLocate database interaction code:\n- SQL query construction\n- ORM usage (ActiveRecord, Hibernate, SQLAlchemy, etc.)\n- Stored procedure calls\n- Dynamic query builders\n- User input handling for database operations\n\n**Common patterns to search**:\n- Direct SQL: `SELECT`, `INSERT`, `UPDATE`, `DELETE` statements\n- String concatenation with user input\n- ORM raw query methods\n- Template-based query construction\n\n### 2. Vulnerability Pattern Detection\n\n**Critical SQL Injection Patterns**:\n\n**String Concatenation (Highly Vulnerable)**:\n```python\n# INSECURE: Direct concatenation\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\nquery = \"SELECT * FROM users WHERE username = '\" + user_input + \"'\"\ncursor.execute(query)\n\n# Attacker input: ' OR '1'='1' --\n# Results in: SELECT * FROM users WHERE username = '' OR '1'='1' --'\n```\n\n**Formatted Strings (Vulnerable)**:\n```javascript\n// INSECURE: Template literals\nconst query = `SELECT * FROM products WHERE id = ${productId}`;\ndb.query(query);\n```\n\n**Dynamic WHERE Clauses (Vulnerable)**:\n```php\n// INSECURE: Building conditions dynamically\n$sql = \"SELECT * FROM orders WHERE status = \" . $_GET['status'];\nmysqli_query($conn, $sql);\n```\n\n### 3. Secure Pattern Validation\n\n**Parameterized Queries (Secure)**:\n```python\n# SECURE: Parameterized query\nquery = \"SELECT * FROM users WHERE username = %s\"\ncursor.execute(query, (user_input,))\n```\n\n**Prepared Statements (Secure)**:\n```java\n// SECURE: Prepared statement\nString query = \"SELECT * FROM products WHERE category = ?\";\nPreparedStatement stmt = conn.prepareStatement(query);\nstmt.setString(1, userCategory);\n```\n\n**ORM Query Builders (Secure when used properly)**:\n```javascript\n// SECURE: ORM with parameter binding\nconst user = await User.findOne({\n  where: { username: userInput }\n});\n```\n\n### 4. Severity Classification\n\nRate SQL injection risks:\n- **Critical**: Direct user input in SQL without sanitization (authentication bypass, data exfiltration)\n- **High**: Partially sanitized input or weak escaping (potential bypass)\n- **Medium**: ORM misuse or raw queries with limited exposure\n- **Low**: SQL in administrative interfaces with access control\n\n### 5. Context Analysis\n\nFor each potential vulnerability:\n- Input source (GET/POST parameters, cookies, headers)\n- Query purpose (SELECT, INSERT, UPDATE, DELETE)\n- Data sensitivity (user data, financial records, PII)\n- Authentication requirements\n- Exploitability assessment\n\n### 6. Generate Security Report\n\nDocument findings with:\n- Vulnerability location (file, line number)\n- Vulnerable code snippet\n- Attack vector examples\n- Impact assessment\n- Secure code replacement\n- Framework-specific remediation\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: SQL injection vulnerability report saved to {baseDir}/security-reports/sqli-scan-YYYYMMDD.md\n\n**Report Structure**:\n```\n# SQL Injection Vulnerability Report\nScan Date: 2024-01-15\nApplication: E-commerce Platform\nFramework: Django 4.2\n\n## Executive Summary\n- Total Vulnerabilities: 12\n- Critical: 3\n- High: 5\n- Medium: 3\n- Low: 1\n\n## Critical Findings\n\n### 1. Authentication Bypass via SQL Injection\n**File**: {baseDir}/src/auth/login.py\n**Line**: 45\n**Severity**: CRITICAL (CVSS 9.8)\n\n**Vulnerable Code**:\n```python\ndef authenticate_user(username, password):\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    user = db.execute(query).fetchone()\n    return user is not None\n```\n\n**Attack Vector**:\n```\nUsername: admin' --\nPassword: anything\n\nResulting Query: SELECT * FROM users WHERE username='admin' --' AND password='anything'\nEffect: Password check bypassed, authentication as admin succeeds\n```\n\n**Impact**:\n- Complete authentication bypass\n- Unauthorized access to any account\n- Administrative privilege escalation\n- No audit trail of compromise\n\n**Remediation**:\n```python\ndef authenticate_user(username, password):\n    query = \"SELECT * FROM users WHERE username=%s AND password=%s\"\n    user = db.execute(query, (username, password)).fetchone()\n    return user is not None\n```\n\n**Additional Recommendations**:\n- Use password hashing (bcrypt, Argon2)\n- Implement account lockout after failed attempts\n- Add MFA for admin accounts\n\n### 2. Data Exfiltration via UNION-based SQLi\n**File**: {baseDir}/src/api/products.py\n**Line**: 78\n**Severity**: CRITICAL (CVSS 8.6)\n\n[Similar detailed structure...]\n\n## Summary by File\n- {baseDir}/src/auth/: 4 vulnerabilities (2 critical)\n- {baseDir}/src/api/: 6 vulnerabilities (1 critical, 5 high)\n- {baseDir}/src/reports/: 2 vulnerabilities (2 medium)\n\n## Remediation Checklist\n- [ ] Replace all string concatenation with parameterized queries\n- [ ] Audit ORM raw query usage\n- [ ] Implement input validation layer\n- [ ] Enable SQL query logging for monitoring\n- [ ] Deploy WAF rules for SQLi detection\n- [ ] Conduct penetration testing after fixes\n```\n\n**Secondary Outputs**:\n- SARIF format for GitHub Security scanning\n- JSON for vulnerability management systems\n- CWE mapping (CWE-89: SQL Injection)\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Framework Not Recognized**\n   - Error: \"Unknown ORM or database framework\"\n   - Resolution: Apply generic SQL injection pattern detection\n   - Note: Framework-specific recommendations unavailable\n\n2. **Obfuscated or Minified Code**\n   - Error: \"Cannot analyze compiled/minified code\"\n   - Resolution: Request source code or unminified version\n   - Limitation: Reduced detection accuracy\n\n3. **False Positives on Sanitized Input**\n   - Error: \"Flagged code that uses proper sanitization\"\n   - Resolution: Manual review required, check sanitization implementation\n   - Enhancement: Whitelist known-safe patterns\n\n4. **Dynamic Query Construction**\n   - Error: \"Complex query building logic difficult to analyze\"\n   - Resolution: Trace data flow manually, flag for manual review\n   - Recommendation: Refactor to simpler, auditable patterns\n\n5. **Stored Procedures**\n   - Error: \"Cannot analyze stored procedure definitions\"\n   - Resolution: Request SQL files or database exports\n   - Alternative: Focus on application-level code\n\n## Examples\n\n- \"Scan our codebase for SQL injection risks in dynamic query construction.\"\n- \"Review these query snippets and propose parameterized equivalents with tests.\"\n\n## Resources\n\n**OWASP Resources**:\n- SQL Injection Prevention Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html\n- OWASP Top 10 - Injection: https://owasp.org/www-project-top-ten/\n\n**Vulnerability Databases**:\n- CWE-89: SQL Injection: https://cwe.mitre.org/data/definitions/89.html\n- CAPEC-66: SQL Injection: https://capec.mitre.org/data/definitions/66.html\n\n**Framework-Specific Guides**:\n- Django Security: https://docs.djangoproject.com/en/stable/topics/security/\n- Rails Security: https://guides.rubyonrails.org/security.html\n- Node.js Best Practices: https://nodejs.org/en/docs/guides/security/\n\n**Testing Tools**:\n- SQLMap: Automated SQL injection testing\n- Burp Suite: Manual testing and exploitation\n- OWASP ZAP: Automated scanning\n\n**Secure Coding Examples**:\n- Parameterized queries by language/framework\n- Input validation patterns\n- Escaping techniques (when parameterization impossible)\n- Least privilege database user configuration",
      "parentPlugin": {
        "name": "sql-injection-detector",
        "category": "security",
        "path": "plugins/security/sql-injection-detector",
        "version": "1.0.0",
        "description": "Detect SQL injection vulnerabilities"
      },
      "filePath": "plugins/security/sql-injection-detector/skills/detecting-sql-injection-vulnerabilities/SKILL.md"
    },
    {
      "slug": "emitting-api-events",
      "name": "emitting-api-events",
      "description": "Build event-driven APIs with webhooks, Server-Sent Events, and real-time notifications. Use when building event-driven API architectures. Trigger with phrases like \"add webhooks\", \"implement events\", or \"create event-driven API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:events-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Event Emitter\n\nThis skill provides automated assistance for api event emitter tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:events-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api event emitter tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-event-emitter",
        "category": "api-development",
        "path": "plugins/api-development/api-event-emitter",
        "version": "1.0.0",
        "description": "Implement event-driven APIs with message queues and event streaming"
      },
      "filePath": "plugins/api-development/api-event-emitter/skills/emitting-api-events/SKILL.md"
    },
    {
      "slug": "encrypting-and-decrypting-data",
      "name": "encrypting-and-decrypting-data",
      "description": "Validate encryption implementations and cryptographic practices. Use when reviewing data security measures. Trigger with 'check encryption', 'validate crypto', or 'review security keys'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Encryption Tool\n\nThis skill provides automated assistance for encryption tool tasks.\n\n## Overview\n\nThis skill empowers Claude to handle data encryption and decryption tasks seamlessly. It leverages the encryption-tool plugin to provide a secure way to protect sensitive information, ensuring confidentiality and integrity.\n\n## How It Works\n\n1. **Identify Encryption/Decryption Request**: Claude analyzes the user's request to determine whether encryption or decryption is required.\n2. **Select Encryption Method**: Claude prompts the user to specify the desired encryption algorithm (e.g., AES, RSA). If not specified, a default secure method is chosen.\n3. **Execute Encryption/Decryption**: Claude utilizes the encryption-tool plugin to perform the encryption or decryption operation on the provided data or file.\n4. **Return Encrypted/Decrypted Data**: Claude presents the encrypted or decrypted data to the user, or saves the result to a file as requested.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Encrypt sensitive data before storage or transmission.\n- Decrypt previously encrypted data for access or processing.\n- Generate encrypted files for secure archiving.\n\n## Examples\n\n### Example 1: Encrypting a Text File\n\nUser request: \"Encrypt the file 'sensitive_data.txt' using AES.\"\n\nThe skill will:\n1. Activate the encryption-tool plugin.\n2. Encrypt the contents of 'sensitive_data.txt' using AES encryption.\n3. Save the encrypted data to a new file (e.g., 'sensitive_data.txt.enc').\n\n### Example 2: Decrypting an Encrypted File\n\nUser request: \"Decrypt the file 'confidential.txt.enc'.\"\n\nThe skill will:\n1. Activate the encryption-tool plugin.\n2. Decrypt the contents of 'confidential.txt.enc' using the appropriate decryption key (assumed to be available or prompted for).\n3. Save the decrypted data to a new file (e.g., 'confidential.txt').\n\n## Best Practices\n\n- **Key Management**: Always store encryption keys securely and avoid hardcoding them in scripts.\n- **Algorithm Selection**: Choose encryption algorithms based on the sensitivity of the data and the required security level. Consider industry best practices and compliance requirements.\n- **Data Integrity**: Implement mechanisms to verify the integrity of encrypted data to detect tampering or corruption.\n\n## Integration\n\nThis skill can be integrated with other Claude Code plugins, such as file management tools, to automate the encryption and decryption of files during data processing workflows. It can also be combined with security auditing tools to ensure compliance with security policies.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "encryption-tool",
        "category": "security",
        "path": "plugins/security/encryption-tool",
        "version": "1.0.0",
        "description": "Encrypt and decrypt data with various algorithms"
      },
      "filePath": "plugins/security/encryption-tool/skills/encrypting-and-decrypting-data/SKILL.md"
    },
    {
      "slug": "engineering-features-for-machine-learning",
      "name": "engineering-features-for-machine-learning",
      "description": "Create, select, and transform features to improve machine learning model performance. Handles feature scaling, encoding, and importance analysis. Use when asked to \"engineer features\" or \"select features\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Feature Engineering Toolkit\n\nThis skill provides automated assistance for feature engineering toolkit tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for feature engineering toolkit tasks.\nThis skill enables Claude to leverage the feature-engineering-toolkit plugin to enhance machine learning models. It automates the process of creating new features, selecting the most relevant ones, and transforming existing features to better suit the model's needs. Use this skill to improve the accuracy, efficiency, and interpretability of machine learning models.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude analyzes the user's request and identifies the specific feature engineering task required.\n2. **Generating Code**: Claude generates Python code using the feature-engineering-toolkit plugin to perform the requested task. This includes data validation and error handling.\n3. **Executing Task**: The generated code is executed, creating, selecting, or transforming features as requested.\n4. **Providing Insights**: Claude provides performance metrics and insights related to the feature engineering process, such as the importance of newly created features or the impact of transformations on model performance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create new features from existing data to improve model accuracy.\n- Select the most relevant features from a dataset to reduce model complexity and improve efficiency.\n- Transform features to better suit the assumptions of a machine learning model (e.g., scaling, normalization, encoding).\n\n## Examples\n\n### Example 1: Improving Model Accuracy\n\nUser request: \"Create new features from the existing 'age' and 'income' columns to improve the accuracy of a customer churn prediction model.\"\n\nThe skill will:\n1. Generate code to create interaction terms between 'age' and 'income' (e.g., age * income, age / income).\n2. Execute the code and evaluate the impact of the new features on model performance.\n\n### Example 2: Reducing Model Complexity\n\nUser request: \"Select the top 10 most important features from the dataset to reduce the complexity of a fraud detection model.\"\n\nThe skill will:\n1. Generate code to calculate feature importance using a suitable method (e.g., Random Forest, SelectKBest).\n2. Execute the code and select the top 10 features based on their importance scores.\n\n## Best Practices\n\n- **Data Validation**: Always validate the input data to ensure it is clean and consistent before performing feature engineering.\n- **Feature Scaling**: Scale numerical features to prevent features with larger ranges from dominating the model.\n- **Encoding Categorical Features**: Encode categorical features appropriately (e.g., one-hot encoding, label encoding) to make them suitable for machine learning models.\n\n## Integration\n\nThis skill integrates with the feature-engineering-toolkit plugin, providing a seamless way to create, select, and transform features for machine learning models. It can be used in conjunction with other Claude Code skills to build complete machine learning pipelines.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "feature-engineering-toolkit",
        "category": "ai-ml",
        "path": "plugins/ai-ml/feature-engineering-toolkit",
        "version": "1.0.0",
        "description": "Feature creation, selection, and transformation tools"
      },
      "filePath": "plugins/ai-ml/feature-engineering-toolkit/skills/engineering-features-for-machine-learning/SKILL.md"
    },
    {
      "slug": "evaluating-machine-learning-models",
      "name": "evaluating-machine-learning-models",
      "description": "This skill allows AI assistant to evaluate machine learning models using a comprehensive suite of metrics. it should be used when the user requests model performance analysis, validation, or testing. AI assistant can use this skill to assess model accuracy, p... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Model Evaluation Suite\n\nThis skill provides automated assistance for model evaluation suite tasks.\n\n## Overview\n\nThis skill empowers Claude to perform thorough evaluations of machine learning models, providing detailed performance insights. It leverages the `model-evaluation-suite` plugin to generate a range of metrics, enabling informed decisions about model selection and optimization.\n\n## How It Works\n\n1. **Analyzing Context**: Claude analyzes the user's request to identify the model to be evaluated and any specific metrics of interest.\n2. **Executing Evaluation**: Claude uses the `/eval-model` command to initiate the model evaluation process within the `model-evaluation-suite` plugin.\n3. **Presenting Results**: Claude presents the generated metrics and insights to the user, highlighting key performance indicators and potential areas for improvement.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess the performance of a machine learning model.\n- Compare the performance of multiple models.\n- Identify areas where a model can be improved.\n- Validate a model's performance before deployment.\n\n## Examples\n\n### Example 1: Evaluating Model Accuracy\n\nUser request: \"Evaluate the accuracy of my image classification model.\"\n\nThe skill will:\n1. Invoke the `/eval-model` command.\n2. Analyze the model's performance on a held-out dataset.\n3. Report the accuracy score and other relevant metrics.\n\n### Example 2: Comparing Model Performance\n\nUser request: \"Compare the F1-score of model A and model B.\"\n\nThe skill will:\n1. Invoke the `/eval-model` command for both models.\n2. Extract the F1-score from the evaluation results.\n3. Present a comparison of the F1-scores for model A and model B.\n\n## Best Practices\n\n- **Specify Metrics**: Clearly define the specific metrics of interest for the evaluation.\n- **Data Validation**: Ensure the data used for evaluation is representative of the real-world data the model will encounter.\n- **Interpret Results**: Provide context and interpretation of the evaluation results to facilitate informed decision-making.\n\n## Integration\n\nThis skill integrates seamlessly with the `model-evaluation-suite` plugin, providing a comprehensive solution for model evaluation within the Claude Code environment. It can be combined with other skills to build automated machine learning workflows.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "model-evaluation-suite",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-evaluation-suite",
        "version": "1.0.0",
        "description": "Comprehensive model evaluation with multiple metrics"
      },
      "filePath": "plugins/ai-ml/model-evaluation-suite/skills/evaluating-machine-learning-models/SKILL.md"
    },
    {
      "slug": "excel-dcf-modeler",
      "name": "excel-dcf-modeler",
      "description": "Build discounted cash flow (DCF) valuation models in Excel with free cash flow projections, WACC calculations, and sensitivity analysis for investment banking and corporate finance teams Activates when you request \"excel dcf modeler\" functionality. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "ClaudeCodePlugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Excel DCF Modeler\n\nCreates professional DCF valuation models following investment banking standards and best practices.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## When to Invoke This Skill\n\nAutomatically load this Skill when the user asks to:\n- \"Create a DCF model\"\n- \"Build a valuation model\"\n- \"Calculate enterprise value\"\n- \"Value [company name]\"\n- \"DCF for [company]\"\n- \"Discounted cash flow analysis\"\n- \"What's the intrinsic value\"\n\n## Model Structure\n\nThis Skill creates a complete 4-sheet Excel DCF model:\n\n### Sheet 1: Assumptions\n- **Company Information**: Name, ticker, base year, fiscal year end\n- **Revenue Growth Rates**: Year 1-5 projections (%)\n- **Profitability Metrics**: EBITDA margin, D&A as % of revenue\n- **Working Capital**: NWC as % of revenue\n- **Capital Expenditures**: CapEx as % of revenue\n- **Tax Rate**: Corporate tax rate (%)\n- **Terminal Growth**: Long-term growth rate (typically 2-3%)\n- **Discount Rate (WACC)**: Weighted average cost of capital\n\n### Sheet 2: Free Cash Flow Projections\n```\nRevenue (Year 0 - Year 5)\n  Less: Operating Expenses\n= EBITDA\n  Less: Depreciation & Amortization\n= EBIT\n  Less: Taxes (EBIT √ó Tax Rate)\n= NOPAT (Net Operating Profit After Tax)\n  Add: Depreciation & Amortization\n  Less: Capital Expenditures\n  Less: Change in Net Working Capital\n= Unlevered Free Cash Flow\n```\n\n### Sheet 3: Valuation\n```\nPresent Value of FCF (Years 1-5)\n  Year 1 FCF / (1 + WACC)^1\n  Year 2 FCF / (1 + WACC)^2\n  ...\n  Year 5 FCF / (1 + WACC)^5\n= Sum of PV(FCF)\n\nTerminal Value Calculation\n  Terminal FCF = Year 5 FCF √ó (1 + Terminal Growth Rate)\n  Terminal Value = Terminal FCF / (WACC - Terminal Growth Rate)\n  PV of Terminal Value = Terminal Value / (1 + WACC)^5\n\nEnterprise Value\n  = Sum of PV(FCF) + PV(Terminal Value)\n\nEquity Value\n  = Enterprise Value\n  - Net Debt\n  + Non-Operating Assets\n\nEquity Value per Share\n  = Equity Value / Shares Outstanding\n```\n\n### Sheet 4: Sensitivity Analysis\nTwo-way sensitivity table showing Enterprise Value sensitivity to:\n- **Rows**: WACC (ranging from -2% to +2% of base case)\n- **Columns**: Terminal Growth Rate (ranging from 1.5% to 3.5%)\n- **Output**: Enterprise Value at each combination\n\n## Step-by-Step Workflow\n\n### 1. Gather Inputs\nAsk the user for the following information (provide defaults based on industry averages if user is uncertain):\n\n**Required Inputs:**\n- Company name and ticker symbol\n- Base year revenue (most recent fiscal year)\n- Revenue growth rates for Years 1-5 (e.g., 15%, 12%, 10%, 8%, 6%)\n- EBITDA margin % (e.g., 20%)\n- Tax rate % (e.g., 21% for US corporations)\n\n**Optional Inputs (use defaults if not provided):**\n- D&A as % of revenue (default: 5%)\n- CapEx as % of revenue (default: 4%)\n- NWC as % of revenue (default: 10%)\n- Terminal growth rate (default: 2.5%)\n- WACC/discount rate (default: 10%)\n- Net debt amount (default: $0)\n- Shares outstanding (if calculating per-share value)\n\n### 2. Validate Inputs\nEnsure the following before building the model:\n- Revenue growth rates are reasonable (typically 0-30%)\n- EBITDA margin is positive\n- Tax rate is between 0-40%\n- Terminal growth < WACC (model won't work if g >= WACC)\n- WACC is reasonable (typically 7-15%)\n\n### 3. Build Excel Model\nUse the Excel MCP server to:\n1. Create new workbook\n2. Create 4 sheets: \"Assumptions\", \"FCF Projections\", \"Valuation\", \"Sensitivity\"\n3. Populate assumptions in Sheet 1\n4. Build FCF projection formulas in Sheet 2 (link to assumptions)\n5. Calculate PV of FCF and Terminal Value in Sheet 3\n6. Create sensitivity table in Sheet 4\n7. Apply professional formatting:\n   - Currency format for monetary values\n   - Percentage format for rates\n   - Conditional formatting on sensitivity table (green = high value, red = low value)\n   - Bold headers, freeze panes\n   - Cell borders for clarity\n\n### 4. Calculate Results\nReturn to the user:\n- Enterprise Value (in millions/billions)\n- Equity Value (if net debt provided)\n- Equity Value per Share (if shares outstanding provided)\n- Key assumptions used\n- Link to sensitivity analysis\n\n### 5. Provide Insights\nAfter building the model, provide brief commentary:\n- Is the implied valuation reasonable given industry comparables?\n- What are the key value drivers? (usually revenue growth and margins)\n- What's the sensitivity to WACC? (typically highest sensitivity)\n- Recommend focus areas for further analysis\n\n## Example Formulas\n\n### Free Cash Flow Calculation\n```excel\n// Revenue (Year 1)\n=Assumptions!B5 * (1 + Assumptions!B10)\n\n// EBITDA\n=Revenue * Assumptions!B15\n\n// EBIT\n=EBITDA - (Revenue * Assumptions!B16)\n\n// NOPAT\n=EBIT * (1 - Assumptions!B18)\n\n// Free Cash Flow\n=NOPAT + (Revenue * Assumptions!B16) - (Revenue * Assumptions!B17) - ŒîNWCapital\n```\n\n### Present Value\n```excel\n// PV of Year 1 FCF\n=FCF_Year1 / (1 + WACC)^1\n\n// Terminal Value\n=(FCF_Year5 * (1 + Terminal_Growth)) / (WACC - Terminal_Growth)\n\n// PV of Terminal Value\n=Terminal_Value / (1 + WACC)^5\n```\n\n### Sensitivity Table\n```excel\n// Use Excel's data table feature\n// Row input: Terminal Growth Rate\n// Column input: WACC\n// Formula: =Enterprise_Value\n```\n\n## Example Interaction\n\n**User:** \"Create a DCF model for Tesla\"\n\n**Skill Response:**\n```\nI'll build a DCF valuation model for Tesla. Let me gather some inputs.\n\nWhat base year revenue should we use for Tesla?\n(For reference, Tesla's 2023 revenue was approximately $96.8B)\n\nUser: $96.8 billion\n\nGreat! What revenue growth rates should we assume for Years 1-5?\n(Recommended: Start high and taper down, e.g., 20%, 18%, 15%, 12%, 10%)\n\nUser: 25%, 20%, 15%, 12%, 10%\n\nWhat EBITDA margin should we assume?\n(Tesla's recent EBITDA margin has been around 15-17%)\n\nUser: 16%\n\nPerfect! I'll use the following assumptions:\n- Base year revenue: $96.8B\n- Growth rates: 25%, 20%, 15%, 12%, 10%\n- EBITDA margin: 16%\n- Tax rate: 21% (standard US rate)\n- WACC: 10% (typical for automotive/tech)\n- Terminal growth: 2.5%\n- D&A: 5% of revenue\n- CapEx: 4% of revenue\n- NWC: 10% of revenue\n\nBuilding your DCF model...\n\n‚úÖ DCF Model Complete!\n\nüìä Valuation Results:\n- Enterprise Value: $847.3 billion\n- Terminal Value represents: 68% of EV\n- Implied equity value per share: $243 (assuming 3.48B shares)\n\nüéØ Key Insights:\n- High growth assumptions drive significant value creation\n- Model is sensitive to WACC (see sensitivity table)\n- Terminal value is major component - validate terminal assumptions\n- Revenue growth is the primary value driver\n\nüìÅ Model saved to: Tesla_DCF_Model.xlsx\n\nThe sensitivity table shows Enterprise Value ranges from $612B (high WACC, low growth) to $1,124B (low WACC, high growth).\n\nWould you like me to adjust any assumptions or add additional analysis?\n```\n\n## Common Variations\n\n### Request: \"Build a DCF but I don't have all the numbers\"\n\n**Response:**\n```\nNo problem! Build a DCF model with industry-average assumptions, then adjust later.\n\nWhat company are you valuing?\n\nUser: Amazon\n\nWhat's the approximate current revenue? (or I can look up recent financials)\n\nI'll use industry-standard assumptions for a large-cap tech company:\n- Revenue growth: 15%, 12%, 10%, 8%, 6% (tapering)\n- EBITDA margin: 25% (tech average)\n- WACC: 9%\n- Terminal growth: 2.5%\n\nBuild the model with these defaults, then adjust the assumptions in the \"Assumptions\" sheet.\n```\n\n### Request: \"Explain the DCF model you built\"\n\n**Response:**\n```\nSure! Let me walk through the DCF model:\n\nüìä What is a DCF?\nA DCF values a company based on its future cash flows, discounted to present value.\n\nüî¢ The Model:\n1. Projects free cash flow for 5 years\n2. Calculates a \"terminal value\" for years beyond Year 5\n3. Discounts all cash flows to today using WACC\n4. Sums everything to get Enterprise Value\n\nüí∞ Key Components:\n- Free Cash Flow = Cash the company generates after all investments\n- WACC = Cost of capital (what investors expect to earn)\n- Terminal Value = Value of cash flows beyond Year 5 (usually 60-80% of total)\n\n‚ö†Ô∏è Important Notes:\n- This is a simplified DCF (doesn't include debt schedules, detailed NWC)\n- Garbage in, garbage out: Assumptions drive everything\n- Use this as a starting point, not final answer\n- Always compare to comparable company analysis (comps)\n\nWould you like me to explain any specific part in more detail?\n```\n\n## Error Handling\n\n### Invalid Inputs\nIf user provides invalid inputs:\n```\n‚ö†Ô∏è Issue detected: Terminal growth rate (4%) is >= WACC (3.5%)\n\nThis will cause a mathematical error in the terminal value calculation.\nThe terminal growth rate must be less than WACC.\n\nTypical ranges:\n- WACC: 7-15%\n- Terminal growth: 2-3%\n\nWould you like me to adjust these values?\n```\n\n### Missing Information\nIf critical information is missing:\n```\nI need at least the following to build a DCF:\n- Company name or industry\n- Approximate current revenue (or I can use industry average)\n\nAll other assumptions can use industry defaults.\n\nWould you like me to proceed with defaults, or would you prefer to provide specific assumptions?\n```\n\n## Best Practices Embedded\n\nThis Skill follows investment banking best practices:\n\n1. **Conservative Terminal Growth**: Default to 2.5% (GDP growth rate)\n2. **Tapering Growth Rates**: Revenue growth declines over projection period\n3. **Sensitivity Analysis**: Always include WACC and terminal growth sensitivity\n4. **Clear Labeling**: All assumptions clearly labeled and linked\n5. **Professional Formatting**: Currency/percentage formats, frozen panes, borders\n6. **Audit Trail**: Formulas link back to assumptions (no hard-coded values)\n7. **Reasonableness Checks**: Validate inputs before building model\n\n## Resources\n\nSee the resources folder for:\n- `dcf-template.xlsx`: Pre-built DCF template\n- `REFERENCE.md`: Financial modeling best practices\n- `formulas.txt`: Common DCF formulas for reference\n\n## Limitations\n\nThis Skill creates a simplified DCF model suitable for:\n- Initial valuation analysis\n- Pitch decks and presentations\n- Academic exercises\n- Quick \"back of envelope\" valuations\n\nFor detailed investment committee presentations or official fairness opinions:\n- Add detailed debt schedules\n- Include multiple scenarios (base, bull, bear)\n- Add more granular operating assumptions\n- Validate with third-party data\n- Have a finance professional review\n\n## Version History\n\n- v1.0.0 (2025-10-27): Initial release with core DCF functionality",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-dcf-modeler/SKILL.md"
    },
    {
      "slug": "excel-lbo-modeler",
      "name": "excel-lbo-modeler",
      "description": "Create leveraged buyout (LBO) models in Excel with sources & uses, debt schedules, cash flow waterfalls, and IRR calculations for private equity analysis Activates when you request \"excel lbo modeler\" functionality. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "ClaudeCodePlugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Excel LBO Modeler\n\nBuilds comprehensive LBO models for private equity transactions following industry-standard practices.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## When to Invoke This Skill\n\nAutomatically load this Skill when the user asks to:\n- \"Create an LBO model\"\n- \"Build a leveraged buyout model\"\n- \"Private equity analysis for [company]\"\n- \"Calculate IRR for acquisition\"\n- \"LBO for [company]\"\n- \"Buyout model\"\n- \"What returns can we get on this deal?\"\n\n## Model Structure\n\nThis Skill creates a complete 6-sheet Excel LBO model:\n\n### Sheet 1: Transaction Summary\n- **Deal Terms**: Purchase price, entry multiple, equity check\n- **Sources & Uses**: How the deal is financed\n- **Returns Summary**: IRR, MoM, hold period\n\n### Sheet 2: Sources & Uses\n\n**Uses of Funds:**\n```\nPurchase Equity Value\n+ Estimated Net Debt\n= Enterprise Value\n+ Transaction Fees (2-3%)\n+ Financing Fees (2-3%)\n= Total Uses\n```\n\n**Sources of Funds:**\n```\nRevolver (typically 0% at close)\n+ Term Loan A (2-3x EBITDA)\n+ Term Loan B (2-3x EBITDA)\n+ Subordinated Debt (1-2x EBITDA)\n+ Preferred Equity (optional)\n+ Sponsor Equity (remainder)\n= Total Sources\n```\n\n### Sheet 3: Operating Model (5 Years)\n```\nRevenue\n  √ó Revenue Growth %\n  √ó EBITDA Margin %\n= EBITDA\n  - CapEx\n  - Change in NWC\n  - Cash Taxes\n= Cash Flow Available for Debt Service\n```\n\n### Sheet 4: Debt Schedule\n\n**For Each Debt Tranche:**\n```\nBeginning Balance\n+ Draws (if revolver)\n- Mandatory Amortization\n- Excess Cash Flow Sweep\n- Optional Prepayment\n= Ending Balance\n\nInterest Expense = Avg Balance √ó Interest Rate\n```\n\n**Debt Paydown Waterfall:**\n1. Revolver paydown\n2. Term Loan A amortization\n3. Term Loan B amortization\n4. Excess cash ‚Üí Revolver\n5. Remaining excess ‚Üí Optional prepayments\n\n### Sheet 5: Returns Analysis\n\n**Exit Valuation:**\n```\nExit Year EBITDA\n  √ó Exit Multiple\n= Exit Enterprise Value\n  - Net Debt at Exit\n= Exit Equity Value\n```\n\n**Returns Calculation:**\n```\nExit Equity Value\n  √∑ Initial Equity Investment\n= Money-on-Money Multiple (MoM)\n\nIRR = ((Exit Value / Entry Value)^(1/Years)) - 1\n```\n\n**Sensitivity Tables:**\n- Exit Multiple vs Hold Period ‚Üí IRR\n- Exit Multiple vs Entry Multiple ‚Üí IRR\n- EBITDA Growth vs Exit Multiple ‚Üí MoM\n\n### Sheet 6: Debt Covenants\n\n**Leverage Covenants:**\n```\nTotal Debt / EBITDA (typically ‚â§ 6.0x at entry, step down over time)\nSenior Debt / EBITDA (typically ‚â§ 4.0x)\n```\n\n**Coverage Covenants:**\n```\nEBITDA / Interest Expense (typically ‚â• 2.0x)\n(EBITDA - CapEx) / Debt Service (typically ‚â• 1.2x)\n```\n\n## Step-by-Step Workflow\n\n### 1. Gather Transaction Inputs\n\n**Required Inputs:**\n- Target company name\n- Current year EBITDA (or trailing twelve months)\n- Entry valuation multiple (EV/EBITDA, typically 8-12x)\n- Expected revenue growth (Years 1-5)\n- Expected EBITDA margin expansion (if any)\n- Exit multiple assumption (typically = entry multiple or slight premium)\n- Hold period (typically 5 years)\n\n**Optional Inputs (use defaults):**\n- CapEx as % of revenue (default: 3%)\n- NWC as % of revenue (default: 10%)\n- Tax rate (default: 25%)\n- Transaction fees (default: 2.5%)\n\n### 2. Structure Financing\n\n**Typical LBO Debt Structure:**\n- **Revolver**: 1-2x EBITDA, undrawn at close (cash buffer)\n- **Term Loan A**: 2-2.5x EBITDA, 5-7 year amortization\n- **Term Loan B**: 2-3x EBITDA, minimal amortization\n- **Subordinated/Mezzanine**: 1-2x EBITDA (if needed)\n- **Total Debt**: 5-6x EBITDA\n- **Equity**: Remainder (typically 30-40% of purchase price)\n\n**Interest Rates (as of 2025):**\n- Revolver: SOFR + 3.00% (default: 8.0%)\n- Term Loan A: SOFR + 3.50% (default: 8.5%)\n- Term Loan B: SOFR + 4.50% (default: 9.5%)\n- Subordinated: 12-14% (default: 13.0%)\n\n### 3. Build Operating Projections\n\nProject 5 years of operations:\n```\nYear 0 Revenue\n  √ó (1 + Growth Rate) for each year\n= Projected Revenue\n\nProjected Revenue\n  √ó EBITDA Margin\n= Projected EBITDA\n\nEBITDA\n  - CapEx (Revenue √ó CapEx %)\n  - Œî NWC (Change in Revenue √ó NWC %)\n  - Cash Taxes (assume % of EBITDA)\n= Cash Flow to Equity (before debt service)\n```\n\n### 4. Calculate Debt Paydown\n\nFor each year:\n1. Calculate cash available after operations\n2. Pay mandatory debt amortization\n3. Pay interest on all tranches\n4. Use excess cash to pay down revolver first\n5. Then pay down term loans (highest rate first)\n6. Track ending debt balances\n\n### 5. Calculate Returns\n\nAt exit (typically Year 5):\n```\nExit EBITDA\n  √ó Exit Multiple\n= Exit Enterprise Value\n  - Remaining Net Debt\n  + Excess Cash\n= Exit Equity Value\n\nMoney-on-Money = Exit Equity Value / Initial Equity\nIRR = (MoM)^(1/Years) - 1\n```\n\n### 6. Create Sensitivity Tables\n\nBuild 3 sensitivity analyses:\n1. **IRR Sensitivity**: Exit Multiple (rows) vs Hold Period (columns)\n2. **MoM Sensitivity**: Exit Multiple (rows) vs Entry Multiple (columns)\n3. **Value Creation**: EBITDA Growth (rows) vs Margin Expansion (columns)\n\n## Example Formulas\n\n### Sources & Uses\n```excel\n// Enterprise Value\n=EBITDA * Entry_Multiple\n\n// Transaction Fees\n=Enterprise_Value * 2.5%\n\n// Term Loan A\n=EBITDA * 2.0  // 2.0x EBITDA\n\n// Sponsor Equity\n=Total_Uses - Total_Debt\n```\n\n### Debt Schedule\n```excel\n// Beginning Balance Year 1\n=Sources_and_Uses!Term_Loan_A\n\n// Interest Expense\n=AVERAGE(Beginning_Balance, Ending_Balance) * Interest_Rate\n\n// Mandatory Amortization\n=Original_Balance / Amortization_Period\n\n// Ending Balance\n=Beginning_Balance - Mandatory_Amortization - Excess_Paydown\n```\n\n### Returns\n```excel\n// Exit Enterprise Value\n=Exit_EBITDA * Exit_Multiple\n\n// Exit Equity Value\n=Exit_Enterprise_Value - Exit_Net_Debt\n\n// IRR\n=IRR(Initial_Equity (negative), Year1_CF, Year2_CF, ..., Final_CF_with_Exit)\n\n// Money-on-Money\n=Exit_Equity_Value / Initial_Equity_Investment\n```\n\n## Example Interaction\n\n**User:** \"Build an LBO model for a $50M EBITDA software company\"\n\n**Skill Response:**\n```\nI'll create an LBO model for a software company with $50M EBITDA.\n\nLet me gather a few inputs:\n\nWhat entry multiple should we assume (EV/EBITDA)?\n(Software companies typically trade at 10-15x EBITDA)\n\nUser: 12x\n\nGreat! What revenue growth should we assume for Years 1-5?\n(Software companies often grow 15-25% annually)\n\nUser: 20%, 18%, 15%, 12%, 10%\n\nWhat EBITDA margin should we assume?\n(Current or target margin)\n\nUser: Start at 25%, expand to 30% by Year 5\n\nPerfect! I'll use these assumptions:\n\nüìä Transaction:\n- Entry EBITDA: $50M\n- Entry Multiple: 12.0x EV/EBITDA\n- Enterprise Value: $600M\n- Equity Check: ~$240M (40%)\n\nüí∞ Financing Structure:\n- Revolver: $50M (undrawn)\n- Term Loan A: $100M @ 8.5%\n- Term Loan B: $150M @ 9.5%\n- Subordinated: $50M @ 13.0%\n- Total Debt: $350M (7.0x EBITDA)\n- Sponsor Equity: $265M\n\nüìà Operating Assumptions:\n- Revenue growth: 20% ‚Üí 10% (tapering)\n- EBITDA margin: 25% ‚Üí 30% (expansion)\n- CapEx: 3% of revenue\n- NWC: 15% of revenue (working capital heavy)\n\nBuilding LBO model...\n\n‚úÖ LBO Model Complete!\n\nüìä Base Case Returns (5-year hold, 12x exit):\n- Exit EBITDA: $97.8M\n- Exit Enterprise Value: $1,174M\n- Debt Paydown: $350M ‚Üí $89M\n- Exit Equity Value: $1,124M\n- Money-on-Money: 4.2x\n- IRR: 34.2%\n\nüéØ Key Insights:\n- Strong returns driven by EBITDA growth (95% increase)\n- Significant deleveraging (7.0x ‚Üí 0.9x)\n- Margin expansion adds $24M in exit EBITDA\n- Multiple arbitrage: None (flat entry/exit multiple)\n\nüìÅ Model saved to: Software_LBO_Model.xlsx\n\n‚ö†Ô∏è Sensitivity Check:\n- At 10x exit: IRR = 26.8% (still attractive)\n- At 14x exit: IRR = 41.1% (exceptional)\n- Downside protected by strong cash generation\n\nWould you like me to adjust any assumptions or run additional scenarios?\n```\n\n## Common Variations\n\n### Request: \"What returns do we need to hit our 25% IRR hurdle?\"\n\n**Response:**\n```\nLet me build a reverse sensitivity to find the required exit conditions.\n\nWith current assumptions (12x entry, 5-year hold):\n\nTo achieve 25% IRR, you need:\n1. 10.5x exit multiple (13% discount to entry), OR\n2. 15% EBITDA CAGR (vs current 14%), OR\n3. 4.2 year hold period (vs 5 years)\n\nThe deal has multiple paths to your hurdle rate.\n\nWould you like me to add a scenario analysis showing these paths?\n```\n\n### Request: \"Model a dividend recap in Year 3\"\n\n**Response:**\n```\nI'll add a dividend recapitalization to the model.\n\nIn Year 3, after debt paydown, we could refinance and distribute a dividend.\n\nAssumptions:\n- Refi to 5.0x EBITDA (from current 3.8x)\n- Distribute ~$150M to equity sponsors\n- Reset debt maturity clocks\n\nThis would:\n- Boost IRR by ~300bps (partial liquidity in Year 3)\n- Reduce final exit proceeds\n- De-risk investment with early cash return\n\nShould I add this to the model?\n```\n\n## Error Handling\n\n### Over-Leveraged Structure\n```\n‚ö†Ô∏è Warning: Total debt of 8.5x EBITDA exceeds typical LBO leverage (5-7x).\n\nThis financing structure may:\n- Not be achievable in current market\n- Violate debt covenants\n- Leave insufficient cash flow for operations\n\nRecommended: Reduce debt to 6.0x EBITDA maximum.\n\nWould you like me to adjust the capital structure?\n```\n\n### Negative Cash Flow\n```\n‚ö†Ô∏è Issue: Model shows negative cash flow in Year 2.\n\nCauses:\n- Interest expense ($62M) + Debt amortization ($25M) > Cash Flow ($78M)\n- Insufficient EBITDA growth to service debt\n\nSolutions:\n1. Reduce entry leverage (currently 7.0x)\n2. Increase revenue growth assumptions\n3. Extend amortization schedule\n4. Add PIK interest option\n\nWould you like me to adjust the model?\n```\n\n## Best Practices Embedded\n\nThis Skill follows PE industry standards:\n\n1. **Debt Structure**: Typical 5-7x EBITDA total leverage\n2. **Conservative Assumptions**: Exit multiple ‚â§ entry multiple\n3. **Covenant Headroom**: Maintain >15% cushion on covenants\n4. **Cash Flow Sweep**: Model 75-100% excess cash flow to debt paydown\n5. **Multiple Scenarios**: Always include sensitivity tables\n6. **Professional Formatting**: Clear sections, color-coding, audit trail\n7. **Reasonableness Checks**: Validate leverage, coverage, growth rates\n\n## Resources\n\nSee the resources folder for:\n- `lbo-template.xlsx`: Pre-built LBO template\n- `REFERENCE.md`: Private equity modeling best practices\n- `debt-structures.txt`: Common debt structures by industry\n\n## Limitations\n\nThis Skill creates a standard LBO model suitable for:\n- Initial investment committee presentations\n- First-round analysis\n- Learning/training purposes\n- Quick deal screening\n\nFor detailed IC memos or final investment decisions, add:\n- Multiple scenarios (base, downside, upside)\n- Management option pool\n- Detailed working capital build\n- Quarterly debt schedules\n- Covenant compliance analysis throughout hold period\n- Transaction expense detail\n\n## Version History\n\n- v1.0.0 (2025-10-27): Initial release with core LBO functionality",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-lbo-modeler/SKILL.md"
    },
    {
      "slug": "excel-pivot-wizard",
      "name": "excel-pivot-wizard",
      "description": "Generate pivot tables and charts from raw data using natural language - analyze sales by region, summarize data by category, and create visualizations effortlessly Activates when you request \"excel pivot wizard\" functionality. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "ClaudeCodePlugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Excel Pivot Wizard\n\nCreates pivot tables and visualizations from raw data using natural language commands.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## When to Invoke This Skill\n\nAutomatically load this Skill when the user asks to:\n- \"Create a pivot table\"\n- \"Analyze [data] by [dimension]\"\n- \"Summarize sales by region\"\n- \"Show revenue breakdown\"\n- \"Group data by category\"\n- \"Cross-tab analysis\"\n- \"Compare [X] across [Y]\"\n\n## Capabilities\n\n### Pivot Table Generation\n- **Rows**: Group data by one or more fields\n- **Columns**: Cross-tabulate across another dimension\n- **Values**: Aggregate functions (sum, average, count, min, max)\n- **Filters**: Slice data by specific criteria\n- **Calculated Fields**: Create custom formulas\n\n### Visualization\n- Column/bar charts for comparisons\n- Line charts for trends over time\n- Pie charts for composition\n- Combo charts for multiple metrics\n- Conditional formatting for heatmaps\n\n## Common Analysis Patterns\n\n### Pattern 1: Single Dimension Summary\n**Request:** \"Show total sales by region\"\n\n**Output:**\n```\n| Region    | Total Sales |\n|-----------|-------------|\n| Northeast | $1,250,000  |\n| Southeast | $980,000    |\n| Midwest   | $1,100,000  |\n| West      | $1,450,000  |\n| Total     | $4,780,000  |\n```\n\n### Pattern 2: Cross-Tabulation\n**Request:** \"Sales by region and product category\"\n\n**Output:**\n```\n| Region    | Electronics | Clothing | Home Goods | Total     |\n|-----------|-------------|----------|------------|-----------|\n| Northeast | $400K       | $500K    | $350K      | $1,250K   |\n| Southeast | $300K       | $380K    | $300K      | $980K     |\n| Midwest   | $450K       | $350K    | $300K      | $1,100K   |\n| West      | $550K       | $500K    | $400K      | $1,450K   |\n| Total     | $1,700K     | $1,730K  | $1,350K    | $4,780K   |\n```\n\n### Pattern 3: Time-Based Trending\n**Request:** \"Monthly revenue trend for 2024\"\n\n**Output:**\n```\nLine chart showing:\n- X-axis: Jan, Feb, Mar, ..., Dec\n- Y-axis: Revenue\n- Line: Monthly revenue with data labels\n```\n\n### Pattern 4: Top N Analysis\n**Request:** \"Top 10 products by revenue\"\n\n**Output:**\n```\n| Rank | Product       | Revenue   | % of Total |\n|------|---------------|-----------|------------|\n| 1    | Product A     | $450,000  | 9.4%       |\n| 2    | Product B     | $380,000  | 7.9%       |\n| 3    | Product C     | $350,000  | 7.3%       |\n| ...  | ...           | ...       | ...        |\n| 10   | Product J     | $180,000  | 3.8%       |\n|      | Top 10 Total  | $2,850,000| 59.6%      |\n|      | All Others    | $1,930,000| 40.4%      |\n|      | Grand Total   | $4,780,000| 100.0%     |\n```\n\n## Step-by-Step Workflow\n\n### 1. Understand the Data\n\nAsk clarifying questions if needed:\n- What does each column represent?\n- What grain is the data? (transaction-level, daily summary, etc.)\n- What fields should be aggregated vs grouped?\n\n### 2. Interpret the Request\n\nParse natural language into pivot table structure:\n\n**\"Show sales by region and month\"** ‚Üí\n```\nRows: Region\nColumns: Month\nValues: Sum of Sales\n```\n\n**\"Average order value by customer segment\"** ‚Üí\n```\nRows: Customer Segment\nValues: Average of Order Value\n```\n\n**\"Count of orders by product category and sales rep\"** ‚Üí\n```\nRows: Product Category\nColumns: Sales Rep\nValues: Count of Orders\n```\n\n### 3. Build the Pivot Table\n\nUse Excel MCP server to:\n1. Create pivot table on new sheet\n2. Set row fields\n3. Set column fields (if cross-tab)\n4. Set value fields with aggregation function\n5. Add filters/slicers if requested\n6. Apply formatting (currency, thousands separator, etc.)\n\n### 4. Add Enhancements\n\n**Conditional Formatting:**\n```\n- Top 10% values: Dark green\n- Bottom 10% values: Dark red\n- Color scale for heatmap effect\n```\n\n**Calculated Fields:**\n```\n- Profit Margin % = (Revenue - Cost) / Revenue\n- Growth % = (This Year - Last Year) / Last Year\n- % of Total = Value / Grand Total\n```\n\n**Sorting:**\n```\n- Largest to smallest by default\n- Or chronological for dates\n```\n\n### 5. Create Visualization\n\nIf visualization is requested or would be helpful:\n\n**For Comparisons:** Column chart\n**For Trends:** Line chart\n**For Composition:** Pie/donut chart\n**For Correlations:** Scatter plot\n**For Multiple Metrics:** Combo chart (column + line)\n\n## Example Interactions\n\n### Example 1: Sales Analysis\n\n**User:** \"Analyze sales by region and product category\"\n\n**Skill Response:**\n```\nI'll create a pivot table to analyze sales by region and product category.\n\nLooking at your data, I see:\n- 5,234 transaction rows\n- Date range: Jan 1 - Oct 27, 2025\n- 4 regions, 12 product categories\n\nCreating pivot table...\n\n‚úÖ Pivot Table Created!\n\nüìä Sales by Region and Product Category:\n\nTop Insights:\n1. West region leads with $1.45M (30.3% of total)\n2. Electronics is #1 category at $1.70M (35.6%)\n3. Northeast + Electronics = $400K (highest cell)\n4. Southeast underperforming in Home Goods ($300K vs avg $338K)\n\nVisualization: Added column chart comparing regions\n\nWould you like me to:\n- Add a slicer to filter by quarter?\n- Calculate % of total for each cell?\n- Create a separate view for top products?\n```\n\n### Example 2: Time Trend\n\n**User:** \"Show monthly revenue trend\"\n\n**Skill Response:**\n```\nCreating monthly revenue trend analysis...\n\nI see data spanning 10 months (Jan-Oct 2025).\n\n‚úÖ Trend Analysis Complete!\n\nüìà Monthly Revenue:\n\nKey Findings:\n- Strong start: Q1 averaged $520K/month\n- Dip in summer: Jun-Aug averaged $420K/month\n- Recovery: Sep-Oct back to $510K/month\n- YTD Total: $4.78M\n- Average: $478K/month\n\nCreated line chart with:\n- Trend line showing slight upward trajectory\n- Data labels for easy reading\n- Target line at $500K (if provided)\n\nüìä Growth Rates:\n- Jan-Oct: +8.3% overall\n- Best month: March ($575K)\n- Weakest month: August ($385K)\n\nRecommendation: Investigate summer seasonality (Jun-Aug dip)\n```\n\n### Example 3: Customer Segmentation\n\n**User:** \"Show average order value by customer segment\"\n\n**Skill Response:**\n```\nAnalyzing average order value (AOV) across customer segments...\n\n‚úÖ Customer Segmentation Analysis Complete!\n\n| Segment       | Orders | Total Revenue | Avg Order Value | % of Revenue |\n|---------------|--------|---------------|-----------------|--------------|\n| Enterprise    | 245    | $1,850,000    | $7,551         | 38.7%        |\n| Mid-Market    | 892    | $1,920,000    | $2,152         | 40.2%        |\n| Small Business| 2,456  | $835,000      | $340           | 17.5%        |\n| Consumer      | 1,641  | $175,000      | $107           | 3.7%         |\n| Total         | 5,234  | $4,780,000    | $913           | 100.0%       |\n\nüéØ Key Insights:\n- Enterprise: Highest AOV ($7.5K) but only 4.7% of orders\n- Mid-Market: Sweet spot (40% of revenue, $2.2K AOV)\n- Small Business: High volume (47% of orders) but low AOV\n- Consumer: Explore if worth serving (low AOV, high volume)\n\nCreated bar chart showing AOV by segment with order count overlay.\n\nüí° Strategic Questions:\n1. Should we focus more resources on Enterprise (high AOV)?\n2. Can we up-sell Small Business to Mid-Market?\n3. Is Consumer segment profitable at $107 AOV?\n```\n\n### Example 4: Product Performance\n\n**User:** \"Which products are driving the most revenue?\"\n\n**Skill Response:**\n```\nCreating product revenue ranking...\n\n‚úÖ Top Products Analysis Complete!\n\nüìä Top 10 Products (out of 147 total):\n\n| Rank | Product          | Revenue   | Orders | AOV    | % of Total |\n|------|------------------|-----------|--------|--------|------------|\n| 1    | Pro Laptop 15\"   | $450,000  | 385    | $1,169 | 9.4%       |\n| 2    | Business Suite   | $380,000  | 892    | $426   | 7.9%       |\n| 3    | Smart Display    | $350,000  | 1,245  | $281   | 7.3%       |\n| 4    | Wireless Buds    | $310,000  | 2,156  | $144   | 6.5%       |\n| 5    | Office Chair Pro | $295,000  | 428    | $689   | 6.2%       |\n| 6    | Standing Desk    | $280,000  | 356    | $787   | 5.9%       |\n| 7    | Monitor 27\"      | $265,000  | 487    | $544   | 5.5%       |\n| 8    | Keyboard Mech    | $245,000  | 1,823  | $134   | 5.1%       |\n| 9    | Webcam 4K        | $220,000  | 678    | $324   | 4.6%       |\n| 10   | Mouse Wireless   | $195,000  | 2,145  | $91    | 4.1%       |\n|      | **Top 10 Total** | **$2,990,000** | | | **62.5%** |\n|      | All Others (137) | $1,790,000 |    |        | 37.5%       |\n|      | **Grand Total**  | **$4,780,000** | | | **100.0%** |\n\nüéØ Insights:\n- Top 10 products = 62.5% of revenue (strong concentration)\n- Pro Laptop alone = 9.4% of total revenue\n- High-volume low-AOV products: Wireless Buds, Keyboard, Mouse\n- High-AOV low-volume products: Pro Laptop, Standing Desk\n\nCreated:\n- Bar chart: Top 10 products by revenue\n- Pareto chart: Cumulative % of revenue\n\nüí° Strategic Implications:\n1. Focus inventory on Top 20 products (likely 80% of revenue)\n2. Bundle high-volume + high-margin items\n3. Wireless Buds: Explore upsell to Pro Buds (higher margin?)\n```\n\n## Advanced Features\n\n### Calculated Fields\n\n**Request:** \"Show profit margin by product category\"\n\n**Response:**\n```\nI'll add a calculated field for profit margin.\n\nAssuming you have Cost and Revenue columns, I'll calculate:\nProfit Margin % = (Revenue - Cost) / Revenue √ó 100%\n\n| Category    | Revenue | Cost    | Profit  | Margin % |\n|-------------|---------|---------|---------|----------|\n| Electronics | $1,700K | $1,020K | $680K   | 40.0%    |\n| Clothing    | $1,730K | $1,125K | $605K   | 35.0%    |\n| Home Goods  | $1,350K | $945K   | $405K   | 30.0%    |\n| Total       | $4,780K | $3,090K | $1,690K | 35.4%    |\n\nElectronics has the highest margin (40%) despite not being the highest revenue category.\n```\n\n### Slicers & Filters\n\n**Request:** \"Add filters for date range and region\"\n\n**Response:**\n```\nI've added slicers for:\n1. Date Range (select quarters or specific months)\n2. Region (select one or multiple regions)\n3. Product Category (filter to specific categories)\n\nThese slicers are linked to the pivot table - any selection will update the data automatically.\n\nTry clicking Q3 in the date slicer to see summer performance!\n```\n\n### Multiple Aggregations\n\n**Request:** \"Show sum and average of sales by region\"\n\n**Response:**\n```\n| Region    | Total Sales | Avg Sale | Order Count |\n|-----------|-------------|----------|-------------|\n| Northeast | $1,250,000  | $625     | 2,000       |\n| Southeast | $980,000    | $490     | 2,000       |\n| Midwest   | $1,100,000  | $524     | 2,100       |\n| West      | $1,450,000  | $690     | 2,100       |\n\nInteresting: Southeast has lowest average sale despite same order count as Northeast.\nThis suggests smaller basket sizes in Southeast - worth investigating!\n```\n\n## Formatting Best Practices\n\nThe Skill applies professional formatting:\n\n### Numbers\n```\nRevenue: $1,250,000 or $1.25M (use M for millions)\nCounts: 2,000 (thousands separator)\nPercentages: 35.0% (1 decimal)\n```\n\n### Conditional Formatting\n```\nTop performers: Green highlight\nBottom performers: Red highlight\nHeatmap: Color gradient from red (low) to green (high)\n```\n\n### Layout\n```\n- Bold headers\n- Freeze top row and left column\n- Subtotals and grand totals\n- Alternating row colors for readability\n```\n\n## Resources\n\nSee resources folder for:\n- `REFERENCE.md`: Pivot table best practices\n- `examples/`: Sample pivot tables for common analyses\n\n## Limitations\n\nThis Skill creates standard pivot tables for:\n- Summarization and aggregation\n- Cross-tabulation\n- Basic calculations (sum, average, count)\n\nFor advanced analysis, you may need:\n- Power Pivot (for complex data models)\n- Pivot charts with custom formatting\n- Integration with external data sources\n- Real-time data refresh\n\n## Version History\n\n- v1.0.0 (2025-10-27): Initial release with core pivot table generation",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-pivot-wizard/SKILL.md"
    },
    {
      "slug": "excel-variance-analyzer",
      "name": "excel-variance-analyzer",
      "description": "Automate budget vs actual variance analysis in excel with flagging, commentary, and executive summaries for financial reporting and fp&a teams activates when you request \"excel variance analyzer\" functionality. Use when analyzing code or data. Trigger with phrases like 'analyze', 'review', or 'examine'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "ClaudeCodePlugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Excel Variance Analyzer\n\nAutomates variance analysis for monthly/quarterly financial reporting and budget reviews.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## When to Invoke This Skill\n\nAutomatically load this Skill when the user asks to:\n- \"Analyze budget variance\"\n- \"Compare actual vs forecast\"\n- \"Create variance report\"\n- \"Explain budget differences\"\n- \"Why are we over/under budget?\"\n- \"Variance analysis for [period]\"\n- \"Budget vs actual\"\n\n## Report Structure\n\nCreates a comprehensive variance report with 3 sheets:\n\n### Sheet 1: Variance Summary\n```\n| Line Item       | Budget  | Actual  | Variance | % Var | Flag | Commentary |\n|-----------------|---------|---------|----------|-------|------|------------|\n| Revenue         | $1,000K | $950K   | $(50K)   | -5.0% | ‚ö†Ô∏è   | Below plan |\n| COGS            | $600K   | $580K   | $(20K)   | -3.3% | ‚úÖ   | Favorable  |\n| Gross Profit    | $400K   | $370K   | $(30K)   | -7.5% | üî¥   | Investigate|\n| Operating Exp   | $250K   | $280K   | $30K     | 12.0% | üî¥   | Over budget|\n| EBITDA          | $150K   | $90K    | $(60K)   | -40.0%| üî¥   | Miss       |\n```\n\n### Sheet 2: Executive Summary\n```\nüìä Performance Highlights\n- Total Revenue: $950K (5.0% below budget)\n- EBITDA: $90K (40.0% below budget)\n- Key Driver: Operating expenses 12% over budget\n\nüî¥ Top 5 Unfavorable Variances:\n1. EBITDA: $(60K) / -40.0%\n2. Revenue: $(50K) / -5.0%\n3. Operating Expenses: $30K / 12.0%\n4. Gross Profit: $(30K) / -7.5%\n5. Marketing: $25K / 25.0%\n\n‚úÖ Top 5 Favorable Variances:\n1. COGS: $(20K) / -3.3%\n2. Rent: $(5K) / -10.0%\n3. Utilities: $(2K) / -8.0%\n```\n\n### Sheet 3: Trend Analysis (if multiple periods)\n```\n| Line Item | Jan Var% | Feb Var% | Mar Var% | Q1 Var% | Trend |\n|-----------|----------|----------|----------|---------|-------|\n| Revenue   | -3%      | -5%      | -7%      | -5%     | ‚¨áÔ∏è    |\n| COGS      | -2%      | -4%      | -3%      | -3%     | ‚û°Ô∏è    |\n```\n\n## Step-by-Step Workflow\n\n### 1. Load Data\n\nAsk the user for:\n- **Budget data**: Can be Excel file, CSV, or pasted table\n- **Actual data**: Same format as budget\n- **Period**: Month, quarter, YTD\n- **Threshold settings** (or use defaults):\n  - Percentage threshold: 10% (flag items >10% variance)\n  - Dollar threshold: $50K (flag items >$50K absolute variance)\n  - Categories to exclude: (e.g., non-cash items like depreciation)\n\n### 2. Validate Data\n\nBefore analysis, check:\n- Budget and actual have matching line items\n- All values are numeric\n- No missing data for key categories (revenue, expenses, profit)\n- Budget data is reasonable (no zeros where there should be values)\n\n### 3. Calculate Variances\n\nFor each line item:\n```\nAbsolute Variance = Actual - Budget\nPercentage Variance = (Actual - Budget) / Budget √ó 100%\n\nSign Convention:\n- Positive variance for revenue/profit = Favorable (‚úÖ)\n- Negative variance for revenue/profit = Unfavorable (üî¥)\n- Positive variance for expenses = Unfavorable (üî¥)\n- Negative variance for expenses = Favorable (‚úÖ)\n```\n\n### 4. Flag Material Items\n\nApply flagging rules:\n```\nüî¥ Red Flag (Critical):\n- Revenue/profit >10% below budget\n- Expenses >10% over budget\n- Absolute variance >$100K\n\n‚ö†Ô∏è Yellow Flag (Warning):\n- Revenue/profit 5-10% below budget\n- Expenses 5-10% over budget\n- Absolute variance $50K-$100K\n\n‚úÖ Green (On Track):\n- Variance within ¬±5%\n- Absolute variance <$50K\n```\n\n### 5. Generate Commentary\n\nFor each flagged item, provide automated commentary:\n\n**Revenue Below Budget:**\n```\nRevenue is $50K (5%) below budget. Possible drivers:\n- Lower than expected sales volume\n- Pricing pressure\n- Delayed revenue recognition\n- Seasonal factors\nRecommend: Review sales pipeline and backlog\n```\n\n**Expenses Over Budget:**\n```\nOperating expenses are $30K (12%) over budget. Breakdown:\n- Marketing: $25K over (new campaign)\n- Travel: $8K over\n- Utilities: $2K under\nRecommend: Review Q2 marketing spend and ROI\n```\n\n**Favorable Variances:**\n```\nCOGS is $20K (3.3%) under budget. Drivers:\n- Lower raw material costs\n- Improved operational efficiency\n- Volume discounts from suppliers\n```\n\n### 6. Create Executive Summary\n\nSummarize for leadership:\n```\nüìä [Period] Financial Performance\n\nBottom Line:\n- Revenue: [X%] vs budget ([favorable/unfavorable])\n- EBITDA: [X%] vs budget ([favorable/unfavorable])\n- Net Variance Impact: $[XXX]K\n\nKey Takeaways:\n1. [Primary driver of miss/beat]\n2. [Secondary factor]\n3. [Outlook for next period]\n\nAction Items:\n1. [Specific action needed]\n2. [Follow-up required]\n```\n\n## Example Interaction\n\n**User:** \"Analyze Q1 budget vs actual\"\n\n**Skill Response:**\n```\nI'll analyze your Q1 variance. Let me load the data.\n\nDo you have budget and actual data files, or would you like to paste them?\n\nUser: *pastes Excel data*\n\nPerfect! I see data for Q1 2025. Setting analysis parameters:\n- Period: Q1 2025 (Jan-Mar)\n- Line items: 15 categories detected\n- Materiality threshold: ¬±10% or ¬±$50K\n\nRunning variance analysis...\n\n‚úÖ Variance Analysis Complete!\n\nüìä Q1 2025 PERFORMANCE SUMMARY\n\nüéØ Overall Results:\n- Revenue: $2,850K vs budget $3,000K (-5.0%) üî¥\n- Gross Profit: $1,110K vs budget $1,200K (-7.5%) üî¥\n- EBITDA: $270K vs budget $450K (-40.0%) üî¥\n\nüìâ Key Drivers of Underperformance:\n1. Revenue Miss: $(150K) - 5% below plan\n   ‚Üí Slower than expected customer acquisition\n   ‚Üí Delayed Q1 enterprise deals to Q2\n\n2. Operating Expenses Over: $90K - 12% above plan\n   ‚Üí Marketing campaign: $75K incremental spend\n   ‚Üí Headcount additions brought forward to Q1\n\nüîç Detailed Findings:\n\nüî¥ CRITICAL VARIANCES (>10%):\n- Operating Expenses: $840K vs $750K (+12.0%) - $90K over\n- EBITDA: $270K vs $450K (-40.0%) - $180K under\n- Marketing: $325K vs $260K (+25.0%) - $65K over\n\n‚ö†Ô∏è WARNING VARIANCES (5-10%):\n- Revenue: $2,850K vs $3,000K (-5.0%) - $150K under\n- Gross Profit: $1,110K vs $1,200K (-7.5%) - $90K under\n\n‚úÖ FAVORABLE VARIANCES:\n- COGS: $1,740K vs $1,800K (-3.3%) - $60K favorable\n- Facilities: $45K vs $50K (-10.0%) - $5K favorable\n\nüí° RECOMMENDATIONS:\n\nImmediate Actions:\n1. Review Q2 sales pipeline - need $150K catch-up + Q2 plan\n2. Evaluate marketing campaign ROI (Q1 spend +$65K)\n3. Assess if Q1 revenue timing shift impacts full-year plan\n\nStrategic Questions:\n1. Is Q1 revenue miss timing (shifts to Q2) or lost?\n2. Will marketing investment drive Q2-Q4 revenue?\n3. Should we adjust full-year budget guidance?\n\nüìÅ Report saved to: Q1_2025_Variance_Analysis.xlsx\n\nWould you like me to:\n- Add trend analysis vs Q4 2024?\n- Create forecast scenarios for Q2?\n- Drill into specific expense categories?\n```\n\n## Common Variations\n\n### Request: \"Why is marketing over budget?\"\n\n**Response:**\n```\nLet me drill into marketing expenses.\n\nMarketing Variance Detail:\nBudget: $260K\nActual: $325K\nVariance: $65K (25% over)\n\nBreakdown by subcategory:\n- Digital Ads: $180K vs $150K (+$30K)\n  ‚Üí Google Ads campaign expanded\n- Events: $75K vs $60K (+$15K)\n  ‚Üí Added trade show booth\n- Content: $45K vs $40K (+$5K)\n  ‚Üí Video production\n- Agencies: $25K vs $10K (+$15K)\n  ‚Üí New creative agency retainer\n\nPrimary driver: Digital ads campaign expansion (+$30K)\n\nWas this spend planned but not budgeted, or truly incremental?\nWould you like me to analyze marketing ROI?\n```\n\n### Request: \"Compare last 3 months\"\n\n**Response:**\n```\nI'll create a 3-month trend analysis.\n\nRevenue Trend (Budget vs Actual):\n| Month | Budget | Actual | Variance | Var% | Trend |\n|-------|--------|--------|----------|------|-------|\n| Jan   | $1,000K| $970K  | $(30K)   | -3%  | ‚¨áÔ∏è    |\n| Feb   | $1,000K| $950K  | $(50K)   | -5%  | ‚¨áÔ∏è    |\n| Mar   | $1,000K| $930K  | $(70K)   | -7%  | ‚¨áÔ∏è    |\n| Q1    | $3,000K| $2,850K| $(150K)  | -5%  | ‚¨áÔ∏è    |\n\n‚ö†Ô∏è ALERT: Revenue variance is worsening month-over-month\n\nThis suggests a structural issue, not timing:\n- Jan: 97% of budget\n- Feb: 95% of budget\n- Mar: 93% of budget\n\nRecommendation: Immediate deep-dive on sales execution\n```\n\n## Formatting Rules\n\nThe Skill applies professional formatting:\n\n### Conditional Formatting\n```\n- Green cells: Favorable variances (>5% better than budget)\n- Yellow cells: Variances within ¬±5%\n- Red cells: Unfavorable variances (>5% worse than budget)\n```\n\n### Number Formatting\n```\n- Currency: $1,000K or $1.0M (use K for thousands, M for millions)\n- Percentages: 1 decimal place (5.0%)\n- Variance: Show sign ($(50K) or $50K)\n```\n\n### Icons\n```\n‚úÖ = On track / Favorable\n‚ö†Ô∏è = Warning / Needs attention\nüî¥ = Critical / Unfavorable\n‚¨ÜÔ∏è = Improving trend\n‚¨áÔ∏è = Worsening trend\n‚û°Ô∏è = Flat trend\n```\n\n## Best Practices Embedded\n\n1. **Materiality Thresholds**: Don't flag every small variance\n2. **Commentary Not Just Numbers**: Explain \"why\", not just \"what\"\n3. **Action-Oriented**: Recommend next steps\n4. **Executive Summary**: Leadership wants top 5-10 items\n5. **Trend Analysis**: Show if variance is new or ongoing\n6. **Sign Conventions**: Consistent favorable/unfavorable labeling\n7. **Audit Trail**: Show calculations and formulas\n\n## Resources\n\nSee resources folder for:\n- `REFERENCE.md`: Variance analysis best practices\n- `templates/`: Sample variance reports\n\n## Limitations\n\nThis Skill provides automated variance analysis for:\n- Standard income statement formats\n- Monthly/quarterly reporting\n- Budget vs actual comparisons\n\nFor more complex analysis, you may need:\n- Statistical variance analysis (standard deviations)\n- Multi-year trend analysis\n- Driver-based variance decomposition\n- Forecast vs forecast comparisons\n\n## Version History\n\n- v1.0.0 (2025-10-27): Initial release with core variance analysis functionality",
      "parentPlugin": {
        "name": "excel-analyst-pro",
        "category": "business-tools",
        "path": "plugins/business-tools/excel-analyst-pro",
        "version": "1.0.0",
        "description": "Professional financial modeling toolkit for Claude Code with auto-invoked Skills and Excel MCP integration. Build DCF models, LBO analysis, variance reports, and pivot tables using natural language."
      },
      "filePath": "plugins/business-tools/excel-analyst-pro/skills/excel-variance-analyzer/SKILL.md"
    },
    {
      "slug": "explaining-machine-learning-models",
      "name": "explaining-machine-learning-models",
      "description": "This skill enables AI assistant to provide interpretability and explainability for machine learning models. it is triggered when the user requests explanations for model predictions, insights into feature importance, or help understanding model behavior... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Model Explainability Tool\n\nThis skill provides automated assistance for model explainability tool tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze and explain machine learning models. It helps users understand why a model makes certain predictions, identify the most influential features, and gain insights into the model's overall behavior.\n\n## How It Works\n\n1. **Analyze Context**: Claude analyzes the user's request and the available model data.\n2. **Select Explanation Technique**: Claude chooses the most appropriate explanation technique (e.g., SHAP, LIME) based on the model type and the user's needs.\n3. **Generate Explanations**: Claude uses the selected technique to generate explanations for model predictions.\n4. **Present Results**: Claude presents the explanations in a clear and concise format, highlighting key insights and feature importances.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Understand why a machine learning model made a specific prediction.\n- Identify the most important features influencing a model's output.\n- Debug model performance issues by identifying unexpected feature interactions.\n- Communicate model insights to non-technical stakeholders.\n- Ensure fairness and transparency in model predictions.\n\n## Examples\n\n### Example 1: Understanding Loan Application Decisions\n\nUser request: \"Explain why this loan application was rejected.\"\n\nThe skill will:\n1. Analyze the loan application data and the model's prediction.\n2. Calculate SHAP values to determine the contribution of each feature to the rejection decision.\n3. Present the results, highlighting the features that most strongly influenced the outcome, such as credit score or debt-to-income ratio.\n\n### Example 2: Identifying Key Factors in Customer Churn\n\nUser request: \"Interpret the customer churn model and identify the most important factors.\"\n\nThe skill will:\n1. Analyze the customer churn model and its predictions.\n2. Use LIME to generate local explanations for individual customer churn predictions.\n3. Aggregate the LIME explanations to identify the most important features driving churn, such as customer tenure or service usage.\n\n## Best Practices\n\n- **Model Type**: Choose the explanation technique that is most appropriate for the model type (e.g., tree-based models, neural networks).\n- **Data Preprocessing**: Ensure that the data used for explanation is properly preprocessed and aligned with the model's input format.\n- **Visualization**: Use visualizations to effectively communicate model insights and feature importances.\n\n## Integration\n\nThis skill integrates with other data analysis and visualization plugins to provide a comprehensive model understanding workflow. It can be used in conjunction with data cleaning and preprocessing plugins to ensure data quality and with visualization tools to present the explanation results in an informative way.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "model-explainability-tool",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-explainability-tool",
        "version": "1.0.0",
        "description": "Model interpretability and explainability"
      },
      "filePath": "plugins/ai-ml/model-explainability-tool/skills/explaining-machine-learning-models/SKILL.md"
    },
    {
      "slug": "exploring-blockchain-data",
      "name": "exploring-blockchain-data",
      "description": "Query and analyze blockchain data including blocks, transactions, and smart contracts. Use when querying blockchain data and transactions. Trigger with phrases like \"explore blockchain\", \"query transactions\", or \"check on-chain data\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:explorer-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Blockchain Explorer Cli\n\nThis skill provides automated assistance for blockchain explorer cli tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:explorer-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "blockchain-explorer-cli",
        "category": "crypto",
        "path": "plugins/crypto/blockchain-explorer-cli",
        "version": "1.0.0",
        "description": "Command-line blockchain explorer for transactions, addresses, and contracts"
      },
      "filePath": "plugins/crypto/blockchain-explorer-cli/skills/exploring-blockchain-data/SKILL.md"
    },
    {
      "slug": "fairdb-backup-manager",
      "name": "fairdb-backup-manager",
      "description": "Use when you need to work with backup and recovery. This skill provides backup automation and disaster recovery with comprehensive guidance and automation. Trigger with phrases like \"create backups\", \"automate backups\", or \"implement disaster recovery\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(tar:*), Bash(rsync:*), Bash(aws:s3:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Fairdb Backup Manager\n\nThis skill provides automated assistance for fairdb backup manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/fairdb-backup-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/fairdb-backup-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/fairdb-backup-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/fairdb-backup-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/fairdb-backup-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/fairdb-backup-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "fairdb-operations-kit",
        "category": "devops",
        "path": "plugins/devops/fairdb-operations-kit",
        "version": "1.0.0",
        "description": "Complete operations kit for FairDB PostgreSQL as a Service - VPS setup, PostgreSQL management, customer provisioning, monitoring, and backup automation"
      },
      "filePath": "plugins/devops/fairdb-operations-kit/skills/fairdb-backup-manager/SKILL.md"
    },
    {
      "slug": "finding-arbitrage-opportunities",
      "name": "finding-arbitrage-opportunities",
      "description": "Detect profitable arbitrage opportunities across exchanges and DEXs in real-time. Use when discovering profitable arbitrage across exchanges. Trigger with phrases like \"find arbitrage\", \"scan for arb opportunities\", or \"check arbitrage\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:arbitrage-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Arbitrage Opportunity Finder\n\nThis skill provides automated assistance for arbitrage opportunity finder tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:arbitrage-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "arbitrage-opportunity-finder",
        "category": "crypto",
        "path": "plugins/crypto/arbitrage-opportunity-finder",
        "version": "1.0.0",
        "description": "Find and analyze arbitrage opportunities across exchanges and DeFi protocols"
      },
      "filePath": "plugins/crypto/arbitrage-opportunity-finder/skills/finding-arbitrage-opportunities/SKILL.md"
    },
    {
      "slug": "finding-security-misconfigurations",
      "name": "finding-security-misconfigurations",
      "description": "Identify security misconfigurations in infrastructure-as-code, application settings, and system configurations. Use when you need to audit Terraform/CloudFormation templates, check application config files, validate system security settings, or ensure compliance with security best practices. Trigger with phrases like \"find security misconfigurations\", \"audit infrastructure security\", \"check config security\", or \"scan for misconfigured settings\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(config-scan:*), Bash(iac-check:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Security Misconfiguration Finder\n\nThis skill provides automated assistance for security misconfiguration finder tasks.\n\n## Overview\n\nScans configuration and IaC sources for common security misconfigurations, prioritizes findings by severity, and recommends minimal-change remediations.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Configuration files accessible in {baseDir}/ (Terraform, CloudFormation, YAML, JSON)\n- Infrastructure-as-code files (.tf, .yaml, .json, .template)\n- Application configuration files (application.yml, config.json, .env.example)\n- System configuration exports available\n- Write permissions for findings report in {baseDir}/security-findings/\n\n## Instructions\n\n1. Identify the target system/service and gather current configuration.\n2. Compare settings against baseline hardening guidance.\n3. Flag risky defaults, drift, and missing controls with severity.\n4. Provide a minimal-change remediation plan and verification steps.\n\n### 1. Configuration Discovery Phase\n\nLocate configuration files to analyze:\n- Infrastructure-as-code: Terraform (.tf), CloudFormation (.yaml/.json), Ansible, Kubernetes\n- Application configs: application.yml, config.json, web.config, .properties\n- Cloud provider configs: AWS, GCP, Azure resource definitions\n- Container configs: Dockerfile, docker-compose.yml, Kubernetes manifests\n- Web server configs: nginx.conf, httpd.conf, .htaccess\n\n### 2. IaC Misconfiguration Checks\n\n**Cloud Storage**:\n- S3 buckets with public read/write access\n- Storage accounts without encryption\n- Publicly accessible blob containers\n- Missing versioning on data stores\n\n**Network Security**:\n- Security groups allowing 0.0.0.0/0 on sensitive ports (22, 3389, 3306, 5432)\n- Network ACLs with overly permissive rules\n- VPCs without flow logs enabled\n- Missing network segmentation\n\n**Identity and Access**:\n- IAM policies with wildcard (*) permissions\n- Service accounts with admin privileges\n- Missing MFA enforcement\n- Overly broad role assignments\n- Hardcoded credentials in code\n\n**Compute Resources**:\n- EC2 instances with public IPs unnecessarily\n- Unencrypted EBS volumes\n- Missing instance metadata service v2\n- Outdated AMIs/images\n\n**Database Security**:\n- RDS instances publicly accessible\n- Databases without encryption at rest\n- Missing automated backups\n- Weak password policies\n- Default ports exposed\n\n### 3. Application Configuration Checks\n\n**Authentication/Authorization**:\n- Debug mode enabled in production\n- Default credentials present\n- Weak session timeout values\n- Missing CSRF protection\n- Insecure password policies\n\n**API Security**:\n- API keys in configuration files\n- CORS configured with wildcard (*)\n- Missing rate limiting\n- Unencrypted API endpoints\n- Disabled authentication\n\n**Data Protection**:\n- Sensitive data in plain text\n- Missing encryption configuration\n- Insecure cookie settings (no HttpOnly, Secure flags)\n- Logging sensitive information\n\n**Dependencies**:\n- Outdated library versions with CVEs\n- Unmaintained packages\n- Unnecessary dependencies\n\n### 4. System Configuration Checks\n\n**Operating System**:\n- Unnecessary services enabled\n- Weak SSH configurations\n- Missing security updates\n- Insecure file permissions\n- Disabled firewalls\n\n**Web Servers**:\n- Directory listing enabled\n- Server tokens exposed\n- Missing security headers\n- Weak TLS configurations\n- Default error pages revealing information\n\n### 5. Severity Classification\n\nRate findings by severity:\n- **Critical**: Immediate exploitation risk (public S3, hardcoded secrets)\n- **High**: Significant security impact (weak auth, missing encryption)\n- **Medium**: Configuration weaknesses (overly permissive, missing logs)\n- **Low**: Best practice violations (information disclosure, outdated configs)\n\n### 6. Generate Findings Report\n\nDocument all misconfigurations with:\n- Severity and category\n- Specific configuration issue\n- Security impact explanation\n- Remediation steps with code examples\n- Compliance implications (CIS, NIST, PCI-DSS)\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Security misconfigurations report saved to {baseDir}/security-findings/misconfig-YYYYMMDD.md\n\n**Report Structure**:\n```\n# Security Misconfiguration Findings\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\nScan Date: 2024-01-15\nFiles Analyzed: 42\nFindings: 15 (3 Critical, 5 High, 4 Medium, 3 Low)\n\n## Critical Findings\n\n### 1. Publicly Accessible S3 Bucket\n**File**: {baseDir}/terraform/storage.tf\n**Line**: 23\n**Issue**: S3 bucket allows public read access\n**Code**:\n```hcl\nresource \"aws_s3_bucket\" \"data\" {\n  acl = \"public-read\"  # CRITICAL: Public access\n}\n```\n**Impact**: Sensitive data exposed to internet\n**Remediation**:\n```hcl\nresource \"aws_s3_bucket\" \"data\" {\n  acl = \"private\"\n\n  public_access_block {\n    block_public_acls       = true\n    block_public_policy     = true\n    ignore_public_acls      = true\n    restrict_public_buckets = true\n  }\n}\n```\n**Compliance**: Violates CIS AWS 2.1.5, NIST AC-3\n\n## High Findings\n\n### 2. Security Group Allows SSH from Anywhere\n**File**: {baseDir}/terraform/network.tf\n**Line**: 45\n**Issue**: Port 22 open to 0.0.0.0/0\n**Impact**: Brute force attack surface\n**Remediation**: Restrict to specific IP ranges or use bastion host\n\n[Additional findings follow similar structure]\n\n## Summary by Category\n- IAM/Access Control: 4 findings\n- Network Security: 6 findings\n- Data Protection: 3 findings\n- Logging/Monitoring: 2 findings\n\n## Compliance Impact\n- CIS Benchmarks: 8 violations\n- NIST 800-53: 5 controls affected\n- PCI-DSS: 3 requirements unmet\n```\n\n**Secondary Outputs**:\n- JSON for CI/CD integration: {baseDir}/security-findings/misconfig-YYYYMMDD.json\n- CSV for spreadsheet tracking\n- SARIF format for GitHub Security tab\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Unable to Parse Configuration File**\n   - Error: \"Syntax error in {baseDir}/terraform/main.tf\"\n   - Resolution: Validate file syntax first, report parse errors separately\n   - Fallback: Skip malformed files, note in report\n\n2. **Missing Cloud Provider Context**\n   - Error: \"Cannot determine cloud provider from configuration\"\n   - Resolution: Look for provider blocks, file naming conventions\n   - Fallback: Apply generic security checks only\n\n3. **Encrypted or Binary Configuration Files**\n   - Error: \"Cannot read encrypted configuration\"\n   - Resolution: Request decrypted version or configuration export\n   - Note: Document inability to audit in report\n\n4. **Large Configuration Sets**\n   - Error: \"Too many files to analyze ({baseDir}/ has 500+ configs)\"\n   - Resolution: Prioritize by file type and location\n   - Strategy: Start with IaC, then app configs, then system configs\n\n5. **False Positives**\n   - Error: \"Flagged configuration is intentional (dev environment)\"\n   - Resolution: Allow environment-specific exceptions\n   - Enhancement: Support ignore/exception rules file\n\n## Examples\n\n- \"Scan our Terraform in {baseDir}/ for overly permissive security groups and IAM wildcards.\"\n- \"Review Kubernetes manifests for insecure defaults and propose fixes.\"\n\n## Resources\n\n**Security Benchmarks**:\n- CIS Benchmarks: https://www.cisecurity.org/cis-benchmarks/\n- OWASP Configuration Guide: https://cheatsheetseries.owasp.org/cheatsheets/Infrastructure_as_Code_Security_Cheatsheet.html\n- Cloud Security Alliance: https://cloudsecurityalliance.org/\n\n**IaC Security Tools**:\n- tfsec (Terraform): https://github.com/aquasecurity/tfsec\n- Checkov (Multi-cloud): https://www.checkov.io/\n- cfn-nag (CloudFormation): https://github.com/stelligent/cfn_nag\n- kube-bench (Kubernetes): https://github.com/aquasecurity/kube-bench\n\n**Configuration Best Practices**:\n- AWS Security Best Practices: https://aws.amazon.com/architecture/security-identity-compliance/\n- Azure Security Baseline: https://docs.microsoft.com/en-us/security/benchmark/azure/\n- GCP Security Best Practices: https://cloud.google.com/security/best-practices\n\n**Compliance Frameworks**:\n- CIS Controls: https://www.cisecurity.org/controls/\n- NIST 800-53: https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final\n- PCI-DSS Requirements: https://www.pcisecuritystandards.org/\n\n**Remediation Examples**:\n- Terraform security modules: {baseDir}/templates/terraform-secure/\n- CloudFormation secure templates: {baseDir}/templates/cfn-secure/\n- Kubernetes security policies: {baseDir}/templates/k8s-policies/",
      "parentPlugin": {
        "name": "security-misconfiguration-finder",
        "category": "security",
        "path": "plugins/security/security-misconfiguration-finder",
        "version": "1.0.0",
        "description": "Find security misconfigurations"
      },
      "filePath": "plugins/security/security-misconfiguration-finder/skills/finding-security-misconfigurations/SKILL.md"
    },
    {
      "slug": "firebase-vertex-ai",
      "name": "firebase-vertex-ai",
      "description": "Firebase platform expert with Vertex AI Gemini integration for Authentication, Firestore, Storage, Functions, Hosting, and AI-powered features. Use when asked to \"setup firebase\", \"deploy to firebase\", or \"integrate vertex ai with firebase\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Firebase Vertex AI\n\nOperate Firebase projects end-to-end (Auth, Firestore, Functions, Hosting) and integrate Gemini/Vertex AI safely for AI-powered features.\n\n## Overview\n\nUse this skill to design, implement, and deploy Firebase applications that call Vertex AI/Gemini from Cloud Functions (or other GCP services) with secure secrets handling, least-privilege IAM, and production-ready observability.\n\n## Prerequisites\n\n- Node.js runtime and Firebase CLI access for the target project\n- A Firebase project (billing enabled for Functions/Vertex AI as needed)\n- Vertex AI API enabled and permissions to call Gemini/Vertex AI from your backend\n- Secrets managed via env vars or Secret Manager (never in client code)\n\n## Instructions\n\n1. Initialize Firebase (or validate an existing repo): Hosting/Functions/Firestore as required.\n2. Implement backend integration:\n   - add a Cloud Function/HTTP endpoint that calls Gemini/Vertex AI\n   - validate inputs and return structured responses\n3. Configure data and security:\n   - Firestore rules + indexes\n   - Storage rules (if applicable)\n   - Auth providers and authorization checks\n4. Deploy and verify:\n   - deploy Functions/Hosting\n   - run smoke tests against deployed endpoints\n5. Add ops guardrails:\n   - logging/metrics\n   - alerting for error spikes\n   - basic cost controls (budgets/quotas) where appropriate\n\n## Output\n\n- A deployable Firebase project structure (configs + Functions/Hosting as needed)\n- Secure backend code that calls Gemini/Vertex AI (with secrets handled correctly)\n- Firestore/Storage rules and index guidance\n- A verification checklist (local + deployed) and CI-ready commands\n\n## Error Handling\n\n- Auth failures: identify the principal and missing permission/role; fix with least privilege.\n- Billing/API issues: detect which API or quota is blocking and provide remediation steps.\n- Firestore rule/index problems: provide minimal repro queries and rule fixes.\n- Vertex AI call failures: surface model/region mismatches and add retries/backoff for transient errors.\n\n## Examples\n\n**Example: Gemini-backed chat API on Firebase**\n- Request: ‚ÄúDeploy Hosting + a Function that powers a Gemini chat endpoint.‚Äù\n- Result: `/api/chat` function, Secret Manager wiring, and smoke tests.\n\n**Example: Firestore-powered RAG**\n- Request: ‚ÄúBuild a RAG flow that embeds docs and answers with citations.‚Äù\n- Result: ingestion plan, embedding + index strategy, and evaluation prompts.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Firebase docs: https://firebase.google.com/docs\n- Cloud Functions for Firebase: https://firebase.google.com/docs/functions\n- Vertex AI docs: https://cloud.google.com/vertex-ai/docs",
      "parentPlugin": {
        "name": "jeremy-firebase",
        "category": "community",
        "path": "plugins/community/jeremy-firebase",
        "version": "1.0.0",
        "description": "Firebase platform expert for Firestore, Auth, Functions, and Vertex AI integration"
      },
      "filePath": "plugins/community/jeremy-firebase/skills/firebase-vertex-ai/SKILL.md"
    },
    {
      "slug": "firestore-operations-manager",
      "name": "firestore-operations-manager",
      "description": "Manage Firebase/Firestore operations including CRUD, queries, batch processing, and index/rule guidance. Use when you need to create/update/query Firestore documents, run batch writes, troubleshoot missing indexes, or plan migrations. Trigger with phrases like \"firestore operations\", \"create firestore document\", \"batch write\", \"missing index\", or \"fix firestore query\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Firestore Operations Manager\n\nOperate Firestore safely in production: schema-aware CRUD, query/index tuning, batch processing, and guardrails for permissions and cost.\n\n## Overview\n\nUse this skill to design Firestore data access patterns and implement changes with the right indexes, security rules, and operational checks (emulator tests, monitoring, and rollback plans).\n\n## Prerequisites\n\n- A Firebase project with Firestore enabled (or a local emulator setup)\n- A clear collection/document schema (or permission to propose one)\n- Credentials for the target environment (service account / ADC) and a plan for secrets\n\n## Instructions\n\n1. Identify the operation: create/update/delete/query/batch/migration.\n2. Confirm schema expectations and security rules constraints.\n3. Implement the change (or propose a patch) using safe patterns:\n   - prefer batched writes/transactions where consistency matters\n   - add pagination for large queries\n4. Check indexes:\n   - detect required composite indexes and provide `firestore.indexes.json` updates\n5. Validate:\n   - run emulator tests or a minimal smoke query\n   - confirm cost/perf implications for the query pattern\n\n## Output\n\n- Code changes or snippets for the requested Firestore operation\n- Index recommendations (and config updates when needed)\n- A validation checklist (emulator commands and production smoke tests)\n\n## Error Handling\n\n- Permission denied: identify the rule/role blocking the operation and propose least-privilege changes.\n- Missing index: provide the exact composite index needed for the query.\n- Hotspot/latency issues: propose sharding, pagination, or query redesign.\n\n## Examples\n\n**Example: Fix a failing query**\n- Request: ‚ÄúThis query needs a composite index‚Äîwhat do I add?‚Äù\n- Result: the exact index definition and a safer query pattern if needed.\n\n**Example: Batch migration**\n- Request: ‚ÄúBackfill a new field across 100k docs.‚Äù\n- Result: batched write strategy, checkpoints, and rollback guidance.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Firestore docs: https://firebase.google.com/docs/firestore\n- Firestore indexes: https://firebase.google.com/docs/firestore/query-data/indexing",
      "parentPlugin": {
        "name": "jeremy-firestore",
        "category": "community",
        "path": "plugins/community/jeremy-firestore",
        "version": "1.0.0",
        "description": "Firestore database specialist for schema design, queries, and real-time sync"
      },
      "filePath": "plugins/community/jeremy-firestore/skills/firestore-operations-manager/SKILL.md"
    },
    {
      "slug": "forecasting-time-series-data",
      "name": "forecasting-time-series-data",
      "description": "This skill enables AI assistant to forecast future values based on historical time series data. it analyzes time-dependent data to identify trends, seasonality, and other patterns. use this skill when the user asks to predict future values of a time ser... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Time Series Forecaster\n\nThis skill provides automated assistance for time series forecaster tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for time series forecaster tasks.\nThis skill empowers Claude to perform time series forecasting, providing insights into future trends and patterns. It automates the process of data analysis, model selection, and prediction generation, delivering valuable information for decision-making.\n\n## How It Works\n\n1. **Data Analysis**: Claude analyzes the provided time series data, identifying key characteristics such as trends, seasonality, and autocorrelation.\n2. **Model Selection**: Based on the data characteristics, Claude selects an appropriate forecasting model (e.g., ARIMA, Prophet).\n3. **Prediction Generation**: The selected model is trained on the historical data, and future values are predicted along with confidence intervals.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Forecast future sales based on past sales data.\n- Predict website traffic for the next month.\n- Analyze trends in stock prices over the past year.\n\n## Examples\n\n### Example 1: Forecasting Sales\n\nUser request: \"Forecast sales for the next quarter based on the past 3 years of monthly sales data.\"\n\nThe skill will:\n1. Analyze the historical sales data to identify trends and seasonality.\n2. Select and train a suitable forecasting model (e.g., ARIMA or Prophet).\n3. Generate a forecast of sales for the next quarter, including confidence intervals.\n\n### Example 2: Predicting Website Traffic\n\nUser request: \"Predict weekly website traffic for the next month based on the last 6 months of data.\"\n\nThe skill will:\n1. Analyze the website traffic data to identify patterns and seasonality.\n2. Choose an appropriate time series forecasting model.\n3. Generate a forecast of weekly website traffic for the next month.\n\n## Best Practices\n\n- **Data Quality**: Ensure the time series data is clean, complete, and accurate for optimal forecasting results.\n- **Model Selection**: Choose a forecasting model appropriate for the characteristics of the data (e.g., ARIMA for stationary data, Prophet for data with strong seasonality).\n- **Evaluation**: Evaluate the performance of the forecasting model using appropriate metrics (e.g., Mean Absolute Error, Root Mean Squared Error).\n\n## Integration\n\nThis skill can be integrated with other data analysis and visualization tools within the Claude Code ecosystem to provide a comprehensive solution for time series analysis and forecasting.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "time-series-forecaster",
        "category": "ai-ml",
        "path": "plugins/ai-ml/time-series-forecaster",
        "version": "1.0.0",
        "description": "Time series forecasting and analysis"
      },
      "filePath": "plugins/ai-ml/time-series-forecaster/skills/forecasting-time-series-data/SKILL.md"
    },
    {
      "slug": "fuzzing-apis",
      "name": "fuzzing-apis",
      "description": "Perform API fuzzing to discover edge cases, crashes, and security vulnerabilities. Use when performing specialized testing. Trigger with phrases like \"fuzz the API\", \"run fuzzing tests\", or \"discover edge cases\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:fuzz-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Api Fuzzer\n\nThis skill provides automated assistance for api fuzzer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:fuzz-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for api fuzzer tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-fuzzer",
        "category": "testing",
        "path": "plugins/testing/api-fuzzer",
        "version": "1.0.0",
        "description": "Fuzz testing for APIs with malformed inputs, edge cases, and security vulnerability detection"
      },
      "filePath": "plugins/testing/api-fuzzer/skills/fuzzing-apis/SKILL.md"
    },
    {
      "slug": "gcp-examples-expert",
      "name": "gcp-examples-expert",
      "description": "Generate production-ready Google Cloud code examples from official repositories including ADK samples, Genkit templates, Vertex AI notebooks, and Gemini patterns. Use when asked to \"show ADK example\" or \"provide GCP starter kit\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Gcp Examples Expert\n\nThis skill provides automated assistance for gcp examples expert tasks.\n\n## What This Skill Does\n\nExpert aggregator of production-ready code examples from official Google Cloud repositories. Provides battle-tested starter kits, templates, and best practices for building AI agents, workflows, and applications on Google Cloud Platform.\n\n## When This Skill Activates\n\n### Trigger Phrases\n- \"Show me ADK sample code\"\n- \"Genkit starter template\"\n- \"Vertex AI code example\"\n- \"Agent Starter Pack template\"\n- \"Gemini function calling example\"\n- \"Multi-agent orchestration pattern\"\n- \"Google Cloud starter kit\"\n- \"Production agent template\"\n- \"How to implement RAG with Genkit\"\n- \"A2A protocol code example\"\n\n### Use Cases\n- Quick access to official Google Cloud code examples\n- Production-ready agent templates\n- Genkit flow patterns (RAG, multi-step workflows, tool calling)\n- Vertex AI training and deployment code\n- Gemini API integration examples\n- Multi-agent system orchestration\n- Infrastructure as Code (Terraform) templates\n\n## Code Example Categories\n\n### 1. ADK (Agent Development Kit) Samples\n\n**Source**: google/adk-samples\n\n**Examples Provided**:\n- Basic agent creation with Code Execution Sandbox\n- Memory Bank configuration for stateful agents\n- A2A protocol implementation for inter-agent communication\n- Multi-tool agent configuration\n- VPC Service Controls integration\n- IAM least privilege patterns\n\n**Sample Pattern**:\n```python\nfrom google.cloud.aiplatform import agent_builder\n\ndef create_adk_agent(project_id: str, location: str):\n    agent_config = {\n        \"display_name\": \"production-agent\",\n        \"model\": \"gemini-2.5-flash\",\n        \"code_execution_config\": {\n            \"enabled\": True,\n            \"state_ttl_days\": 14\n        },\n        \"memory_bank_config\": {\n            \"enabled\": True\n        }\n    }\n    # Implementation from google/adk-samples\n```\n\n### 2. Agent Starter Pack\n\n**Source**: GoogleCloudPlatform/agent-starter-pack\n\n**Examples Provided**:\n- Production agent with monitoring and observability\n- Auto-scaling configuration\n- Security best practices (Model Armor, VPC-SC)\n- Cloud Monitoring dashboards\n- Alerting policies\n- Error tracking setup\n\n**Sample Pattern**:\n```python\ndef production_agent_with_observability(project_id: str):\n    agent = aiplatform.Agent.create(\n        config={\n            \"auto_scaling\": {\n                \"min_instances\": 2,\n                \"max_instances\": 10\n            },\n            \"vpc_service_controls\": {\"enabled\": True},\n            \"model_armor\": {\"enabled\": True}\n        }\n    )\n    # Full implementation from agent-starter-pack\n```\n\n### 3. Firebase Genkit\n\n**Source**: firebase/genkit\n\n**Examples Provided**:\n- RAG flows with vector search\n- Multi-step workflows\n- Tool calling integration\n- Prompt templates\n- Evaluation frameworks\n- Deployment patterns (Cloud Run, Functions)\n\n**Sample Pattern**:\n```typescript\nimport { genkit, z } from 'genkit';\nimport { googleAI, gemini15ProLatest } from '@genkit-ai/googleai';\n\nconst ragFlow = ai.defineFlow({\n  name: 'ragSearchFlow',\n  inputSchema: z.object({ query: z.string() }),\n  outputSchema: z.object({ answer: z.string() })\n}, async (input) => {\n  // Implementation from firebase/genkit examples\n});\n```\n\n### 4. Vertex AI Samples\n\n**Source**: GoogleCloudPlatform/vertex-ai-samples\n\n**Examples Provided**:\n- Custom model training with Gemini\n- Batch prediction jobs\n- Hyperparameter tuning\n- Model evaluation\n- Endpoint deployment with auto-scaling\n- A/B testing patterns\n\n**Sample Pattern**:\n```python\ndef fine_tune_gemini_model(project_id: str, training_data_uri: str):\n    job = aiplatform.CustomTrainingJob(\n        training_config={\n            \"base_model\": \"gemini-2.5-flash\",\n            \"learning_rate\": 0.001,\n            \"adapter_size\": 8  # LoRA\n        }\n    )\n    # Full implementation from vertex-ai-samples\n```\n\n### 5. Generative AI Examples\n\n**Source**: GoogleCloudPlatform/generative-ai\n\n**Examples Provided**:\n- Gemini multimodal analysis (text, images, video)\n- Function calling with live APIs\n- Structured output generation\n- Grounding with Google Search\n- Safety filters and content moderation\n- Token counting and cost optimization\n\n**Sample Pattern**:\n```python\nfrom vertexai.generative_models import GenerativeModel, Part\n\ndef analyze_multimodal_content(video_uri: str, question: str):\n    model = GenerativeModel(\"gemini-2.5-pro\")\n    video_part = Part.from_uri(video_uri, mime_type=\"video/mp4\")\n    response = model.generate_content([video_part, question])\n    # Implementation from generative-ai examples\n```\n\n### 6. AgentSmithy\n\n**Source**: GoogleCloudPlatform/agentsmithy\n\n**Examples Provided**:\n- Multi-agent orchestration\n- Supervisory agent patterns\n- Agent-to-agent communication\n- Workflow coordination (sequential, parallel, conditional)\n- Task delegation strategies\n- Error handling and retry logic\n\n**Sample Pattern**:\n```python\nfrom agentsmithy import Agent, Orchestrator, Task\n\ndef create_multi_agent_system(project_id: str):\n    orchestrator = Orchestrator(\n        agents=[research_agent, analysis_agent, writer_agent],\n        strategy=\"sequential\"\n    )\n    # Full implementation from agentsmithy\n```\n\n## Workflow\n\n### Phase 1: Identify Use Case\n```\n1. Listen for trigger phrases in user request\n2. Determine which repository has relevant examples\n3. Identify specific code pattern needed\n4. Select appropriate framework (ADK, Genkit, Vertex AI)\n```\n\n### Phase 2: Provide Code Example\n```\n1. Fetch relevant code snippet from knowledge base\n2. Adapt to user's specific requirements\n3. Include imports and dependencies\n4. Add configuration details\n5. Cite source repository\n```\n\n### Phase 3: Explain Best Practices\n```\n1. Highlight security considerations (IAM, VPC-SC, Model Armor)\n2. Show monitoring and observability setup\n3. Demonstrate error handling patterns\n4. Include infrastructure deployment code\n5. Provide cost optimization tips\n```\n\n### Phase 4: Deployment Guidance\n```\n1. Provide Terraform/IaC templates\n2. Show Cloud Build CI/CD configuration\n3. Include testing strategies\n4. Document environment variables\n5. Link to official documentation\n```\n\n## Tool Permissions\n\nThis skill uses the following tools:\n- **Read**: Access code examples and documentation\n- **Write**: Create starter template files\n- **Edit**: Modify templates for user's project\n- **Grep**: Search for specific patterns in examples\n- **Glob**: Find related code files\n- **Bash**: Run setup commands and validation\n\n## Example Interactions\n\n### Example 1: ADK Agent Creation\n**User**: \"Show me how to create an ADK agent with Code Execution\"\n\n**Skill Activates**:\n- Provides code example from google/adk-samples\n- Includes Code Execution Sandbox configuration\n- Shows 14-day state persistence setup\n- Demonstrates security best practices\n- Links to official ADK documentation\n\n### Example 2: Genkit RAG Flow\n**User**: \"I need a Genkit starter template for RAG\"\n\n**Skill Activates**:\n- Provides RAG flow code from firebase/genkit\n- Shows vector search integration\n- Demonstrates embedding generation\n- Includes context retrieval logic\n- Provides deployment configuration\n\n### Example 3: Production Agent Template\n**User**: \"What's the best way to deploy a production agent?\"\n\n**Skill Activates**:\n- Provides Agent Starter Pack template\n- Shows auto-scaling configuration\n- Includes monitoring dashboard setup\n- Demonstrates alerting policies\n- Provides Terraform deployment code\n\n### Example 4: Gemini Multimodal\n**User**: \"How do I analyze video with Gemini?\"\n\n**Skill Activates**:\n- Provides multimodal code from generative-ai repo\n- Shows video part creation\n- Demonstrates prompt engineering\n- Includes error handling\n- Provides cost optimization tips\n\n### Example 5: Multi-Agent System\n**User**: \"I want to build a multi-agent system\"\n\n**Skill Activates**:\n- Provides AgentSmithy orchestration code\n- Shows supervisory agent pattern\n- Demonstrates A2A protocol usage\n- Includes workflow coordination\n- Provides testing strategies\n\n## Best Practices Applied\n\n### Security\n‚úÖ IAM least privilege service accounts\n‚úÖ VPC Service Controls for enterprise isolation\n‚úÖ Model Armor for prompt injection protection\n‚úÖ Encrypted data at rest and in transit\n‚úÖ No hardcoded credentials (use Secret Manager)\n\n### Performance\n‚úÖ Auto-scaling configuration (min/max instances)\n‚úÖ Appropriate machine types and accelerators\n‚úÖ Caching strategies for repeated queries\n‚úÖ Batch processing for high throughput\n‚úÖ Token optimization for cost efficiency\n\n### Observability\n‚úÖ Cloud Monitoring dashboards\n‚úÖ Alerting policies for errors and latency\n‚úÖ Structured logging with severity levels\n‚úÖ Distributed tracing with Cloud Trace\n‚úÖ Error tracking with Cloud Error Reporting\n\n### Reliability\n‚úÖ Multi-region deployment for high availability\n‚úÖ Circuit breaker patterns for fault tolerance\n‚úÖ Retry logic with exponential backoff\n‚úÖ Health check endpoints\n‚úÖ Graceful degradation strategies\n\n### Cost Optimization\n‚úÖ Use Gemini 2.5 Flash for simple tasks (cheaper)\n‚úÖ Gemini 2.5 Pro for complex reasoning (higher quality)\n‚úÖ Batch predictions for bulk processing\n‚úÖ Preemptible instances for non-critical workloads\n‚úÖ Token counting to estimate costs\n\n## Integration with Other Plugins\n\n### Works with jeremy-genkit-pro\n- Provides Genkit code examples\n- Complements Genkit flow architect agent\n- Shares Genkit production best practices\n\n### Works with jeremy-adk-orchestrator\n- Provides ADK sample code\n- Shows A2A protocol implementation\n- Demonstrates multi-agent patterns\n\n### Works with jeremy-vertex-validator\n- Provides production-ready code that passes validation\n- Follows security and performance best practices\n- Includes monitoring from the start\n\n### Works with jeremy-*-terraform plugins\n- Provides infrastructure code examples\n- Shows Terraform module patterns\n- Demonstrates resource configuration\n\n## Version History\n\n- **1.0.0** (2025): Initial release with 6 official Google Cloud repository integrations\n\n## References\n\n- **google/adk-samples**: https://github.com/google/adk-samples\n- **GoogleCloudPlatform/agent-starter-pack**: https://github.com/GoogleCloudPlatform/agent-starter-pack\n- **firebase/genkit**: https://github.com/firebase/genkit\n- **GoogleCloudPlatform/vertex-ai-samples**: https://github.com/GoogleCloudPlatform/vertex-ai-samples\n- **GoogleCloudPlatform/generative-ai**: https://github.com/GoogleCloudPlatform/generative-ai\n- **GoogleCloudPlatform/agentsmithy**: https://github.com/GoogleCloudPlatform/agentsmithy\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-gcp-starter-examples",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-gcp-starter-examples",
        "version": "1.0.0",
        "description": "Google Cloud starter kits and example code aggregator with ADK samples"
      },
      "filePath": "plugins/ai-ml/jeremy-gcp-starter-examples/skills/gcp-examples-expert/SKILL.md"
    },
    {
      "slug": "generating-api-contracts",
      "name": "generating-api-contracts",
      "description": "Generate API contracts and OpenAPI specifications from code or design documents. Use when documenting API contracts and specifications. Trigger with phrases like \"generate API contract\", \"create OpenAPI spec\", or \"document API contract\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:contract-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Contract Generator\n\nThis skill provides automated assistance for api contract generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:contract-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api contract generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-contract-generator",
        "category": "api-development",
        "path": "plugins/api-development/api-contract-generator",
        "version": "1.0.0",
        "description": "Generate API contracts for consumer-driven contract testing"
      },
      "filePath": "plugins/api-development/api-contract-generator/skills/generating-api-contracts/SKILL.md"
    },
    {
      "slug": "generating-api-docs",
      "name": "generating-api-docs",
      "description": "Create comprehensive API documentation with examples, authentication guides, and SDKs. Use when creating comprehensive API documentation. Trigger with phrases like \"generate API docs\", \"create API documentation\", or \"document the API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:docs-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Documentation Generator\n\nThis skill provides automated assistance for api documentation generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:docs-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api documentation generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-documentation-generator",
        "category": "api-development",
        "path": "plugins/api-development/api-documentation-generator",
        "version": "1.0.0",
        "description": "Generate comprehensive API documentation from OpenAPI/Swagger specs"
      },
      "filePath": "plugins/api-development/api-documentation-generator/skills/generating-api-docs/SKILL.md"
    },
    {
      "slug": "generating-api-sdks",
      "name": "generating-api-sdks",
      "description": "Generate client SDKs in multiple languages from OpenAPI specifications. Use when generating client libraries for API consumption. Trigger with phrases like \"generate SDK\", \"create client library\", or \"build API SDK\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:sdk-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Sdk Generator\n\nThis skill provides automated assistance for api sdk generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:sdk-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api sdk generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-sdk-generator",
        "category": "api-development",
        "path": "plugins/api-development/api-sdk-generator",
        "version": "1.0.0",
        "description": "Generate client SDKs from OpenAPI specs for multiple languages"
      },
      "filePath": "plugins/api-development/api-sdk-generator/skills/generating-api-sdks/SKILL.md"
    },
    {
      "slug": "generating-compliance-reports",
      "name": "generating-compliance-reports",
      "description": "Generate comprehensive compliance reports for security standards. Use when creating compliance documentation. Trigger with 'generate compliance report', 'compliance status', or 'audit compliance'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Compliance Report Generator\n\nThis skill provides automated assistance for compliance report generator tasks.\n\n## Overview\n\nThis skill empowers Claude to create detailed compliance reports, saving time and ensuring accuracy in documenting security practices. It automates the process of gathering information and formatting it into a standardized report, making compliance audits easier and more efficient.\n\n## How It Works\n\n1. **Identify Report Type**: Claude analyzes the user's request to determine the required compliance standard (e.g., PCI DSS, HIPAA).\n2. **Gather Data**: The plugin collects relevant data from the system or prompts the user for necessary information.\n3. **Generate Report**: The plugin formats the collected data into a comprehensive compliance report, including necessary sections and documentation.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Generate a report for a specific compliance standard (e.g., \"generate a HIPAA compliance report\").\n- Create a security audit report.\n- Document adherence to a security policy.\n- Prepare for a compliance audit.\n\n## Examples\n\n### Example 1: Generating a PCI DSS Compliance Report\n\nUser request: \"Generate a PCI DSS compliance report for our e-commerce platform.\"\n\nThe skill will:\n1. Activate the compliance-report-generator plugin.\n2. Prompt the user for information about their e-commerce platform's security controls and processes.\n3. Generate a detailed PCI DSS compliance report based on the provided information.\n\n### Example 2: Creating a HIPAA Compliance Report\n\nUser request: \"Create a HIPAA compliance report to demonstrate our adherence to privacy regulations.\"\n\nThe skill will:\n1. Activate the compliance-report-generator plugin.\n2. Guide the user through a series of questions related to HIPAA requirements.\n3. Compile the answers into a structured HIPAA compliance report.\n\n## Best Practices\n\n- **Specificity**: Be specific about the compliance standard you need a report for (e.g., \"SOC 2 report\").\n- **Completeness**: Provide all the necessary information requested by the plugin to ensure a comprehensive and accurate report.\n- **Review**: Always review the generated report to ensure its accuracy and completeness before submitting it for an audit.\n\n## Integration\n\nThis skill can be integrated with other plugins that provide security assessment or vulnerability scanning capabilities. The results from those plugins can be incorporated into the compliance reports generated by this skill, providing a more comprehensive view of the organization's security posture.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "compliance-report-generator",
        "category": "security",
        "path": "plugins/security/compliance-report-generator",
        "version": "1.0.0",
        "description": "Generate compliance reports"
      },
      "filePath": "plugins/security/compliance-report-generator/skills/generating-compliance-reports/SKILL.md"
    },
    {
      "slug": "generating-conventional-commits",
      "name": "generating-conventional-commits",
      "description": "Generates conventional commit messages using AI. It analyzes code changes and suggests a commit message adhering to the conventional commits specification. Use this skill when you need help writing clear, standardized commit messages, especially a... Use when managing version control. Trigger with phrases like 'commit', 'branch', or 'git'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Devops Automation Pack\n\nThis skill provides automated assistance for devops automation pack tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for devops automation pack tasks.\nThis skill helps you create well-formatted, informative commit messages that follow the conventional commits standard, improving collaboration and automation in your Git workflow. It saves you time and ensures consistency across your project.\n\n## How It Works\n\n1. **Analyze Changes**: The skill analyzes the staged changes in your Git repository.\n2. **Generate Suggestion**: It uses AI to generate a commit message based on the analyzed changes, adhering to the conventional commits format (e.g., `feat: add new feature`, `fix: correct bug`).\n3. **Present to User**: The generated commit message is presented to you for review and acceptance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a commit message after making code changes.\n- Ensure your commit messages follow the conventional commits standard.\n- Save time writing commit messages manually.\n\n## Examples\n\n### Example 1: Adding a New Feature\n\nUser request: \"Generate a commit message for these changes.\"\n\nThe skill will:\n1. Analyze the staged changes related to a new feature.\n2. Generate a commit message like `feat: Implement user authentication`.\n\n### Example 2: Fixing a Bug\n\nUser request: \"Create a commit for the bug fix.\"\n\nThe skill will:\n1. Analyze the staged changes related to a bug fix.\n2. Generate a commit message like `fix: Resolve issue with incorrect password reset`.\n\n## Best Practices\n\n- **Stage Changes**: Ensure all relevant changes are staged before using the skill.\n- **Review Carefully**: Always review the generated commit message before accepting it.\n- **Customize if Needed**: Feel free to customize the generated message to provide more context.\n\n## Integration\n\nThis skill integrates with your Git workflow, providing a convenient way to generate commit messages directly within Claude Code. It complements other Git-related skills in the DevOps Automation Pack, such as `/branch-create` and `/pr-create`.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "devops-automation-pack",
        "category": "packages",
        "path": "plugins/packages/devops-automation-pack",
        "version": "1.0.0",
        "description": "25 professional DevOps plugins for CI/CD, deployment, Docker, Kubernetes, and infrastructure automation. Save 20+ hours of manual work."
      },
      "filePath": "plugins/packages/devops-automation-pack/skills/generating-conventional-commits/SKILL.md"
    },
    {
      "slug": "generating-database-seed-data",
      "name": "generating-database-seed-data",
      "description": "This skill enables AI assistant to generate realistic test data and database seed scripts for development and testing environments. it uses faker libraries to create realistic data, maintains relational integrity, and allows configurable data volumes. u... Use when working with databases or data models. Trigger with phrases like 'database', 'query', or 'schema'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Data Seeder Generator\n\nThis skill provides automated assistance for data seeder generator tasks.\n\n## Overview\n\nThis skill automates the creation of database seed scripts, populating your database with realistic and consistent test data. It leverages Faker libraries to generate diverse and believable data, ensuring relational integrity and configurable data volumes.\n\n## How It Works\n\n1. **Analyze Schema**: Claude analyzes the database schema to understand table structures and relationships.\n2. **Generate Data**: Using Faker libraries, Claude generates realistic data for each table, respecting data types and constraints.\n3. **Maintain Relationships**: Claude ensures foreign key relationships are maintained, creating consistent and valid data across tables.\n4. **Create Seed Script**: Claude generates a database seed script (e.g., SQL, JavaScript) containing the generated data.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Populate a development database with realistic data.\n- Create a seed script for automated database setup.\n- Generate test data for application testing.\n- Demonstrate an application with pre-populated data.\n\n## Examples\n\n### Example 1: Populating a User Database\n\nUser request: \"Create a seed script to populate my users table with 50 realistic users.\"\n\nThe skill will:\n1. Analyze the 'users' table schema (name, email, password, etc.).\n2. Generate 50 sets of realistic user data using Faker libraries.\n3. Create a SQL seed script to insert the generated user data into the 'users' table.\n\n### Example 2: Seeding a Blog Database\n\nUser request: \"Generate test data for my blog database, including posts, comments, and users.\"\n\nThe skill will:\n1. Analyze the 'posts', 'comments', and 'users' table schemas and their relationships.\n2. Generate realistic data for each table, ensuring foreign key relationships are maintained (e.g., comments linked to posts, posts linked to users).\n3. Create a seed script (e.g., JavaScript with TypeORM) to insert the generated data into the database.\n\n## Best Practices\n\n- **Data Volume**: Start with a small data volume and gradually increase it to avoid performance issues.\n- **Data Consistency**: Ensure the Faker libraries used are appropriate for the data types and formats required by your database.\n- **Idempotency**: Design your seed scripts to be idempotent, so they can be run multiple times without causing errors or duplicate data.\n\n## Integration\n\nThis skill integrates well with database migration tools and frameworks, allowing you to automate the entire database setup process, including schema creation and data seeding. It can also be used in conjunction with testing frameworks to generate realistic test data for automated testing.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "data-seeder-generator",
        "category": "database",
        "path": "plugins/database/data-seeder-generator",
        "version": "1.0.0",
        "description": "Generate realistic test data and database seed scripts for development and testing environments"
      },
      "filePath": "plugins/database/data-seeder-generator/skills/generating-database-seed-data/SKILL.md"
    },
    {
      "slug": "generating-docker-compose-files",
      "name": "generating-docker-compose-files",
      "description": "Use when you need to work with Docker Compose. This skill provides Docker Compose file generation with comprehensive guidance and automation. Trigger with phrases like \"generate docker-compose\", \"create compose file\", or \"configure multi-container app\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Docker Compose Generator\n\nThis skill provides automated assistance for docker compose generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/docker-compose-generator/`\n\n**Documentation and Guides**: `{baseDir}/docs/docker-compose-generator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/docker-compose-generator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/docker-compose-generator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/docker-compose-generator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/docker-compose-generator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "docker-compose-generator",
        "category": "devops",
        "path": "plugins/devops/docker-compose-generator",
        "version": "1.0.0",
        "description": "Generate Docker Compose configurations for multi-container applications with best practices"
      },
      "filePath": "plugins/devops/docker-compose-generator/skills/generating-docker-compose-files/SKILL.md"
    },
    {
      "slug": "generating-grpc-services",
      "name": "generating-grpc-services",
      "description": "Generate gRPC service definitions, stubs, and implementations from Protocol Buffers. Use when creating high-performance gRPC services. Trigger with phrases like \"generate gRPC service\", \"create gRPC API\", or \"build gRPC server\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:grpc-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Grpc Service Generator\n\nThis skill provides automated assistance for grpc service generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:grpc-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for grpc service generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "grpc-service-generator",
        "category": "api-development",
        "path": "plugins/api-development/grpc-service-generator",
        "version": "1.0.0",
        "description": "Generate gRPC services with Protocol Buffers and streaming support"
      },
      "filePath": "plugins/api-development/grpc-service-generator/skills/generating-grpc-services/SKILL.md"
    },
    {
      "slug": "generating-helm-charts",
      "name": "generating-helm-charts",
      "description": "Use when generating Helm charts for Kubernetes applications. Trigger with phrases like \"create Helm chart\", \"generate chart for app\", \"package Kubernetes deployment\", or \"helm template\". Produces production-ready charts with Chart.yaml, values.yaml, templates, and best practices for multi-environment deployments. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(helm:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Helm Chart Generator\n\nThis skill provides automated assistance for helm chart generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Helm 3+ is installed on the system\n- Kubernetes cluster access is configured\n- Application container images are available\n- Understanding of application resource requirements\n- Chart repository access (if publishing)\n\n## Instructions\n\n1. **Gather Requirements**: Identify application type, dependencies, configuration needs\n2. **Create Chart Structure**: Generate Chart.yaml with metadata and version info\n3. **Define Values**: Create values.yaml with configurable parameters and defaults\n4. **Build Templates**: Generate deployment, service, ingress, and configmap templates\n5. **Add Helpers**: Create _helpers.tpl for reusable template functions\n6. **Configure Resources**: Set resource limits, security contexts, and health checks\n7. **Test Chart**: Validate with `helm lint` and `helm template` commands\n8. **Document Usage**: Add README with installation instructions and configuration options\n\n## Output\n\nGenerates complete Helm chart structure:\n\n```\n{baseDir}/helm-charts/app-name/\n‚îú‚îÄ‚îÄ Chart.yaml          # Chart metadata\n‚îú‚îÄ‚îÄ values.yaml         # Default configuration\n‚îú‚îÄ‚îÄ templates/\n‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ service.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ _helpers.tpl    # Template helpers\n‚îÇ   ‚îî‚îÄ‚îÄ NOTES.txt       # Post-install notes\n‚îú‚îÄ‚îÄ charts/             # Dependencies\n‚îî‚îÄ‚îÄ README.md\n```\n\n**Example Chart.yaml:**\n```yaml\napiVersion: v2\nname: my-app\ndescription: Production-ready application chart\ntype: application\nversion: 1.0.0\nappVersion: \"1.0.0\"\n```\n\n**Example values.yaml:**\n```yaml\nreplicaCount: 3\nimage:\n  repository: registry/app\n  tag: \"1.0.0\"\n  pullPolicy: IfNotPresent\nresources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 250m\n    memory: 256Mi\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Chart Validation Errors**\n- Error: \"Chart.yaml: version is required\"\n- Solution: Ensure Chart.yaml contains valid apiVersion, name, and version fields\n\n**Template Rendering Failures**\n- Error: \"parse error in deployment.yaml\"\n- Solution: Validate template syntax with `helm template` and check Go template formatting\n\n**Missing Dependencies**\n- Error: \"dependency not found\"\n- Solution: Run `helm dependency update` in chart directory\n\n**Values Override Issues**\n- Error: \"failed to render values\"\n- Solution: Check values.yaml syntax and ensure proper YAML indentation\n\n**Installation Failures**\n- Error: \"release failed: timed out waiting for condition\"\n- Solution: Increase timeout or check pod logs for application startup issues\n\n## Resources\n\n- Helm documentation: https://helm.sh/docs/\n- Chart best practices guide: https://helm.sh/docs/chart_best_practices/\n- Template function reference: https://helm.sh/docs/chart_template_guide/\n- Example charts repository: https://github.com/helm/charts\n- Chart testing guide in {baseDir}/docs/helm-testing.md\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "helm-chart-generator",
        "category": "devops",
        "path": "plugins/devops/helm-chart-generator",
        "version": "1.0.0",
        "description": "Generate Helm charts for Kubernetes applications"
      },
      "filePath": "plugins/devops/helm-chart-generator/skills/generating-helm-charts/SKILL.md"
    },
    {
      "slug": "generating-infrastructure-as-code",
      "name": "generating-infrastructure-as-code",
      "description": "Use when generating infrastructure as code configurations. Trigger with phrases like \"create Terraform config\", \"generate CloudFormation template\", \"write Pulumi code\", or \"IaC for AWS/GCP/Azure\". Produces production-ready code for Terraform, CloudFormation, Pulumi, ARM templates, and CDK across multiple cloud providers. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(aws:*), Bash(gcloud:*), Bash(az:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Infrastructure As Code Generator\n\nThis skill provides automated assistance for infrastructure as code generator tasks.\n\n## Overview\n\nGenerates production-ready IaC (Terraform/CloudFormation/Pulumi/etc.) with modular structure, variables, outputs, and deployment guidance for common cloud stacks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Target cloud provider CLI is installed (aws-cli, gcloud, az)\n- IaC tool is installed (Terraform, Pulumi, AWS CDK)\n- Cloud credentials are configured locally\n- Understanding of target infrastructure architecture\n- Version control system for IaC storage\n\n## Instructions\n\n1. **Identify Platform**: Determine IaC tool (Terraform, CloudFormation, Pulumi, ARM, CDK)\n2. **Define Resources**: Specify cloud resources needed (compute, network, storage, database)\n3. **Establish Structure**: Create modular file structure for maintainability\n4. **Generate Code**: Write IaC configurations with proper syntax and formatting\n5. **Add Variables**: Define input variables for environment-specific values\n6. **Configure Outputs**: Specify outputs for resource references and integrations\n7. **Implement State**: Set up remote state storage for team collaboration\n8. **Document Usage**: Add README with deployment instructions and prerequisites\n\n## Output\n\nGenerates infrastructure as code files:\n\n**Terraform Example:**\n```hcl\n# {baseDir}/terraform/main.tf\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\nterraform {\n  required_version = \">= 1.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nresource \"aws_vpc\" \"main\" {\n  cidr_block = var.vpc_cidr\n  enable_dns_hostnames = true\n\n  tags = {\n    Name = \"${var.project}-vpc\"\n    Environment = var.environment\n  }\n}\n```\n\n**CloudFormation Example:**\n```yaml\n# {baseDir}/cloudformation/template.yaml\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: Production VPC infrastructure\n\nParameters:\n  VpcCidr:\n    Type: String\n    Default: 10.0.0.0/16\n\nResources:\n  VPC:\n    Type: AWS::EC2::VPC\n    Properties:\n      CidrBlock: !Ref VpcCidr\n      EnableDnsHostnames: true\n```\n\n**Pulumi Example:**\n```typescript\n// {baseDir}/pulumi/index.ts\nimport * as aws from \"@pulumi/aws\";\n\nconst vpc = new aws.ec2.Vpc(\"main\", {\n    cidrBlock: \"10.0.0.0/16\",\n    enableDnsHostnames: true,\n    tags: {\n        Name: \"production-vpc\"\n    }\n});\n\nexport const vpcId = vpc.id;\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Syntax Errors**\n- Error: \"Invalid resource syntax in configuration\"\n- Solution: Validate syntax with `terraform validate` or respective tool linter\n\n**Provider Authentication**\n- Error: \"Unable to authenticate with cloud provider\"\n- Solution: Configure credentials via environment variables or CLI login\n\n**Resource Conflicts**\n- Error: \"Resource already exists\"\n- Solution: Import existing resources or use data sources instead of creating new ones\n\n**State Lock Issues**\n- Error: \"Error acquiring state lock\"\n- Solution: Ensure no other process is running, or force unlock if safe\n\n**Dependency Errors**\n- Error: \"Resource depends on resource that does not exist\"\n- Solution: Check resource references and ensure proper dependency ordering\n\n## Examples\n\n- \"Generate Terraform for a VPC + private subnets + NAT + EKS cluster on AWS.\"\n- \"Create a minimal CloudFormation template for an S3 bucket with encryption and public access blocked.\"\n\n## Resources\n\n- Terraform documentation: https://www.terraform.io/docs/\n- AWS CloudFormation guide: https://docs.aws.amazon.com/cloudformation/\n- Pulumi documentation: https://www.pulumi.com/docs/\n- Azure ARM templates: https://docs.microsoft.com/azure/azure-resource-manager/\n- IaC best practices guide in {baseDir}/docs/iac-standards.md",
      "parentPlugin": {
        "name": "infrastructure-as-code-generator",
        "category": "devops",
        "path": "plugins/devops/infrastructure-as-code-generator",
        "version": "1.0.0",
        "description": "Generate Infrastructure as Code for Terraform, CloudFormation, Pulumi, and more"
      },
      "filePath": "plugins/devops/infrastructure-as-code-generator/skills/generating-infrastructure-as-code/SKILL.md"
    },
    {
      "slug": "generating-orm-code",
      "name": "generating-orm-code",
      "description": "Use when you need to work with ORM code generation. This skill provides ORM model and code generation with comprehensive guidance and automation. Trigger with phrases like \"generate ORM models\", \"create entity classes\", or \"scaffold database models\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Orm Code Generator\n\nThis skill provides automated assistance for orm code generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/orm-code-generator/`\n\n**Documentation and Guides**: `{baseDir}/docs/orm-code-generator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/orm-code-generator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/orm-code-generator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/orm-code-generator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/orm-code-generator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "orm-code-generator",
        "category": "database",
        "path": "plugins/database/orm-code-generator",
        "version": "1.0.0",
        "description": "Generate ORM models from database schemas or create database schemas from models for TypeORM, Prisma, Sequelize, SQLAlchemy, and more"
      },
      "filePath": "plugins/database/orm-code-generator/skills/generating-orm-code/SKILL.md"
    },
    {
      "slug": "generating-rest-apis",
      "name": "generating-rest-apis",
      "description": "Generate complete REST API implementations from OpenAPI specifications or database schemas. Use when generating RESTful API implementations. Trigger with phrases like \"generate REST API\", \"create RESTful API\", or \"build REST endpoints\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:rest-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Rest Api Generator\n\nThis skill provides automated assistance for rest api generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:rest-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for rest api generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "rest-api-generator",
        "category": "api-development",
        "path": "plugins/api-development/rest-api-generator",
        "version": "1.0.0",
        "description": "Generate RESTful APIs from schemas with proper routing, validation, and documentation"
      },
      "filePath": "plugins/api-development/rest-api-generator/skills/generating-rest-apis/SKILL.md"
    },
    {
      "slug": "generating-security-audit-reports",
      "name": "generating-security-audit-reports",
      "description": "Generate comprehensive security audit reports for applications and systems. Use when you need to assess security posture, identify vulnerabilities, evaluate compliance status, or create formal security documentation. Trigger with phrases like \"create security audit report\", \"generate security assessment\", \"audit security posture\", or \"PCI-DSS compliance report\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(security-scan:*), Bash(report-gen:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Security Audit Reporter\n\nThis skill provides automated assistance for security audit reporter tasks.\n\n## Overview\n\nGenerates audit-ready security reports (findings, risk rating, compliance mapping, and remediation plan) from scanner outputs and configuration evidence.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Security scan data or logs are available in {baseDir}/security/\n- Access to application configuration files\n- Security tool outputs (e.g., vulnerability scanners, SAST/DAST results)\n- Compliance framework documentation (if applicable)\n- Write permissions for generating report files\n\n## Instructions\n\n1. Collect available security signals (scanner outputs, configs, logs).\n2. Analyze findings and map to risk + compliance requirements.\n3. Generate a report with prioritized remediation guidance.\n4. Format outputs (Markdown/HTML/PDF) and include evidence links.\n\n### 1. Data Collection Phase\n\nGather security information from available sources:\n- Read vulnerability scan results\n- Analyze security configurations\n- Review access control policies\n- Check encryption implementations\n- Examine authentication mechanisms\n\n### 2. Analysis Phase\n\nProcess collected data to identify:\n- Critical vulnerabilities (CVSS scores, exploitability)\n- Security misconfigurations\n- Compliance gaps against standards (PCI-DSS, GDPR, HIPAA, SOC 2)\n- Access control weaknesses\n- Data protection issues\n\n### 3. Report Generation Phase\n\nCreate structured audit report with:\n- Executive summary with risk overview\n- Detailed vulnerability findings with severity ratings\n- Compliance status matrix\n- Risk assessment and prioritization\n- Remediation recommendations with timelines\n- Technical appendices with evidence\n\n### 4. Output Formatting\n\nGenerate report in requested format:\n- Markdown for version control\n- HTML for stakeholder review\n- JSON for integration with ticketing systems\n- PDF-ready structure for formal documentation\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Comprehensive security audit report saved to {baseDir}/reports/security-audit-YYYYMMDD.md\n\n**Report Structure**:\n```\n# Security Audit Report - [System Name]\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## Executive Summary\n- Overall risk rating\n- Critical findings count\n- Compliance status\n\n## Vulnerability Findings\n### Critical (CVSS 9.0+)\n- [CVE-XXXX-XXXX] Description\n- Impact assessment\n- Remediation steps\n\n### High (CVSS 7.0-8.9)\n[Similar structure]\n\n## Compliance Assessment\n- PCI-DSS: 85% compliant (gaps identified)\n- GDPR: 92% compliant\n- SOC 2: In progress\n\n## Remediation Plan\nPriority matrix with timelines\n\n## Technical Appendices\nEvidence and scan outputs\n```\n\n**Secondary Outputs**:\n- Vulnerability tracking JSON for issue systems\n- Executive summary slide deck outline\n- Remediation tracking checklist\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Missing Scan Data**\n   - Error: \"No security scan results found\"\n   - Resolution: Specify alternate data sources or run preliminary scans\n   - Fallback: Generate report from configuration analysis only\n\n2. **Incomplete Compliance Framework**\n   - Error: \"Cannot assess [STANDARD] compliance - requirements unavailable\"\n   - Resolution: Request framework checklist or use general best practices\n   - Fallback: Note limitation in report with partial assessment\n\n3. **Access Denied to Configuration Files**\n   - Error: \"Permission denied reading {baseDir}/config/\"\n   - Resolution: Request elevated permissions or provide configuration exports\n   - Fallback: Generate report with available data, note gaps\n\n4. **Large Dataset Processing**\n   - Error: \"Scan results exceed processing capacity\"\n   - Resolution: Process in batches by severity or component\n   - Fallback: Focus on critical/high findings first\n\n## Examples\n\n- \"Generate a SOC 2 security audit report for our API using the scan results in {baseDir}/security/.\"\n- \"Create a PCI-DSS compliance-focused security assessment and remediation plan.\"\n\n## Resources\n\n**Security Standards References**:\n- OWASP Top 10: https://owasp.org/www-project-top-ten/\n- CWE Top 25: https://cwe.mitre.org/top25/\n- NIST Cybersecurity Framework: https://www.nist.gov/cyberframework\n\n**Compliance Frameworks**:\n- PCI-DSS Requirements: https://www.pcisecuritystandards.org/\n- GDPR Compliance Checklist: https://gdpr.eu/checklist/\n- HIPAA Security Rule: https://www.hhs.gov/hipaa/for-professionals/security/\n\n**Vulnerability Databases**:\n- National Vulnerability Database: https://nvd.nist.gov/\n- CVE Details: https://www.cvedetails.com/\n\n**Report Templates**:\n- Use {baseDir}/templates/security-audit-template.md if available\n- Default structure follows NIST SP 800-115 guidelines\n\n**Integration Points**:\n- Export findings to JIRA/GitHub Issues for tracking\n- Generate compliance evidence for SOC 2 audits\n- Link to SIEM/logging systems for evidence validation",
      "parentPlugin": {
        "name": "security-audit-reporter",
        "category": "security",
        "path": "plugins/security/security-audit-reporter",
        "version": "1.0.0",
        "description": "Generate comprehensive security audit reports"
      },
      "filePath": "plugins/security/security-audit-reporter/skills/generating-security-audit-reports/SKILL.md"
    },
    {
      "slug": "generating-smart-commits",
      "name": "generating-smart-commits",
      "description": "Use when generating conventional commit messages from staged git changes. Trigger with phrases like \"create commit message\", \"generate smart commit\", \"/commit-smart\", or \"/gc\". Automatically analyzes changes to determine commit type (feat, fix, docs), identifies breaking changes, and formats according to conventional commit standards. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Git Commit Smart\n\nThis skill provides automated assistance for git commit smart tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Git repository is initialized in {baseDir}\n- Changes are staged using `git add`\n- User has permission to create commits\n- Git user name and email are configured\n\n## Instructions\n\n1. **Analyze Staged Changes**: Examine git diff output to understand modifications\n2. **Determine Commit Type**: Classify changes as feat, fix, docs, style, refactor, test, or chore\n3. **Identify Scope**: Extract affected module or component from file paths\n4. **Detect Breaking Changes**: Look for API changes, removed features, or incompatible modifications\n5. **Format Message**: Construct message following pattern: `type(scope): description`\n6. **Present for Review**: Show generated message and ask for confirmation before committing\n\n## Output\n\nGenerates conventional commit messages in this format:\n\n```\ntype(scope): brief description\n\n- Detailed explanation of changes\n- Why the change was necessary\n- Impact on existing functionality\n\nBREAKING CHANGE: description if applicable\n```\n\nExamples:\n- `feat(auth): implement JWT authentication middleware`\n- `fix(api): resolve null pointer exception in user endpoint`\n- `docs(readme): update installation instructions`\n\n## Error Handling\n\nCommon issues and solutions:\n\n**No Staged Changes**\n- Error: \"No changes staged for commit\"\n- Solution: Stage files using `git add <files>` before generating commit message\n\n**Git Not Initialized**\n- Error: \"Not a git repository\"\n- Solution: Initialize git with `git init` or navigate to repository root\n\n**Uncommitted Changes**\n- Warning: \"Unstaged changes detected\"\n- Solution: Stage relevant changes or use `git stash` for unrelated modifications\n\n**Invalid Commit Format**\n- Error: \"Generated message doesn't follow conventional format\"\n- Solution: Review and manually adjust type, scope, or description\n\n## Resources\n\n- Conventional Commits specification: https://www.conventionalcommits.org/\n- Git commit best practices documentation\n- Repository commit history for style consistency\n- Project-specific commit guidelines in {baseDir}/000-docs/007-DR-GUID-contributing.md\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "git-commit-smart",
        "category": "devops",
        "path": "plugins/devops/git-commit-smart",
        "version": "1.0.0",
        "description": "AI-powered conventional commit message generator with smart analysis"
      },
      "filePath": "plugins/devops/git-commit-smart/skills/generating-smart-commits/SKILL.md"
    },
    {
      "slug": "generating-stored-procedures",
      "name": "generating-stored-procedures",
      "description": "Use when you need to work with stored procedure generation. This skill provides stored procedure code generation with comprehensive guidance and automation. Trigger with phrases like \"generate stored procedures\", \"create database functions\", or \"write SQL procedures\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Stored Procedure Generator\n\nThis skill provides automated assistance for stored procedure generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/stored-procedure-generator/`\n\n**Documentation and Guides**: `{baseDir}/docs/stored-procedure-generator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/stored-procedure-generator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/stored-procedure-generator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/stored-procedure-generator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/stored-procedure-generator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "stored-procedure-generator",
        "category": "database",
        "path": "plugins/database/stored-procedure-generator",
        "version": "1.0.0",
        "description": "Database plugin for stored-procedure-generator"
      },
      "filePath": "plugins/database/stored-procedure-generator/skills/generating-stored-procedures/SKILL.md"
    },
    {
      "slug": "generating-test-data",
      "name": "generating-test-data",
      "description": "Generate realistic test data including edge cases and boundary conditions. Use when creating realistic fixtures or edge case test data. Trigger with phrases like \"generate test data\", \"create fixtures\", or \"setup test database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:data-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Test Data Generator\n\nThis skill provides automated assistance for test data generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:data-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test data generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-data-generator",
        "category": "testing",
        "path": "plugins/testing/test-data-generator",
        "version": "1.0.0",
        "description": "Generate realistic test data including users, products, orders, and custom schemas for comprehensive testing"
      },
      "filePath": "plugins/testing/test-data-generator/skills/generating-test-data/SKILL.md"
    },
    {
      "slug": "generating-test-doubles",
      "name": "generating-test-doubles",
      "description": "Generate mocks, stubs, spies, and fakes for dependency isolation. Use when creating mocks, stubs, or test isolation fixtures. Trigger with phrases like \"generate mocks\", \"create test doubles\", or \"setup stubs\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:doubles-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Test Doubles Generator\n\nThis skill provides automated assistance for test doubles generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:doubles-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test doubles generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-doubles-generator",
        "category": "testing",
        "path": "plugins/testing/test-doubles-generator",
        "version": "1.0.0",
        "description": "Generate mocks, stubs, spies, and fakes for unit testing with Jest, Sinon, and test frameworks"
      },
      "filePath": "plugins/testing/test-doubles-generator/skills/generating-test-doubles/SKILL.md"
    },
    {
      "slug": "generating-test-reports",
      "name": "generating-test-reports",
      "description": "Generate comprehensive test reports with metrics, coverage, and visualizations. Use when performing specialized testing. Trigger with phrases like \"generate test report\", \"create test documentation\", or \"show test metrics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:report-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Test Report Generator\n\nThis skill provides automated assistance for test report generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:report-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test report generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-report-generator",
        "category": "testing",
        "path": "plugins/testing/test-report-generator",
        "version": "1.0.0",
        "description": "Generate comprehensive test reports with coverage, trends, and stakeholder-friendly formats"
      },
      "filePath": "plugins/testing/test-report-generator/skills/generating-test-reports/SKILL.md"
    },
    {
      "slug": "generating-trading-signals",
      "name": "generating-trading-signals",
      "description": "Generate trading signals using technical indicators and on-chain metrics. Use when receiving trading signals and alerts. Trigger with phrases like \"get trading signals\", \"check indicators\", or \"analyze signals\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:signals-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Crypto Signal Generator\n\nThis skill provides automated assistance for crypto signal generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:signals-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "crypto-signal-generator",
        "category": "crypto",
        "path": "plugins/crypto/crypto-signal-generator",
        "version": "1.0.0",
        "description": "Generate trading signals from technical indicators and market analysis"
      },
      "filePath": "plugins/crypto/crypto-signal-generator/skills/generating-trading-signals/SKILL.md"
    },
    {
      "slug": "generating-unit-tests",
      "name": "generating-unit-tests",
      "description": "Automatically generate comprehensive unit tests from source code covering happy paths, edge cases, and error conditions. Use when creating test coverage for functions, classes, or modules. Trigger with phrases like \"generate unit tests\", \"create tests for\", or \"add test coverage\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:unit-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Unit Test Generator\n\nThis skill provides automated assistance for unit test generator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Source code files requiring test coverage\n- Testing framework installed (Jest, Mocha, pytest, JUnit, etc.)\n- Understanding of code dependencies and external services to mock\n- Test directory structure established (e.g., `tests/`, `__tests__/`, `spec/`)\n- Package configuration updated with test scripts\n\n## Instructions\n\n### Step 1: Analyze Source Code\nExamine code structure and identify test requirements:\n1. Use Read tool to load source files from {baseDir}/src/\n2. Identify all functions, classes, and methods requiring tests\n3. Document function signatures, parameters, return types, and side effects\n4. Note external dependencies requiring mocking or stubbing\n\n### Step 2: Determine Testing Framework\nSelect appropriate testing framework based on language:\n- JavaScript/TypeScript: Jest, Mocha, Jasmine, Vitest\n- Python: pytest, unittest, nose2\n- Java: JUnit 5, TestNG\n- Go: testing package with testify assertions\n- Ruby: RSpec, Minitest\n\n### Step 3: Generate Test Cases\nCreate comprehensive test suite covering:\n1. Happy path tests with valid inputs and expected outputs\n2. Edge case tests with boundary values (empty arrays, null, zero, max values)\n3. Error condition tests with invalid inputs\n4. Mock external dependencies (databases, APIs, file systems)\n5. Setup and teardown fixtures for test isolation\n\n### Step 4: Write Test File\nGenerate test file in {baseDir}/tests/ with structure:\n- Import statements for code under test and testing framework\n- Mock declarations for external dependencies\n- Describe/context blocks grouping related tests\n- Individual test cases with arrange-act-assert pattern\n- Cleanup logic in afterEach/tearDown hooks\n\n## Output\n\nThe skill generates complete test files:\n\n### Test File Structure\n```javascript\n// Example Jest test file\nimport { validator } from '../src/utils/validator';\n\ndescribe('Validator', () => {\n  describe('validateEmail', () => {\n    it('should accept valid email addresses', () => {\n      expect(validator.validateEmail('test@example.com')).toBe(true);\n    });\n\n    it('should reject invalid email formats', () => {\n      expect(validator.validateEmail('invalid-email')).toBe(false);\n    });\n\n    it('should handle null and undefined', () => {\n      expect(validator.validateEmail(null)).toBe(false);\n      expect(validator.validateEmail(undefined)).toBe(false);\n    });\n  });\n});\n```\n\n### Coverage Metrics\n- Line coverage percentage (target: 80%+)\n- Branch coverage showing tested conditional paths\n- Function coverage ensuring all exports are tested\n- Statement coverage for comprehensive validation\n\n### Mock Implementations\nGenerated mocks for:\n- Database connections and queries\n- HTTP requests to external APIs\n- File system operations (read/write)\n- Environment variables and configuration\n- Time-dependent functions (Date.now(), setTimeout)\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Module Import Errors**\n- Error: Cannot find module or dependencies\n- Solution: Install missing packages; verify import paths match project structure; check TypeScript configuration\n\n**Mock Setup Failures**\n- Error: Mock not properly intercepting calls\n- Solution: Ensure mocks are defined before imports; use proper mocking syntax for framework; clear mocks between tests\n\n**Async Test Timeouts**\n- Error: Test exceeded timeout before completing\n- Solution: Increase timeout for slow operations; ensure async/await or done callbacks are used correctly; check for unresolved promises\n\n**Test Isolation Issues**\n- Error: Tests pass individually but fail when run together\n- Solution: Add proper cleanup in afterEach hooks; avoid shared mutable state; reset mocks between tests\n\n## Resources\n\n### Testing Frameworks\n- Jest documentation for JavaScript testing\n- pytest documentation for Python testing\n- JUnit 5 User Guide for Java testing\n- Go testing package and testify library\n\n### Best Practices\n- Follow AAA pattern (Arrange, Act, Assert) for test structure\n- Write tests before fixing bugs (test-driven bug fixing)\n- Use descriptive test names that explain the scenario\n- Keep tests independent and avoid test interdependencies\n- Mock external dependencies for unit test isolation\n- Aim for 80%+ code coverage on critical paths\n\n## Overview\n\n\nThis skill provides automated assistance for unit test generator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "unit-test-generator",
        "category": "testing",
        "path": "plugins/testing/unit-test-generator",
        "version": "1.0.0",
        "description": "Automatically generate comprehensive unit tests from source code with multiple testing framework support"
      },
      "filePath": "plugins/testing/unit-test-generator/skills/generating-unit-tests/SKILL.md"
    },
    {
      "slug": "genkit-infra-expert",
      "name": "genkit-infra-expert",
      "description": "Use when deploying Genkit applications to production with Terraform. Trigger with phrases like \"deploy genkit terraform\", \"provision genkit infrastructure\", \"firebase functions terraform\", \"cloud run deployment\", or \"genkit production infrastructure\". Provisions Firebase Functions, Cloud Run services, GKE clusters, monitoring dashboards, and CI/CD for AI workflows. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Genkit Infra Expert\n\nThis skill provides automated assistance for genkit infra expert tasks.\n\n## Overview\n\nDeploy Genkit applications to production with Terraform (Firebase Functions, Cloud Run, or GKE) with secure secrets handling and observability. Use this skill to choose a target, generate the Terraform baseline, wire up Secret Manager, and provide a validation checklist for your Genkit flows.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Google Cloud project with Firebase enabled\n- Terraform 1.0+ installed\n- gcloud and firebase CLI authenticated\n- Genkit application built and containerized\n- API keys for Gemini or other AI models\n- Understanding of Genkit flows and deployment options\n\n## Instructions\n\n1. **Choose Deployment Target**: Firebase Functions, Cloud Run, or GKE\n2. **Configure Terraform Backend**: Set up remote state in GCS\n3. **Define Variables**: Project ID, region, Genkit app configuration\n4. **Provision Compute**: Deploy functions or containers\n5. **Configure Secrets**: Store API keys in Secret Manager\n6. **Set Up Monitoring**: Create dashboards for token usage and latency\n7. **Enable Auto-scaling**: Configure min/max instances\n8. **Validate Deployment**: Test Genkit flows via HTTP endpoints\n\n## Examples\n\n**Example: Cloud Run deployment for a Genkit API**\n- Inputs: container image, region, min/max instances, and required model API keys.\n- Outputs: Terraform for Cloud Run + Secret Manager bindings and a smoke test curl command that hits a health endpoint / flow route.\n\n## Output\n\n**Firebase Functions:**\n```hcl\n# {baseDir}/terraform/functions.tf\nresource \"google_cloudfunctions2_function\" \"genkit_function\" {\n  name     = \"genkit-ai-flow\"\n  location = var.region\n\n  build_config {\n    runtime     = \"nodejs20\"\n    entry_point = \"genkitFlow\"\n  }\n\n  service_config {\n    max_instance_count = 100\n    available_memory   = \"512Mi\"\n    timeout_seconds    = 300\n  }\n}\n```\n\n**Cloud Run Service:**\n```hcl\nresource \"google_cloud_run_v2_service\" \"genkit_service\" {\n  name     = \"genkit-api\"\n  location = var.region\n\n  template {\n    scaling {\n      min_instance_count = 1\n      max_instance_count = 10\n    }\n    containers {\n      image = \"gcr.io/${var.project_id}/genkit-app:latest\"\n      resources {\n        limits = {\n          cpu = \"2\"\n          memory = \"1Gi\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Error Handling\n\n**Build Failures**\n- Error: \"Cloud Function build failed\"\n- Solution: Check package.json dependencies and Node.js runtime version\n\n**Cold Start Latency**\n- Warning: \"High latency on first request\"\n- Solution: Set min_instance_count >= 1 to keep warm instances\n\n**Secret Access Denied**\n- Error: \"Permission denied accessing secret\"\n- Solution: Grant secretAccessor role to Cloud Run/Functions service account\n\n**Memory Exceeded**\n- Error: \"Container killed: out of memory\"\n- Solution: Increase available_memory or optimize Genkit flow memory usage\n\n## Resources\n\n- Genkit Deployment: https://genkit.dev/docs/deployment\n- Firebase Terraform: https://registry.terraform.io/providers/hashicorp/google/latest\n- Genkit examples in {baseDir}/genkit-examples/",
      "parentPlugin": {
        "name": "jeremy-genkit-terraform",
        "category": "devops",
        "path": "plugins/devops/jeremy-genkit-terraform",
        "version": "1.0.0",
        "description": "Terraform modules for Firebase Genkit infrastructure and deployments"
      },
      "filePath": "plugins/devops/jeremy-genkit-terraform/skills/genkit-infra-expert/SKILL.md"
    },
    {
      "slug": "genkit-production-expert",
      "name": "genkit-production-expert",
      "description": "Build production Firebase Genkit applications including RAG systems, multi-step flows, and tool calling for Node.js/Python/Go. Deploy to Firebase Functions or Cloud Run with AI monitoring. Use when asked to \"create genkit flow\" or \"implement RAG\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Genkit Production Expert\n\nThis skill provides automated assistance for genkit production expert tasks.\n\n## What This Skill Does\n\nThis skill provides comprehensive expertise in building production-ready Firebase Genkit applications across Node.js (1.0), Python (Alpha), and Go (1.0). It handles the complete lifecycle from initialization to deployment with AI monitoring.\n\n### Core Capabilities\n\n1. **Project Initialization**: Set up properly structured Genkit projects with best practices\n2. **Flow Architecture**: Design multi-step AI workflows with proper error handling\n3. **RAG Implementation**: Build retrieval-augmented generation systems with vector search\n4. **Tool Integration**: Implement function calling and custom tools\n5. **Monitoring Setup**: Configure AI monitoring for Firebase Console\n6. **Multi-Language Support**: Expert guidance for TypeScript, Python, and Go implementations\n7. **Production Deployment**: Deploy to Firebase Functions or Google Cloud Run\n\n## When This Skill Activates\n\nThis skill automatically activates when you mention:\n\n### Trigger Phrases\n- \"Create a Genkit flow\"\n- \"Implement RAG with Genkit\"\n- \"Deploy Genkit to Firebase\"\n- \"Set up Gemini integration\"\n- \"Configure AI monitoring\"\n- \"Build Genkit application\"\n- \"Design AI workflow\"\n- \"Genkit tool calling\"\n- \"Vector search with Genkit\"\n- \"Genkit production deployment\"\n\n### Use Case Patterns\n- Setting up new Genkit projects\n- Implementing RAG systems with embedding models\n- Integrating Gemini 2.5 Pro/Flash models\n- Creating multi-step AI workflows\n- Deploying to production with monitoring\n- Debugging Genkit flows\n- Optimizing token usage and costs\n\n## How It Works\n\n### Phase 1: Requirements Analysis\n```\nUser Request ‚Üí Analyze needs ‚Üí Determine:\n- Target language (Node.js/Python/Go)\n- Flow complexity (simple/multi-step/RAG)\n- Model requirements (Gemini version, custom models)\n- Deployment target (Firebase/Cloud Run/local)\n```\n\n### Phase 2: Project Setup\n```\nCheck existing project ‚Üí If new:\n  - Initialize project structure\n  - Install dependencies\n  - Configure environment variables\n  - Set up TypeScript/Python/Go config\n\nIf existing:\n  - Analyze current structure\n  - Identify integration points\n  - Preserve existing code\n```\n\n### Phase 3: Implementation\n```\nDesign flow architecture ‚Üí Implement:\n  - Input/output schemas (Zod/Pydantic/Go structs)\n  - Model configuration\n  - Tool definitions (if needed)\n  - Retriever setup (for RAG)\n  - Error handling\n  - Tracing configuration\n```\n\n### Phase 4: Testing & Validation\n```\nCreate test cases ‚Üí Run locally:\n  - Genkit Developer UI\n  - Unit tests\n  - Integration tests\n  - Token usage analysis\n```\n\n### Phase 5: Production Deployment\n```\nConfigure deployment ‚Üí Deploy:\n  - Firebase Functions (with AI monitoring)\n  - Cloud Run (with auto-scaling)\n  - Set up monitoring dashboards\n  - Configure alerting\n```\n\n## Workflow Examples\n\n### Example 1: Simple Question-Answering Flow\n\n**User Request**: \"Create a Genkit flow that answers user questions using Gemini 2.5 Flash\"\n\n**Skill Response**:\n1. Creates TypeScript project (default)\n2. Implements flow with input validation:\n```typescript\nconst qaFlow = ai.defineFlow(\n  {\n    name: 'qaFlow',\n    inputSchema: z.object({ question: z.string() }),\n    outputSchema: z.object({ answer: z.string() }),\n  },\n  async (input) => {\n    const { text } = await ai.generate({\n      model: gemini25Flash,\n      prompt: `Answer this question: ${input.question}`,\n      config: { temperature: 0.3 }, // Lower for factual answers\n    });\n    return { answer: text };\n  }\n);\n```\n3. Sets up local testing\n4. Provides deployment instructions\n\n### Example 2: RAG System with Vector Search\n\n**User Request**: \"Implement RAG with Genkit for our documentation search\"\n\n**Skill Response**:\n1. Analyzes document storage needs\n2. Implements retriever with embeddings:\n```typescript\nconst docRetriever = ai.defineRetriever(\n  {\n    name: 'docRetriever',\n    configSchema: z.object({ k: z.number().default(5) }),\n  },\n  async (query, config) => {\n    // Generate embedding\n    const embedding = await ai.embed({\n      embedder: textEmbeddingGecko,\n      content: query,\n    });\n\n    // Search vector database\n    const results = await vectorDB.search(embedding, config.k);\n    return results.map(doc => ({\n      content: doc.text,\n      metadata: { source: doc.source },\n    }));\n  }\n);\n```\n3. Creates RAG flow combining retrieval + generation\n4. Sets up vector database connection\n5. Implements caching for efficiency\n\n### Example 3: Multi-Agent Tool Calling\n\n**User Request**: \"Create a Genkit agent with weather and calendar tools\"\n\n**Skill Response**:\n1. Defines tools with proper schemas:\n```typescript\nconst weatherTool = ai.defineTool({\n  name: 'getWeather',\n  description: 'Get current weather for a location',\n  inputSchema: z.object({ location: z.string() }),\n  outputSchema: z.object({\n    temp: z.number(),\n    conditions: z.string(),\n  }),\n}, async ({ location }) => {\n  // Call weather API\n});\n\nconst calendarTool = ai.defineTool({\n  name: 'checkCalendar',\n  description: 'Check calendar availability',\n  inputSchema: z.object({ date: z.string() }),\n  outputSchema: z.object({ available: z.boolean() }),\n}, async ({ date }) => {\n  // Check calendar API\n});\n```\n2. Creates agent flow with tool access:\n```typescript\nconst agentFlow = ai.defineFlow(async (userQuery) => {\n  const { text } = await ai.generate({\n    model: gemini25Flash,\n    prompt: userQuery,\n    tools: [weatherTool, calendarTool],\n  });\n  return text;\n});\n```\n3. Implements proper error handling\n4. Sets up tool execution tracing\n\n## Production Best Practices Applied\n\n### 1. Schema Validation\n- All inputs/outputs use Zod (TS), Pydantic (Python), or structs (Go)\n- Prevents runtime errors from malformed data\n\n### 2. Error Handling\n```typescript\ntry {\n  const result = await ai.generate({...});\n  return result;\n} catch (error) {\n  if (error.code === 'SAFETY_BLOCK') {\n    // Handle safety filters\n  } else if (error.code === 'QUOTA_EXCEEDED') {\n    // Handle rate limits\n  }\n  throw error;\n}\n```\n\n### 3. Cost Optimization\n- Context caching for repeated prompts\n- Token usage monitoring\n- Temperature tuning for use case\n- Model selection (Flash vs Pro)\n\n### 4. Monitoring\n- OpenTelemetry tracing enabled\n- Custom span attributes\n- Firebase Console integration\n- Alert configuration\n\n### 5. Security\n- Environment variable management\n- API key rotation support\n- Input sanitization\n- Output filtering\n\n## Integration with Other Tools\n\n### Works With ADK Plugin\nWhen complex multi-agent orchestration is needed:\n- Use Genkit for individual specialized flows\n- Use ADK for orchestrating multiple Genkit flows\n- Pass results via A2A protocol\n\n### Works With Vertex AI Validator\nFor production deployment:\n- Genkit implements the flows\n- Validator ensures production readiness\n- Validates monitoring configuration\n- Checks security compliance\n\n## Tool Permissions\n\nThis skill uses the following tools:\n- **Read**: Analyze existing code and configuration\n- **Write**: Create new flow files and configs\n- **Edit**: Modify existing Genkit implementations\n- **Grep**: Search for integration points\n- **Glob**: Find related files\n- **Bash**: Install dependencies, run tests, deploy\n\n## Troubleshooting Guide\n\n### Common Issue 1: API Key Not Found\n**Symptoms**: Error \"API key not provided\"\n**Solution**:\n1. Check `.env` file exists\n2. Verify `GOOGLE_API_KEY` is set\n3. Ensure `dotenv` is loaded\n\n### Common Issue 2: Flow Not Appearing in UI\n**Symptoms**: Flow not visible in Genkit Developer UI\n**Solution**:\n1. Ensure flow is exported\n2. Restart Genkit server\n3. Check console for errors\n\n### Common Issue 3: High Token Usage\n**Symptoms**: Unexpected costs\n**Solution**:\n1. Implement context caching\n2. Use Gemini 2.5 Flash instead of Pro\n3. Lower temperature\n4. Compress prompts\n\n## Version History\n\n- **1.0.0** (2025): Initial release with Node.js 1.0, Python Alpha, Go 1.0 support\n- Supports Gemini 2.5 Pro/Flash\n- AI monitoring integration\n- Production deployment patterns\n\n## References\n\n- Official Docs: https://genkit.dev/\n- Node.js 1.0 Announcement: https://firebase.blog/posts/2025/02/announcing-genkit/\n- Go 1.0 Announcement: https://developers.googleblog.com/en/announcing-genkit-go-10/\n- Vertex AI Plugin: https://genkit.dev/docs/integrations/vertex-ai/\n\n## Overview\n\n\nThis skill provides automated assistance for genkit production expert tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-genkit-pro",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-genkit-pro",
        "version": "1.0.0",
        "description": "Firebase Genkit expert for production-ready AI workflows with RAG and tool calling"
      },
      "filePath": "plugins/ai-ml/jeremy-genkit-pro/skills/genkit-production-expert/SKILL.md"
    },
    {
      "slug": "gh-actions-validator",
      "name": "gh-actions-validator",
      "description": "Use when validating GitHub Actions workflows for Google Cloud and Vertex AI deployments. Trigger with phrases like \"validate github actions\", \"setup workload identity federation\", \"github actions security\", \"deploy agent with ci/cd\", or \"automate vertex ai deployment\". Enforces Workload Identity Federation (WIF), validates OIDC permissions, ensures least privilege IAM, and implements security best practices. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Gh Actions Validator\n\nThis skill provides automated assistance for gh actions validator tasks.\n\n## Overview\n\nValidate and harden GitHub Actions workflows that deploy to Google Cloud (especially Vertex AI) using Workload Identity Federation (OIDC) instead of long-lived service account keys. Use this to audit existing workflows, propose a secure replacement, and add CI checks that prevent common credential and permission mistakes.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- GitHub repository with Actions enabled\n- Google Cloud project with billing enabled\n- gcloud CLI authenticated with admin permissions\n- Understanding of Workload Identity Federation concepts\n- GitHub repository secrets configured\n- Appropriate IAM roles for CI/CD automation\n\n## Instructions\n\n1. **Audit Existing Workflows**: Scan .github/workflows/ for security issues\n2. **Validate WIF Usage**: Ensure no JSON service account keys are used\n3. **Check OIDC Permissions**: Verify id-token: write is present\n4. **Review IAM Roles**: Confirm least privilege (no owner/editor roles)\n5. **Add Security Scans**: Include secret detection and vulnerability scanning\n6. **Validate Deployments**: Add post-deployment health checks\n7. **Configure Monitoring**: Set up alerts for deployment failures\n8. **Document WIF Setup**: Provide one-time WIF configuration commands\n\n## Examples\n\n**Example: Harden an existing deployment workflow**\n- Input: `.github/workflows/deploy.yml` that uses `credentials_json` or a downloaded service account key.\n- Output: a WIF-based workflow using `google-github-actions/auth@v2`, minimal IAM roles, and a guardrail job that fails PRs when JSON keys appear in workflows.\n\n## Output\n\n**Secure Workflow Template:**\n```yaml\n# {baseDir}/.github/workflows/deploy-vertex-ai.yml\nname: Deploy Vertex AI Agent\n\non:\n  push:\n    branches: [main]\n    paths: ['agent/**']\n\npermissions:\n  contents: read\n  id-token: write  # REQUIRED for WIF\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Authenticate to GCP (WIF)\n        uses: google-github-actions/auth@v2\n        with:\n          workload_identity_provider: ${{ secrets.WIF_PROVIDER }}\n          service_account: ${{ secrets.WIF_SERVICE_ACCOUNT }}\n\n      - name: Deploy to Vertex AI\n        run: |\n          gcloud ai agents deploy \\\n            --project=${{ secrets.GCP_PROJECT_ID }} \\\n            --region=us-central1\n\n      - name: Validate Deployment\n        run: |\n          python scripts/validate-deployment.py\n```\n\n**WIF Setup Commands:**\n```bash\n# One-time WIF configuration\ngcloud iam workload-identity-pools create github-pool \\\n  --location=global \\\n  --display-name=\"GitHub Actions Pool\"\n\ngcloud iam workload-identity-pools providers create-oidc github-provider \\\n  --location=global \\\n  --workload-identity-pool=github-pool \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.repository=assertion.repository\"\n```\n\n**Security Validation Checks:**\n```yaml\n# {baseDir}/.github/workflows/security-check.yml\nname: Security Validation\n\non: [pull_request, push]\n\npermissions:\n  contents: read\n  security-events: write\n\njobs:\n  security:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Scan for secrets\n        uses: trufflesecurity/trufflehog@main\n\n      - name: Vulnerability scan\n        uses: aquasecurity/trivy-action@master\n\n      - name: Validate no JSON keys\n        run: |\n          if find . -name \"*service-account*.json\"; then\n            echo \"ERROR: Service account keys detected\"\n            exit 1\n          fi\n\n      - name: Validate WIF usage\n        run: |\n          if grep -r \"credentials_json\" .github/workflows/; then\n            echo \"ERROR: Use WIF instead of JSON keys\"\n            exit 1\n          fi\n```\n\n## Error Handling\n\n**WIF Authentication Failed**\n- Error: \"Failed to generate Google Cloud access token\"\n- Solution: Verify WIF provider and service account email are correct\n\n**OIDC Token Error**\n- Error: \"Unable to get ACTIONS_ID_TOKEN_REQUEST_URL env variable\"\n- Solution: Add `id-token: write` permission to workflow\n\n**IAM Permission Denied**\n- Error: \"does not have required permission\"\n- Solution: Grant service account minimum required roles (run.admin, aiplatform.user)\n\n**Attribute Condition Failed**\n- Error: \"Token does not match attribute condition\"\n- Solution: Update attribute mapping to include repository restriction\n\n**Deployment Validation Failed**\n- Error: \"Agent not in RUNNING state\"\n- Solution: Check agent configuration and deployment logs\n\n## Resources\n\n- Workload Identity Federation: https://cloud.google.com/iam/docs/workload-identity-federation\n- GitHub OIDC: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments\n- Vertex AI Agent Engine: https://cloud.google.com/vertex-ai/docs/agent-engine\n- google-github-actions/auth: https://github.com/google-github-actions/auth\n- WIF setup guide in {baseDir}/docs/wif-setup.md",
      "parentPlugin": {
        "name": "jeremy-github-actions-gcp",
        "category": "devops",
        "path": "plugins/devops/jeremy-github-actions-gcp",
        "version": "1.0.0",
        "description": "GitHub Actions CI/CD workflows for Google Cloud and Vertex AI deployments"
      },
      "filePath": "plugins/devops/jeremy-github-actions-gcp/skills/gh-actions-validator/SKILL.md"
    },
    {
      "slug": "google-cloud-agent-sdk-master",
      "name": "google-cloud-agent-sdk-master",
      "description": "Automatic activation for all google cloud agent development kit (adk) Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, WebFetch, WebSearch, Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Google Cloud Agent SDK Master\n\nMaster Google‚Äôs Agent Development Kit (ADK) patterns for building and deploying production-grade agents with clear tool contracts, validation, and operational guardrails.\n\n## Overview\n\nUse this skill to quickly answer ‚Äúhow do I do X with Google ADK?‚Äù and to produce a safe, production-oriented plan (structure, patterns, deployment, verification) rather than ad-hoc snippets.\n\n## Examples\n\n**Example: Pick the right ADK pattern**\n- Request: ‚ÄúShould this be a single agent or a multi-agent orchestrator?‚Äù\n- Output: an architecture recommendation with tradeoffs, plus a minimal scaffold plan.\n\n## Prerequisites\n\n- The target environment (local-only vs Vertex AI Agent Engine)\n- The agent‚Äôs core job, expected inputs/outputs, and required tools\n- Any constraints (latency, cost, compliance/security)\n\n## Instructions\n\n1. Clarify requirements and choose an ADK architecture (single vs multi-agent; orchestration pattern).\n2. Define tool interfaces (inputs, outputs, and error contracts) and how secrets are managed.\n3. Provide an implementation plan with a minimal scaffold and incremental milestones.\n4. Add validation: smoke prompts, regression tests, and deployment verification steps.\n\n## Output\n\n- A recommended ADK architecture and scaffold layout\n- A checklist of commands to validate locally and in CI\n- Optional: deployment steps and post-deploy health checks\n\n## Error Handling\n\n- If documentation conflicts, prefer the latest canonical standards in `000-docs/6767-*`.\n- If an API feature is unavailable in a region/version, propose a compatible alternative.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- ADK / Agent Engine docs: https://cloud.google.com/vertex-ai/docs/agent-engine\n- Canonical repo standards: `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`",
      "parentPlugin": {
        "name": "004-jeremy-google-cloud-agent-sdk",
        "category": "productivity",
        "path": "plugins/productivity/004-jeremy-google-cloud-agent-sdk",
        "version": "1.0.0",
        "description": "Google Cloud Agent Development Kit (ADK) and Agent Starter Pack mastery - build containerized multi-agent systems with production-ready templates, deploy to Cloud Run/GKE/Agent Engine, RAG agents, ReAct agents, and multi-agent orchestration."
      },
      "filePath": "plugins/productivity/004-jeremy-google-cloud-agent-sdk/skills/google-cloud-agent-sdk-master/SKILL.md"
    },
    {
      "slug": "handling-api-errors",
      "name": "handling-api-errors",
      "description": "Implement standardized error handling with proper HTTP status codes and error responses. Use when implementing standardized error handling. Trigger with phrases like \"add error handling\", \"standardize errors\", or \"implement error responses\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:error-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Error Handler\n\nThis skill provides automated assistance for api error handler tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:error-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api error handler tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-error-handler",
        "category": "api-development",
        "path": "plugins/api-development/api-error-handler",
        "version": "1.0.0",
        "description": "Implement standardized error handling with proper HTTP status codes"
      },
      "filePath": "plugins/api-development/api-error-handler/skills/handling-api-errors/SKILL.md"
    },
    {
      "slug": "implementing-backup-strategies",
      "name": "implementing-backup-strategies",
      "description": "Use when you need to work with backup and recovery. This skill provides backup automation and disaster recovery with comprehensive guidance and automation. Trigger with phrases like \"create backups\", \"automate backups\", or \"implement disaster recovery\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(tar:*), Bash(rsync:*), Bash(aws:s3:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Backup Strategy Implementor\n\nThis skill provides automated assistance for backup strategy implementor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/backup-strategy-implementor/`\n\n**Documentation and Guides**: `{baseDir}/docs/backup-strategy-implementor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/backup-strategy-implementor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/backup-strategy-implementor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/backup-strategy-implementor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/backup-strategy-implementor-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "backup-strategy-implementor",
        "category": "devops",
        "path": "plugins/devops/backup-strategy-implementor",
        "version": "1.0.0",
        "description": "Implement backup strategies for databases and applications"
      },
      "filePath": "plugins/devops/backup-strategy-implementor/skills/implementing-backup-strategies/SKILL.md"
    },
    {
      "slug": "implementing-database-audit-logging",
      "name": "implementing-database-audit-logging",
      "description": "Use when you need to track database changes for compliance and security monitoring. This skill implements audit logging using triggers, application-level logging, CDC, or native logs. Trigger with phrases like \"implement database audit logging\", \"add audit trails\", \"track database changes\", or \"monitor database activity for compliance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Audit Logger\n\nThis skill provides automated assistance for database audit logger tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Database credentials with CREATE TABLE and CREATE TRIGGER permissions\n- Understanding of compliance requirements (GDPR, HIPAA, SOX, PCI-DSS)\n- Sufficient storage for audit logs (estimate 10-30% of data size)\n- Decision on audit log retention period\n- Access to database documentation for table schemas\n- Monitoring tools configured for audit log analysis\n\n## Instructions\n\n### Step 1: Define Audit Requirements\n1. Identify tables requiring audit logging based on compliance needs\n2. Determine events to audit (INSERT, UPDATE, DELETE, SELECT for sensitive data)\n3. Define which columns contain sensitive data requiring audit\n4. Document retention requirements for audit logs\n5. Identify users/roles whose actions need auditing\n\n### Step 2: Choose Audit Strategy\n1. **Trigger-Based Auditing**: Best for comprehensive row-level tracking\n   - Pros: Automatic, no application changes, captures all changes\n   - Cons: Performance overhead, complex trigger maintenance\n2. **Application-Level Auditing**: Best for selective auditing\n   - Pros: Flexible, lower database overhead, easier debugging\n   - Cons: Requires application changes, can miss direct database changes\n3. **Change Data Capture (CDC)**: Best for real-time streaming\n   - Pros: Minimal performance impact, real-time analysis, external processing\n   - Cons: Complex setup, requires CDC infrastructure\n4. **Native Database Logs**: Best for general monitoring\n   - Pros: No setup, captures everything, built-in\n   - Cons: High volume, limited retention, difficult to query\n\n### Step 3: Design Audit Table Schema\n1. Create audit log table with these core columns:\n   - audit_id (primary key), table_name, action (INSERT/UPDATE/DELETE)\n   - record_id (reference to audited record), old_values (JSON), new_values (JSON)\n   - changed_by (user), changed_at (timestamp), client_ip, application_context\n2. Add indexes on table_name, changed_at, changed_by for query performance\n3. Partition audit table by date for efficient archival\n4. Configure tablespace for audit logs separate from primary data\n\n### Step 4: Implement Audit Mechanism\n1. For trigger-based: Create AFTER INSERT/UPDATE/DELETE triggers on each table\n2. Capture old and new row values as JSON in trigger body\n3. Record user context (CURRENT_USER, application user, IP address)\n4. Handle trigger failures gracefully (log but don't block operations)\n5. Test triggers with sample data modifications\n\n### Step 5: Configure Audit Log Management\n1. Set up automated archival of old audit logs to cold storage\n2. Implement audit log analysis queries for common compliance reports\n3. Create alerts for suspicious activities (bulk deletes, off-hours changes)\n4. Document audit log query procedures for compliance auditors\n5. Schedule periodic audit log reviews with security team\n\n### Step 6: Validate Audit Implementation\n1. Perform test operations on audited tables\n2. Verify audit log entries are created with complete data\n3. Test audit log queries for performance\n4. Confirm audit logs cannot be modified by regular users\n5. Document audit implementation for compliance documentation\n\n## Output\n\nThis skill produces:\n\n**Audit Table Schema**: SQL DDL for audit log table with proper indexes and partitioning\n\n**Audit Triggers**: Database triggers for automatic audit log population on data changes\n\n**Audit Log Queries**: Pre-built SQL queries for compliance reports and change tracking\n\n**Implementation Documentation**: Configuration details, trigger logic, and maintenance procedures\n\n**Compliance Report Templates**: SQL queries for GDPR access logs, SOX change reports, etc.\n\n## Error Handling\n\n**Trigger Performance Issues**:\n- Audit only critical tables, not all tables\n- Use asynchronous audit logging with queue systems\n- Batch audit log inserts instead of individual inserts\n- Monitor trigger execution time and optimize trigger logic\n\n**Audit Table Growth**:\n- Implement automated archival of audit logs older than retention period\n- Partition audit table by month or quarter\n- Compress old audit log partitions\n- Move historical audit logs to cheaper storage tiers\n\n**Missing Audit Context**:\n- Set application context in database session before operations\n- Use database session variables to pass user identity\n- Implement connection pooling with session initialization\n- Log application user separately from database user\n\n**Permission Issues**:\n- Ensure audit log table is writable by trigger execution context\n- Grant INSERT on audit table to all database users\n- Protect audit table from modifications (no UPDATE/DELETE grants)\n- Use separate schema for audit tables with restricted access\n\n## Resources\n\n**Audit Table Templates**:\n- PostgreSQL audit trigger: `{baseDir}/templates/postgresql-audit-trigger.sql`\n- MySQL audit trigger: `{baseDir}/templates/mysql-audit-trigger.sql`\n- Audit table schema: `{baseDir}/templates/audit-table-schema.sql`\n\n**Compliance Report Queries**: `{baseDir}/queries/compliance-reports/`\n- GDPR data access report\n- SOX change audit report\n- User activity summary\n- Suspicious activity detection\n\n**Audit Strategy Guide**: `{baseDir}/docs/audit-strategy-selection.md`\n**Performance Tuning**: `{baseDir}/docs/audit-performance-optimization.md`\n**Archival Procedures**: `{baseDir}/scripts/audit-archival.sh`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-audit-logger",
        "category": "database",
        "path": "plugins/database/database-audit-logger",
        "version": "1.0.0",
        "description": "Database plugin for database-audit-logger"
      },
      "filePath": "plugins/database/database-audit-logger/skills/implementing-database-audit-logging/SKILL.md"
    },
    {
      "slug": "implementing-database-caching",
      "name": "implementing-database-caching",
      "description": "Use when you need to implement multi-tier caching to improve database performance. This skill sets up Redis, in-memory caching, and CDN layers to reduce database load. Trigger with phrases like \"implement database caching\", \"add Redis cache layer\", \"improve query performance with caching\", or \"reduce database load\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(redis-cli:*), Bash(docker:redis:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Cache Layer\n\nThis skill provides automated assistance for database cache layer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Redis server available or ability to deploy Redis container\n- Understanding of application data access patterns and hotspots\n- Knowledge of which queries/data benefit most from caching\n- Monitoring tools to measure cache hit rates and performance\n- Development environment for testing caching implementation\n- Understanding of cache invalidation requirements for data consistency\n\n## Instructions\n\n### Step 1: Analyze Caching Requirements\n1. Profile database queries to identify slow or frequently executed queries\n2. Determine which data is read-heavy vs write-heavy\n3. Identify data that can tolerate eventual consistency\n4. Calculate expected cache size and Redis memory requirements\n5. Document current database load and target performance metrics\n\n### Step 2: Choose Caching Strategy\n1. **Cache-Aside (Lazy Loading)**: Application checks cache first, loads from DB on miss\n   - Best for: Read-heavy workloads, unpredictable access patterns\n   - Pros: Only caches requested data, simple to implement\n   - Cons: Cache misses incur database hit, stale data possible\n2. **Write-Through**: Application writes to cache and database simultaneously\n   - Best for: Write-heavy workloads needing consistency\n   - Pros: Cache always consistent, no stale data\n   - Cons: Write latency, unnecessary caching of rarely-read data\n3. **Write-Behind (Write-Back)**: Application writes to cache, async writes to database\n   - Best for: High write throughput requirements\n   - Pros: Low write latency, batched database writes\n   - Cons: Risk of data loss, complexity in implementation\n\n### Step 3: Design Cache Architecture\n1. Set up Redis as distributed cache layer (L2 cache)\n2. Implement in-memory LRU cache in application (L1 cache)\n3. Configure CDN for static assets (images, CSS, JS)\n4. Design cache key naming convention (e.g., `user:123:profile`)\n5. Define TTL (Time To Live) for different data types\n\n### Step 4: Implement Caching Code\n1. Add Redis client library to application dependencies\n2. Create cache wrapper functions (get, set, delete, invalidate)\n3. Modify database query code to check cache before DB query\n4. Implement cache population on cache miss\n5. Add error handling for cache failures (fail gracefully to database)\n\n### Step 5: Configure Cache Invalidation\n1. Implement TTL-based expiration for time-sensitive data\n2. Add explicit cache invalidation on data updates/deletes\n3. Use cache tags or patterns for bulk invalidation\n4. Implement cache warming for critical data after deployments\n5. Set up cache stampede prevention (lock/queue on miss)\n\n### Step 6: Monitor and Optimize\n1. Track cache hit rate, miss rate, and eviction rate\n2. Monitor Redis memory usage and eviction policy\n3. Analyze query performance improvements\n4. Adjust TTLs based on data update frequency\n5. Identify and cache additional hot data\n\n## Output\n\nThis skill produces:\n\n**Redis Configuration**: Docker Compose or config files for Redis deployment with appropriate memory and eviction settings\n\n**Caching Code**: Application code implementing cache-aside, write-through, or write-behind patterns\n\n**Cache Key Schema**: Documentation of cache key naming conventions and TTL settings\n\n**Monitoring Dashboards**: Metrics for cache hit rates, memory usage, and performance improvements\n\n**Cache Invalidation Logic**: Code for explicit and implicit cache invalidation on data changes\n\n## Error Handling\n\n**Cache Connection Failures**:\n- Implement circuit breaker pattern to prevent cascading failures\n- Fall back to database when cache is unavailable\n- Log cache connection errors for monitoring\n- Retry cache connections with exponential backoff\n- Consider read-replica or cache cluster for high availability\n\n**Cache Stampede**:\n- Implement probabilistic early expiration (PER) for TTLs\n- Use distributed locks (Redis SETNX) to prevent concurrent cache population\n- Queue cache refresh requests instead of parallel execution\n- Add jitter to TTLs to spread expiration times\n- Use stale-while-revalidate pattern for acceptable delays\n\n**Stale Data Issues**:\n- Implement versioning in cache keys (e.g., `user:123:v2`)\n- Use cache tags for related data invalidation\n- Set aggressive TTLs for frequently changing data\n- Implement active cache invalidation on data updates\n- Monitor data consistency between cache and database\n\n**Memory Pressure**:\n- Configure Redis eviction policy (allkeys-lru recommended)\n- Monitor Redis memory usage and set max memory limits\n- Implement tiered caching (hot data in Redis, warm data in DB)\n- Reduce TTLs for less critical data\n- Scale Redis horizontally with cluster mode\n\n## Resources\n\n**Redis Configuration Templates**:\n- Docker Compose: `{baseDir}/docker/redis-compose.yml`\n- Redis config: `{baseDir}/config/redis.conf`\n- Cluster config: `{baseDir}/config/redis-cluster.conf`\n\n**Caching Code Examples**: `{baseDir}/examples/caching/`\n- Cache-aside pattern (Node.js, Python, Java)\n- Write-through pattern\n- Cache invalidation strategies\n- Distributed locking\n\n**Cache Key Design Guide**: `{baseDir}/docs/cache-key-design.md`\n**Performance Tuning**: `{baseDir}/docs/cache-performance-tuning.md`\n**Monitoring Setup**: `{baseDir}/monitoring/redis-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-cache-layer",
        "category": "database",
        "path": "plugins/database/database-cache-layer",
        "version": "1.0.0",
        "description": "Database plugin for database-cache-layer"
      },
      "filePath": "plugins/database/database-cache-layer/skills/implementing-database-caching/SKILL.md"
    },
    {
      "slug": "implementing-real-user-monitoring",
      "name": "implementing-real-user-monitoring",
      "description": "Implement Real User Monitoring (RUM) to capture actual user performance data including Core Web Vitals and page load times. Use when setting up user experience monitoring or tracking custom performance events. Trigger with phrases like \"setup RUM\", \"track Core Web Vitals\", or \"monitor real user performance\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(npm:*)",
        "Bash(rum:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Real User Monitoring\n\nThis skill provides automated assistance for real user monitoring tasks.\n\n## Overview\n\nThis skill streamlines the process of setting up Real User Monitoring (RUM) for web applications. It guides you through the essential steps of choosing a platform, defining metrics, and implementing the tracking code to capture valuable user experience data.\n\n## How It Works\n\n1. **Platform Selection**: Helps you consider available RUM platforms (e.g., Google Analytics, Datadog RUM, New Relic).\n2. **Instrumentation Design**: Guides you in defining the key performance metrics to track, including Core Web Vitals and custom events.\n3. **Tracking Code Implementation**: Assists in implementing the necessary JavaScript code to collect and transmit performance data.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement Real User Monitoring on a website or web application.\n- Track Core Web Vitals (LCP, FID, CLS) to improve user experience.\n- Monitor page load times (FCP, TTI, TTFB) for performance optimization.\n\n## Examples\n\n### Example 1: Setting up RUM for a new website\n\nUser request: \"setup RUM for my new website\"\n\nThe skill will:\n1. Guide the user through selecting a RUM platform.\n2. Provide code snippets for implementing basic tracking.\n\n### Example 2: Tracking custom performance metrics\n\nUser request: \"I want to track how long it takes users to complete a purchase\"\n\nThe skill will:\n1. Help define a custom performance metric for purchase completion time.\n2. Generate JavaScript code to track the metric.\n\n## Best Practices\n\n- **Privacy Compliance**: Ensure compliance with privacy regulations (e.g., GDPR, CCPA) when collecting user data.\n- **Sampling**: Implement sampling to reduce data volume and impact on performance.\n- **Error Handling**: Implement robust error handling to prevent tracking code from breaking the website.\n\n## Integration\n\nThis skill can be used in conjunction with other monitoring and analytics tools to provide a comprehensive view of application performance.\n\n## Prerequisites\n\n- Access to web application frontend code in {baseDir}/\n- RUM platform account (Google Analytics, Datadog, New Relic)\n- Understanding of Core Web Vitals metrics\n- Privacy compliance documentation (GDPR, CCPA)\n\n## Instructions\n\n1. Select appropriate RUM platform for requirements\n2. Define key metrics to track (Core Web Vitals, custom events)\n3. Implement tracking code in application frontend\n4. Configure data sampling and privacy settings\n5. Set up dashboards for metric visualization\n6. Define alerts for performance degradation\n\n## Output\n\n- RUM implementation code snippets\n- Platform configuration documentation\n- Custom event tracking examples\n- Dashboard definitions for key metrics\n- Privacy compliance checklist\n\n## Error Handling\n\nIf RUM implementation fails:\n- Verify platform API credentials\n- Check JavaScript bundle integration\n- Validate metric collection permissions\n- Review privacy consent configuration\n- Ensure network connectivity for data transmission\n\n## Resources\n\n- Core Web Vitals measurement guide\n- RUM platform documentation\n- Privacy compliance best practices\n- Performance monitoring strategies",
      "parentPlugin": {
        "name": "real-user-monitoring",
        "category": "performance",
        "path": "plugins/performance/real-user-monitoring",
        "version": "1.0.0",
        "description": "Implement Real User Monitoring for actual performance data"
      },
      "filePath": "plugins/performance/real-user-monitoring/skills/implementing-real-user-monitoring/SKILL.md"
    },
    {
      "slug": "integrating-secrets-managers",
      "name": "integrating-secrets-managers",
      "description": "This skill enables AI assistant to seamlessly integrate with various secrets managers like hashicorp vault and aws secrets manager. it generates configurations and setup code, ensuring best practices for secure credential management. use this skill when... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Secrets Manager Integrator\n\nThis skill provides automated assistance for secrets manager integrator tasks.\n\n## Overview\n\nThis skill empowers Claude to automate the integration of secrets managers into your infrastructure. It generates the necessary configuration files and setup code, ensuring a secure and efficient workflow for managing sensitive credentials.\n\n## How It Works\n\n1. **Identify Requirements**: Claude analyzes the user's request to determine the specific secrets manager and desired configurations.\n2. **Generate Configuration**: Based on the identified requirements, Claude generates the appropriate configuration files (e.g., Vault policies, AWS IAM roles) and setup code.\n3. **Provide Instructions**: Claude provides clear instructions on how to deploy and configure the generated code and integrate it into the existing infrastructure.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Integrate HashiCorp Vault into your infrastructure.\n- Set up AWS Secrets Manager for secure credential storage.\n- Generate configuration files for managing secrets.\n- Implement best practices for secrets management.\n\n## Examples\n\n### Example 1: Integrating Vault with a Kubernetes Cluster\n\nUser request: \"Integrate Vault with my Kubernetes cluster for managing database credentials.\"\n\nThe skill will:\n1. Generate Vault policies for accessing database credentials.\n2. Create Kubernetes service accounts with appropriate annotations for Vault integration.\n3. Provide instructions for deploying the Vault agent injector to the Kubernetes cluster.\n\n### Example 2: Setting up AWS Secrets Manager for API Keys\n\nUser request: \"Set up AWS Secrets Manager to securely store API keys for my application.\"\n\nThe skill will:\n1. Generate an IAM role with permissions to access AWS Secrets Manager.\n2. Create a Secrets Manager secret containing the API keys.\n3. Provide code snippets for retrieving the API keys from Secrets Manager within the application.\n\n## Best Practices\n\n- **Least Privilege**: Generate configurations that grant only the necessary permissions for accessing secrets.\n- **Secure Storage**: Ensure that secrets are stored securely within the chosen secrets manager.\n- **Regular Rotation**: Implement a strategy for regularly rotating secrets to minimize the impact of potential breaches.\n\n## Integration\n\nThis skill can be used in conjunction with other skills for deploying applications, configuring infrastructure, and automating DevOps workflows. It provides a secure foundation for managing sensitive information across your entire infrastructure.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "secrets-manager-integrator",
        "category": "devops",
        "path": "plugins/devops/secrets-manager-integrator",
        "version": "1.0.0",
        "description": "Integrate with secrets managers (Vault, AWS Secrets Manager, etc)"
      },
      "filePath": "plugins/devops/secrets-manager-integrator/skills/integrating-secrets-managers/SKILL.md"
    },
    {
      "slug": "load-testing-apis",
      "name": "load-testing-apis",
      "description": "Execute comprehensive load and stress testing to validate API performance and scalability. Use when validating API performance under load. Trigger with phrases like \"load test the API\", \"stress test API\", or \"benchmark API performance\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:load-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Load Tester\n\nThis skill provides automated assistance for api load tester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:load-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api load tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-load-tester",
        "category": "api-development",
        "path": "plugins/api-development/api-load-tester",
        "version": "1.0.0",
        "description": "Load test APIs with k6, Gatling, or Artillery"
      },
      "filePath": "plugins/api-development/api-load-tester/skills/load-testing-apis/SKILL.md"
    },
    {
      "slug": "logging-api-requests",
      "name": "logging-api-requests",
      "description": "Monitor and log API requests with correlation IDs, performance metrics, and security audit trails. Use when auditing API requests and responses. Trigger with phrases like \"log API requests\", \"add API logging\", or \"track API calls\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:log-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Request Logger\n\nThis skill provides automated assistance for api request logger tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:log-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api request logger tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-request-logger",
        "category": "api-development",
        "path": "plugins/api-development/api-request-logger",
        "version": "1.0.0",
        "description": "Log API requests with structured logging and correlation IDs"
      },
      "filePath": "plugins/api-development/api-request-logger/skills/logging-api-requests/SKILL.md"
    },
    {
      "slug": "managing-api-cache",
      "name": "managing-api-cache",
      "description": "Implement intelligent API response caching with Redis, Memcached, and CDN integration. Use when optimizing API performance with caching. Trigger with phrases like \"add caching\", \"optimize API performance\", or \"implement cache layer\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:cache-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Cache Manager\n\nThis skill provides automated assistance for api cache manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:cache-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api cache manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-cache-manager",
        "category": "api-development",
        "path": "plugins/api-development/api-cache-manager",
        "version": "1.0.0",
        "description": "Implement caching strategies with Redis, CDN, and HTTP headers"
      },
      "filePath": "plugins/api-development/api-cache-manager/skills/managing-api-cache/SKILL.md"
    },
    {
      "slug": "managing-autonomous-development",
      "name": "managing-autonomous-development",
      "description": "Enables AI assistant to manage sugar's autonomous development workflows. it allows AI assistant to create tasks, view the status of the system, review pending tasks, and start autonomous execution mode. use this skill when the user asks to create a new develo... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "",
      "license": "MIT",
      "content": "# Sugar\n\nThis skill provides automated assistance for sugar tasks.\n\n## Overview\n\nThis skill empowers Claude to orchestrate and monitor autonomous development processes within the Sugar environment. It provides a set of commands to create, manage, and execute tasks, ensuring efficient and automated software development workflows.\n\n## How It Works\n\n1. **Command Recognition**: Claude identifies the appropriate Sugar command (e.g., `/sugar-task`, `/sugar-status`, `/sugar-review`, `/sugar-run`).\n2. **Parameter Extraction**: Claude extracts relevant parameters from the user's request, such as task type, priority, and execution flags.\n3. **Execution**: Claude executes the corresponding Sugar command with the extracted parameters, interacting with the Sugar plugin.\n4. **Response Generation**: Claude presents the results of the command execution to the user in a clear and informative manner.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create a new development task with specific requirements.\n- Check the current status of the Sugar system and task queue.\n- Review and manage pending tasks in the queue.\n- Start or manage the autonomous execution mode.\n\n## Examples\n\n### Example 1: Creating a New Feature Task\n\nUser request: \"/sugar-task Implement user authentication --type feature --priority 4\"\n\nThe skill will:\n1. Parse the request and identify the command as `/sugar-task` with parameters \"Implement user authentication\", `--type feature`, and `--priority 4`.\n2. Execute the `sugar` command to create a new task with the specified parameters.\n3. Confirm the successful creation of the task to the user.\n\n### Example 2: Checking System Status\n\nUser request: \"/sugar-status\"\n\nThe skill will:\n1. Identify the command as `/sugar-status`.\n2. Execute the `sugar` command to retrieve the system status.\n3. Display the system status, including task queue information, to the user.\n\n## Best Practices\n\n- **Clarity**: Always confirm the parameters before executing a command to ensure accuracy.\n- **Safety**: When using `/sugar-run`, strongly advise the user to use `--dry-run --once` first.\n- **Validation**: Recommend validating the Sugar configuration before starting autonomous mode.\n\n## Integration\n\nThis skill integrates directly with the Sugar plugin, leveraging its command-line interface to manage autonomous development workflows. It can be combined with other skills to provide a more comprehensive development experience.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "sugar",
        "category": "devops",
        "path": "plugins/devops/sugar",
        "version": "unknown",
        "description": ""
      },
      "filePath": "plugins/devops/sugar/skills/managing-autonomous-development/SKILL.md"
    },
    {
      "slug": "managing-container-registries",
      "name": "managing-container-registries",
      "description": "Use when you need to work with containerization. This skill provides container management and orchestration with comprehensive guidance and automation. Trigger with phrases like \"containerize app\", \"manage containers\", or \"orchestrate deployment\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Container Registry Manager\n\nThis skill provides automated assistance for container registry manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/container-registry-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/container-registry-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/container-registry-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/container-registry-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/container-registry-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/container-registry-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "container-registry-manager",
        "category": "devops",
        "path": "plugins/devops/container-registry-manager",
        "version": "1.0.0",
        "description": "Manage container registries (ECR, GCR, Harbor)"
      },
      "filePath": "plugins/devops/container-registry-manager/skills/managing-container-registries/SKILL.md"
    },
    {
      "slug": "managing-database-migrations",
      "name": "managing-database-migrations",
      "description": "Use when you need to work with database migrations. This skill provides schema migration management with comprehensive guidance and automation. Trigger with phrases like \"create migration\", \"run migrations\", or \"manage schema versions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Migration Manager\n\nThis skill provides automated assistance for database migration manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-migration-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-migration-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-migration-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-migration-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-migration-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-migration-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-migration-manager",
        "category": "database",
        "path": "plugins/database/database-migration-manager",
        "version": "1.0.0",
        "description": "Manage database migrations with version control, rollback capabilities, and automated schema evolution tracking"
      },
      "filePath": "plugins/database/database-migration-manager/skills/managing-database-migrations/SKILL.md"
    },
    {
      "slug": "managing-database-partitions",
      "name": "managing-database-partitions",
      "description": "Use when you need to work with database partitioning. This skill provides table partitioning strategies with comprehensive guidance and automation. Trigger with phrases like \"partition tables\", \"implement partitioning\", or \"optimize large tables\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Partition Manager\n\nThis skill provides automated assistance for database partition manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-partition-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-partition-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-partition-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-partition-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-partition-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-partition-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-partition-manager",
        "category": "database",
        "path": "plugins/database/database-partition-manager",
        "version": "1.0.0",
        "description": "Database plugin for database-partition-manager"
      },
      "filePath": "plugins/database/database-partition-manager/skills/managing-database-partitions/SKILL.md"
    },
    {
      "slug": "managing-database-recovery",
      "name": "managing-database-recovery",
      "description": "Use when you need to work with database operations. This skill provides database management and optimization with comprehensive guidance and automation. Trigger with phrases like \"manage database\", \"optimize database\", or \"configure database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(tar:*), Bash(rsync:*), Bash(aws:s3:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Recovery Manager\n\nThis skill provides automated assistance for database recovery manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-recovery-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-recovery-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-recovery-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-recovery-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-recovery-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-recovery-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-recovery-manager",
        "category": "database",
        "path": "plugins/database/database-recovery-manager",
        "version": "1.0.0",
        "description": "Database plugin for database-recovery-manager"
      },
      "filePath": "plugins/database/database-recovery-manager/skills/managing-database-recovery/SKILL.md"
    },
    {
      "slug": "managing-database-replication",
      "name": "managing-database-replication",
      "description": "Use when you need to work with database scalability. This skill provides replication and sharding with comprehensive guidance and automation. Trigger with phrases like \"set up replication\", \"implement sharding\", or \"scale database\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Replication Manager\n\nThis skill provides automated assistance for database replication manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-replication-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-replication-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-replication-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-replication-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-replication-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-replication-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-replication-manager",
        "category": "database",
        "path": "plugins/database/database-replication-manager",
        "version": "1.0.0",
        "description": "Manage database replication, failover, and high availability configurations"
      },
      "filePath": "plugins/database/database-replication-manager/skills/managing-database-replication/SKILL.md"
    },
    {
      "slug": "managing-database-sharding",
      "name": "managing-database-sharding",
      "description": "Use when you need to work with database sharding. This skill provides horizontal sharding strategies with comprehensive guidance and automation. Trigger with phrases like \"implement sharding\", \"shard database\", or \"distribute data\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Sharding Manager\n\nThis skill provides automated assistance for database sharding manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-sharding-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-sharding-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-sharding-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-sharding-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-sharding-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-sharding-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-sharding-manager",
        "category": "database",
        "path": "plugins/database/database-sharding-manager",
        "version": "1.0.0",
        "description": "Database plugin for database-sharding-manager"
      },
      "filePath": "plugins/database/database-sharding-manager/skills/managing-database-sharding/SKILL.md"
    },
    {
      "slug": "managing-database-tests",
      "name": "managing-database-tests",
      "description": "Database testing including fixtures, transactions, and rollback management. Use when performing specialized testing. Trigger with phrases like \"test the database\", \"run database tests\", or \"validate data integrity\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:db-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Database Test Manager\n\nThis skill provides automated assistance for database test manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:db-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for database test manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-test-manager",
        "category": "testing",
        "path": "plugins/testing/database-test-manager",
        "version": "1.0.0",
        "description": "Database testing utilities with test data setup, transaction rollback, and schema validation"
      },
      "filePath": "plugins/testing/database-test-manager/skills/managing-database-tests/SKILL.md"
    },
    {
      "slug": "managing-deployment-rollbacks",
      "name": "managing-deployment-rollbacks",
      "description": "Use when you need to work with deployment and CI/CD. This skill provides deployment automation and orchestration with comprehensive guidance and automation. Trigger with phrases like \"deploy application\", \"create pipeline\", or \"automate deployment\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*), Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Deployment Rollback Manager\n\nThis skill provides automated assistance for deployment rollback manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/deployment-rollback-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/deployment-rollback-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/deployment-rollback-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/deployment-rollback-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/deployment-rollback-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/deployment-rollback-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "deployment-rollback-manager",
        "category": "devops",
        "path": "plugins/devops/deployment-rollback-manager",
        "version": "1.0.0",
        "description": "Manage and execute deployment rollbacks with safety checks"
      },
      "filePath": "plugins/devops/deployment-rollback-manager/skills/managing-deployment-rollbacks/SKILL.md"
    },
    {
      "slug": "managing-environment-configurations",
      "name": "managing-environment-configurations",
      "description": "Implement environment and configuration management with comprehensive guidance and automation. Use when you need to work with environment configuration. Trigger with phrases like \"manage environments\", \"configure environments\", or \"sync configurations\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Environment Config Manager\n\nThis skill provides automated assistance for environment config manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/environment-config-manager/`\n\n**Documentation and Guides**: `{baseDir}/docs/environment-config-manager/`\n\n**Example Scripts and Code**: `{baseDir}/examples/environment-config-manager/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/environment-config-manager-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/environment-config-manager-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/environment-config-manager-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "environment-config-manager",
        "category": "devops",
        "path": "plugins/devops/environment-config-manager",
        "version": "1.0.0",
        "description": "Manage environment configurations and secrets across deployments"
      },
      "filePath": "plugins/devops/environment-config-manager/skills/managing-environment-configurations/SKILL.md"
    },
    {
      "slug": "managing-network-policies",
      "name": "managing-network-policies",
      "description": "Use when managing Kubernetes network policies and firewall rules. Trigger with phrases like \"create network policy\", \"configure firewall rules\", \"restrict pod communication\", or \"setup ingress/egress rules\". Generates Kubernetes NetworkPolicy manifests following least privilege and zero-trust principles. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Network Policy Manager\n\nThis skill provides automated assistance for network policy manager tasks.\n\n## Overview\n\nCreates Kubernetes NetworkPolicy manifests to enforce least-privilege ingress/egress between pods and namespaces, and helps validate connectivity after changes.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Kubernetes cluster has network policy support enabled\n- Network plugin supports policies (Calico, Cilium, Weave)\n- Pod labels are properly defined for policy selectors\n- Understanding of application communication patterns\n- Namespace isolation strategy is defined\n\n## Instructions\n\n1. **Identify Requirements**: Determine which pods need to communicate\n2. **Define Selectors**: Use pod/namespace labels for policy targeting\n3. **Configure Ingress**: Specify allowed incoming traffic sources and ports\n4. **Configure Egress**: Define allowed outgoing traffic destinations\n5. **Test Policies**: Verify connectivity works as expected\n6. **Monitor Denials**: Check for blocked traffic in network plugin logs\n7. **Iterate**: Refine policies based on application behavior\n\n## Output\n\n**Network Policy Examples:**\n```yaml\n# {baseDir}/network-policies/allow-frontend-to-backend.yaml\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n      - podSelector:\n          matchLabels:\n            app: frontend\n      ports:\n      - protocol: TCP\n        port: 8080\n---\n# Deny all ingress by default\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n    - Ingress\n```\n\n**Egress Policy:**\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-external-api\nspec:\n  podSelector:\n    matchLabels:\n      app: api-client\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n      - namespaceSelector:\n          matchLabels:\n            name: external-services\n      ports:\n      - protocol: TCP\n        port: 443\n```\n\n## Error Handling\n\n**Policy Not Applied**\n- Error: Traffic still blocked/allowed contrary to policy\n- Solution: Verify network plugin supports policies and policy is applied to correct namespace\n\n**DNS Resolution Fails**\n- Error: Pods cannot resolve DNS after applying policy\n- Solution: Add egress rule allowing DNS traffic to kube-dns/coredns\n\n**No Communication After Policy**\n- Error: All traffic blocked unexpectedly\n- Solution: Check for default-deny policies and ensure explicit allow rules exist\n\n**Label Mismatch**\n- Error: Policy not targeting intended pods\n- Solution: Verify pod labels match policy selectors using `kubectl get pods --show-labels`\n\n## Examples\n\n- \"Restrict namespace `prod` so only the ingress controller can reach the web pods on 443.\"\n- \"Create egress rules that allow the API to talk only to Postgres and Redis.\"\n\n## Resources\n\n- Kubernetes NetworkPolicy: https://kubernetes.io/docs/concepts/services-networking/network-policies/\n- Calico documentation: https://docs.projectcalico.org/\n- Example policies in {baseDir}/network-policy-examples/",
      "parentPlugin": {
        "name": "network-policy-manager",
        "category": "devops",
        "path": "plugins/devops/network-policy-manager",
        "version": "1.0.0",
        "description": "Manage Kubernetes network policies and firewall rules"
      },
      "filePath": "plugins/devops/network-policy-manager/skills/managing-network-policies/SKILL.md"
    },
    {
      "slug": "managing-snapshot-tests",
      "name": "managing-snapshot-tests",
      "description": "Create and validate component snapshots for UI regression testing. Use when performing specialized testing. Trigger with phrases like \"update snapshots\", \"test UI snapshots\", or \"validate component snapshots\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:snapshot-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Snapshot Test Manager\n\nThis skill provides automated assistance for snapshot test manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:snapshot-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for snapshot test manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "snapshot-test-manager",
        "category": "testing",
        "path": "plugins/testing/snapshot-test-manager",
        "version": "1.0.0",
        "description": "Manage and update snapshot tests with intelligent diff analysis and selective updates"
      },
      "filePath": "plugins/testing/snapshot-test-manager/skills/managing-snapshot-tests/SKILL.md"
    },
    {
      "slug": "managing-ssltls-certificates",
      "name": "managing-ssltls-certificates",
      "description": "This skill enables AI assistant to manage and monitor ssl/tls certificates using the ssl-certificate-manager plugin. it is activated when the user requests actions related to ssl certificates, such as checking certificate expiry, renewing certificates, ... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Bash(cmd:*), Grep, Glob version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Ssl Certificate Manager\n\nThis skill provides automated assistance for ssl certificate manager tasks.\n\n## Overview\n\nThis skill empowers Claude to seamlessly interact with the ssl-certificate-manager plugin, facilitating efficient management and monitoring of SSL/TLS certificates. It allows for quick checks of certificate expiry dates, automated renewal processes, and comprehensive listings of installed certificates.\n\n## How It Works\n\n1. **Identify Intent**: Claude analyzes the user's request for keywords related to SSL/TLS certificate management.\n2. **Plugin Activation**: The ssl-certificate-manager plugin is automatically activated.\n3. **Command Execution**: Based on the user's request, Claude executes the appropriate command within the plugin (e.g., checking expiry, renewing certificate, listing certificates).\n4. **Result Presentation**: Claude presents the results of the command execution to the user in a clear and concise format.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Check the expiry date of an SSL/TLS certificate.\n- Renew an SSL/TLS certificate.\n- List all installed SSL/TLS certificates.\n- Investigate SSL/TLS certificate issues.\n\n## Examples\n\n### Example 1: Checking Certificate Expiry\n\nUser request: \"Check the expiry date of my SSL certificate for example.com\"\n\nThe skill will:\n1. Activate the ssl-certificate-manager plugin.\n2. Execute the command to check the expiry date for the specified domain.\n3. Display the expiry date to the user.\n\n### Example 2: Renewing a Certificate\n\nUser request: \"Renew my SSL certificate for api.example.org\"\n\nThe skill will:\n1. Activate the ssl-certificate-manager plugin.\n2. Execute the command to renew the SSL certificate for the specified domain.\n3. Confirm the renewal process to the user.\n\n## Best Practices\n\n- **Specificity**: Provide the full domain name when requesting certificate checks or renewals.\n- **Context**: If encountering errors, provide the full error message to Claude for better troubleshooting.\n- **Verification**: After renewal, always verify the new certificate is correctly installed and functioning.\n\n## Integration\n\nThis skill can be used in conjunction with other security-related plugins to provide a comprehensive security overview. For example, it can be integrated with vulnerability scanning tools to identify potential weaknesses related to outdated or misconfigured certificates.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "ssl-certificate-manager",
        "category": "security",
        "path": "plugins/security/ssl-certificate-manager",
        "version": "1.0.0",
        "description": "Manage and monitor SSL/TLS certificates"
      },
      "filePath": "plugins/security/ssl-certificate-manager/skills/managing-ssltls-certificates/SKILL.md"
    },
    {
      "slug": "managing-test-environments",
      "name": "managing-test-environments",
      "description": "Provision and manage isolated test environments with configuration and data. Use when performing specialized testing. Trigger with phrases like \"manage test environment\", \"provision test env\", or \"setup test infrastructure\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:env-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Test Environment Manager\n\nThis skill provides automated assistance for test environment manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:env-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test environment manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-environment-manager",
        "category": "testing",
        "path": "plugins/testing/test-environment-manager",
        "version": "1.0.0",
        "description": "Manage test environments with Docker Compose, Testcontainers, and environment isolation"
      },
      "filePath": "plugins/testing/test-environment-manager/skills/managing-test-environments/SKILL.md"
    },
    {
      "slug": "memory",
      "name": "memory",
      "description": "Extract and use project memories from previous sessions for context-aware assistance. Use when recalling past decisions, checking project conventions, or understanding user preferences. Trigger with phrases like \"remember when\", \"like before\", or \"what was our decision about\". allowed-tools: Read, Write version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "yldrmahmet",
      "license": "MIT",
      "content": "# Memory\n\nThis skill provides automated assistance for memory tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Project memory file at `{baseDir}/.memories/project_memory.json`\n- Read permissions for the memory storage location\n- Understanding that memories persist across sessions\n- Knowledge of slash commands for manual memory management\n\n## Instructions\n\n### Step 1: Access Project Memories\nRetrieve stored memories from previous sessions:\n1. Locate memory file using Read tool\n2. Parse JSON structure containing memory entries\n3. Identify relevant memories based on current context\n4. Extract applicable decisions, conventions, or preferences\n\n### Step 2: Apply Memories to Current Context\nIntegrate past decisions into current work:\n- Use remembered library/tool choices when making similar decisions\n- Apply architectural patterns established in prior sessions\n- Reference user preferences for coding style or conventions\n- Consider past decisions as context for new features\n\n### Step 3: Update Memories When Needed\nStore new decisions for future reference:\n- Add significant architectural choices\n- Document tool or library selections with rationale\n- Record user preferences and conventions\n- Update changed decisions to avoid conflicts\n\n### Step 4: Resolve Memory Conflicts\nHandle situations where memories conflict with current requests:\n- Prioritize current explicit user requests over stored memories\n- Flag conflicts for user awareness when appropriate\n- Update memories that have become outdated\n- Remove memories that are no longer relevant\n\n## Output\n\nThe skill provides seamless memory-enhanced responses:\n\n### Silent Integration\n- Memories applied automatically without announcement\n- Decisions informed by historical context\n- Consistent behavior aligned with past choices\n- Natural incorporation of established patterns\n\n### Memory Status\nWhen using slash commands:\n- List of all stored memories with timestamps\n- Confirmation of newly added memories\n- Notification of removed or updated memories\n- Summary of applicable memories for current task\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Memory File Not Found**\n- Error: Cannot locate project memory file\n- Solution: Initialize new memory file in standard location, prompt user to set up memory persistence\n\n**Conflicting Memories**\n- Error: Multiple memories contradict each other\n- Solution: Apply most recent memory, allow current request to override, suggest cleanup\n\n**Invalid Memory Format**\n- Error: Memory file corrupted or improperly formatted\n- Solution: Backup existing file, recreate with valid JSON structure, restore recoverable entries\n\n**Permission Denied**\n- Error: Cannot read or write memory file\n- Solution: Check file permissions, request necessary access, use alternative storage location\n\n## Resources\n\n### Memory Management Commands\n- `/remember [text]` - Add new memory to manual_memories array\n- `/forget [text]` - Remove matching memory from storage\n- `/memories` - Display all currently stored memories\n\n### Best Practices\n- Apply memories silently without announcing to user\n- Current explicit requests always override stored memories\n- Store significant decisions that affect future work\n- Regularly review and clean up outdated memories\n- Use memories as context, not rigid constraints\n\n### Memory Categories\n- **Architecture decisions**: Framework choices, design patterns\n- **Tool selections**: Libraries, dependencies, build tools\n- **Code conventions**: Style preferences, naming patterns\n- **User preferences**: Communication style, detail level\n- **Project constraints**: Performance targets, compatibility requirements\n\n### Integration Guidelines\n- Memory retrieval happens automatically during task analysis\n- Memories inform recommendations and implementation choices\n- User can override any memory-based decision at any time\n- Regular memory updates keep context current and relevant\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "claude-never-forgets",
        "category": "community",
        "path": "plugins/community/claude-never-forgets",
        "version": "1.0.0",
        "description": "Persistent memory across sessions. Learns preferences, conventions, and corrections automatically."
      },
      "filePath": "plugins/community/claude-never-forgets/skills/memory/SKILL.md"
    },
    {
      "slug": "migrating-apis",
      "name": "migrating-apis",
      "description": "Implement API migrations between versions, platforms, or frameworks with minimal downtime. Use when upgrading APIs between versions. Trigger with phrases like \"migrate the API\", \"upgrade API version\", or \"migrate to new API\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:migrate-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Migration Tool\n\nThis skill provides automated assistance for api migration tool tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:migrate-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api migration tool tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-migration-tool",
        "category": "api-development",
        "path": "plugins/api-development/api-migration-tool",
        "version": "1.0.0",
        "description": "Migrate APIs between versions with backward compatibility"
      },
      "filePath": "plugins/api-development/api-migration-tool/skills/migrating-apis/SKILL.md"
    },
    {
      "slug": "mocking-apis",
      "name": "mocking-apis",
      "description": "Generate mock API servers for testing and development with realistic response data. Use when creating mock APIs for development and testing. Trigger with phrases like \"create mock API\", \"generate API mock\", or \"setup mock server\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:mock-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Mock Server\n\nThis skill provides automated assistance for api mock server tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:mock-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api mock server tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-mock-server",
        "category": "api-development",
        "path": "plugins/api-development/api-mock-server",
        "version": "1.0.0",
        "description": "Create mock API servers from OpenAPI specs for testing"
      },
      "filePath": "plugins/api-development/api-mock-server/skills/mocking-apis/SKILL.md"
    },
    {
      "slug": "modeling-nosql-data",
      "name": "modeling-nosql-data",
      "description": "Use when you need to work with NoSQL data modeling. This skill provides NoSQL database design with comprehensive guidance and automation. Trigger with phrases like \"model NoSQL data\", \"design document structure\", or \"optimize NoSQL schema\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Nosql Data Modeler\n\nThis skill provides automated assistance for nosql data modeler tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/nosql-data-modeler/`\n\n**Documentation and Guides**: `{baseDir}/docs/nosql-data-modeler/`\n\n**Example Scripts and Code**: `{baseDir}/examples/nosql-data-modeler/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/nosql-data-modeler-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/nosql-data-modeler-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/nosql-data-modeler-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "nosql-data-modeler",
        "category": "database",
        "path": "plugins/database/nosql-data-modeler",
        "version": "1.0.0",
        "description": "Database plugin for nosql-data-modeler"
      },
      "filePath": "plugins/database/nosql-data-modeler/skills/modeling-nosql-data/SKILL.md"
    },
    {
      "slug": "monitoring-apis",
      "name": "monitoring-apis",
      "description": "Build real-time API monitoring dashboards with metrics, alerts, and health checks. Use when tracking API health and performance metrics. Trigger with phrases like \"monitor the API\", \"add API metrics\", or \"setup API monitoring\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:monitor-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Monitoring Dashboard\n\nThis skill provides automated assistance for api monitoring dashboard tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:monitor-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api monitoring dashboard tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-monitoring-dashboard",
        "category": "api-development",
        "path": "plugins/api-development/api-monitoring-dashboard",
        "version": "1.0.0",
        "description": "Create monitoring dashboards for API health, metrics, and alerts"
      },
      "filePath": "plugins/api-development/api-monitoring-dashboard/skills/monitoring-apis/SKILL.md"
    },
    {
      "slug": "monitoring-cpu-usage",
      "name": "monitoring-cpu-usage",
      "description": "This skill enables AI assistant to monitor and analyze cpu usage patterns within applications. it helps identify cpu hotspots, analyze algorithmic complexity, and detect blocking operations. use this skill when the user asks to \"monitor cpu usage\", \"opt... Use when setting up monitoring or observability. Trigger with phrases like 'monitor', 'metrics', or 'alerts'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Cpu Usage Monitor\n\nThis skill provides automated assistance for cpu usage monitor tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze code for CPU-intensive operations, offering detailed optimization recommendations to improve processor utilization. By pinpointing areas of high CPU usage, it facilitates targeted improvements for enhanced application performance.\n\n## How It Works\n\n1. **Initiate CPU Monitoring**: Claude activates the `cpu-usage-monitor` plugin.\n2. **Code Analysis**: The plugin analyzes the codebase for computationally expensive operations, synchronous blocking calls, inefficient loops, and regex patterns.\n3. **Optimization Recommendations**: Claude provides a detailed report outlining areas for optimization, including suggestions for algorithmic improvements, asynchronous processing, and regex optimization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify CPU bottlenecks in your application.\n- Optimize application performance by reducing CPU load.\n- Analyze code for computationally intensive operations.\n\n## Examples\n\n### Example 1: Identifying CPU Hotspots\n\nUser request: \"Monitor CPU usage in my Python script and suggest optimizations.\"\n\nThe skill will:\n1. Analyze the provided Python script for CPU-intensive functions.\n2. Identify potential bottlenecks such as inefficient loops or complex regex patterns.\n3. Provide recommendations for optimizing the code, such as using more efficient algorithms or asynchronous operations.\n\n### Example 2: Analyzing Algorithmic Complexity\n\nUser request: \"Analyze the CPU load of this Java code and identify areas with high algorithmic complexity.\"\n\nThe skill will:\n1. Analyze the provided Java code, focusing on algorithmic complexity (e.g., O(n^2) or worse).\n2. Pinpoint specific methods or sections of code with high complexity.\n3. Suggest alternative algorithms or data structures to improve performance.\n\n## Best Practices\n\n- **Targeted Analysis**: Focus the analysis on specific sections of code known to be CPU-intensive.\n- **Asynchronous Operations**: Consider using asynchronous operations to prevent blocking the main thread.\n- **Regex Optimization**: Carefully review and optimize regular expressions for performance.\n\n## Integration\n\nThis skill can be used in conjunction with other code analysis and refactoring tools to implement the suggested optimizations. It can also be integrated into CI/CD pipelines to automatically monitor CPU usage and identify performance regressions.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "cpu-usage-monitor",
        "category": "performance",
        "path": "plugins/performance/cpu-usage-monitor",
        "version": "1.0.0",
        "description": "Monitor and analyze CPU usage patterns in applications"
      },
      "filePath": "plugins/performance/cpu-usage-monitor/skills/monitoring-cpu-usage/SKILL.md"
    },
    {
      "slug": "monitoring-cross-chain-bridges",
      "name": "monitoring-cross-chain-bridges",
      "description": "Monitor cross-chain bridge security, liquidity, and transaction status across networks. Use when monitoring cross-chain asset transfers. Trigger with phrases like \"monitor bridges\", \"check cross-chain\", or \"track bridge transfers\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:bridge-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Cross Chain Bridge Monitor\n\nThis skill provides automated assistance for cross chain bridge monitor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:bridge-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "cross-chain-bridge-monitor",
        "category": "crypto",
        "path": "plugins/crypto/cross-chain-bridge-monitor",
        "version": "1.0.0",
        "description": "Monitor cross-chain bridge activity, track transfers, analyze security, and detect bridge exploits"
      },
      "filePath": "plugins/crypto/cross-chain-bridge-monitor/skills/monitoring-cross-chain-bridges/SKILL.md"
    },
    {
      "slug": "monitoring-database-health",
      "name": "monitoring-database-health",
      "description": "Use when you need to work with monitoring and observability. This skill provides health monitoring and alerting with comprehensive guidance and automation. Trigger with phrases like \"monitor system health\", \"set up alerts\", or \"track metrics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Health Monitor\n\nThis skill provides automated assistance for database health monitor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-health-monitor/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-health-monitor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-health-monitor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-health-monitor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-health-monitor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-health-monitor-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-health-monitor",
        "category": "database",
        "path": "plugins/database/database-health-monitor",
        "version": "1.0.0",
        "description": "Database plugin for database-health-monitor"
      },
      "filePath": "plugins/database/database-health-monitor/skills/monitoring-database-health/SKILL.md"
    },
    {
      "slug": "monitoring-database-transactions",
      "name": "monitoring-database-transactions",
      "description": "Use when you need to work with monitoring and observability. This skill provides health monitoring and alerting with comprehensive guidance and automation. Trigger with phrases like \"monitor system health\", \"set up alerts\", or \"track metrics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Transaction Monitor\n\nThis skill provides automated assistance for database transaction monitor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-transaction-monitor/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-transaction-monitor/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-transaction-monitor/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-transaction-monitor-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-transaction-monitor-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-transaction-monitor-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-transaction-monitor",
        "category": "database",
        "path": "plugins/database/database-transaction-monitor",
        "version": "1.0.0",
        "description": "Database plugin for database-transaction-monitor"
      },
      "filePath": "plugins/database/database-transaction-monitor/skills/monitoring-database-transactions/SKILL.md"
    },
    {
      "slug": "monitoring-error-rates",
      "name": "monitoring-error-rates",
      "description": "Monitor and analyze application error rates to improve reliability. Use when tracking errors in applications including HTTP errors, exceptions, and database issues. Trigger with phrases like \"monitor error rates\", \"track application errors\", or \"analyze error patterns\".",
      "allowedTools": [
        "\"Read",
        "Bash(monitoring:*)",
        "Bash(metrics:*)",
        "Bash(logs:*)",
        "Grep",
        "Glob\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Error Rate Monitor\n\nThis skill provides automated assistance for error rate monitor tasks.\n\n## Overview\n\nThis skill automates the process of setting up comprehensive error monitoring and alerting for various components of an application. It helps identify, track, and analyze different types of errors, enabling proactive identification and resolution of issues before they impact users.\n\n## How It Works\n\n1. **Analyze Error Sources**: Identifies potential error sources within the application architecture, including HTTP endpoints, database queries, external APIs, background jobs, and client-side code.\n2. **Define Monitoring Criteria**: Establishes specific error types and thresholds for each source, such as HTTP status codes (4xx, 5xx), exception types, query timeouts, and API response failures.\n3. **Configure Alerting**: Sets up alerts to trigger when error rates exceed defined thresholds, notifying relevant teams or individuals for investigation and remediation.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Set up error monitoring for a new application.\n- Analyze existing error rates and identify areas for improvement.\n- Configure alerts to be notified of critical errors in real-time.\n- Establish error budgets and track progress towards reliability goals.\n\n## Examples\n\n### Example 1: Setting up Error Monitoring for a Web Application\n\nUser request: \"Monitor errors in my web application, especially 500 errors and database connection issues.\"\n\nThe skill will:\n1. Analyze the web application's architecture to identify potential error sources (e.g., HTTP endpoints, database connections).\n2. Configure monitoring for 500 errors and database connection failures, setting appropriate thresholds and alerts.\n\n### Example 2: Analyzing Error Rates in a Background Job Processor\n\nUser request: \"Analyze error rates for my background job processor. I'm seeing a lot of failed jobs.\"\n\nThe skill will:\n1. Focus on the background job processor and identify the types of errors occurring (e.g., task failures, timeouts, resource exhaustion).\n2. Analyze the frequency and patterns of these errors to identify potential root causes.\n\n## Best Practices\n\n- **Granularity**: Monitor errors at a granular level to identify specific problem areas.\n- **Thresholding**: Set appropriate alert thresholds to avoid alert fatigue and focus on critical issues.\n- **Context**: Include relevant context in error messages and alerts to facilitate troubleshooting.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools, such as Prometheus, Grafana, and PagerDuty, to provide a comprehensive view of application health and performance. It can also be used in conjunction with incident management tools to streamline incident response workflows.\n\n## Prerequisites\n\n- Access to application logs and metrics\n- Monitoring infrastructure (Prometheus, Grafana, or similar)\n- Read permissions for log files in {baseDir}/logs/\n- Network access to monitoring endpoints\n\n## Instructions\n\n1. Identify error sources by analyzing application architecture\n2. Define error types and monitoring thresholds\n3. Configure alerting rules with appropriate severity levels\n4. Set up dashboards for error rate visualization\n5. Establish notification channels for critical errors\n6. Document error baselines and SLO targets\n\n## Output\n\n- Error rate metrics and trends\n- Alert configurations for critical thresholds\n- Dashboard definitions for error monitoring\n- Reports on error patterns and root causes\n- Recommendations for error reduction strategies\n\n## Error Handling\n\nIf monitoring setup fails:\n- Verify log file permissions and paths\n- Check monitoring service connectivity\n- Validate metric export configurations\n- Review alert rule syntax\n- Ensure notification channels are configured\n\n## Resources\n\n- Monitoring platform documentation (Prometheus, Grafana)\n- Application log format specifications\n- Error taxonomy and classification guides\n- SLO/SLI definition best practices",
      "parentPlugin": {
        "name": "error-rate-monitor",
        "category": "performance",
        "path": "plugins/performance/error-rate-monitor",
        "version": "1.0.0",
        "description": "Monitor and analyze application error rates"
      },
      "filePath": "plugins/performance/error-rate-monitor/skills/monitoring-error-rates/SKILL.md"
    },
    {
      "slug": "monitoring-whale-activity",
      "name": "monitoring-whale-activity",
      "description": "Track large crypto transactions and whale wallet movements across blockchains. Use when tracking large holder movements. Trigger with phrases like \"track whales\", \"monitor large transfers\", or \"check whale activity\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:whale-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Whale Alert Monitor\n\nThis skill provides automated assistance for whale alert monitor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:whale-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "whale-alert-monitor",
        "category": "crypto",
        "path": "plugins/crypto/whale-alert-monitor",
        "version": "1.0.0",
        "description": "Monitor large crypto transactions and whale wallet movements in real-time"
      },
      "filePath": "plugins/crypto/whale-alert-monitor/skills/monitoring-whale-activity/SKILL.md"
    },
    {
      "slug": "ollama-setup",
      "name": "ollama-setup",
      "description": "Auto-configure Ollama when user needs local LLM deployment, free AI alternatives, or wants to eliminate hosted API costs. Trigger phrases: \"install ollama\", \"local AI\", \"free LLM\", \"self-hosted AI\", \"replace OpenAI\", \"no API costs\". Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Ollama Setup Skill\n\n\n\nThis skill provides automated assistance for ollama setup tasks.\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## Purpose\n\nAutomatically detect when users need local AI deployment and guide them through Ollama installation. Eliminates paid API dependencies.\n\n## Activation Triggers\n\n- \"install ollama\"\n- \"local AI models\"\n- \"free alternative to OpenAI\"\n- \"self-hosted LLM\"\n- \"run AI locally\"\n- \"eliminate API costs\"\n- \"privacy-first AI\"\n- \"offline AI models\"\n\n## Skill Workflow\n\n### 1. Detect Need for Local AI\n\nWhen user mentions:\n- High API costs\n- Privacy concerns\n- Offline requirements\n- OpenAI/Anthropic alternatives\n- Self-hosted infrastructure\n\n**‚Üí Activate this skill**\n\n### 2. Assess System Requirements\n\n```bash\n# Check OS\nuname -s\n\n# Check available memory\nfree -h  # Linux\nvm_stat  # macOS\n\n# Check GPU\nnvidia-smi  # NVIDIA\nsystem_profiler SPDisplaysDataType  # macOS\n```\n\n### 3. Recommend Appropriate Models\n\n**8GB RAM:**\n- llama3.2:7b (4GB)\n- mistral:7b (4GB)\n- phi3:14b (8GB)\n\n**16GB RAM:**\n- codellama:13b (7GB)\n- mixtral:8x7b (26GB quantized)\n\n**32GB+ RAM:**\n- llama3.2:70b (40GB)\n- codellama:34b (20GB)\n\n### 4. Installation Process\n\n**macOS:**\n```bash\nbrew install ollama\nbrew services start ollama\nollama pull llama3.2\n```\n\n**Linux:**\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\nsudo systemctl start ollama\nollama pull llama3.2\n```\n\n**Docker:**\n```bash\ndocker run -d \\\\\n  -v ollama:/root/.ollama \\\\\n  -p 11434:11434 \\\\\n  --name ollama \\\\\n  ollama/ollama\n\ndocker exec -it ollama ollama pull llama3.2\n```\n\n### 5. Verify Installation\n\n```bash\nollama list\nollama run llama3.2 \"Say hello\"\ncurl http://localhost:11434/api/tags\n```\n\n### 6. Integration Examples\n\n**Python:**\n```python\nimport ollama\n\nresponse = ollama.chat(\n    model='llama3.2',\n    messages=[{'role': 'user', 'content': 'Hello!'}]\n)\nprint(response['message']['content'])\n```\n\n**Node.js:**\n```javascript\nconst ollama = require('ollama')\n\nconst response = await ollama.chat({\n  model: 'llama3.2',\n  messages: [{ role: 'user', content: 'Hello!' }]\n})\n```\n\n**cURL:**\n```bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Hello!\"\n}'\n```\n\n## Common Use Cases\n\n### Replace OpenAI\n```python\n# Before (Paid)\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"...\")\n\n# After (Free)\nimport ollama\nresponse = ollama.chat(model='llama3.2', ...)\n```\n\n### Replace Anthropic Claude\n```python\n# Before (Paid)\nfrom anthropic import Anthropic\nclient = Anthropic(api_key=\"...\")\n\n# After (Free)\nimport ollama\nresponse = ollama.chat(model='mistral', ...)\n```\n\n### Code Generation\n```bash\nollama pull codellama\nollama run codellama \"Write a Python REST API\"\n```\n\n## Cost Comparison\n\n**OpenAI GPT-4:**\n- Input: $0.03/1K tokens\n- Output: $0.06/1K tokens\n- 1M tokens/month = $30-60\n\n**Ollama:**\n- Setup: Free\n- Usage: $0 (hardware you already own)\n- Savings: $30-60/month ‚úì\n\n## Performance Expectations\n\n**With GPU (NVIDIA/Metal):**\n- 7B models: 50-100 tokens/sec\n- 13B models: 30-60 tokens/sec\n- 34B models: 20-40 tokens/sec\n\n**CPU Only:**\n- 7B models: 10-20 tokens/sec\n- 13B models: 5-10 tokens/sec\n- 34B models: 2-5 tokens/sec\n\n## Troubleshooting\n\n### Out of Memory\n```bash\n# Use quantized models\nollama pull llama3.2:7b-q4  # 4-bit (smaller)\n```\n\n### Slow Performance\n```bash\n# Use smaller model\nollama pull mistral:7b  # Faster than 70B\n```\n\n### Model Not Found\n```bash\n# Pull model first\nollama pull llama3.2\nollama list  # Verify\n```\n\n## Success Metrics\n\nAfter skill execution, user should have:\n- ‚úÖ Ollama installed and running\n- ‚úÖ At least one model downloaded\n- ‚úÖ Successful test inference\n- ‚úÖ Integration code examples\n- ‚úÖ Zero ongoing API costs\n\n## Privacy Benefits\n\n- Data never leaves local machine\n- No API keys required\n- No usage tracking\n- GDPR/HIPAA compliant (local only)\n- Offline capable\n\n## When NOT to Use This Skill\n\n- User needs latest GPT-4 specifically\n- User has <8GB RAM\n- User needs real-time updates (like web search)\n- User wants Claude Code itself (requires Anthropic API)\n\n## Related Skills\n\n- `local-llm-wrapper` - Generic local LLM integration\n- `ai-sdk-agents` - AI SDK with Ollama support\n- `privacy-first-ai` - Privacy-focused AI workflows",
      "parentPlugin": {
        "name": "ollama-local-ai",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ollama-local-ai",
        "version": "1.0.0",
        "description": "Run AI models locally with Ollama - free alternative to OpenAI, Anthropic, and other paid LLM APIs. Zero-cost, privacy-first AI infrastructure."
      },
      "filePath": "plugins/ai-ml/ollama-local-ai/skills/ollama-setup/SKILL.md"
    },
    {
      "slug": "optimizing-cache-performance",
      "name": "optimizing-cache-performance",
      "description": "This skill enables AI assistant to analyze and improve application caching strategies. it optimizes cache hit rates, ttl configurations, cache key design, and invalidation strategies. use this skill when the user requests to \"optimize cache performance\"... Use when optimizing performance. Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Write, Bash(cmd:*), Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Cache Performance Optimizer\n\nThis skill provides automated assistance for cache performance optimizer tasks.\n\n## Overview\n\nThis skill empowers Claude to diagnose and resolve caching-related performance issues. It guides users through a comprehensive optimization process, ensuring efficient use of caching resources.\n\n## How It Works\n\n1. **Identify Caching Implementation**: Locates the caching implementation within the project (e.g., Redis, Memcached, in-memory caches).\n2. **Analyze Cache Configuration**: Examines the existing cache configuration, including TTL values, eviction policies, and key structures.\n3. **Recommend Optimizations**: Suggests improvements to cache hit rates, TTLs, key design, invalidation strategies, and memory usage.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Improve application performance by optimizing caching mechanisms.\n- Identify and resolve caching-related bottlenecks.\n- Review and improve cache key design for better hit rates.\n\n## Examples\n\n### Example 1: Optimizing Redis Cache\n\nUser request: \"Optimize Redis cache performance.\"\n\nThe skill will:\n1. Analyze the Redis configuration, including TTLs and memory usage.\n2. Recommend optimal TTL values based on data access patterns.\n\n### Example 2: Improving Cache Hit Rate\n\nUser request: \"Improve cache hit rate in my application.\"\n\nThe skill will:\n1. Analyze cache key design and identify potential areas for improvement.\n2. Suggest more effective cache key structures to increase hit rates.\n\n## Best Practices\n\n- **TTL Management**: Set appropriate TTL values to balance data freshness and cache hit rates.\n- **Key Design**: Use consistent and well-structured cache keys for efficient retrieval.\n- **Invalidation Strategies**: Implement proper cache invalidation strategies to avoid serving stale data.\n\n## Integration\n\nThis skill can integrate with code analysis tools to automatically identify caching implementations and configuration. It can also work with monitoring tools to track cache hit rates and performance metrics.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "cache-performance-optimizer",
        "category": "performance",
        "path": "plugins/performance/cache-performance-optimizer",
        "version": "1.0.0",
        "description": "Optimize caching strategies for improved performance"
      },
      "filePath": "plugins/performance/cache-performance-optimizer/skills/optimizing-cache-performance/SKILL.md"
    },
    {
      "slug": "optimizing-cloud-costs",
      "name": "optimizing-cloud-costs",
      "description": "Use when you need to work with cloud cost optimization. This skill provides cost analysis and optimization with comprehensive guidance and automation. Trigger with phrases like \"optimize costs\", \"analyze spending\", or \"reduce costs\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(aws:*), Bash(gcloud:*), Bash(az:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Cloud Cost Optimizer\n\nThis skill provides automated assistance for cloud cost optimizer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/cloud-cost-optimizer/`\n\n**Documentation and Guides**: `{baseDir}/docs/cloud-cost-optimizer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/cloud-cost-optimizer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/cloud-cost-optimizer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/cloud-cost-optimizer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/cloud-cost-optimizer-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "cloud-cost-optimizer",
        "category": "devops",
        "path": "plugins/devops/cloud-cost-optimizer",
        "version": "1.0.0",
        "description": "Optimize cloud costs and generate cost reports"
      },
      "filePath": "plugins/devops/cloud-cost-optimizer/skills/optimizing-cloud-costs/SKILL.md"
    },
    {
      "slug": "optimizing-database-connection-pooling",
      "name": "optimizing-database-connection-pooling",
      "description": "Use when you need to work with connection management. This skill provides connection pooling and management with comprehensive guidance and automation. Trigger with phrases like \"manage connections\", \"configure pooling\", or \"optimize connection usage\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Connection Pooler\n\nThis skill provides automated assistance for database connection pooler tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-connection-pooler/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-connection-pooler/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-connection-pooler/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-connection-pooler-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-connection-pooler-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-connection-pooler-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-connection-pooler",
        "category": "database",
        "path": "plugins/database/database-connection-pooler",
        "version": "1.0.0",
        "description": "Implement and optimize database connection pooling for improved performance and resource management"
      },
      "filePath": "plugins/database/database-connection-pooler/skills/optimizing-database-connection-pooling/SKILL.md"
    },
    {
      "slug": "optimizing-deep-learning-models",
      "name": "optimizing-deep-learning-models",
      "description": "Optimize deep learning models using Adam, SGD, and learning rate scheduling to improve accuracy and reduce training time. Use when asked to \"optimize deep learning model\" or \"improve model performance\". Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Deep Learning Optimizer\n\nThis skill provides automated assistance for deep learning optimizer tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for deep learning optimizer tasks.\nThis skill empowers Claude to automatically optimize deep learning models, enhancing their performance and efficiency. It intelligently applies various optimization techniques based on the model's characteristics and the user's objectives.\n\n## How It Works\n\n1. **Analyze Model**: Examines the deep learning model's architecture, training data, and performance metrics.\n2. **Identify Optimizations**: Determines the most effective optimization strategies based on the analysis, such as adjusting the learning rate, applying regularization techniques, or modifying the optimizer.\n3. **Apply Optimizations**: Generates optimized code that implements the chosen strategies.\n4. **Evaluate Performance**: Assesses the impact of the optimizations on model performance, providing metrics like accuracy, training time, and resource consumption.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Optimize the performance of a deep learning model.\n- Reduce the training time of a deep learning model.\n- Improve the accuracy of a deep learning model.\n- Optimize the learning rate for a deep learning model.\n- Reduce resource consumption during deep learning model training.\n\n## Examples\n\n### Example 1: Improving Model Accuracy\n\nUser request: \"Optimize this deep learning model for improved image classification accuracy.\"\n\nThe skill will:\n1. Analyze the model and identify potential areas for improvement, such as adjusting the learning rate or adding regularization.\n2. Apply the selected optimization techniques and generate optimized code.\n3. Evaluate the model's performance and report the improved accuracy.\n\n### Example 2: Reducing Training Time\n\nUser request: \"Reduce the training time of this deep learning model.\"\n\nThe skill will:\n1. Analyze the model and identify bottlenecks in the training process.\n2. Apply techniques like batch size adjustment or optimizer selection to reduce training time.\n3. Evaluate the model's performance and report the reduced training time.\n\n## Best Practices\n\n- **Optimizer Selection**: Experiment with different optimizers (e.g., Adam, SGD) to find the best fit for the model and dataset.\n- **Learning Rate Scheduling**: Implement learning rate scheduling to dynamically adjust the learning rate during training.\n- **Regularization**: Apply regularization techniques (e.g., L1, L2 regularization) to prevent overfitting.\n\n## Integration\n\nThis skill can be integrated with other plugins that provide model building and data preprocessing capabilities. It can also be used in conjunction with monitoring tools to track the performance of optimized models.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "deep-learning-optimizer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/deep-learning-optimizer",
        "version": "1.0.0",
        "description": "Deep learning optimization techniques"
      },
      "filePath": "plugins/ai-ml/deep-learning-optimizer/skills/optimizing-deep-learning-models/SKILL.md"
    },
    {
      "slug": "optimizing-defi-yields",
      "name": "optimizing-defi-yields",
      "description": "Find and compare DeFi yield opportunities across protocols with APY calculations. Use when finding optimal DeFi yield opportunities. Trigger with phrases like \"find yield\", \"optimize returns\", or \"compare APY\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:yield-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Defi Yield Optimizer\n\nThis skill provides automated assistance for defi yield optimizer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:yield-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "defi-yield-optimizer",
        "category": "crypto",
        "path": "plugins/crypto/defi-yield-optimizer",
        "version": "1.0.0",
        "description": "Optimize DeFi yield farming strategies across protocols with APY tracking and risk assessment"
      },
      "filePath": "plugins/crypto/defi-yield-optimizer/skills/optimizing-defi-yields/SKILL.md"
    },
    {
      "slug": "optimizing-gas-fees",
      "name": "optimizing-gas-fees",
      "description": "Predict optimal gas prices and transaction timing to minimize blockchain transaction costs. Use when optimizing blockchain transaction costs. Trigger with phrases like \"optimize gas\", \"check gas prices\", or \"minimize fees\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:gas-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Gas Fee Optimizer\n\nThis skill provides automated assistance for gas fee optimizer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:gas-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "gas-fee-optimizer",
        "category": "crypto",
        "path": "plugins/crypto/gas-fee-optimizer",
        "version": "1.0.0",
        "description": "Optimize transaction gas fees with timing and routing recommendations"
      },
      "filePath": "plugins/crypto/gas-fee-optimizer/skills/optimizing-gas-fees/SKILL.md"
    },
    {
      "slug": "optimizing-prompts",
      "name": "optimizing-prompts",
      "description": "This skill optimizes prompts for large language models (llms) to reduce token usage, lower costs, and improve performance. it analyzes the prompt, identifies areas for simplification and redundancy removal, and rewrites the prompt to be more conci... Use when optimizing performance. Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Ai Ml Engineering Pack\n\nThis skill provides automated assistance for ai ml engineering pack tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for ai ml engineering pack tasks.\nThis skill empowers Claude to refine prompts for optimal LLM performance. It streamlines prompts to minimize token count, thereby reducing costs and enhancing response speed, all while maintaining or improving output quality.\n\n## How It Works\n\n1. **Analyzing Prompt**: The skill analyzes the input prompt to identify areas of redundancy, verbosity, and potential for simplification.\n2. **Rewriting Prompt**: It rewrites the prompt using techniques like concise language, targeted instructions, and efficient phrasing.\n3. **Suggesting Alternatives**: The skill provides the optimized prompt along with an explanation of the changes made and their expected impact.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Reduce the cost of using an LLM.\n- Improve the speed of LLM responses.\n- Enhance the quality or clarity of LLM outputs by refining the prompt.\n\n## Examples\n\n### Example 1: Reducing LLM Costs\n\nUser request: \"Optimize this prompt for cost and quality: 'I would like you to create a detailed product description for a new ergonomic office chair, highlighting its features, benefits, and target audience, and also include information about its warranty and return policy.'\"\n\nThe skill will:\n1. Analyze the prompt for redundancies and areas for simplification.\n2. Rewrite the prompt to be more concise: \"Create a product description for an ergonomic office chair. Include features, benefits, target audience, warranty, and return policy.\"\n3. Provide the optimized prompt and explain the token reduction achieved.\n\n### Example 2: Improving Prompt Performance\n\nUser request: \"Optimize this prompt for better summarization: 'Please read the following document and provide a comprehensive summary of all the key points, main arguments, supporting evidence, and overall conclusion, ensuring that the summary is accurate, concise, and easy to understand.'\"\n\nThe skill will:\n1. Identify areas for improvement in the prompt's clarity and focus.\n2. Rewrite the prompt to be more direct: \"Summarize this document, including key points, arguments, evidence, and the conclusion.\"\n3. Present the optimized prompt and explain how it enhances summarization performance.\n\n## Best Practices\n\n- **Clarity**: Ensure the original prompt is clear and well-defined before optimization.\n- **Context**: Provide sufficient context to the skill so it can understand the prompt's purpose.\n- **Iteration**: Iterate on the optimized prompt based on the LLM's output to fine-tune performance.\n\n## Integration\n\nThis skill integrates with the `prompt-architect` agent to leverage advanced prompt engineering techniques. It can also be used in conjunction with the `llm-integration-expert` to optimize prompts for specific LLM APIs.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "ai-ml-engineering-pack",
        "category": "packages",
        "path": "plugins/packages/ai-ml-engineering-pack",
        "version": "1.0.0",
        "description": "Professional AI/ML Engineering toolkit: Prompt engineering, LLM integration, RAG systems, AI safety with 12 expert plugins"
      },
      "filePath": "plugins/packages/ai-ml-engineering-pack/skills/optimizing-prompts/SKILL.md"
    },
    {
      "slug": "optimizing-sql-queries",
      "name": "optimizing-sql-queries",
      "description": "Use when you need to work with query optimization. This skill provides query performance analysis with comprehensive guidance and automation. Trigger with phrases like \"optimize queries\", \"analyze performance\", or \"improve query speed\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Sql Query Optimizer\n\nThis skill provides automated assistance for sql query optimizer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/sql-query-optimizer/`\n\n**Documentation and Guides**: `{baseDir}/docs/sql-query-optimizer/`\n\n**Example Scripts and Code**: `{baseDir}/examples/sql-query-optimizer/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/sql-query-optimizer-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/sql-query-optimizer-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/sql-query-optimizer-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "sql-query-optimizer",
        "category": "database",
        "path": "plugins/database/sql-query-optimizer",
        "version": "1.0.0",
        "description": "Analyze and optimize SQL queries for better performance, suggesting indexes, query rewrites, and execution plan improvements"
      },
      "filePath": "plugins/database/sql-query-optimizer/skills/optimizing-sql-queries/SKILL.md"
    },
    {
      "slug": "optimizing-staking-rewards",
      "name": "optimizing-staking-rewards",
      "description": "Compare staking rewards across validators and networks with ROI calculations. Use when optimizing proof-of-stake rewards. Trigger with phrases like \"optimize staking\", \"compare validators\", or \"calculate rewards\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:staking-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Staking Rewards Optimizer\n\nThis skill provides automated assistance for staking rewards optimizer tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:staking-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "staking-rewards-optimizer",
        "category": "crypto",
        "path": "plugins/crypto/staking-rewards-optimizer",
        "version": "1.0.0",
        "description": "Optimize staking rewards across multiple protocols and chains"
      },
      "filePath": "plugins/crypto/staking-rewards-optimizer/skills/optimizing-staking-rewards/SKILL.md"
    },
    {
      "slug": "orchestrating-deployment-pipelines",
      "name": "orchestrating-deployment-pipelines",
      "description": "Use when you need to work with deployment and CI/CD. This skill provides deployment automation and orchestration with comprehensive guidance and automation. Trigger with phrases like \"deploy application\", \"create pipeline\", or \"automate deployment\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(git:*), Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Deployment Pipeline Orchestrator\n\nThis skill provides automated assistance for deployment pipeline orchestrator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/deployment-pipeline-orchestrator/`\n\n**Documentation and Guides**: `{baseDir}/docs/deployment-pipeline-orchestrator/`\n\n**Example Scripts and Code**: `{baseDir}/examples/deployment-pipeline-orchestrator/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/deployment-pipeline-orchestrator-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/deployment-pipeline-orchestrator-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/deployment-pipeline-orchestrator-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "deployment-pipeline-orchestrator",
        "category": "devops",
        "path": "plugins/devops/deployment-pipeline-orchestrator",
        "version": "1.0.0",
        "description": "Orchestrate complex multi-stage deployment pipelines"
      },
      "filePath": "plugins/devops/deployment-pipeline-orchestrator/skills/orchestrating-deployment-pipelines/SKILL.md"
    },
    {
      "slug": "orchestrating-multi-agent-systems",
      "name": "orchestrating-multi-agent-systems",
      "description": "Orchestrate multi-agent systems with handoffs, routing, and workflows across AI providers. Use when building complex AI systems requiring agent collaboration, task delegation, or workflow coordination. Trigger with phrases like \"create multi-agent system\", \"orchestrate agents\", or \"coordinate agent workflows\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(npm:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Ai Sdk Agents\n\nThis skill provides automated assistance for ai sdk agents tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Node.js 18+ installed for TypeScript agent development\n- AI SDK v5 package installed (`npm install ai`)\n- API keys for AI providers (OpenAI, Anthropic, Google, etc.)\n- Understanding of agent-based architecture patterns\n- TypeScript knowledge for agent implementation\n- Project directory structure for multi-agent systems\n\n## Instructions\n\n### Step 1: Initialize Project Structure\nSet up the foundation for your multi-agent system:\n1. Create project directory with necessary subdirectories\n2. Initialize npm project with TypeScript configuration\n3. Install AI SDK v5 and provider-specific packages\n4. Set up configuration files for agent orchestration\n\n### Step 2: Define Agent Roles\nIdentify and specify specialized agents needed:\n- Determine agent responsibilities and capabilities\n- Define agent system prompts with clear instructions\n- Specify tools each agent can access\n- Establish agent communication protocols\n\n### Step 3: Implement Agents\nCreate individual agent files with proper configuration:\n1. Write agent initialization code with AI SDK\n2. Configure system prompts for agent behavior\n3. Define tool functions for agent capabilities\n4. Implement handoff rules for inter-agent delegation\n\n### Step 4: Configure Orchestration\nSet up coordination between agents:\n- Define workflow sequences for task processing\n- Implement routing logic for task distribution\n- Configure handoff mechanisms between agents\n- Set up state management for multi-step workflows\n\n### Step 5: Test and Refine\nValidate the multi-agent system functionality:\n- Test individual agent responses and behaviors\n- Verify handoff execution between agents\n- Validate routing logic with different input scenarios\n- Monitor coordination and identify bottlenecks\n\n## Output\n\nThe skill generates a complete multi-agent system including:\n\n### Project Structure\n```\n{baseDir}/\n‚îú‚îÄ‚îÄ agents/\n‚îÇ   ‚îú‚îÄ‚îÄ coordinator.ts       # Main orchestration agent\n‚îÇ   ‚îú‚îÄ‚îÄ specialist-1.ts      # Domain-specific agent\n‚îÇ   ‚îú‚îÄ‚îÄ specialist-2.ts      # Domain-specific agent\n‚îÇ   ‚îî‚îÄ‚îÄ [additional agents]\n‚îú‚îÄ‚îÄ orchestration/\n‚îÇ   ‚îú‚îÄ‚îÄ workflow.ts          # Workflow definitions\n‚îÇ   ‚îú‚îÄ‚îÄ routing.ts           # Routing logic\n‚îÇ   ‚îî‚îÄ‚îÄ handoffs.ts          # Handoff configurations\n‚îú‚îÄ‚îÄ tools/\n‚îÇ   ‚îî‚îÄ‚îÄ [agent tools]        # Shared tool implementations\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îî‚îÄ‚îÄ agents.config.ts     # Agent configurations\n‚îî‚îÄ‚îÄ package.json             # Dependencies\n```\n\n### Agent Implementation Files\n- TypeScript files with AI SDK v5 integration\n- System prompts tailored to each agent role\n- Tool definitions and implementations\n- Handoff rules and coordination logic\n\n### Orchestration Configuration\n- Workflow definitions for task sequences\n- Routing rules for intelligent task distribution\n- State management for multi-step processes\n- Error handling and fallback mechanisms\n\n### Documentation\n- Agent role descriptions and capabilities\n- Workflow diagrams showing agent interactions\n- API documentation for agent endpoints\n- Usage examples for common scenarios\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Agent Initialization Failures**\n- Error: AI SDK provider configuration invalid\n- Solution: Verify API keys in environment variables, check provider-specific setup requirements\n\n**Handoff Execution Errors**\n- Error: Agent handoff fails or creates circular dependencies\n- Solution: Review handoff rules for clarity, implement handoff depth limits, add fallback agents\n\n**Routing Logic Failures**\n- Error: Tasks routed to incorrect agent or no agent\n- Solution: Refine routing criteria, add default routing rules, implement topic classification improvement\n\n**Tool Access Violations**\n- Error: Agent attempts to use unauthorized tools\n- Solution: Review tool permissions per agent, implement proper access control, validate tool configurations\n\n**Workflow Deadlocks**\n- Error: Multi-agent workflow stalls without completion\n- Solution: Implement timeout mechanisms, add workflow monitoring, design escape conditions for stuck states\n\n## Resources\n\n### AI SDK Documentation\n- AI SDK v5 official documentation for agent creation\n- Provider-specific integration guides (OpenAI, Anthropic, Google)\n- Tool definition and implementation examples\n- Handoff and routing pattern references\n\n### Multi-Agent Architecture Patterns\n- Coordinator-worker pattern for task distribution\n- Pipeline pattern for sequential processing\n- Hub-and-spoke pattern for centralized coordination\n- Peer-to-peer pattern for collaborative agents\n\n### Agent Design Best Practices\n- Single responsibility principle for agent specialization\n- Clear handoff criteria and routing rules\n- Comprehensive error handling and fallbacks\n- State management for complex workflows\n- Testing strategies for multi-agent systems\n\n### Example Use Cases\n- Code generation pipelines with specialized agents\n- Customer support routing systems\n- Research and analysis workflows\n- Content creation and review pipelines\n- Data processing and validation systems\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "ai-sdk-agents",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ai-sdk-agents",
        "version": "1.0.0",
        "description": "Multi-agent orchestration with AI SDK v5 - handoffs, routing, and coordination for any AI provider (OpenAI, Anthropic, Google)"
      },
      "filePath": "plugins/ai-ml/ai-sdk-agents/skills/orchestrating-multi-agent-systems/SKILL.md"
    },
    {
      "slug": "orchestrating-test-execution",
      "name": "orchestrating-test-execution",
      "description": "Coordinate parallel test execution across multiple environments and frameworks. Use when performing specialized testing. Trigger with phrases like \"orchestrate tests\", \"run parallel tests\", or \"coordinate test execution\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:orchestrate-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Test Orchestrator\n\nThis skill provides automated assistance for test orchestrator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:orchestrate-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for test orchestrator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "test-orchestrator",
        "category": "testing",
        "path": "plugins/testing/test-orchestrator",
        "version": "1.0.0",
        "description": "Orchestrate complex test workflows with dependencies, parallel execution, and smart test selection"
      },
      "filePath": "plugins/testing/test-orchestrator/skills/orchestrating-test-execution/SKILL.md"
    },
    {
      "slug": "overnight-development",
      "name": "overnight-development",
      "description": "Automates software development overnight using git hooks to enforce test-driven Use when appropriate context detected. Trigger with relevant phrases based on skill purpose.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(general:*)",
        "Bash(util:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Overnight Development\n\nThis skill provides automated assistance for overnight dev tasks.\n\n## Overview\n\nThis skill automates software development overnight by leveraging Git hooks to enforce test-driven development (TDD). It ensures that all code changes are fully tested and meet specified quality standards before being committed. This approach allows Claude to work autonomously, building new features, refactoring existing code, or fixing bugs while adhering to a rigorous TDD process.\n\n## Prerequisites\n\n- A git repository with a working test command (at least one passing test)\n- A lint/format command (recommended) and a clear CI-style ‚Äúquality gate‚Äù\n- Permissions to create/edit git hooks (or a hooks manager like Husky)\n- A clearly defined overnight task with measurable success criteria (tests/coverage/acceptance checks)\n\n## Instructions\n\n1. Install and enable the `overnight-dev` plugin, then run `/overnight-setup` to install hooks and config.\n2. Define the overnight task as a testable goal (what to build/fix, what ‚Äúdone‚Äù means, and which commands must pass).\n3. Start implementation with TDD: write a failing test, implement the minimum change, repeat.\n4. Let the hooks block commits until the quality gate passes; iterate on failures until green.\n5. In the morning, review the session log and diffs, then run the same quality gate once locally/CI for confidence.\n\n## Output\n\n- Installed/updated git hooks and any repo config files used by the plugin\n- A progress log for the overnight run (for example `.overnight-dev-log.txt`)\n- A set of commits that only land when the configured quality gate passes (tests/lint/coverage as configured)\n\n## Error Handling\n\nCommon failures are handled by making the failure actionable (show the exact command output) and retrying until the gate passes:\n- Hooks not executable or not wired correctly\n- Tests failing (new failures or flaky tests)\n- Lint/format failures\n- Missing dependencies or misconfigured commands\n\n## Core Capabilities\n\n-   Enforces test-driven development (TDD) using Git hooks.\n-   Automates debugging and code fixing until all tests pass.\n-   Tracks progress and logs activities during overnight sessions.\n-   Supports flexible configuration for various testing frameworks and languages.\n-   Provides guidance and support through the `overnight-dev-coach` agent.\n\n## Workflow\n\n### Phase 1: Project Setup and Configuration\n\nTo prepare the project for overnight development:\n\n1.  **Verify Prerequisites:** Ensure the project is a Git repository, has a configured test framework, and includes at least one passing test.\n    ```bash\n    git init\n    npm install --save-dev jest # Example for Node.js\n    ```\n\n2.  **Install the Plugin:** Add the Claude Code Plugin marketplace and install the `overnight-dev` plugin.\n    ```bash\n    /plugin marketplace add jeremylongshore/claude-code-plugins\n    /plugin install overnight-dev@claude-code-plugins-plus\n    ```\n\n3.  **Run Setup Command:** Execute the `/overnight-setup` command to create necessary Git hooks and configuration files.\n    ```bash\n    /overnight-setup\n    ```\n\n### Phase 2: Task Definition and Planning\n\nTo define the task for the overnight session:\n\n1.  **Define a Clear Goal:** Specify a clear and testable goal for the overnight session, such as \"Build user authentication with JWT (90% coverage).\"\n    ```text\n    Task: Build user authentication with JWT (90% coverage)\n    Success: All tests pass, 90%+ coverage, fully documented\n    ```\n\n2.  **Start Coding:** Begin implementing the feature by writing tests first, following the TDD approach.\n    ```javascript\n    // Example test case (Node.js with Jest)\n    it('should authenticate a user with valid credentials', async () => {\n      // Test implementation\n    });\n    ```\n\n3.  **Attempt to Commit:** Try to commit the changes, which will trigger the Git hooks and run the tests.\n    ```bash\n    git commit -m \"feat: implement user authentication\"\n    ```\n\n### Phase 3: Autonomous Development and Debugging\n\nTo allow Claude to work autonomously:\n\n1.  **Git Hooks Enforcement:** The Git hooks will block the commit if any tests fail, providing Claude with the error messages.\n    ```text\n    Overnight Dev: Running pre-commit checks...\n    Running linting...\n    Linting passed\n    Running tests...\n    12 tests failing\n    Commit blocked!\n    ```\n\n2.  **Automated Debugging:** Claude analyzes the error messages, identifies the issues, and attempts to fix the code.\n    ```text\n    Claude: Fixing test failures in user authentication module.\n    ```\n\n3.  **Retry Commits:** Claude retries the commit after making the necessary fixes, repeating the process until all tests pass.\n    ```bash\n    git commit -m \"fix: address test failures in user authentication\"\n    ```\n\n### Phase 4: Progress Tracking and Completion\n\nTo monitor the progress and finalize the session:\n\n1.  **Monitor Progress:** Track the progress of the overnight session by viewing the log file.\n    ```bash\n    cat .overnight-dev-log.txt\n    ```\n\n2.  **Review Results:** Wake up to fully tested code, complete features, and a clean Git history.\n    ```text\n    7 AM: You wake up to:\n    - 47 passing tests (0 failing)\n    - 94% test coverage\n    - Clean conventional commit history\n    - Fully documented JWT authentication\n    - Production-ready code\n    ```\n\n3.  **Session Completion:** The session completes when all tests pass, the code meets the specified quality standards, and the changes are committed.\n\n## Resources\n\n### Scripts\n\nTo automate the setup process, use the `overnight-setup.sh` script:\n\n```bash\n./scripts/overnight-setup.sh\n```\n\nTo track the progress of the overnight session, use the `progress-tracker.py` script:\n\n```bash\n./scripts/progress-tracker.py --log .overnight-dev-log.txt\n```\n\n### References\n\nFor detailed configuration options, load:\n\n-   [Configuration Reference](./references/configuration_reference.md)\n\nFor best practices on writing effective tests, load:\n\n-   [Testing Best Practices](./references/testing_best_practices.md)\n\n### Assets\n\nAvailable templates:\n\n-   `assets/commit-template.txt` - Template for generating commit messages.\n-   `assets/readme-template.md` - Template for generating README files.\n\n## Examples\n\n### Example 1: Building JWT Authentication\n\nUser request: \"Implement JWT authentication with 90% test coverage overnight.\"\n\nWorkflow:\n\n1.  Claude writes failing authentication tests (TDD).\n2.  Claude implements JWT signing (tests still failing).\n3.  Claude debugs token generation (commit blocked, keeps trying).\n4.  Tests pass! Commit succeeds.\n5.  Claude adds middleware (writes tests first).\n6.  Integration tests (debugging edge cases).\n7.  All tests green (Coverage: 94%).\n8.  Claude adds docs, refactors, still green.\n9.  Session complete.\n\n### Example 2: Refactoring Database Layer\n\nUser request: \"Refactor the database layer to use the repository pattern overnight.\"\n\nWorkflow:\n\n1.  Claude analyzes existing tests to ensure no regression.\n2.  Claude implements the repository pattern.\n3.  Tests are run; some fail due to changes in data access.\n4.  Claude updates tests to align with the new repository pattern.\n5.  All tests pass; commit succeeds.\n6.  Claude documents the refactored database layer.\n7.  Session complete.\n\n### Example 3: Fixing a Bug in Payment Processing\n\nUser request: \"Fix the bug in payment processing that causes incorrect amounts to be charged overnight.\"\n\nWorkflow:\n\n1.  Claude reproduces the bug and writes a failing test case.\n2.  Claude analyzes the code and identifies the root cause of the bug.\n3.  Claude fixes the bug and runs the tests.\n4.  The failing test case now passes; all other tests also pass.\n5.  Commit succeeds.\n6.  Claude adds a comment to the code explaining the fix.\n7.  Session complete.\n\n## Best Practices\n\n-   Ensure that the task is well-defined and testable.\n-   Follow the TDD approach by writing tests before implementing features.\n-   Monitor the progress of the overnight session by viewing the log file.\n-   Configure the Git hooks and settings appropriately for the project.\n-   Use the `overnight-dev-coach` agent for guidance and support.\n\n## Troubleshooting\n\n**Issue:** Hooks are not running.\n\n**Solution:** Make sure the hooks are executable:\n\n```bash\nchmod +x .git/hooks/pre-commit\nchmod +x .git/hooks/commit-msg\n```\n\n**Issue:** Tests are failing immediately.\n\n**Solution:** Ensure you have at least one passing test:\n\n```bash\nnpm test # Should see: Tests passed\n```\n\n**Issue:** Lint errors are blocking everything.\n\n**Solution:** Enable auto-fix:\n\n```json\n{\n  \"autoFix\": true\n}\n```\n\nOr fix manually:\n\n```bash\nnpm run lint -- --fix\n```\n\n## Integration\n\nThis skill integrates with Git repositories and various testing frameworks. It uses Git hooks to enforce TDD and ensure that all code changes are fully tested. The `overnight-dev-coach` agent provides guidance and support throughout the process.\n\n```bash\n# Example integration with Jest (Node.js)\n{\n  \"testCommand\": \"npm test -- --coverage --watchAll=false\",\n  \"lintCommand\": \"npm run lint\",\n  \"autoFix\": true\n}\n```\n\n```bash\n# Example integration with pytest (Python)\n{\n  \"testCommand\": \"pytest --cov=. --cov-report=term-missing\",\n  \"lintCommand\": \"flake8 . && black --check .\",\n  \"autoFix\": false\n}\n```\n```\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nIf execution fails:\n- Verify prerequisites are met\n- Check input parameters and formats\n- Validate file paths and permissions\n- Review error messages for root cause\n- Consult documentation for troubleshooting\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "overnight-dev",
        "category": "productivity",
        "path": "plugins/productivity/overnight-dev",
        "version": "1.0.0",
        "description": "Run Claude autonomously for 6-8 hours overnight using Git hooks that enforce TDD - wake up to fully tested features"
      },
      "filePath": "plugins/productivity/overnight-dev/skills/overnight-development/SKILL.md"
    },
    {
      "slug": "performing-penetration-testing",
      "name": "performing-penetration-testing",
      "description": "Perform security penetration testing to identify vulnerabilities. Use when conducting security assessments. Trigger with 'run pentest', 'security testing', or 'find vulnerabilities'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Penetration Tester\n\nThis skill provides automated assistance for penetration tester tasks.\n\n## Overview\n\nThis skill automates the process of penetration testing for web applications, identifying vulnerabilities and suggesting exploitation techniques. It leverages the penetration-tester plugin to assess web application security posture.\n\n## How It Works\n\n1. **Target Identification**: Analyzes the user's request to identify the target web application or API endpoint.\n2. **Vulnerability Scanning**: Executes automated scans to discover potential vulnerabilities, covering OWASP Top 10 risks.\n3. **Reporting**: Generates a detailed penetration test report, including identified vulnerabilities, risk ratings, and remediation recommendations.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform a penetration test on a web application.\n- Identify vulnerabilities in a web application or API.\n- Assess the security posture of a web application.\n- Generate a report detailing security flaws and remediation steps.\n\n## Examples\n\n### Example 1: Performing a Full Penetration Test\n\nUser request: \"Run a penetration test on example.com\"\n\nThe skill will:\n1. Initiate a comprehensive penetration test on the specified domain.\n2. Generate a detailed report outlining identified vulnerabilities, including SQL injection, XSS, and CSRF.\n\n### Example 2: Assessing API Security\n\nUser request: \"Perform vulnerability assessment on the /api/users endpoint\"\n\nThe skill will:\n1. Target the specified API endpoint for vulnerability scanning.\n2. Identify potential security flaws in the API, such as authentication bypass or authorization issues, and provide remediation advice.\n\n## Best Practices\n\n- **Authorization**: Always ensure you have explicit authorization before performing penetration testing on any system.\n- **Scope Definition**: Clearly define the scope of the penetration test to avoid unintended consequences.\n- **Safe Exploitation**: Use exploitation techniques carefully to demonstrate vulnerabilities without causing damage.\n\n## Integration\n\nThis skill can be integrated with other security tools and plugins to enhance vulnerability management and remediation efforts. For example, findings can be exported to vulnerability tracking systems.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "penetration-tester",
        "category": "security",
        "path": "plugins/security/penetration-tester",
        "version": "1.0.0",
        "description": "Automated penetration testing for web applications with OWASP Top 10 coverage"
      },
      "filePath": "plugins/security/penetration-tester/skills/performing-penetration-testing/SKILL.md"
    },
    {
      "slug": "performing-regression-analysis",
      "name": "performing-regression-analysis",
      "description": "This skill empowers AI assistant to perform regression analysis and modeling using the regression-analysis-tool plugin. it analyzes datasets, generates appropriate regression models (linear, polynomial, etc.), validates the models, and provides performa... Use when analyzing code or data. Trigger with phrases like 'analyze', 'review', or 'examine'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Regression Analysis Tool\n\nThis skill provides automated assistance for regression analysis tool tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for regression analysis tool tasks.\nThis skill enables Claude to analyze data, build regression models, and provide insights into the relationships between variables. It leverages the regression-analysis-tool plugin to automate the process and ensure best practices are followed.\n\n## How It Works\n\n1. **Data Analysis**: Claude analyzes the provided data to understand its structure and identify potential relationships between variables.\n2. **Model Generation**: Based on the data, Claude generates appropriate regression models (e.g., linear, polynomial).\n3. **Model Validation**: Claude validates the generated models to ensure their accuracy and reliability.\n4. **Performance Reporting**: Claude provides performance metrics and insights into the model's effectiveness.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform regression analysis on a given dataset.\n- Predict future values based on existing data using regression models.\n- Understand the relationship between independent and dependent variables.\n- Evaluate the performance of a regression model.\n\n## Examples\n\n### Example 1: Predicting House Prices\n\nUser request: \"Can you build a regression model to predict house prices based on square footage and number of bedrooms?\"\n\nThe skill will:\n1. Analyze the provided data on house prices, square footage, and number of bedrooms.\n2. Generate a regression model (likely multiple to compare) to predict house prices.\n3. Provide performance metrics such as R-squared and RMSE.\n\n### Example 2: Analyzing Sales Trends\n\nUser request: \"I need to analyze the sales data for the past year and identify any trends using regression analysis.\"\n\nThe skill will:\n1. Analyze the provided sales data.\n2. Generate a regression model to identify trends and patterns in the sales data.\n3. Visualize the trend and report the equation and R-squared value.\n\n## Best Practices\n\n- **Data Preparation**: Ensure the data is clean and preprocessed before performing regression analysis.\n- **Model Selection**: Choose the appropriate regression model based on the data and the problem.\n- **Validation**: Always validate the model to ensure its accuracy and reliability.\n\n## Integration\n\nThis skill works independently using the regression-analysis-tool plugin. It can be used in conjunction with other data analysis and visualization tools to provide a comprehensive understanding of the data.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "regression-analysis-tool",
        "category": "ai-ml",
        "path": "plugins/ai-ml/regression-analysis-tool",
        "version": "1.0.0",
        "description": "Regression analysis and modeling"
      },
      "filePath": "plugins/ai-ml/regression-analysis-tool/skills/performing-regression-analysis/SKILL.md"
    },
    {
      "slug": "performing-security-audits",
      "name": "performing-security-audits",
      "description": "Analyze code, infrastructure, and configurations by conducting comprehensive security audits. It leverages tools within the security-pro-pack plugin, including vulnerability scanning, compliance checking, and cryptography review. Use when assessing security or running audits. Trigger with phrases like 'security scan', 'audit', or 'vulnerability'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Security Pro Pack\n\nThis skill provides automated assistance for security pro pack tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for security pro pack tasks.\nThis skill empowers Claude to perform in-depth security audits across various domains, from code vulnerability scanning to compliance verification and infrastructure security assessment. It utilizes the specialized tools within the security-pro-pack to provide a comprehensive security posture analysis.\n\n## How It Works\n\n1. **Analysis Selection**: Claude determines the appropriate security-pro-pack tool (e.g., `Security Auditor Expert`, `Compliance Checker`, `Crypto Audit`) based on the user's request and the context of the code or system being analyzed.\n2. **Execution**: Claude executes the selected tool, providing it with the relevant code, configuration files, or API endpoints.\n3. **Reporting**: Claude aggregates and presents the findings in a clear, actionable report, highlighting vulnerabilities, compliance issues, and potential security risks, along with suggested remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess the security of code for vulnerabilities like those in the OWASP Top 10.\n- Evaluate compliance with standards such as HIPAA, PCI DSS, GDPR, or SOC 2.\n- Review cryptographic implementations for weaknesses.\n- Perform container security scans or API security audits.\n\n## Examples\n\n### Example 1: Vulnerability Assessment\n\nUser request: \"Please perform a security audit on this authentication code to find any potential vulnerabilities.\"\n\nThe skill will:\n1. Invoke the `Security Auditor Expert` agent.\n2. Analyze the provided authentication code for common vulnerabilities.\n3. Generate a report detailing any identified vulnerabilities, their severity, and recommended fixes.\n\n### Example 2: Compliance Check\n\nUser request: \"Check this application against GDPR compliance requirements.\"\n\nThe skill will:\n1. Invoke the `Compliance Checker` agent.\n2. Evaluate the application's architecture and code against GDPR guidelines.\n3. Generate a report highlighting any non-compliant areas and suggesting necessary changes.\n\n## Best Practices\n\n- **Specificity**: Provide clear and specific instructions about the scope of the audit (e.g., \"audit this specific function\" instead of \"audit the whole codebase\").\n- **Context**: Include relevant context about the application, infrastructure, or data being audited to enable more accurate and relevant results.\n- **Iteration**: Use the skill iteratively, addressing the most critical findings first and then progressively improving the overall security posture.\n\n## Integration\n\nThis skill seamlessly integrates with all other components of the security-pro-pack plugin. It also works well with Claude's existing code analysis capabilities, allowing for a holistic and integrated security review process.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "security-pro-pack",
        "category": "packages",
        "path": "plugins/packages/security-pro-pack",
        "version": "1.0.0",
        "description": "Professional security tools for Claude Code: vulnerability scanning, compliance, cryptography audit, container & API security"
      },
      "filePath": "plugins/packages/security-pro-pack/skills/performing-security-audits/SKILL.md"
    },
    {
      "slug": "performing-security-code-review",
      "name": "performing-security-code-review",
      "description": "This skill enables AI assistant to conduct a security-focused code review using the security-agent plugin. it analyzes code for potential vulnerabilities like sql injection, xss, authentication flaws, and insecure dependencies. AI assistant uses this skill wh... Use when assessing security or running audits. Trigger with phrases like 'security scan', 'audit', or 'vulnerability'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore",
      "license": "MIT",
      "content": "# Security Agent\n\nThis skill provides automated assistance for security agent tasks.\n\n## Overview\n\nThis skill empowers Claude to act as a security expert, identifying and explaining potential vulnerabilities within code. It leverages the security-agent plugin to provide detailed security analysis, helping developers improve the security posture of their applications.\n\n## How It Works\n\n1. **Receiving Request**: Claude identifies a user's request for a security review or audit of code.\n2. **Activating Security Agent**: Claude invokes the security-agent plugin to analyze the provided code.\n3. **Generating Security Report**: The security-agent produces a structured report detailing identified vulnerabilities, their severity, affected code locations, and recommended remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Review code for security vulnerabilities.\n- Perform a security audit of a codebase.\n- Identify potential security risks in a software application.\n\n## Examples\n\n### Example 1: Identifying SQL Injection Vulnerability\n\nUser request: \"Please review this database query code for SQL injection vulnerabilities.\"\n\nThe skill will:\n1. Activate the security-agent plugin to analyze the database query code.\n2. Generate a report identifying potential SQL injection vulnerabilities, including the vulnerable code snippet, its severity, and suggested remediation, such as using parameterized queries.\n\n### Example 2: Checking for Insecure Dependencies\n\nUser request: \"Can you check this project's dependencies for known security vulnerabilities?\"\n\nThe skill will:\n1. Utilize the security-agent plugin to scan the project's dependencies against known vulnerability databases.\n2. Produce a report listing any vulnerable dependencies, their Common Vulnerabilities and Exposures (CVE) identifiers, and recommendations for updating to secure versions.\n\n## Best Practices\n\n- **Specificity**: Provide the exact code or project you want reviewed.\n- **Context**: Clearly state the security concerns you have regarding the code.\n- **Iteration**: Use the findings to address vulnerabilities and request further reviews.\n\n## Integration\n\nThis skill integrates with Claude's code understanding capabilities and leverages the security-agent plugin to provide specialized security analysis. It can be used in conjunction with other code analysis tools to provide a comprehensive assessment of code quality and security.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "security-agent",
        "category": "examples",
        "path": "plugins/examples/security-agent",
        "version": "1.0.0",
        "description": "Specialized security review subagent"
      },
      "filePath": "plugins/examples/security-agent/skills/performing-security-code-review/SKILL.md"
    },
    {
      "slug": "performing-security-testing",
      "name": "performing-security-testing",
      "description": "Automate security vulnerability testing covering OWASP Top 10, SQL injection, XSS, CSRF, and authentication issues. Use when performing security assessments, penetration tests, or vulnerability scans. Trigger with phrases like \"scan for vulnerabilities\", \"test security\", or \"run penetration test\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:security-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Security Test Scanner\n\nThis skill provides automated assistance for security test scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Target application or API endpoint URLs accessible for testing\n- Authentication credentials if testing protected resources\n- Appropriate authorization to perform security testing on the target system\n- Test environment configured (avoid production without explicit approval)\n- Security testing tools installed (OWASP ZAP, sqlmap, or equivalent)\n\n## Instructions\n\n### Step 1: Define Test Scope\nIdentify the security testing parameters:\n- Target URLs and endpoints to scan\n- Authentication requirements and test credentials\n- Specific vulnerability types to focus on (OWASP Top 10, injection, XSS, etc.)\n- Testing depth level (passive scan vs. active exploitation)\n\n### Step 2: Execute Security Scan\nRun automated vulnerability detection:\n1. Use Read tool to analyze application structure and identify entry points\n2. Execute security testing tools via Bash(test:security-*) with proper scope\n3. Monitor scan progress and capture all findings\n4. Document identified vulnerabilities with severity ratings\n\n### Step 3: Analyze Vulnerabilities\nProcess scan results to identify:\n- SQL injection vulnerabilities in database queries\n- Cross-Site Scripting (XSS) in user input fields\n- Cross-Site Request Forgery (CSRF) token weaknesses\n- Authentication and authorization bypass opportunities\n- Security misconfigurations and exposed sensitive data\n\n### Step 4: Generate Security Report\nCreate comprehensive documentation in {baseDir}/security-reports/:\n- Executive summary with risk overview\n- Detailed vulnerability findings with CVSS scores\n- Proof-of-concept exploit examples where applicable\n- Prioritized remediation recommendations\n- Compliance assessment against security standards\n\n## Output\n\nThe skill generates structured security assessment reports:\n\n### Vulnerability Summary\n- Total vulnerabilities discovered by severity (Critical, High, Medium, Low)\n- OWASP Top 10 category mapping for each finding\n- Attack surface analysis showing exposed endpoints\n\n### Detailed Findings\nEach vulnerability includes:\n- Unique identifier and CVSS score\n- Affected URLs, parameters, and HTTP methods\n- Technical description of the security weakness\n- Proof-of-concept demonstration or reproduction steps\n- Potential impact on confidentiality, integrity, and availability\n\n### Remediation Guidance\n- Specific code fixes or configuration changes required\n- Secure coding best practices to prevent recurrence\n- Priority recommendations based on risk and effort\n- Verification testing procedures after remediation\n\n### Compliance Assessment\n- Alignment with OWASP Application Security Verification Standard (ASVS)\n- PCI DSS requirements if applicable to payment processing\n- General Data Protection Regulation (GDPR) security considerations\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Access Denied**\n- Error: HTTP 403 or authentication failures during scan\n- Solution: Verify credentials are valid and have sufficient permissions; use provided test accounts\n\n**Rate Limiting**\n- Error: Too many requests blocked by WAF or rate limiter\n- Solution: Configure scan throttling to reduce request rate; use authenticated sessions to increase limits\n\n**False Positives**\n- Error: Reported vulnerabilities that cannot be exploited\n- Solution: Manually verify each finding; adjust scanner sensitivity; whitelist known safe patterns\n\n**Tool Installation Missing**\n- Error: Security testing tools not found on system\n- Solution: Install required tools using Bash(test:security-install) with package manager\n\n## Resources\n\n### Security Testing Tools\n- OWASP ZAP for automated vulnerability scanning\n- Burp Suite for manual penetration testing\n- sqlmap for SQL injection detection and exploitation\n- Nikto for web server vulnerability scanning\n\n### Vulnerability Databases\n- Common Vulnerabilities and Exposures (CVE) database\n- National Vulnerability Database (NVD) for CVSS scoring\n- OWASP Top 10 documentation and remediation guides\n\n### Secure Coding Guidelines\n- OWASP Secure Coding Practices checklist\n- CWE (Common Weakness Enumeration) catalog\n- SANS Top 25 Most Dangerous Software Errors\n\n### Best Practices\n- Always test in non-production environments first\n- Obtain written authorization before security testing\n- Document all testing activities for audit trails\n- Validate remediation effectiveness with regression testing\n\n## Overview\n\n\nThis skill provides automated assistance for security test scanner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "security-test-scanner",
        "category": "testing",
        "path": "plugins/testing/security-test-scanner",
        "version": "1.0.0",
        "description": "Automated security vulnerability testing covering OWASP Top 10, SQL injection, XSS, CSRF, and authentication issues"
      },
      "filePath": "plugins/testing/security-test-scanner/skills/performing-security-testing/SKILL.md"
    },
    {
      "slug": "planning-disaster-recovery",
      "name": "planning-disaster-recovery",
      "description": "Use when you need to work with backup and recovery. This skill provides backup automation and disaster recovery with comprehensive guidance and automation. Trigger with phrases like \"create backups\", \"automate backups\", or \"implement disaster recovery\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(tar:*), Bash(rsync:*), Bash(aws:s3:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Disaster Recovery Planner\n\nThis skill provides automated assistance for disaster recovery planner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/disaster-recovery-planner/`\n\n**Documentation and Guides**: `{baseDir}/docs/disaster-recovery-planner/`\n\n**Example Scripts and Code**: `{baseDir}/examples/disaster-recovery-planner/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/disaster-recovery-planner-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/disaster-recovery-planner-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/disaster-recovery-planner-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "disaster-recovery-planner",
        "category": "devops",
        "path": "plugins/devops/disaster-recovery-planner",
        "version": "1.0.0",
        "description": "Plan and implement disaster recovery procedures"
      },
      "filePath": "plugins/devops/disaster-recovery-planner/skills/planning-disaster-recovery/SKILL.md"
    },
    {
      "slug": "plugin-auditor",
      "name": "plugin-auditor",
      "description": "Automatically audits AI assistant code plugins for security vulnerabilities, best practices, AI assistant.md compliance, and quality standards when user mentions audit plugin, security review, or best practices check. specific to AI assistant-code-plugins repositor... Use when assessing security or running audits. Trigger with phrases like 'security scan', 'audit', or 'vulnerability'. allowed-tools: Read, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Plugin Auditor\n\n\n\nThis skill provides automated assistance for plugin auditor tasks.\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## Purpose\nAutomatically audits Claude Code plugins for security vulnerabilities, best practice violations, CLAUDE.md compliance, and quality standards - optimized for claude-code-plugins repository requirements.\n\n## Trigger Keywords\n- \"audit plugin\"\n- \"security review\" or \"security audit\"\n- \"best practices check\"\n- \"plugin quality\"\n- \"compliance check\"\n- \"plugin security\"\n\n## Audit Categories\n\n### 1. Security Audit\n\n**Critical Checks:**\n- ‚ùå No hardcoded secrets (passwords, API keys, tokens)\n- ‚ùå No AWS keys (AKIA...)\n- ‚ùå No private keys (BEGIN PRIVATE KEY)\n- ‚ùå No dangerous commands (rm -rf /, eval(), exec())\n- ‚ùå No command injection vectors\n- ‚ùå No suspicious URLs (IP addresses, non-HTTPS)\n- ‚ùå No obfuscated code (base64 decode, hex encoding)\n\n**Security Patterns:**\n```bash\n# Check for hardcoded secrets\ngrep -r \"password\\s*=\\s*['\\\"]\" --exclude-dir=node_modules\ngrep -r \"api_key\\s*=\\s*['\\\"]\" --exclude-dir=node_modules\ngrep -r \"secret\\s*=\\s*['\\\"]\" --exclude-dir=node_modules\n\n# Check for AWS keys\ngrep -r \"AKIA[0-9A-Z]{16}\" --exclude=README.md\n\n# Check for private keys\ngrep -r \"BEGIN.*PRIVATE KEY\" --exclude=README.md\n\n# Check for dangerous patterns\ngrep -r \"rm -rf /\" | grep -v \"/var/\" | grep -v \"{baseDir}/tmp/\"\ngrep -r \"eval\\s*\\(\" --exclude=README.md\n```\n\n### 2. Best Practices Audit\n\n**Plugin Structure:**\n- ‚úÖ Proper directory hierarchy\n- ‚úÖ Required files present\n- ‚úÖ Semantic versioning (x.y.z)\n- ‚úÖ Clear, concise descriptions\n- ‚úÖ Proper LICENSE file (MIT/Apache-2.0)\n- ‚úÖ Comprehensive README\n- ‚úÖ At least 5 keywords\n\n**Code Quality:**\n- ‚úÖ No TODO/FIXME without issue links\n- ‚úÖ No console.log() in production code\n- ‚úÖ No hardcoded paths (/home/, /Users/)\n- ‚úÖ Uses `${CLAUDE_PLUGIN_ROOT}` in hooks\n- ‚úÖ Scripts have proper shebangs\n- ‚úÖ All scripts are executable\n\n**Documentation:**\n- ‚úÖ README has installation section\n- ‚úÖ README has usage examples\n- ‚úÖ README has clear description\n- ‚úÖ Commands have proper frontmatter\n- ‚úÖ Agents have model specified\n- ‚úÖ Skills have trigger keywords\n\n### 3. CLAUDE.md Compliance\n\n**Repository Standards:**\n- ‚úÖ Follows plugin structure from CLAUDE.md\n- ‚úÖ Uses correct marketplace slug\n- ‚úÖ Proper category assignment\n- ‚úÖ Valid plugin.json schema\n- ‚úÖ Marketplace catalog entry exists\n- ‚úÖ Version consistency\n\n**Skills Compliance (if applicable):**\n- ‚úÖ SKILL.md has proper frontmatter\n- ‚úÖ Description includes trigger keywords\n- ‚úÖ allowed-tools specified (if restricted)\n- ‚úÖ Clear purpose and instructions\n- ‚úÖ Examples provided\n\n### 4. Marketplace Compliance\n\n**Catalog Requirements:**\n- ‚úÖ Plugin listed in marketplace.extended.json\n- ‚úÖ Source path matches actual location\n- ‚úÖ Version matches plugin.json\n- ‚úÖ Category is valid\n- ‚úÖ No duplicate plugin names\n- ‚úÖ Author information complete\n\n### 5. Git Hygiene\n\n**Repository Practices:**\n- ‚úÖ No large binary files\n- ‚úÖ No node_modules/ committed\n- ‚úÖ No .env files\n- ‚úÖ Proper .gitignore\n- ‚úÖ No merge conflicts\n- ‚úÖ Clean commit history\n\n### 6. MCP Plugin Audit (if applicable)\n\n**MCP-Specific Checks:**\n- ‚úÖ Valid package.json with @modelcontextprotocol/sdk\n- ‚úÖ TypeScript configured correctly\n- ‚úÖ dist/ in .gitignore\n- ‚úÖ Proper mcp/*.json configuration\n- ‚úÖ Build scripts present\n- ‚úÖ No dependency vulnerabilities\n\n### 7. Performance Audit\n\n**Efficiency Checks:**\n- ‚úÖ No unnecessary file reads\n- ‚úÖ Efficient glob patterns\n- ‚úÖ No recursive loops\n- ‚úÖ Reasonable timeout values\n- ‚úÖ No memory leaks (event listeners)\n\n### 8. Accessibility & UX\n\n**User Experience:**\n- ‚úÖ Clear error messages\n- ‚úÖ Helpful command descriptions\n- ‚úÖ Proper usage examples\n- ‚úÖ Good README formatting\n- ‚úÖ Working demo commands\n\n## Audit Process\n\nWhen activated, I will:\n\n1. **Security Scan**\n   ```bash\n   # Run security checks\n   grep -r \"password\\|secret\\|api_key\" plugins/plugin-name/\n   grep -r \"AKIA[0-9A-Z]{16}\" plugins/plugin-name/\n   grep -r \"BEGIN.*PRIVATE KEY\" plugins/plugin-name/\n   grep -r \"rm -rf /\" plugins/plugin-name/\n   grep -r \"eval\\(\" plugins/plugin-name/\n   ```\n\n2. **Structure Validation**\n   ```bash\n   # Check required files\n   test -f .claude-plugin/plugin.json\n   test -f README.md\n   test -f LICENSE\n\n   # Check component directories\n   ls -d commands/ agents/ skills/ hooks/ mcp/ 2>/dev/null\n   ```\n\n3. **Best Practices Check**\n   ```bash\n   # Check for TODO/FIXME\n   grep -r \"TODO\\|FIXME\" --exclude=README.md\n\n   # Check for console.log\n   grep -r \"console\\.log\" --exclude=README.md\n\n   # Check script permissions\n   find . -name \"*.sh\" ! -perm -u+x\n   ```\n\n4. **Compliance Verification**\n   ```bash\n   # Check marketplace entry\n   jq '.plugins[] | select(.name == \"plugin-name\")' .claude-plugin/marketplace.extended.json\n\n   # Verify version consistency\n   plugin_version=$(jq -r '.version' .claude-plugin/plugin.json)\n   market_version=$(jq -r '.plugins[] | select(.name == \"plugin-name\") | .version' .claude-plugin/marketplace.extended.json)\n   ```\n\n5. **Generate Audit Report**\n\n## Audit Report Format\n\n```\nüîç PLUGIN AUDIT REPORT\nPlugin: plugin-name\nVersion: 1.0.0\nCategory: security\nAudit Date: 2025-10-16\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüîí SECURITY AUDIT\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n‚úÖ PASSED (7/7)\n- No hardcoded secrets\n- No AWS keys\n- No private keys\n- No dangerous commands\n- No command injection vectors\n- HTTPS URLs only\n- No obfuscated code\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüìã BEST PRACTICES\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n‚úÖ PASSED (10/12)\n- Proper directory structure\n- Required files present\n- Semantic versioning\n- Clear descriptions\n- Comprehensive README\n\n‚ö†Ô∏è  WARNINGS (2)\n- 3 scripts missing execute permission\n  Fix: chmod +x scripts/*.sh\n\n- 2 TODO items without issue links\n  Location: commands/scan.md:45, agents/analyzer.md:67\n  Recommendation: Create GitHub issues or remove TODOs\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚úÖ CLAUDE.MD COMPLIANCE\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n‚úÖ PASSED (6/6)\n- Follows plugin structure\n- Uses correct marketplace slug\n- Proper category assignment\n- Valid plugin.json schema\n- Marketplace entry exists\n- Version consistency\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüìä QUALITY SCORE\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nSecurity:        10/10 ‚úÖ\nBest Practices:   8/10 ‚ö†Ô∏è\nCompliance:      10/10 ‚úÖ\nDocumentation:   10/10 ‚úÖ\n\nOVERALL SCORE: 9.5/10 (EXCELLENT)\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüéØ RECOMMENDATIONS\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nPriority: MEDIUM\n1. Fix script permissions (2 min)\n2. Resolve TODO items (10 min)\n\nOptional Improvements:\n- Add more usage examples in README\n- Include troubleshooting section\n- Add GIF/video demo\n\n‚úÖ AUDIT COMPLETE\nPlugin is production-ready with minor improvements needed.\n```\n\n## Severity Levels\n\n**Critical (üî¥):**\n- Security vulnerabilities\n- Hardcoded secrets\n- Dangerous commands\n- Missing required files\n\n**High (üü†):**\n- Best practice violations\n- Missing documentation\n- Broken functionality\n- Schema violations\n\n**Medium (üü°):**\n- Code quality issues\n- Missing optional features\n- Performance concerns\n- UX improvements\n\n**Low (üü¢):**\n- Style inconsistencies\n- Minor documentation gaps\n- Nice-to-have features\n\n## Auto-Fix Capabilities\n\nI can automatically fix:\n- ‚úÖ Script permissions\n- ‚úÖ JSON formatting\n- ‚úÖ Markdown formatting\n- ‚úÖ Version sync issues\n\n## Repository-Specific Checks\n\n**For claude-code-plugins repo:**\n- Validates against CLAUDE.md standards\n- Checks marketplace integration\n- Verifies category structure\n- Ensures quality for featured plugins\n- Checks contributor guidelines compliance\n\n## Examples\n\n**User says:** \"Audit the security-scanner plugin\"\n\n**I automatically:**\n1. Run full security scan\n2. Check best practices\n3. Verify CLAUDE.md compliance\n4. Generate comprehensive report\n5. Provide recommendations\n\n**User says:** \"Is this plugin safe to publish?\"\n\n**I automatically:**\n1. Security audit (critical)\n2. Marketplace compliance\n3. Quality score calculation\n4. Publish readiness assessment\n\n**User says:** \"Quality review before featured status\"\n\n**I automatically:**\n1. Full audit (all categories)\n2. Higher quality thresholds\n3. Featured plugin requirements\n4. Recommendation: approve/reject",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/plugin-auditor/SKILL.md"
    },
    {
      "slug": "plugin-creator",
      "name": "plugin-creator",
      "description": "Automatically creates new AI assistant code plugins with proper structure, validation, and marketplace integration when user mentions creating a plugin, new plugin, or plugin from template. specific to AI assistant-code-plugins repository workflow. Use when generating or creating new content. Trigger with phrases like 'generate', 'create', or 'scaffold'. allowed-tools: Write, Read, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Plugin Creator\n\n\n\nThis skill provides automated assistance for plugin creator tasks.\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Confirm the desired plugin `name` (kebab-case), category, and what the plugin contains (commands/agents/skills/hooks/mcp).\n2. Scaffold the plugin directory under `plugins/<category>/<plugin-name>/` with required files and minimal working examples.\n3. Populate `.claude-plugin/plugin.json` and the plugin `README.md` with accurate metadata and usage.\n4. Add the plugin entry to `.claude-plugin/marketplace.extended.json` and regenerate the catalog (e.g., `npm run sync-marketplace`).\n5. Run repo validators (frontmatter/schema, catalogs, basic lint/test if applicable) and fix failures.\n6. Report a concise summary with paths created/changed and next steps.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## Purpose\nAutomatically scaffolds new Claude Code plugins with complete directory structure, required files, proper formatting, and marketplace catalog integration - specifically optimized for the claude-code-plugins repository.\n\n## Trigger Keywords\n- \"create plugin\" or \"new plugin\"\n- \"plugin from template\"\n- \"scaffold plugin\"\n- \"generate plugin\"\n- \"add new plugin to marketplace\"\n\n## Plugin Creation Process\n\nWhen activated, I will:\n\n1. **Gather Requirements**\n   - Plugin name (kebab-case)\n   - Category (productivity, security, devops, etc.)\n   - Type (commands, agents, skills, MCP, or combination)\n   - Description and keywords\n   - Author information\n\n2. **Create Directory Structure**\n   ```\n   plugins/[category]/[plugin-name]/\n   ‚îú‚îÄ‚îÄ .claude-plugin/\n   ‚îÇ   ‚îî‚îÄ‚îÄ plugin.json\n   ‚îú‚îÄ‚îÄ README.md\n   ‚îú‚îÄ‚îÄ LICENSE\n   ‚îî‚îÄ‚îÄ [commands|agents|skills|hooks|mcp]/\n   ```\n\n3. **Generate Required Files**\n   - **plugin.json** with proper schema (name, version, description, author)\n   - **README.md** with comprehensive documentation\n   - **LICENSE** (MIT by default)\n   - Component files based on type\n\n4. **Add to Marketplace Catalog**\n   - Update `.claude-plugin/marketplace.extended.json`\n   - Run `npm run sync-marketplace` automatically\n   - Validate catalog schema\n\n5. **Validate Everything**\n   - Run `./scripts/validate-all.sh` on new plugin\n   - Check JSON syntax with `jq`\n   - Verify frontmatter in markdown files\n   - Ensure scripts are executable\n\n## Plugin Types Supported\n\n### Commands Plugin\n- Creates `commands/` directory\n- Generates example command with proper frontmatter\n- Includes `/demo-command` example\n\n### Agents Plugin\n- Creates `agents/` directory\n- Generates example agent with capabilities\n- Includes model specification\n\n### Skills Plugin\n- Creates `skills/skill-name/` directory\n- Generates SKILL.md with proper format\n- Includes trigger keywords and allowed-tools\n\n### MCP Plugin\n- Creates `src/`, `dist/`, `mcp/` directories\n- Generates TypeScript boilerplate\n- Includes package.json with MCP SDK\n- Adds to pnpm workspace\n\n### Full Plugin\n- Combines all types\n- Creates complete example structure\n- Ready for customization\n\n## File Templates\n\n### plugin.json Template\n```json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Clear description\",\n  \"author\": {\n    \"name\": \"Author Name\",\n    \"email\": \"[email protected]\"\n  },\n  \"repository\": \"https://github.com/jeremylongshore/claude-code-plugins\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"keyword1\", \"keyword2\"]\n}\n```\n\n### Command Template\n```markdown\n---\nname: command-name\ndescription: What this command does\nmodel: sonnet\n---\n\n# Command Title\n\nInstructions for Claude...\n```\n\n### Skill Template\n```markdown\n---\nname: Skill Name\ndescription: What it does AND when to use it\nallowed-tools: Read, Write, Grep\n---\n\n# Skill Name\n\n## Purpose\n[What this skill does]\n\n## Trigger Keywords\n- keyword1\n- keyword2\n\n## Instructions\n[Step-by-step for Claude]\n```\n\n## Marketplace Integration\n\nI automatically:\n1. Add plugin entry to `marketplace.extended.json`\n2. Run `npm run sync-marketplace` to update CLI catalog\n3. Validate both catalogs with `jq`\n4. Check for duplicate names\n5. Verify source paths exist\n\n## Validation Steps\n\nAfter creation:\n- ‚úÖ All required files present\n- ‚úÖ Valid JSON (plugin.json, catalogs)\n- ‚úÖ Proper frontmatter in markdown\n- ‚úÖ Scripts executable (`chmod +x`)\n- ‚úÖ No duplicate plugin names\n- ‚úÖ Category is valid\n- ‚úÖ Keywords present\n\n## Repository-Specific Features\n\n**For claude-code-plugins repo:**\n- Follows exact directory structure\n- Uses correct marketplace slug (`claude-code-plugins-plus`)\n- Includes proper LICENSE file\n- Adds to correct category folder\n- Validates against existing plugins\n- Updates version in marketplace\n\n## Output\n\nI provide a deterministic scaffold summary (what was created/changed) plus the exact follow-up commands to run to validate and commit the new plugin.\n```\n‚úÖ Created plugin: plugin-name\nüìÅ Location: plugins/category/plugin-name/\nüìù Files created: 8\nüîç Validation: PASSED\nüì¶ Marketplace: UPDATED\n‚ú® Ready to commit!\n\nNext steps:\n1. Review files in plugins/category/plugin-name/\n2. Customize README.md and component files\n3. Run: git add plugins/category/plugin-name/\n4. Run: git commit -m \"feat: Add plugin-name plugin\"\n```\n\n## Examples\n\n**User says:** \"Create a new security plugin called 'owasp-scanner' with commands\"\n\n**I automatically:**\n1. Create directory: `plugins/security/owasp-scanner/`\n2. Generate plugin.json, README, LICENSE\n3. Create `commands/` with example\n4. Add to marketplace.extended.json\n5. Sync marketplace.json\n6. Validate all files\n7. Report success\n\n**User says:** \"Scaffold a Skills plugin for code review\"\n\n**I automatically:**\n1. Create directory with `skills/` subdirectories\n2. Generate SKILL.md templates\n3. Add trigger keywords for code review\n4. Add to marketplace\n5. Validate and report",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/plugin-creator/SKILL.md"
    },
    {
      "slug": "plugin-validator",
      "name": "plugin-validator",
      "description": "Automatically validates AI assistant code plugin structure, schemas, and compliance when user mentions validate plugin, check plugin, or plugin errors. runs comprehensive validation specific to AI assistant-code-plugins repository standards. Use when validating configurations or code. Trigger with phrases like 'validate', 'check', or 'verify'. allowed-tools: Read, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Plugin Validator\n\n\n\nThis skill provides automated assistance for plugin validator tasks.\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## Purpose\nAutomatically validates Claude Code plugins against repository standards, checking structure, JSON schemas, frontmatter, permissions, security, and marketplace compliance - optimized for claude-code-plugins repository.\n\n## Trigger Keywords\n- \"validate plugin\"\n- \"check plugin\"\n- \"plugin validation\"\n- \"plugin errors\"\n- \"lint plugin\"\n- \"verify plugin\"\n\n## Validation Checks\n\n### 1. Required Files\n- ‚úÖ `.claude-plugin/plugin.json` exists\n- ‚úÖ `README.md` exists and not empty\n- ‚úÖ `LICENSE` file exists\n- ‚úÖ At least one component directory (commands/, agents/, skills/, hooks/, mcp/)\n\n### 2. Plugin.json Schema\n```bash\n# Required fields:\n- name (kebab-case, lowercase, hyphens only)\n- version (semantic versioning x.y.z)\n- description (clear, concise)\n- author.name\n- author.email\n- license (MIT, Apache-2.0, etc.)\n- keywords (array, at least 2)\n\n# Optional but recommended:\n- repository (GitHub URL)\n- homepage (docs URL)\n```\n\n### 3. Frontmatter Validation\n**For Commands (commands/*.md):**\n```yaml\n---\nname: command-name\ndescription: Brief description\nmodel: sonnet|opus|haiku\n---\n```\n\n**For Agents (agents/*.md):**\n```yaml\n---\nname: agent-name\ndescription: Agent purpose\nmodel: sonnet|opus|haiku\n---\n```\n\n**For Skills (skills/*/SKILL.md):**\n```yaml\n---\nname: Skill Name\ndescription: What it does AND when to use it\nallowed-tools: Tool1, Tool2, Tool3  # optional\n---\n```\n\n### 4. Directory Structure\nValidates proper hierarchy:\n```\nplugin-name/\n‚îú‚îÄ‚îÄ .claude-plugin/          # Required\n‚îÇ   ‚îî‚îÄ‚îÄ plugin.json          # Required\n‚îú‚îÄ‚îÄ README.md                 # Required\n‚îú‚îÄ‚îÄ LICENSE                   # Required\n‚îú‚îÄ‚îÄ commands/                 # Optional\n‚îÇ   ‚îî‚îÄ‚îÄ *.md\n‚îú‚îÄ‚îÄ agents/                   # Optional\n‚îÇ   ‚îî‚îÄ‚îÄ *.md\n‚îú‚îÄ‚îÄ skills/                   # Optional\n‚îÇ   ‚îî‚îÄ‚îÄ skill-name/\n‚îÇ       ‚îî‚îÄ‚îÄ SKILL.md\n‚îú‚îÄ‚îÄ hooks/                    # Optional\n‚îÇ   ‚îî‚îÄ‚îÄ hooks.json\n‚îî‚îÄ‚îÄ mcp/                      # Optional\n    ‚îî‚îÄ‚îÄ *.json\n```\n\n### 5. Script Permissions\n```bash\n# All .sh files must be executable\nfind . -name \"*.sh\" ! -perm -u+x\n# Should return empty\n```\n\n### 6. JSON Validation\n```bash\n# All JSON must be valid\njq empty plugin.json\njq empty marketplace.extended.json\njq empty hooks/hooks.json\n```\n\n### 7. Security Scans\n- ‚ùå No hardcoded secrets (API keys, tokens, passwords)\n- ‚ùå No AWS keys (AKIA...)\n- ‚ùå No private keys (BEGIN PRIVATE KEY)\n- ‚ùå No dangerous commands (rm -rf /, eval())\n- ‚ùå No suspicious URLs (non-HTTPS, IP addresses)\n\n### 8. Marketplace Compliance\n- ‚úÖ Plugin listed in marketplace.extended.json\n- ‚úÖ Source path matches actual location\n- ‚úÖ Version matches between plugin.json and catalog\n- ‚úÖ Category is valid\n- ‚úÖ No duplicate plugin names\n\n### 9. README Requirements\n- ‚úÖ Has installation instructions\n- ‚úÖ Has usage examples\n- ‚úÖ Has description section\n- ‚úÖ Proper markdown formatting\n- ‚úÖ No broken links\n\n### 10. Path Variables\nFor hooks:\n- ‚úÖ Uses `${CLAUDE_PLUGIN_ROOT}` not absolute paths\n- ‚úÖ No hardcoded /home/ or /Users/ paths\n\n## Validation Process\n\nWhen activated, I will:\n\n1. **Identify Plugin**\n   - Detect plugin directory from context\n   - Or ask user which plugin to validate\n\n2. **Run Comprehensive Checks**\n   ```bash\n   # Structure validation\n   ./scripts/validate-all.sh plugins/category/plugin-name/\n\n   # JSON validation\n   jq empty .claude-plugin/plugin.json\n\n   # Frontmatter check\n   python3 scripts/check-frontmatter.py\n\n   # Permission check\n   find . -name \"*.sh\" ! -perm -u+x\n\n   # Security scan\n   grep -r \"password\\|secret\\|api_key\" | grep -v placeholder\n   ```\n\n3. **Generate Report**\n   - List all issues by severity (critical, high, medium, low)\n   - Provide fix commands for each issue\n   - Summary: PASSED or FAILED\n\n## Validation Report Format\n\n```\nüîç PLUGIN VALIDATION REPORT\nPlugin: plugin-name\nLocation: plugins/category/plugin-name/\n\n‚úÖ PASSED CHECKS (8/10)\n- Required files present\n- Valid plugin.json schema\n- Proper frontmatter format\n- Directory structure correct\n- No security issues\n- Marketplace compliance\n- README complete\n- JSON valid\n\n‚ùå FAILED CHECKS (2/10)\n- Script permissions: 3 .sh files not executable\n  Fix: chmod +x scripts/*.sh\n\n- Marketplace version mismatch\n  plugin.json: v1.2.0\n  marketplace.extended.json: v1.1.0\n  Fix: Update marketplace.extended.json to v1.2.0\n\n‚ö†Ô∏è  WARNINGS (1)\n- README missing usage examples\n  Recommendation: Add ## Usage section with examples\n\nOVERALL: FAILED (2 critical issues)\nFix issues above before committing.\n```\n\n## Auto-Fix Capabilities\n\nI can automatically fix:\n- ‚úÖ Script permissions (`chmod +x`)\n- ‚úÖ JSON formatting (`jq` reformat)\n- ‚úÖ Marketplace version sync\n- ‚úÖ Missing LICENSE (copy from root)\n\n## Repository-Specific Checks\n\n**For claude-code-plugins repo:**\n- Validates against `.claude-plugin/marketplace.extended.json`\n- Checks category folder matches catalog entry\n- Ensures marketplace slug is `claude-code-plugins-plus`\n- Validates against other plugins (no duplicates)\n- Checks compliance with CLAUDE.md standards\n\n## Integration with CI\n\nValidation results match GitHub Actions:\n- Same checks as `.github/workflows/validate-plugins.yml`\n- Compatible with CI error format\n- Can be run locally before pushing\n\n## Examples\n\n**User says:** \"Validate the skills-powerkit plugin\"\n\n**I automatically:**\n1. Run all validation checks\n2. Identify 2 issues (permissions, version mismatch)\n3. Provide fix commands\n4. Report overall status: FAILED\n\n**User says:** \"Check if my plugin is ready to commit\"\n\n**I automatically:**\n1. Detect plugin from context\n2. Run comprehensive validation\n3. Check marketplace compliance\n4. Report: PASSED or list issues\n\n**User says:** \"Why is my plugin failing CI?\"\n\n**I automatically:**\n1. Run same checks as CI\n2. Identify exact failure\n3. Provide fix command\n4. Validate fix works",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/plugin-validator/SKILL.md"
    },
    {
      "slug": "preprocessing-data-with-automated-pipelines",
      "name": "preprocessing-data-with-automated-pipelines",
      "description": "Automate data cleaning, transformation, and validation for ML tasks. Use when requesting \"preprocess data\", \"clean data\", \"ETL pipeline\", or \"data transformation\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Data Preprocessing Pipeline\n\nThis skill provides automated assistance for data preprocessing pipeline tasks.\n\n## Overview\n\nThis skill enables Claude to construct and execute automated data preprocessing pipelines, ensuring data quality and readiness for machine learning. It streamlines the data preparation process by automating common tasks such as data cleaning, transformation, and validation.\n\n## How It Works\n\n1. **Analyze Requirements**: Claude analyzes the user's request to understand the specific data preprocessing needs, including data sources, target format, and desired transformations.\n2. **Generate Pipeline Code**: Based on the requirements, Claude generates Python code for an automated data preprocessing pipeline using relevant libraries and best practices. This includes data validation and error handling.\n3. **Execute Pipeline**: The generated code is executed, performing the data preprocessing steps.\n4. **Provide Metrics and Insights**: Claude provides performance metrics and insights about the pipeline's execution, including data quality reports and potential issues encountered.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Prepare raw data for machine learning models.\n- Automate data cleaning and transformation processes.\n- Implement a robust ETL (Extract, Transform, Load) pipeline.\n\n## Examples\n\n### Example 1: Cleaning Customer Data\n\nUser request: \"Preprocess the customer data from the CSV file to remove duplicates and handle missing values.\"\n\nThe skill will:\n1. Generate a Python script to read the CSV file, remove duplicate entries, and impute missing values using appropriate techniques (e.g., mean imputation).\n2. Execute the script and provide a summary of the changes made, including the number of duplicates removed and the number of missing values imputed.\n\n### Example 2: Transforming Sensor Data\n\nUser request: \"Create an ETL pipeline to transform the sensor data from the database into a format suitable for time series analysis.\"\n\nThe skill will:\n1. Generate a Python script to extract sensor data from the database, transform it into a time series format (e.g., resampling to a fixed frequency), and load it into a suitable storage location.\n2. Execute the script and provide performance metrics, such as the time taken for each step of the pipeline and the size of the transformed data.\n\n## Best Practices\n\n- **Data Validation**: Always include data validation steps to ensure data quality and catch potential errors early in the pipeline.\n- **Error Handling**: Implement robust error handling to gracefully handle unexpected issues during pipeline execution.\n- **Performance Optimization**: Optimize the pipeline for performance by using efficient algorithms and data structures.\n\n## Integration\n\nThis skill can be integrated with other Claude Code skills for data analysis, model training, and deployment. It provides a standardized way to prepare data for these tasks, ensuring consistency and reliability.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "data-preprocessing-pipeline",
        "category": "ai-ml",
        "path": "plugins/ai-ml/data-preprocessing-pipeline",
        "version": "1.0.0",
        "description": "Automated data preprocessing and cleaning pipelines"
      },
      "filePath": "plugins/ai-ml/data-preprocessing-pipeline/skills/preprocessing-data-with-automated-pipelines/SKILL.md"
    },
    {
      "slug": "processing-api-batches",
      "name": "processing-api-batches",
      "description": "Optimize bulk API requests with batching, throttling, and parallel execution. Use when processing bulk API operations efficiently. Trigger with phrases like \"process bulk requests\", \"batch API calls\", or \"handle batch operations\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:batch-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Batch Processor\n\nThis skill provides automated assistance for api batch processor tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:batch-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api batch processor tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-batch-processor",
        "category": "api-development",
        "path": "plugins/api-development/api-batch-processor",
        "version": "1.0.0",
        "description": "Implement batch API operations with bulk processing and job queues"
      },
      "filePath": "plugins/api-development/api-batch-processor/skills/processing-api-batches/SKILL.md"
    },
    {
      "slug": "processing-computer-vision-tasks",
      "name": "processing-computer-vision-tasks",
      "description": "Process images using object detection, classification, and segmentation. Use when requesting \"analyze image\", \"object detection\", \"image classification\", or \"computer vision\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Computer Vision Processor\n\nThis skill provides automated assistance for computer vision processor tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for computer vision processor tasks.\nThis skill empowers Claude to leverage the computer-vision-processor plugin to analyze images, detect objects, and extract meaningful information. It automates computer vision workflows, optimizes performance, and provides detailed insights based on image content.\n\n## How It Works\n\n1. **Analyzing the Request**: Claude identifies the need for computer vision processing based on the user's request and trigger terms.\n2. **Generating Code**: Claude generates the appropriate Python code to interact with the computer-vision-processor plugin, specifying the desired analysis type (e.g., object detection, image classification).\n3. **Executing the Task**: The generated code is executed using the `/process-vision` command, which processes the image and returns the results.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze an image for specific objects or features.\n- Classify an image into predefined categories.\n- Segment an image to identify different regions or objects.\n\n## Examples\n\n### Example 1: Object Detection\n\nUser request: \"Analyze this image and identify all the cars and pedestrians.\"\n\nThe skill will:\n1. Generate code to perform object detection on the provided image using the computer-vision-processor plugin.\n2. Return a list of bounding boxes and labels for each detected car and pedestrian.\n\n### Example 2: Image Classification\n\nUser request: \"Classify this image. Is it a cat or a dog?\"\n\nThe skill will:\n1. Generate code to perform image classification on the provided image using the computer-vision-processor plugin.\n2. Return the classification result (e.g., \"cat\" or \"dog\") along with a confidence score.\n\n## Best Practices\n\n- **Data Validation**: Always validate the input image to ensure it's in a supported format and resolution.\n- **Error Handling**: Implement robust error handling to gracefully manage potential issues during image processing.\n- **Performance Optimization**: Choose the appropriate computer vision techniques and parameters to optimize performance for the specific task.\n\n## Integration\n\nThis skill utilizes the `/process-vision` command provided by the computer-vision-processor plugin. It can be integrated with other skills to further process the results of the computer vision analysis, such as generating reports or triggering actions based on detected objects.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "computer-vision-processor",
        "category": "ai-ml",
        "path": "plugins/ai-ml/computer-vision-processor",
        "version": "1.0.0",
        "description": "Computer vision image processing and analysis"
      },
      "filePath": "plugins/ai-ml/computer-vision-processor/skills/processing-computer-vision-tasks/SKILL.md"
    },
    {
      "slug": "profiling-application-performance",
      "name": "profiling-application-performance",
      "description": "This skill enables AI assistant to profile application performance, analyzing cpu usage, memory consumption, and execution time. it is triggered when the user requests performance analysis, bottleneck identification, or optimization recommendations. the... Use when optimizing performance. Trigger with phrases like 'optimize', 'performance', or 'speed up'. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Application Profiler\n\nThis skill provides automated assistance for application profiler tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze application performance, pinpoint bottlenecks, and recommend optimizations. By leveraging the application-profiler plugin, it provides insights into CPU usage, memory allocation, and execution time, enabling targeted improvements.\n\n## How It Works\n\n1. **Identify Application Stack**: Determines the application's technology (e.g., Node.js, Python, Java).\n2. **Locate Entry Points**: Identifies main application entry points and critical execution paths.\n3. **Analyze Performance Metrics**: Examines CPU usage, memory allocation, and execution time to detect bottlenecks.\n4. **Generate Profile**: Compiles the analysis into a comprehensive performance profile, highlighting areas for optimization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Analyze application performance for bottlenecks.\n- Identify CPU-intensive operations and memory leaks.\n- Optimize application execution time.\n\n## Examples\n\n### Example 1: Identifying Memory Leaks\n\nUser request: \"Analyze my Node.js application for memory leaks.\"\n\nThe skill will:\n1. Activate the application-profiler plugin.\n2. Analyze the application's memory allocation patterns.\n3. Generate a profile highlighting potential memory leaks.\n\n### Example 2: Optimizing CPU Usage\n\nUser request: \"Profile my Python script and find the most CPU-intensive functions.\"\n\nThe skill will:\n1. Activate the application-profiler plugin.\n2. Analyze the script's CPU usage.\n3. Generate a profile identifying the functions consuming the most CPU time.\n\n## Best Practices\n\n- **Code Instrumentation**: Ensure the application code is instrumented for accurate profiling.\n- **Realistic Workloads**: Use realistic workloads during profiling to simulate real-world scenarios.\n- **Iterative Optimization**: Apply optimizations iteratively and re-profile to measure improvements.\n\n## Integration\n\nThis skill can be used in conjunction with code editing plugins to implement the recommended optimizations directly within the application's source code. It can also integrate with monitoring tools to track performance improvements over time.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "application-profiler",
        "category": "performance",
        "path": "plugins/performance/application-profiler",
        "version": "1.0.0",
        "description": "Profile application performance with CPU, memory, and execution time analysis"
      },
      "filePath": "plugins/performance/application-profiler/skills/profiling-application-performance/SKILL.md"
    },
    {
      "slug": "providing-performance-optimization-advice",
      "name": "providing-performance-optimization-advice",
      "description": "Provide comprehensive prioritized performance optimization recommendations for frontend, backend, and infrastructure. Use when analyzing bottlenecks or seeking improvement strategies. Trigger with phrases like \"optimize performance\", \"improve speed\", or \"performance recommendations\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(profiling:*)",
        "Bash(analysis:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Performance Optimization Advisor\n\nThis skill provides automated assistance for performance optimization advisor tasks.\n\n## Overview\n\nThis skill empowers Claude to act as a performance optimization advisor, delivering a detailed report of potential improvements across various layers of a software application. It prioritizes recommendations based on impact and effort, allowing for a focused and efficient optimization strategy.\n\n## How It Works\n\n1. **Analyze Project**: Claude uses the plugin to analyze the project's codebase, infrastructure configuration, and architecture.\n2. **Identify Optimization Areas**: The plugin identifies potential optimization areas in the frontend, backend, and infrastructure.\n3. **Prioritize Recommendations**: The plugin prioritizes recommendations based on estimated performance gains and implementation effort.\n4. **Generate Report**: Claude presents a comprehensive report with actionable advice, performance gain estimates, and a phased implementation roadmap.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in a software application.\n- Get recommendations for improving website loading speed.\n- Optimize database query performance.\n- Improve API response times.\n- Reduce infrastructure costs.\n\n## Examples\n\n### Example 1: Optimizing a Slow Website\n\nUser request: \"My website is loading very slowly. Can you help me optimize its performance?\"\n\nThe skill will:\n1. Analyze the website's frontend code, backend APIs, and infrastructure configuration.\n2. Identify issues such as unoptimized images, inefficient database queries, and lack of CDN usage.\n3. Generate a report with prioritized recommendations, including image optimization, database query optimization, and CDN implementation.\n\n### Example 2: Improving API Response Time\n\nUser request: \"The API response time is too slow. What can I do to improve it?\"\n\nThe skill will:\n1. Analyze the API code, database queries, and caching strategies.\n2. Identify issues such as inefficient database queries, lack of caching, and slow processing logic.\n3. Generate a report with prioritized recommendations, including database query optimization, caching implementation, and asynchronous processing.\n\n## Best Practices\n\n- **Specificity**: Provide specific details about the project and its performance issues to get more accurate and relevant recommendations.\n- **Context**: Explain the context of the performance problem, such as the expected user load or the specific use case.\n- **Iteration**: Review the recommendations and provide feedback to refine the optimization strategy.\n\n## Integration\n\nThis skill integrates well with other plugins that provide code analysis, infrastructure management, and deployment automation capabilities. For example, it can be used in conjunction with a code linting plugin to identify code-level performance issues or with an infrastructure-as-code plugin to automate infrastructure optimization tasks.\n\n## Prerequisites\n\n- Access to application codebase in {baseDir}/\n- Infrastructure configuration files\n- Performance profiling tools\n- Current performance metrics and baselines\n\n## Instructions\n\n1. Analyze frontend code for rendering and asset optimization\n2. Review backend code for query and processing efficiency\n3. Examine infrastructure for scaling and resource usage\n4. Identify high-impact optimization opportunities\n5. Prioritize recommendations by effort vs impact\n6. Generate phased implementation roadmap\n\n## Output\n\n- Comprehensive optimization report by layer (frontend/backend/infra)\n- Prioritized recommendations with impact estimates\n- Code examples for suggested improvements\n- Performance gain projections\n- Implementation effort estimates and timeline\n\n## Error Handling\n\nIf optimization analysis fails:\n- Verify codebase access permissions\n- Check profiling tool installation\n- Validate configuration file formats\n- Ensure sufficient analysis resources\n- Review project structure completeness\n\n## Resources\n\n- Web performance optimization guides\n- Database query optimization best practices\n- Infrastructure scaling patterns\n- Caching strategies and CDN usage",
      "parentPlugin": {
        "name": "performance-optimization-advisor",
        "category": "performance",
        "path": "plugins/performance/performance-optimization-advisor",
        "version": "1.0.0",
        "description": "Get comprehensive performance optimization recommendations"
      },
      "filePath": "plugins/performance/performance-optimization-advisor/skills/providing-performance-optimization-advice/SKILL.md"
    },
    {
      "slug": "rate-limiting-apis",
      "name": "rate-limiting-apis",
      "description": "Implement sophisticated rate limiting with sliding windows, token buckets, and quotas. Use when protecting APIs from excessive requests. Trigger with phrases like \"add rate limiting\", \"limit API requests\", or \"implement rate limits\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:ratelimit-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Rate Limiter\n\nThis skill provides automated assistance for api rate limiter tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:ratelimit-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api rate limiter tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-rate-limiter",
        "category": "api-development",
        "path": "plugins/api-development/api-rate-limiter",
        "version": "1.0.0",
        "description": "Implement rate limiting with token bucket, sliding window, and Redis"
      },
      "filePath": "plugins/api-development/api-rate-limiter/skills/rate-limiting-apis/SKILL.md"
    },
    {
      "slug": "responding-to-security-incidents",
      "name": "responding-to-security-incidents",
      "description": "Analyze and guide security incident response, investigation, and remediation processes. Use when you need to handle security breaches, classify incidents, develop response playbooks, gather forensic evidence, or coordinate remediation efforts. Trigger with phrases like \"security incident response\", \"ransomware attack response\", \"data breach investigation\", \"incident playbook\", or \"security forensics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(log-analysis:*), Bash(forensics:*), Bash(network-trace:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Security Incident Responder\n\nThis skill provides automated assistance for security incident responder tasks.\n\n## Overview\n\nHelps drive consistent incident response: triage, evidence preservation, containment, eradication, recovery, and post-incident follow-ups.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Access to system and application logs in {baseDir}/logs/\n- Network traffic captures or SIEM data available\n- Incident response team contact information\n- Backup systems operational and accessible\n- Write permissions for incident documentation in {baseDir}/incidents/\n- Communication channels established for stakeholder updates\n\n## Instructions\n\n1. Triage the incident and scope affected systems/data.\n2. Preserve evidence (logs, snapshots, network captures) before making changes.\n3. Contain the blast radius and eradicate root cause.\n4. Recover safely and document follow-ups (AAR + backlog).\n\n### 1. Incident Detection and Triage\n\nClassify the security incident:\n- Incident type (ransomware, data breach, DDoS, insider threat, phishing)\n- Severity level (Critical, High, Medium, Low)\n- Scope assessment (affected systems, data, users)\n- Initial timestamp and detection method\n- Potential business impact\n\n### 2. Immediate Containment Actions\n\nPrevent further damage:\n- Isolate affected systems from network\n- Disable compromised user accounts\n- Block malicious IP addresses at firewall\n- Preserve system state for forensics\n- Activate incident response team\n- Document all containment actions with timestamps\n\n### 3. Evidence Collection Phase\n\nGather forensic data systematically:\n\n**System Evidence**:\n- Memory dumps from affected systems\n- Disk images for forensic analysis\n- Running process listings\n- Network connection states\n- Registry modifications (Windows)\n\n**Log Evidence**:\n- Authentication logs (successful/failed logins)\n- Application logs with error patterns\n- Network traffic logs (firewall, IDS/IPS)\n- Database access logs\n- Web server access/error logs\n\n**Network Evidence**:\n- Packet captures (PCAP files)\n- DNS query logs\n- Proxy server logs\n- Network flow data (NetFlow)\n\n### 4. Investigation and Analysis\n\nReconstruct the attack timeline:\n- Identify initial access vector (how attackers got in)\n- Map lateral movement within network\n- Determine data exfiltration attempts\n- Identify persistence mechanisms\n- Assess privilege escalation methods\n- Document indicators of compromise (IOCs)\n\n### 5. Eradication Phase\n\nRemove threat from environment:\n- Remove malware and backdoors\n- Close exploited vulnerabilities\n- Reset compromised credentials\n- Apply security patches\n- Update firewall rules\n- Verify threat elimination\n\n### 6. Recovery and Restoration\n\nRestore normal operations:\n- Restore systems from clean backups\n- Rebuild compromised systems from scratch\n- Verify system integrity\n- Monitor for reinfection attempts\n- Gradually restore services\n- Validate business operations\n\n### 7. Post-Incident Documentation\n\nCreate comprehensive incident report:\n- Executive summary\n- Detailed timeline\n- Root cause analysis\n- Lessons learned\n- Remediation recommendations\n- Cost impact assessment\n\n## Output\n\nThe skill produces:\n\n**Primary Output**: Incident response playbook saved to {baseDir}/incidents/incident-YYYYMMDD-HHMM.md\n\n**Playbook Structure**:\n```\n# Security Incident Response - [Incident Type]\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\nDate: YYYY-MM-DD HH:MM\nSeverity: CRITICAL\nStatus: Contained\n\n## Executive Summary\n- Incident type: Ransomware attack\n- Detection time: 2024-01-15 08:30 UTC\n- Affected systems: 15 servers, 200 workstations\n- Business impact: Production halted\n- Current status: Contained, recovery in progress\n\n## Timeline of Events\n08:30 - Initial detection via EDR alert\n08:35 - IT team confirms ransomware encryption\n08:40 - Network isolation initiated\n09:00 - Incident response team activated\n[Detailed timeline continues]\n\n## Containment Actions Taken\n‚úÖ Isolated affected network segments\n‚úÖ Disabled compromised accounts\n‚úÖ Blocked C2 server IPs\n‚úÖ Preserved forensic evidence\n\n## Evidence Collected\n- Memory dumps: 15 systems\n- Log files: {baseDir}/incidents/evidence/logs/\n- Network captures: {baseDir}/incidents/evidence/pcaps/\n- Malware samples: Quarantined\n\n## IOCs (Indicators of Compromise)\n- IP addresses: 203.0.113.45, 198.51.100.78\n- File hashes: SHA256 values listed\n- Domain names: malicious-c2.example\n- Registry keys: HKLM\\Software\\[malware]\n\n## Remediation Plan\nPriority 1 (Immediate):\n- Remove ransomware from all systems\n- Reset all domain credentials\n- Patch vulnerable RDP service\n\nPriority 2 (24 hours):\n- Deploy endpoint protection updates\n- Implement network segmentation\n- Enable MFA for all accounts\n\nPriority 3 (1 week):\n- Security awareness training\n- Update incident response procedures\n- Conduct tabletop exercise\n\n## Recovery Status\n- Clean backups identified: 2024-01-14 backup\n- Systems rebuilt: 5/15 servers complete\n- Services restored: Email, file servers online\n- Estimated full recovery: 48 hours\n\n## Communication Log\n- 08:45 - Executive team notified\n- 09:30 - Legal counsel engaged\n- 10:00 - Cyber insurance contacted\n- 12:00 - Customer notification prepared\n```\n\n**Secondary Outputs**:\n- IOC list for threat intelligence sharing (JSON/STIX format)\n- Evidence chain of custody log\n- Stakeholder communication templates\n- Post-incident review agenda\n\n## Error Handling\n\n**Common Issues and Resolutions**:\n\n1. **Incomplete Log Data**\n   - Error: \"Critical logs missing from {baseDir}/logs/\"\n   - Resolution: Work with available data, note gaps in report\n   - Action: Improve logging for future incidents\n\n2. **Evidence Contamination**\n   - Error: \"System state modified before evidence collection\"\n   - Resolution: Document contamination, collect remaining evidence\n   - Best Practice: Immediately isolate before investigation\n\n3. **Ongoing Active Threat**\n   - Error: \"Attacker still has access during investigation\"\n   - Resolution: Prioritize containment over investigation\n   - Action: Implement emergency containment procedures first\n\n4. **Insufficient Access for Forensics**\n   - Error: \"Permission denied accessing system memory\"\n   - Resolution: Escalate to obtain necessary privileges\n   - Fallback: Use available logs and network data\n\n5. **Backup Corruption**\n   - Error: \"Backups also encrypted by ransomware\"\n   - Resolution: Identify offline/air-gapped backups\n   - Contingency: Assess rebuild from scratch vs ransom payment\n\n## Examples\n\n- \"We suspect credential stuffing. Use logs in {baseDir}/logs/ to triage and propose containment steps.\"\n- \"Create an incident response plan for a suspected data breach and list evidence to collect.\"\n\n## Resources\n\n**Incident Response Frameworks**:\n- NIST Computer Security Incident Handling Guide: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf\n- SANS Incident Handler's Handbook: https://www.sans.org/white-papers/33901/\n- CISA Incident Response Guide: https://www.cisa.gov/incident-response\n\n**Forensic Tools**:\n- Memory analysis: Volatility Framework\n- Disk forensics: Autopsy, FTK Imager\n- Network analysis: Wireshark, tcpdump\n- Log analysis: ELK Stack, Splunk\n\n**Threat Intelligence**:\n- MITRE ATT&CK Framework: https://attack.mitre.org/\n- AlienVault OTX: https://otx.alienvault.com/\n- VirusTotal: https://www.virustotal.com/\n\n**Communication Templates**:\n- Breach notification requirements by jurisdiction\n- Customer communication guidelines\n- Media response templates\n- Regulatory reporting formats (GDPR, HIPAA, etc.)\n\n**Playbook Templates**:\n- Ransomware response: {baseDir}/templates/playbook-ransomware.md\n- Data breach response: {baseDir}/templates/playbook-breach.md\n- DDoS response: {baseDir}/templates/playbook-ddos.md\n\n**Legal and Compliance**:\n- Chain of custody documentation\n- eDiscovery preparation\n- Cyber insurance claim procedures\n- Law enforcement coordination",
      "parentPlugin": {
        "name": "security-incident-responder",
        "category": "security",
        "path": "plugins/security/security-incident-responder",
        "version": "1.0.0",
        "description": "Assist with security incident response"
      },
      "filePath": "plugins/security/security-incident-responder/skills/responding-to-security-incidents/SKILL.md"
    },
    {
      "slug": "routing-dex-trades",
      "name": "routing-dex-trades",
      "description": "Optimize trade routing across multiple DEXs to find optimal prices and minimize slippage. Use when routing trades for best execution. Trigger with phrases like \"find best price\", \"route trade\", or \"check DEX prices\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:dex-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Dex Aggregator Router\n\nThis skill provides automated assistance for dex aggregator router tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:dex-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "dex-aggregator-router",
        "category": "crypto",
        "path": "plugins/crypto/dex-aggregator-router",
        "version": "1.0.0",
        "description": "Find optimal DEX routes for token swaps across multiple exchanges"
      },
      "filePath": "plugins/crypto/dex-aggregator-router/skills/routing-dex-trades/SKILL.md"
    },
    {
      "slug": "running-chaos-tests",
      "name": "running-chaos-tests",
      "description": "Execute chaos engineering experiments to test system resilience. Use when performing specialized testing. Trigger with phrases like \"run chaos tests\", \"test resilience\", or \"inject failures\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:chaos-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Chaos Engineering Toolkit\n\nThis skill provides automated assistance for chaos engineering toolkit tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:chaos-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for chaos engineering toolkit tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "chaos-engineering-toolkit",
        "category": "testing",
        "path": "plugins/testing/chaos-engineering-toolkit",
        "version": "1.0.0",
        "description": "Chaos testing for resilience with failure injection, latency simulation, and system resilience validation"
      },
      "filePath": "plugins/testing/chaos-engineering-toolkit/skills/running-chaos-tests/SKILL.md"
    },
    {
      "slug": "running-clustering-algorithms",
      "name": "running-clustering-algorithms",
      "description": "Analyze datasets by running clustering algorithms (K-means, DBSCAN, hierarchical) to identify data groups. Use when requesting \"run clustering\", \"cluster analysis\", or \"group data points\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Clustering Algorithm Runner\n\nThis skill provides automated assistance for clustering algorithm runner tasks.\n\n## Overview\n\nThis skill empowers Claude to perform clustering analysis on provided datasets. It allows for automated execution of various clustering algorithms, providing insights into data groupings and structures.\n\n## How It Works\n\n1. **Analyzing the Context**: Claude analyzes the user's request to determine the dataset, desired clustering algorithm (if specified), and any specific requirements.\n2. **Generating Code**: Claude generates Python code using appropriate ML libraries (e.g., scikit-learn) to perform the clustering task, including data loading, preprocessing, algorithm execution, and result visualization.\n3. **Executing Clustering**: The generated code is executed, and the clustering algorithm is applied to the dataset.\n4. **Providing Results**: Claude presents the results, including cluster assignments, performance metrics (e.g., silhouette score, Davies-Bouldin index), and visualizations (e.g., scatter plots with cluster labels).\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify distinct groups within a dataset.\n- Perform a cluster analysis to understand data structure.\n- Run K-means, DBSCAN, or hierarchical clustering on a given dataset.\n\n## Examples\n\n### Example 1: Customer Segmentation\n\nUser request: \"Run clustering on this customer data to identify customer segments. The data is in customer_data.csv.\"\n\nThe skill will:\n1. Load the customer_data.csv dataset.\n2. Perform K-means clustering to identify distinct customer segments based on their attributes.\n3. Provide a visualization of the customer segments and their characteristics.\n\n### Example 2: Anomaly Detection\n\nUser request: \"Perform DBSCAN clustering on this network traffic data to identify anomalies. The data is available at network_traffic.txt.\"\n\nThe skill will:\n1. Load the network_traffic.txt dataset.\n2. Perform DBSCAN clustering to identify outliers representing anomalous network traffic.\n3. Report the identified anomalies and their characteristics.\n\n## Best Practices\n\n- **Data Preprocessing**: Always preprocess the data (e.g., scaling, normalization) before applying clustering algorithms to improve performance and accuracy.\n- **Algorithm Selection**: Choose the appropriate clustering algorithm based on the data characteristics and the desired outcome. K-means is suitable for spherical clusters, while DBSCAN is better for non-spherical clusters and anomaly detection.\n- **Parameter Tuning**: Tune the parameters of the clustering algorithm (e.g., number of clusters in K-means, epsilon and min_samples in DBSCAN) to optimize the results.\n\n## Integration\n\nThis skill can be integrated with data loading skills to retrieve datasets from various sources. It can also be combined with visualization skills to generate insightful visualizations of the clustering results.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "clustering-algorithm-runner",
        "category": "ai-ml",
        "path": "plugins/ai-ml/clustering-algorithm-runner",
        "version": "1.0.0",
        "description": "Run clustering algorithms on datasets"
      },
      "filePath": "plugins/ai-ml/clustering-algorithm-runner/skills/running-clustering-algorithms/SKILL.md"
    },
    {
      "slug": "running-e2e-tests",
      "name": "running-e2e-tests",
      "description": "Execute end-to-end tests covering full user workflows across frontend and backend. Use when performing specialized testing. Trigger with phrases like \"run end-to-end tests\", \"test user flows\", or \"execute E2E suite\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:e2e-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# E2E Test Framework\n\nThis skill provides automated assistance for e2e test framework tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:e2e-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for e2e test framework tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "e2e-test-framework",
        "category": "testing",
        "path": "plugins/testing/e2e-test-framework",
        "version": "1.0.0",
        "description": "End-to-end test automation with Playwright, Cypress, and Selenium for browser-based testing"
      },
      "filePath": "plugins/testing/e2e-test-framework/skills/running-e2e-tests/SKILL.md"
    },
    {
      "slug": "running-integration-tests",
      "name": "running-integration-tests",
      "description": "Execute integration tests validating component interactions and system integration. Use when performing specialized testing. Trigger with phrases like \"run integration tests\", \"test integration\", or \"validate component interactions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:integration-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Integration Test Runner\n\nThis skill provides automated assistance for integration test runner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:integration-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for integration test runner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "integration-test-runner",
        "category": "testing",
        "path": "plugins/testing/integration-test-runner",
        "version": "1.0.0",
        "description": "Run and manage integration test suites with environment setup, database seeding, and cleanup"
      },
      "filePath": "plugins/testing/integration-test-runner/skills/running-integration-tests/SKILL.md"
    },
    {
      "slug": "running-load-tests",
      "name": "running-load-tests",
      "description": "Create and execute load tests for performance validation using k6, JMeter, and Artillery. Use when validating application performance under load conditions or identifying bottlenecks. Trigger with phrases like \"run load test\", \"create stress test\", or \"validate performance under load\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(k6:*)",
        "Bash(jmeter:*)",
        "Bash(artillery:*)",
        "Bash(performance:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Load Test Runner\n\nThis skill provides automated assistance for load test runner tasks.\n\n## Overview\n\nThis skill empowers Claude to automate the creation and execution of load tests, ensuring applications can handle expected traffic and identify potential performance bottlenecks. It streamlines the process of defining test scenarios, generating scripts, and executing tests for comprehensive performance validation.\n\n## How It Works\n\n1. **Analyze Application**: Claude analyzes the user's request to understand the application's endpoints and critical paths.\n2. **Identify Test Scenarios**: Claude identifies relevant test scenarios, such as baseline load, stress test, spike test, soak test, or scalability test, based on the user's requirements.\n3. **Generate Load Test Scripts**: Claude generates load test scripts (k6, JMeter, Artillery, etc.) based on the selected scenarios and application details.\n4. **Define Performance Thresholds**: Claude defines performance thresholds and provides execution instructions for the generated scripts.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Create load tests for a web application or API.\n- Validate the performance of an application under different load conditions.\n- Identify performance bottlenecks and breaking points.\n\n## Examples\n\n### Example 1: Creating a Stress Test\n\nUser request: \"Create a stress test for the /api/users endpoint to simulate 1000 concurrent users.\"\n\nThe skill will:\n1. Analyze the request and identify the need for a stress test on the /api/users endpoint.\n2. Generate a k6 script that simulates 1000 concurrent users hitting the /api/users endpoint.\n\n### Example 2: Validating Performance After a Code Change\n\nUser request: \"Validate the performance of the application after the recent code changes with a baseline load test.\"\n\nThe skill will:\n1. Identify the need for a baseline load test to validate performance.\n2. Generate a JMeter script that simulates normal traffic patterns for the application.\n\n## Best Practices\n\n- **Realistic Scenarios**: Define load test scenarios that accurately reflect real-world usage patterns.\n- **Threshold Definition**: Establish clear performance thresholds to identify potential issues.\n- **Iterative Testing**: Run load tests iteratively to identify and address performance bottlenecks early in the development cycle.\n\n## Integration\n\nThis skill can be integrated with CI/CD pipelines to automate performance testing as part of the deployment process. It can also be used in conjunction with monitoring tools to correlate performance metrics with application behavior.\n\n## Prerequisites\n\n- Load testing tools installed (k6, JMeter, or Artillery)\n- Access to target application endpoints\n- Test scenario definitions and expected load patterns\n- Results storage location at {baseDir}/load-tests/\n\n## Instructions\n\n1. Analyze application architecture and identify critical endpoints\n2. Define test scenarios (baseline, stress, spike, soak, scalability)\n3. Generate appropriate load test scripts using selected tool\n4. Configure performance thresholds and acceptance criteria\n5. Execute load tests and capture metrics\n6. Analyze results and identify performance bottlenecks\n\n## Output\n\n- Load test scripts (k6, JMeter, or Artillery format)\n- Test execution logs and metrics\n- Performance reports with response times and throughput\n- Threshold violation alerts\n- Recommendations for performance improvements\n\n## Error Handling\n\nIf load test execution fails:\n- Verify tool installation and configuration\n- Check network connectivity to target endpoints\n- Validate authentication and authorization\n- Review test script syntax and parameters\n- Ensure sufficient system resources for test execution\n\n## Resources\n\n- k6 documentation and examples\n- JMeter user manual and best practices\n- Artillery load testing guides\n- Performance testing methodology references",
      "parentPlugin": {
        "name": "load-test-runner",
        "category": "performance",
        "path": "plugins/performance/load-test-runner",
        "version": "1.0.0",
        "description": "Create and execute load tests for performance validation"
      },
      "filePath": "plugins/performance/load-test-runner/skills/running-load-tests/SKILL.md"
    },
    {
      "slug": "running-mutation-tests",
      "name": "running-mutation-tests",
      "description": "Execute mutation testing to evaluate test suite effectiveness. Use when performing specialized testing. Trigger with phrases like \"run mutation tests\", \"test the tests\", or \"validate test effectiveness\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:mutation-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Mutation Test Runner\n\nThis skill provides automated assistance for mutation test runner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:mutation-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for mutation test runner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "mutation-test-runner",
        "category": "testing",
        "path": "plugins/testing/mutation-test-runner",
        "version": "1.0.0",
        "description": "Mutation testing to validate test quality by introducing code changes and verifying tests catch them"
      },
      "filePath": "plugins/testing/mutation-test-runner/skills/running-mutation-tests/SKILL.md"
    },
    {
      "slug": "running-performance-tests",
      "name": "running-performance-tests",
      "description": "Execute load testing, stress testing, and performance benchmarking. Use when performing specialized testing. Trigger with phrases like \"run load tests\", \"test performance\", or \"benchmark the system\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:perf-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Performance Test Suite\n\nThis skill provides automated assistance for performance test suite tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:perf-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for performance test suite tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "performance-test-suite",
        "category": "testing",
        "path": "plugins/testing/performance-test-suite",
        "version": "1.0.0",
        "description": "Load testing and performance benchmarking with metrics analysis and bottleneck identification"
      },
      "filePath": "plugins/testing/performance-test-suite/skills/running-performance-tests/SKILL.md"
    },
    {
      "slug": "running-smoke-tests",
      "name": "running-smoke-tests",
      "description": "Execute fast smoke tests validating critical functionality after deployment. Use when performing specialized testing. Trigger with phrases like \"run smoke tests\", \"quick validation\", or \"test critical paths\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:smoke-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Smoke Test Runner\n\nThis skill provides automated assistance for smoke test runner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:smoke-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for smoke test runner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "smoke-test-runner",
        "category": "testing",
        "path": "plugins/testing/smoke-test-runner",
        "version": "1.0.0",
        "description": "Quick smoke test suites to verify critical functionality after deployments"
      },
      "filePath": "plugins/testing/smoke-test-runner/skills/running-smoke-tests/SKILL.md"
    },
    {
      "slug": "scanning-accessibility",
      "name": "scanning-accessibility",
      "description": "Validate WCAG compliance and accessibility standards (ARIA, keyboard navigation). Use when auditing WCAG compliance or screen reader compatibility. Trigger with phrases like \"scan accessibility\", \"check WCAG compliance\", or \"validate screen readers\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:a11y-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Accessibility Test Scanner\n\nThis skill provides automated assistance for accessibility test scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:a11y-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for accessibility test scanner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "accessibility-test-scanner",
        "category": "testing",
        "path": "plugins/testing/accessibility-test-scanner",
        "version": "1.0.0",
        "description": "A11y compliance testing with WCAG 2.1/2.2 validation, screen reader compatibility, and automated accessibility audits"
      },
      "filePath": "plugins/testing/accessibility-test-scanner/skills/scanning-accessibility/SKILL.md"
    },
    {
      "slug": "scanning-api-security",
      "name": "scanning-api-security",
      "description": "Detect API security vulnerabilities including injection, broken auth, and data exposure. Use when scanning APIs for security vulnerabilities. Trigger with phrases like \"scan API security\", \"check for vulnerabilities\", or \"audit API security\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:security-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Security Scanner\n\nThis skill provides automated assistance for api security scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:security-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api security scanner tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-security-scanner",
        "category": "api-development",
        "path": "plugins/api-development/api-security-scanner",
        "version": "1.0.0",
        "description": "Scan APIs for security vulnerabilities and OWASP API Top 10"
      },
      "filePath": "plugins/api-development/api-security-scanner/skills/scanning-api-security/SKILL.md"
    },
    {
      "slug": "scanning-container-security",
      "name": "scanning-container-security",
      "description": "Use when you need to work with security and compliance. This skill provides security scanning and vulnerability detection with comprehensive guidance and automation. Trigger with phrases like \"scan for vulnerabilities\", \"implement security controls\", or \"audit security\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Container Security Scanner\n\nThis skill provides automated assistance for container security scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/container-security-scanner/`\n\n**Documentation and Guides**: `{baseDir}/docs/container-security-scanner/`\n\n**Example Scripts and Code**: `{baseDir}/examples/container-security-scanner/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/container-security-scanner-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/container-security-scanner-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/container-security-scanner-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "container-security-scanner",
        "category": "devops",
        "path": "plugins/devops/container-security-scanner",
        "version": "1.0.0",
        "description": "Scan containers for vulnerabilities using Trivy, Snyk, and other security tools"
      },
      "filePath": "plugins/devops/container-security-scanner/skills/scanning-container-security/SKILL.md"
    },
    {
      "slug": "scanning-database-security",
      "name": "scanning-database-security",
      "description": "Use when you need to work with security and compliance. This skill provides security scanning and vulnerability detection with comprehensive guidance and automation. Trigger with phrases like \"scan for vulnerabilities\", \"implement security controls\", or \"audit security\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*), Bash(mongosh:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Database Security Scanner\n\nThis skill provides automated assistance for database security scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Required credentials and permissions for the operations\n- Understanding of the system architecture and dependencies\n- Backup of critical data before making structural changes\n- Access to relevant documentation and configuration files\n- Monitoring tools configured for observability\n- Development or staging environment available for testing\n\n## Instructions\n\n### Step 1: Assess Current State\n1. Review current configuration, setup, and baseline metrics\n2. Identify specific requirements, goals, and constraints\n3. Document existing patterns, issues, and pain points\n4. Analyze dependencies and integration points\n5. Validate all prerequisites are met before proceeding\n\n### Step 2: Design Solution\n1. Define optimal approach based on best practices\n2. Create detailed implementation plan with clear steps\n3. Identify potential risks and mitigation strategies\n4. Document expected outcomes and success criteria\n5. Review plan with team or stakeholders if needed\n\n### Step 3: Implement Changes\n1. Execute implementation in non-production environment first\n2. Verify changes work as expected with thorough testing\n3. Monitor for any issues, errors, or performance impacts\n4. Document all changes, decisions, and configurations\n5. Prepare rollback plan and recovery procedures\n\n### Step 4: Validate Implementation\n1. Run comprehensive tests to verify all functionality\n2. Compare performance metrics against baseline\n3. Confirm no unintended side effects or regressions\n4. Update all relevant documentation\n5. Obtain approval before production deployment\n\n### Step 5: Deploy to Production\n1. Schedule deployment during appropriate maintenance window\n2. Execute implementation with real-time monitoring\n3. Watch closely for any issues or anomalies\n4. Verify successful deployment and functionality\n5. Document completion, metrics, and lessons learned\n\n## Output\n\nThis skill produces:\n\n**Implementation Artifacts**: Scripts, configuration files, code, and automation tools\n\n**Documentation**: Comprehensive documentation of changes, procedures, and architecture\n\n**Test Results**: Validation reports, test coverage, and quality metrics\n\n**Monitoring Configuration**: Dashboards, alerts, metrics, and observability setup\n\n**Runbooks**: Operational procedures for maintenance, troubleshooting, and incident response\n\n## Error Handling\n\n**Permission and Access Issues**:\n- Verify credentials and permissions for all operations\n- Request elevated access if required for specific tasks\n- Document all permission requirements for automation\n- Use separate service accounts for privileged operations\n- Implement least-privilege access principles\n\n**Connection and Network Failures**:\n- Check network connectivity, firewalls, and security groups\n- Verify service endpoints, DNS resolution, and routing\n- Test connections using diagnostic and troubleshooting tools\n- Review network policies, ACLs, and security configurations\n- Implement retry logic with exponential backoff\n\n**Resource Constraints**:\n- Monitor resource usage (CPU, memory, disk, network)\n- Implement throttling, rate limiting, or queue mechanisms\n- Schedule resource-intensive tasks during low-traffic periods\n- Scale infrastructure resources if consistently hitting limits\n- Optimize queries, code, or configurations for efficiency\n\n**Configuration and Syntax Errors**:\n- Validate all configuration syntax before applying changes\n- Test configurations thoroughly in non-production first\n- Implement automated configuration validation checks\n- Maintain version control for all configuration files\n- Keep previous working configuration for quick rollback\n\n## Resources\n\n**Configuration Templates**: `{baseDir}/templates/database-security-scanner/`\n\n**Documentation and Guides**: `{baseDir}/docs/database-security-scanner/`\n\n**Example Scripts and Code**: `{baseDir}/examples/database-security-scanner/`\n\n**Troubleshooting Guide**: `{baseDir}/docs/database-security-scanner-troubleshooting.md`\n\n**Best Practices**: `{baseDir}/docs/database-security-scanner-best-practices.md`\n\n**Monitoring Setup**: `{baseDir}/monitoring/database-security-scanner-dashboard.json`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "database-security-scanner",
        "category": "database",
        "path": "plugins/database/database-security-scanner",
        "version": "1.0.0",
        "description": "Database plugin for database-security-scanner"
      },
      "filePath": "plugins/database/database-security-scanner/skills/scanning-database-security/SKILL.md"
    },
    {
      "slug": "scanning-for-data-privacy-issues",
      "name": "scanning-for-data-privacy-issues",
      "description": "Scan for data privacy issues and sensitive information exposure. Use when reviewing data handling practices. Trigger with 'scan privacy issues', 'check sensitive data', or 'validate data protection'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Data Privacy Scanner\n\nThis skill provides automated assistance for data privacy scanner tasks.\n\n## Overview\n\nThis skill automates the process of identifying data privacy risks within a codebase. By leveraging the data-privacy-scanner plugin, Claude can quickly pinpoint potential vulnerabilities, helping developers proactively address compliance requirements and protect sensitive user data.\n\n## How It Works\n\n1. **Initiate Scan**: Upon detecting a privacy-related trigger phrase, Claude activates the data-privacy-scanner plugin.\n2. **Analyze Codebase**: The plugin analyzes the specified files or the entire project for potential data privacy violations.\n3. **Report Findings**: The plugin generates a detailed report outlining identified risks, including the location of the vulnerability and a description of the potential impact.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify potential data privacy vulnerabilities in a codebase.\n- Ensure compliance with data privacy regulations such as GDPR, CCPA, or HIPAA.\n- Perform a privacy audit of a project involving sensitive user data.\n\n## Examples\n\n### Example 1: Identifying PII Leaks\n\nUser request: \"Scan this project for PII leaks.\"\n\nThe skill will:\n1. Activate the data-privacy-scanner plugin to analyze the project.\n2. Generate a report highlighting potential Personally Identifiable Information (PII) leaks, such as exposed email addresses or phone numbers.\n\n### Example 2: Checking GDPR Compliance\n\nUser request: \"Check this configuration file for GDPR compliance issues.\"\n\nThe skill will:\n1. Activate the data-privacy-scanner plugin to analyze the specified configuration file.\n2. Generate a report identifying potential GDPR violations, such as insufficient data anonymization or improper consent management.\n\n## Best Practices\n\n- **Scope**: Specify the relevant files or directories to narrow the scope of the scan and improve performance.\n- **Context**: Provide context about the type of data being processed to help the plugin identify relevant privacy risks.\n- **Review**: Carefully review the generated report to understand the identified vulnerabilities and implement appropriate remediation measures.\n\n## Integration\n\nThis skill can be integrated with other security and compliance tools to provide a comprehensive approach to data privacy. For example, it can be combined with vulnerability scanning tools to identify related security risks or with reporting tools to track progress on remediation efforts.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "data-privacy-scanner",
        "category": "security",
        "path": "plugins/security/data-privacy-scanner",
        "version": "1.0.0",
        "description": "Scan for data privacy issues"
      },
      "filePath": "plugins/security/data-privacy-scanner/skills/scanning-for-data-privacy-issues/SKILL.md"
    },
    {
      "slug": "scanning-for-gdpr-compliance",
      "name": "scanning-for-gdpr-compliance",
      "description": "Scan for GDPR compliance issues in data handling and privacy practices. Use when ensuring EU data protection compliance. Trigger with 'scan GDPR compliance', 'check data privacy', or 'validate GDPR'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Gdpr Compliance Scanner\n\nThis skill provides automated assistance for gdpr compliance scanner tasks.\n\n## Overview\n\nThis skill allows Claude to automatically assess an application's GDPR compliance posture. It provides a comprehensive scan, identifying potential violations and offering actionable recommendations to improve compliance. The skill simplifies the complex process of GDPR auditing, making it easier to identify and address critical gaps.\n\n## How It Works\n\n1. **Initiate Scan**: The user requests a GDPR compliance scan using natural language.\n2. **Plugin Activation**: Claude activates the `gdpr-compliance-scanner` plugin.\n3. **Compliance Assessment**: The plugin scans the application or system based on GDPR requirements.\n4. **Report Generation**: A detailed report is generated, highlighting compliance scores, critical gaps, and recommended actions.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess an application's GDPR compliance.\n- Identify potential GDPR violations.\n- Generate a report outlining compliance gaps and recommendations.\n- Audit data processing activities for adherence to GDPR principles.\n\n## Examples\n\n### Example 1: Assess GDPR Compliance of a Web Application\n\nUser request: \"Scan my web application for GDPR compliance.\"\n\nThe skill will:\n1. Activate the `gdpr-compliance-scanner` plugin.\n2. Scan the web application for GDPR compliance issues related to data collection, storage, and processing.\n3. Generate a report highlighting compliance scores, critical gaps such as missing cookie consent mechanisms, and actionable recommendations like implementing a cookie consent banner.\n\n### Example 2: Audit Data Processing Activities\n\nUser request: \"Check our data processing activities for GDPR compliance.\"\n\nThe skill will:\n1. Activate the `gdpr-compliance-scanner` plugin.\n2. Analyze data processing activities, including data collection methods, storage practices, and security measures.\n3. Generate a report identifying potential violations, such as inadequate data encryption or missing data processing agreements, along with recommendations for remediation.\n\n## Best Practices\n\n- **Specificity**: Provide as much context as possible about the application or system being scanned to improve the accuracy of the assessment.\n- **Regularity**: Schedule regular GDPR compliance scans to ensure ongoing adherence to regulatory requirements.\n- **Actionable Insights**: Prioritize addressing the critical gaps identified in the report to mitigate potential risks.\n\n## Integration\n\nThis skill can be integrated with other security and compliance tools to provide a holistic view of an application's security posture. It can also be used in conjunction with code generation tools to automatically implement recommended changes and improve GDPR compliance.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "gdpr-compliance-scanner",
        "category": "security",
        "path": "plugins/security/gdpr-compliance-scanner",
        "version": "1.0.0",
        "description": "Scan for GDPR compliance issues"
      },
      "filePath": "plugins/security/gdpr-compliance-scanner/skills/scanning-for-gdpr-compliance/SKILL.md"
    },
    {
      "slug": "scanning-for-secrets",
      "name": "scanning-for-secrets",
      "description": "Detect exposed secrets, API keys, and credentials in code. Use when auditing for secret leaks. Trigger with 'scan for secrets', 'find exposed keys', or 'check credentials'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Secret Scanner\n\nThis skill provides automated assistance for secret scanner tasks.\n\n## Overview\n\nThis skill enables Claude to scan your codebase for exposed secrets, API keys, passwords, and other sensitive credentials. It helps you identify and remediate potential security vulnerabilities before they are committed or deployed.\n\n## How It Works\n\n1. **Initiate Scan**: Claude activates the `secret-scanner` plugin.\n2. **Codebase Analysis**: The plugin scans the codebase using pattern matching and entropy analysis.\n3. **Report Generation**: A detailed report is generated, highlighting identified secrets, their locations, and suggested remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Scan your codebase for exposed API keys (e.g., AWS, Google, Azure).\n- Check for hardcoded passwords in configuration files.\n- Identify potential private keys (SSH, PGP) accidentally committed to the repository.\n- Proactively find secrets before committing changes.\n\n## Examples\n\n### Example 1: Identifying Exposed AWS Keys\n\nUser request: \"Scan for AWS keys in the codebase\"\n\nThe skill will:\n1. Activate the `secret-scanner` plugin.\n2. Scan the codebase for patterns matching AWS Access Keys (AKIA[0-9A-Z]{16}).\n3. Generate a report listing any found keys, their file locations, and remediation steps (e.g., revoking the key).\n\n### Example 2: Checking for Hardcoded Passwords\n\nUser request: \"Check for exposed credentials in config files\"\n\nThe skill will:\n1. Activate the `secret-scanner` plugin.\n2. Scan configuration files (e.g., `database.yml`, `.env`) for password patterns.\n3. Generate a report detailing any found passwords and suggesting the use of environment variables.\n\n## Best Practices\n\n- **Regular Scanning**: Schedule regular scans to catch newly introduced secrets.\n- **Pre-Commit Hooks**: Integrate the `secret-scanner` into your pre-commit hooks to prevent committing secrets.\n- **Review Entropy Analysis**: Carefully review results from entropy analysis, as they may indicate potential secrets not caught by pattern matching.\n\n## Integration\n\nThis skill can be integrated with other security tools, such as vulnerability scanners, to provide a comprehensive security assessment of your codebase. It can also be combined with notification plugins to alert you when new secrets are detected.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "secret-scanner",
        "category": "security",
        "path": "plugins/security/secret-scanner",
        "version": "1.0.0",
        "description": "Scan codebase for exposed secrets, API keys, passwords, and sensitive credentials"
      },
      "filePath": "plugins/security/secret-scanner/skills/scanning-for-secrets/SKILL.md"
    },
    {
      "slug": "scanning-for-vulnerabilities",
      "name": "scanning-for-vulnerabilities",
      "description": "This skill enables comprehensive vulnerability scanning using the vulnerability-scanner plugin. it identifies security vulnerabilities in code, dependencies, and configurations, including cve detection. use this skill when the user asks to scan fo... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Vulnerability Scanner\n\nThis skill provides automated assistance for vulnerability scanner tasks.\n\n## Overview\n\nThis skill empowers Claude to automatically scan your codebase for security vulnerabilities. It leverages the vulnerability-scanner plugin to identify potential risks, including code-level flaws, vulnerable dependencies, and insecure configurations.\n\n## How It Works\n\n1. **Initiate Scan**: The skill activates the vulnerability-scanner plugin based on user input.\n2. **Perform Analysis**: The plugin scans the codebase, dependencies, and configurations for vulnerabilities, including CVE detection.\n3. **Generate Report**: The plugin creates a detailed vulnerability report with findings, severity levels, and remediation guidance.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify security vulnerabilities in your code.\n- Check your project's dependencies for known CVEs.\n- Review your project's configurations for security weaknesses.\n\n## Examples\n\n### Example 1: Identifying SQL Injection Risks\n\nUser request: \"Scan my code for SQL injection vulnerabilities.\"\n\nThe skill will:\n1. Activate the vulnerability-scanner plugin.\n2. Analyze the codebase for potential SQL injection flaws.\n3. Generate a report highlighting any identified SQL injection risks and providing remediation steps.\n\n### Example 2: Checking for Vulnerable npm Packages\n\nUser request: \"Check my project's npm dependencies for known vulnerabilities.\"\n\nThe skill will:\n1. Activate the vulnerability-scanner plugin.\n2. Scan the project's `package.json` file and identify any npm packages with known CVEs.\n3. Generate a report listing the vulnerable packages, their CVE identifiers, and recommended updates.\n\n## Best Practices\n\n- **Regular Scanning**: Run vulnerability scans regularly, especially before deployments.\n- **Prioritize Remediation**: Focus on addressing critical and high-severity vulnerabilities first.\n- **Validate Fixes**: After applying fixes, run another scan to ensure the vulnerabilities are resolved.\n\n## Integration\n\nThis skill integrates with the core Claude Code environment by providing automated vulnerability scanning capabilities. It can be used in conjunction with other plugins to create a comprehensive security workflow, such as integrating with a ticketing system to automatically create tickets for identified vulnerabilities.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "vulnerability-scanner",
        "category": "security",
        "path": "plugins/security/vulnerability-scanner",
        "version": "1.0.0",
        "description": "Comprehensive vulnerability scanning for code, dependencies, and configurations with CVE detection"
      },
      "filePath": "plugins/security/vulnerability-scanner/skills/scanning-for-vulnerabilities/SKILL.md"
    },
    {
      "slug": "scanning-for-xss-vulnerabilities",
      "name": "scanning-for-xss-vulnerabilities",
      "description": "This skill enables AI assistant to automatically scan for xss (cross-site scripting) vulnerabilities in code. it is triggered when the user requests to \"scan for xss vulnerabilities\", \"check for xss\", or uses the command \"/xss\". the skill identifies ref... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, WebFetch, WebSearch, Grep version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Xss Vulnerability Scanner\n\nThis skill provides automated assistance for xss vulnerability scanner tasks.\n\n## Overview\n\nThis skill empowers Claude to proactively identify and report XSS vulnerabilities within your codebase. By leveraging advanced detection techniques, including context-aware analysis and WAF bypass testing, this skill ensures your web applications are resilient against common XSS attack vectors. It provides detailed insights into vulnerability types and offers guidance on remediation strategies.\n\n## How It Works\n\n1. **Activation**: Claude recognizes the user's intent to scan for XSS vulnerabilities through specific trigger phrases like \"scan for XSS\" or the shortcut \"/xss\".\n2. **Code Analysis**: The plugin analyzes the codebase, identifying potential XSS vulnerabilities across different contexts (HTML, JavaScript, CSS, URL).\n3. **Vulnerability Detection**: The plugin detects reflected, stored, and DOM-based XSS vulnerabilities by injecting various payloads and analyzing the responses.\n4. **Reporting**: The plugin generates a report highlighting identified vulnerabilities, their location in the code, and recommended remediation steps.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Perform a security audit of your web application.\n- Review code for potential XSS vulnerabilities.\n- Ensure compliance with security standards.\n- Test the effectiveness of your Content Security Policy (CSP).\n- Identify and mitigate XSS vulnerabilities before deploying to production.\n\n## Examples\n\n### Example 1: Detecting Reflected XSS\n\nUser request: \"scan for XSS vulnerabilities in the search functionality\"\n\nThe skill will:\n1. Analyze the code related to the search functionality.\n2. Identify a reflected XSS vulnerability in how search queries are displayed.\n3. Report the vulnerability, including the affected code snippet and a suggested fix using proper sanitization.\n\n### Example 2: Identifying Stored XSS\n\nUser request: \"/xss check the comment submission form\"\n\nThe skill will:\n1. Analyze the comment submission form and its associated backend code.\n2. Detect a stored XSS vulnerability where user comments are saved to the database without sanitization.\n3. Report the vulnerability, highlighting the unsanitized comment storage and suggesting the use of a sanitization library like `sanitizeHtml`.\n\n## Best Practices\n\n- **Sanitization**: Always sanitize user input before displaying it on the page. Use appropriate escaping functions for the specific context (HTML, JavaScript, URL).\n- **Content Security Policy (CSP)**: Implement a strong CSP to restrict the sources from which the browser can load resources, mitigating the impact of XSS vulnerabilities.\n- **Regular Updates**: Keep your web application framework and libraries up to date to patch known XSS vulnerabilities.\n\n## Integration\n\nThis skill complements other security-focused plugins by providing targeted XSS vulnerability detection. It can be integrated with code review tools to automate security checks and provide developers with immediate feedback on potential XSS issues.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "xss-vulnerability-scanner",
        "category": "security",
        "path": "plugins/security/xss-vulnerability-scanner",
        "version": "1.0.0",
        "description": "Scan for XSS vulnerabilities"
      },
      "filePath": "plugins/security/xss-vulnerability-scanner/skills/scanning-for-xss-vulnerabilities/SKILL.md"
    },
    {
      "slug": "scanning-input-validation-practices",
      "name": "scanning-input-validation-practices",
      "description": "Scan for input validation vulnerabilities and injection risks. Use when reviewing user input handling. Trigger with 'scan input validation', 'check injection vulnerabilities', or 'validate sanitization'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Input Validation Scanner\n\nThis skill provides automated assistance for input validation scanner tasks.\n\n## Overview\n\nThis skill automates the process of identifying potential input validation flaws within a codebase. By analyzing how user-provided data is handled, it helps developers proactively address security vulnerabilities before they can be exploited. This skill streamlines security audits and improves the overall security posture of applications.\n\n## How It Works\n\n1. **Initiate Scan**: The user requests an input validation scan, triggering the skill.\n2. **Code Analysis**: The skill uses the input-validation-scanner plugin to analyze the specified codebase or file.\n3. **Vulnerability Identification**: The plugin identifies instances where input validation may be missing or insufficient.\n4. **Report Generation**: The skill presents a report highlighting potential vulnerabilities and their locations in the code.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Audit a codebase for input validation vulnerabilities.\n- Review newly written code for potential XSS or SQL injection flaws.\n- Harden an application against common web security exploits.\n- Ensure compliance with security best practices related to input handling.\n\n## Examples\n\n### Example 1: Identifying XSS Vulnerabilities\n\nUser request: \"Scan the user profile module for potential XSS vulnerabilities.\"\n\nThe skill will:\n1. Activate the input-validation-scanner plugin on the specified module.\n2. Generate a report highlighting areas where user input is directly rendered without proper sanitization, indicating potential XSS vulnerabilities.\n\n### Example 2: Checking for SQL Injection Risks\n\nUser request: \"Check the database access layer for potential SQL injection risks.\"\n\nThe skill will:\n1. Use the input-validation-scanner plugin to examine the database access code.\n2. Identify instances where user input is used directly in SQL queries without proper parameterization or escaping, indicating potential SQL injection vulnerabilities.\n\n## Best Practices\n\n- **Regular Scanning**: Integrate input validation scanning into your regular development workflow.\n- **Contextual Analysis**: Always review the identified vulnerabilities in context to determine their actual impact and severity.\n- **Comprehensive Validation**: Ensure that all user-supplied data is validated, including data from forms, APIs, and external sources.\n\n## Integration\n\nThis skill can be used in conjunction with other security-related skills to provide a more comprehensive security assessment. For example, it can be combined with a static analysis skill to identify other types of vulnerabilities or with a dependency scanning skill to identify vulnerable third-party libraries.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "input-validation-scanner",
        "category": "security",
        "path": "plugins/security/input-validation-scanner",
        "version": "1.0.0",
        "description": "Scan input validation practices"
      },
      "filePath": "plugins/security/input-validation-scanner/skills/scanning-input-validation-practices/SKILL.md"
    },
    {
      "slug": "scanning-market-movers",
      "name": "scanning-market-movers",
      "description": "Detect significant price movements and unusual volume across crypto markets. Use when tracking significant price movements. Trigger with phrases like \"scan market movers\", \"check biggest gainers\", or \"find pumps\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:movers-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Market Movers Scanner\n\nThis skill provides automated assistance for market movers scanner tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:movers-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "market-movers-scanner",
        "category": "crypto",
        "path": "plugins/crypto/market-movers-scanner",
        "version": "1.0.0",
        "description": "Scan for top market movers - gainers, losers, volume spikes, and unusual activity"
      },
      "filePath": "plugins/crypto/market-movers-scanner/skills/scanning-market-movers/SKILL.md"
    },
    {
      "slug": "setting-up-distributed-tracing",
      "name": "setting-up-distributed-tracing",
      "description": "This skill automates the setup of distributed tracing for microservices. it helps developers implement end-to-end request visibility by configuring context propagation, span creation, trace collection, and analysis. use this skill when the user re... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Distributed Tracing Setup\n\nThis skill provides automated assistance for distributed tracing setup tasks.\n\n## Overview\n\nThis skill streamlines the process of setting up distributed tracing in a microservices environment. It guides you through the key steps of instrumenting your services, configuring trace context propagation, and selecting a backend for trace collection and analysis, enabling comprehensive monitoring and debugging.\n\n## How It Works\n\n1. **Backend Selection**: Determines the preferred tracing backend (e.g., Jaeger, Zipkin, Datadog).\n2. **Instrumentation Strategy**: Designs an instrumentation strategy for each service, focusing on key operations and dependencies.\n3. **Configuration Generation**: Generates the necessary configuration files and code snippets to enable distributed tracing.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement distributed tracing in a microservices application.\n- Gain end-to-end visibility into request flows across multiple services.\n- Troubleshoot performance bottlenecks and latency issues.\n\n## Examples\n\n### Example 1: Adding Tracing to a New Microservice\n\nUser request: \"setup tracing for the new payment service\"\n\nThe skill will:\n1. Prompt for the preferred tracing backend (e.g., Jaeger).\n2. Generate code snippets for OpenTelemetry instrumentation in the payment service.\n\n### Example 2: Troubleshooting Performance Issues\n\nUser request: \"implement distributed tracing to debug slow checkout process\"\n\nThe skill will:\n1. Guide the user through instrumenting relevant services in the checkout flow.\n2. Provide configuration examples for context propagation.\n\n## Best Practices\n\n- **Backend Choice**: Select a tracing backend that aligns with your existing infrastructure and monitoring tools.\n- **Sampling Strategy**: Implement a sampling strategy to manage trace volume and cost, especially in high-traffic environments.\n- **Context Propagation**: Ensure proper context propagation across all services to maintain trace continuity.\n\n## Integration\n\nThis skill can be used in conjunction with other plugins to automate the deployment and configuration of tracing infrastructure. For example, it can integrate with infrastructure-as-code tools to provision Jaeger or Zipkin clusters.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "distributed-tracing-setup",
        "category": "performance",
        "path": "plugins/performance/distributed-tracing-setup",
        "version": "1.0.0",
        "description": "Set up distributed tracing for microservices"
      },
      "filePath": "plugins/performance/distributed-tracing-setup/skills/setting-up-distributed-tracing/SKILL.md"
    },
    {
      "slug": "setting-up-experiment-tracking",
      "name": "setting-up-experiment-tracking",
      "description": "Implement machine learning experiment tracking using MLflow or Weights & Biases. Configures environment and provides code for logging parameters, metrics, and artifacts. Use when asked to \"setup experiment tracking\" or \"initialize MLflow\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Experiment Tracking Setup\n\nThis skill provides automated assistance for experiment tracking setup tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for experiment tracking setup tasks.\nThis skill streamlines the process of setting up experiment tracking for machine learning projects. It automates environment configuration, tool initialization, and provides code examples to get you started quickly.\n\n## How It Works\n\n1. **Analyze Context**: The skill analyzes the current project context to determine the appropriate experiment tracking tool (MLflow or W&B) based on user preference or existing project configuration.\n2. **Configure Environment**: It configures the environment by installing necessary Python packages and setting environment variables.\n3. **Initialize Tracking**: The skill initializes the chosen tracking tool, potentially starting a local MLflow server or connecting to a W&B project.\n4. **Provide Code Snippets**: It provides code snippets demonstrating how to log experiment parameters, metrics, and artifacts within your ML code.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Start tracking machine learning experiments in a new project.\n- Integrate experiment tracking into an existing ML project.\n- Quickly set up MLflow or Weights & Biases for experiment management.\n- Automate the process of logging parameters, metrics, and artifacts.\n\n## Examples\n\n### Example 1: Starting a New Project with MLflow\n\nUser request: \"track experiments using mlflow\"\n\nThe skill will:\n1. Install the `mlflow` Python package.\n2. Generate example code for logging parameters, metrics, and artifacts to an MLflow server.\n\n### Example 2: Integrating W&B into an Existing Project\n\nUser request: \"setup experiment tracking with wandb\"\n\nThe skill will:\n1. Install the `wandb` Python package.\n2. Generate example code for initializing W&B and logging experiment data.\n\n## Best Practices\n\n- **Tool Selection**: Consider the scale and complexity of your project when choosing between MLflow and W&B. MLflow is well-suited for local tracking, while W&B offers cloud-based collaboration and advanced features.\n- **Consistent Logging**: Establish a consistent logging strategy for parameters, metrics, and artifacts to ensure comparability across experiments.\n- **Artifact Management**: Utilize artifact logging to track models, datasets, and other relevant files associated with each experiment.\n\n## Integration\n\nThis skill can be used in conjunction with other skills that generate or modify machine learning code, such as skills for model training or data preprocessing. It ensures that all experiments are properly tracked and documented.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "experiment-tracking-setup",
        "category": "ai-ml",
        "path": "plugins/ai-ml/experiment-tracking-setup",
        "version": "1.0.0",
        "description": "Set up ML experiment tracking"
      },
      "filePath": "plugins/ai-ml/experiment-tracking-setup/skills/setting-up-experiment-tracking/SKILL.md"
    },
    {
      "slug": "setting-up-log-aggregation",
      "name": "setting-up-log-aggregation",
      "description": "Use when setting up log aggregation solutions using ELK, Loki, or Splunk. Trigger with phrases like \"setup log aggregation\", \"deploy ELK stack\", \"configure Loki\", or \"install Splunk\". Generates production-ready configurations for data ingestion, processing, storage, and visualization with proper security and scalability. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(docker:*), Bash(kubectl:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Log Aggregation Setup\n\nThis skill provides automated assistance for log aggregation setup tasks.\n\n## Overview\n\nSets up centralized log aggregation (ELK/Loki/Splunk) including ingestion pipelines, parsing, retention policies, dashboards, and security controls.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Target infrastructure is identified (Kubernetes, Docker, VMs)\n- Storage requirements are calculated based on log volume\n- Network connectivity between log sources and aggregation platform\n- Authentication mechanism is defined (LDAP, OAuth, basic auth)\n- Resource allocation planned (CPU, memory, disk)\n\n## Instructions\n\n1. **Select Platform**: Choose ELK, Loki, Grafana Loki, or Splunk\n2. **Configure Ingestion**: Set up log shippers (Filebeat, Promtail, Fluentd)\n3. **Define Storage**: Configure retention policies and index lifecycle\n4. **Set Up Processing**: Create parsing rules and field extractions\n5. **Deploy Visualization**: Configure Kibana/Grafana dashboards\n6. **Implement Security**: Enable authentication, encryption, and RBAC\n7. **Test Pipeline**: Verify logs flow from sources to visualization\n\n## Output\n\n**ELK Stack (Docker Compose):**\n```yaml\n# {baseDir}/elk/docker-compose.yml\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\nversion: '3.8'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=true\n    volumes:\n      - es-data:/usr/share/elasticsearch/data\n    ports:\n      - \"9200:9200\"\n\n  logstash:\n    image: docker.elastic.co/logstash/logstash:8.11.0\n    volumes:\n      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf\n    depends_on:\n      - elasticsearch\n\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.11.0\n    ports:\n      - \"5601:5601\"\n    depends_on:\n      - elasticsearch\n```\n\n**Loki Configuration:**\n```yaml\n# {baseDir}/loki/loki-config.yaml\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n\ningester:\n  lifecycler:\n    ring:\n      kvstore:\n        store: inmemory\n      replication_factor: 1\n  chunk_idle_period: 5m\n  chunk_retain_period: 30s\n\nschema_config:\n  configs:\n    - from: 2024-01-01\n      store: boltdb-shipper\n      object_store: filesystem\n      schema: v11\n      index:\n        prefix: index_\n        period: 24h\n```\n\n## Error Handling\n\n**Out of Memory**\n- Error: \"Elasticsearch heap space exhausted\"\n- Solution: Increase heap size in elasticsearch.yml or add more nodes\n\n**Connection Refused**\n- Error: \"Cannot connect to Elasticsearch\"\n- Solution: Verify network connectivity and firewall rules\n\n**Index Creation Failed**\n- Error: \"Failed to create index\"\n- Solution: Check disk space and index template configuration\n\n**Log Parsing Errors**\n- Error: \"Failed to parse log line\"\n- Solution: Review grok patterns or JSON parsing configuration\n\n## Examples\n\n- \"Deploy Loki + Promtail on Kubernetes with 14-day retention and basic auth.\"\n- \"Set up an ELK stack for app + nginx logs and create a dashboard for 5xx errors.\"\n\n## Resources\n\n- ELK Stack guide: https://www.elastic.co/guide/\n- Loki documentation: https://grafana.com/docs/loki/\n- Example configurations in {baseDir}/log-aggregation-examples/",
      "parentPlugin": {
        "name": "log-aggregation-setup",
        "category": "devops",
        "path": "plugins/devops/log-aggregation-setup",
        "version": "1.0.0",
        "description": "Set up log aggregation (ELK, Loki, Splunk)"
      },
      "filePath": "plugins/devops/log-aggregation-setup/skills/setting-up-log-aggregation/SKILL.md"
    },
    {
      "slug": "setting-up-synthetic-monitoring",
      "name": "setting-up-synthetic-monitoring",
      "description": "Setup synthetic monitoring for proactive performance tracking including uptime checks, transaction monitoring, and API health. Use when implementing availability monitoring or tracking critical user journeys. Trigger with phrases like \"setup synthetic monitoring\", \"monitor uptime\", or \"configure health checks\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(curl:*)",
        "Bash(monitoring:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Synthetic Monitoring Setup\n\nThis skill provides automated assistance for synthetic monitoring setup tasks.\n\n## Overview\n\nThis skill streamlines the process of setting up synthetic monitoring, enabling proactive performance tracking for applications. It guides the user through defining key monitoring scenarios and configuring alerts to ensure optimal application performance and availability.\n\n## How It Works\n\n1. **Identify Monitoring Needs**: Determine the critical endpoints, user journeys, and APIs to monitor based on the user's application requirements.\n2. **Design Monitoring Scenarios**: Create specific monitoring scenarios for uptime, transactions, and API performance, including frequency and location.\n3. **Configure Monitoring**: Set up the synthetic monitoring tool with the designed scenarios, including alerts and dashboards for performance visualization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Implement uptime monitoring for a web application.\n- Track the performance of critical user journeys through transaction monitoring.\n- Monitor the response time and availability of API endpoints.\n\n## Examples\n\n### Example 1: Setting up Uptime Monitoring\n\nUser request: \"Set up uptime monitoring for my website example.com.\"\n\nThe skill will:\n1. Identify example.com as the target endpoint.\n2. Configure uptime monitoring to check the availability of example.com every 5 minutes from multiple locations.\n\n### Example 2: Monitoring API Performance\n\nUser request: \"Configure API monitoring for the /users endpoint of my application.\"\n\nThe skill will:\n1. Identify the /users endpoint as the target for API monitoring.\n2. Set up monitoring to track the response time and status code of the /users endpoint every minute.\n\n## Best Practices\n\n- **Prioritize Critical Endpoints**: Focus on monitoring the most critical endpoints and user journeys that directly impact user experience.\n- **Set Realistic Thresholds**: Configure alerts with realistic thresholds to avoid false positives and ensure timely notifications.\n- **Regularly Review and Adjust**: Periodically review the monitoring configuration and adjust scenarios and thresholds based on application changes and performance trends.\n\n## Integration\n\nThis skill can be integrated with other plugins for incident management and alerting, such as those that handle notifications via Slack or PagerDuty, allowing for automated incident response workflows based on synthetic monitoring results.\n\n## Prerequisites\n\n- Access to synthetic monitoring platform (Pingdom, Datadog, New Relic)\n- List of critical endpoints and user journeys in {baseDir}/monitoring/endpoints.yaml\n- Alerting infrastructure configuration\n- Geographic monitoring location requirements\n\n## Instructions\n\n1. Identify critical endpoints and user journeys to monitor\n2. Design monitoring scenarios (uptime, transactions, API checks)\n3. Configure monitoring frequency and locations\n4. Set up performance and availability thresholds\n5. Configure alerting for failures and degradation\n6. Create dashboards for monitoring visualization\n\n## Output\n\n- Synthetic monitoring configuration files\n- Uptime check definitions for endpoints\n- Transaction monitoring scripts\n- Alert rule configurations\n- Dashboard definitions for monitoring status\n\n## Error Handling\n\nIf synthetic monitoring setup fails:\n- Verify monitoring platform credentials\n- Check endpoint accessibility from monitoring locations\n- Validate transaction script syntax\n- Ensure alert channel configuration\n- Review threshold definitions\n\n## Resources\n\n- Synthetic monitoring best practices\n- Uptime monitoring service documentation\n- Transaction monitoring script examples\n- Alert threshold tuning guides",
      "parentPlugin": {
        "name": "synthetic-monitoring-setup",
        "category": "performance",
        "path": "plugins/performance/synthetic-monitoring-setup",
        "version": "1.0.0",
        "description": "Set up synthetic monitoring for proactive performance tracking"
      },
      "filePath": "plugins/performance/synthetic-monitoring-setup/skills/setting-up-synthetic-monitoring/SKILL.md"
    },
    {
      "slug": "simulating-flash-loans",
      "name": "simulating-flash-loans",
      "description": "Simulate flash loan arbitrage strategies and profitability across DeFi protocols. Use when performing crypto analysis. Trigger with phrases like \"analyze crypto\", \"check blockchain\", or \"monitor market\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:flashloan-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Flash Loan Simulator\n\nThis skill provides automated assistance for flash loan simulator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:flashloan-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "flash-loan-simulator",
        "category": "crypto",
        "path": "plugins/crypto/flash-loan-simulator",
        "version": "1.0.0",
        "description": "Simulate and analyze flash loan strategies including arbitrage, liquidations, and collateral swaps"
      },
      "filePath": "plugins/crypto/flash-loan-simulator/skills/simulating-flash-loans/SKILL.md"
    },
    {
      "slug": "skill-adapter",
      "name": "skill-adapter",
      "description": "Analyzes existing plugins to extract their capabilities, then adapts and applies those skills to the current task. Acts as a universal skill chameleon that learns from other plugins. Activates when you request \"skill adapter\" functionality. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Skill Adapter - Universal Plugin Capability Extractor\n\n\n\nThis skill provides automated assistance for pi pathfinder tasks.\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## Purpose\nAnalyzes plugins in the claude-code-plugins marketplace to understand their capabilities, extracts the core patterns and approaches, then adapts those skills to solve the current user's task. Acts as a \"skill chameleon\" that can adopt any plugin's capabilities.\n\n## How It Works\n\n### 1. Task Analysis\nWhen user presents a task:\n- Identify the core capability needed (e.g., \"analyze code quality\", \"generate documentation\", \"automate deployment\")\n- Determine the domain (security, devops, testing, etc.)\n- Extract key requirements and constraints\n\n### 2. Plugin Discovery\nSearch existing plugins for relevant capabilities:\n\n```bash\n# Find plugins in relevant category\nls plugins/community/ plugins/packages/ plugins/examples/\n\n# Search for keywords in plugin descriptions\ngrep -r \"keyword\" --include=\"plugin.json\" plugins/\n\n# Find similar commands/agents\ngrep -r \"capability-name\" --include=\"*.md\" plugins/\n```\n\n### 3. Capability Extraction\n\nFor each relevant plugin found, analyze:\n\n**Commands (commands/*.md):**\n- Read the markdown content\n- Extract the approach/methodology\n- Identify input/output patterns\n- Note any scripts or tools used\n\n**Agents (agents/*.md):**\n- Understand the agent's role\n- Extract problem-solving approach\n- Note decision-making patterns\n- Identify expertise areas\n\n**Skills (skills/*/SKILL.md):**\n- Read the skill instructions\n- Extract core capability\n- Note trigger conditions\n- Understand tool usage patterns\n\n**Scripts (scripts/*.sh, *.py):**\n- Analyze script logic\n- Extract reusable patterns\n- Identify best practices\n- Note error handling approaches\n\n### 4. Pattern Synthesis\n\nCombine learned patterns:\n- Merge multiple approaches if beneficial\n- Adapt to current context and constraints\n- Simplify or enhance based on user needs\n- Ensure compatibility with current environment\n\n### 5. Skill Application\n\nApply the adapted skill:\n- Use the learned approach\n- Follow the extracted patterns\n- Apply best practices discovered\n- Adapt syntax/tools to current context\n\n## Example Workflows\n\n### Example 1: Learning Code Analysis from Security Plugins\n\n**User task:** \"Analyze this codebase for issues\"\n\n**Process:**\n1. Search for security and code-analysis plugins\n2. Find: `owasp-top-10-scanner`, `code-quality-enforcer`, `security-audit-agent`\n3. Extract patterns:\n   - OWASP scanner checks for: SQL injection, XSS, CSRF, auth issues\n   - Quality enforcer looks at: complexity, duplication, standards\n   - Audit agent examines: dependencies, secrets, permissions\n4. Synthesize approach:\n   - Run multi-layer analysis\n   - Check security patterns first\n   - Then code quality metrics\n   - Then dependency issues\n5. Apply to user's codebase with adapted checks\n\n### Example 2: Adopting Documentation Skills\n\n**User task:** \"Generate API documentation\"\n\n**Process:**\n1. Find documentation plugins\n2. Discover: `api-documenter`, `openapi-generator`, `readme-builder`\n3. Extract approaches:\n   - API documenter: parses code, generates OpenAPI spec\n   - OpenAPI generator: creates interactive docs\n   - README builder: structures documentation hierarchically\n4. Synthesize:\n   - Parse code for endpoints\n   - Generate OpenAPI/Swagger spec\n   - Create interactive documentation\n   - Build comprehensive README\n5. Apply combined approach to user's API\n\n### Example 3: Learning Automation from DevOps Plugins\n\n**User task:** \"Automate deployment process\"\n\n**Process:**\n1. Search DevOps category\n2. Find: `deployment-automation`, `ci-cd-pipeline`, `docker-compose-generator`\n3. Extract patterns:\n   - Deployment automation: build ‚Üí test ‚Üí deploy ‚Üí verify\n   - CI/CD pipeline: trigger conditions, parallel jobs, rollback\n   - Docker compose: service orchestration, environment management\n4. Synthesize deployment workflow\n5. Apply to user's specific tech stack\n\n## Reasoning Process\n\n### When to Use Skill Adapter\n\nTrigger when:\n- User needs capability that might exist in marketplace\n- Task could benefit from existing plugin patterns\n- User asks: \"Is there a plugin for this?\"\n- Similar problems have been solved before\n- Multiple approaches could be combined\n\n### Plugin Selection Criteria\n\nChoose plugins based on:\n1. **Relevance**: Matches task domain/requirements\n2. **Quality**: Well-documented, clear approach\n3. **Simplicity**: Not overly complex for the task\n4. **Recency**: Updated plugins preferred\n5. **Popularity**: Featured or well-maintained plugins\n\n### Adaptation Strategy\n\nWhen adapting skills:\n- **Keep**: Core logic and proven patterns\n- **Adapt**: Syntax, tool names, specific commands\n- **Enhance**: Add error handling, user feedback\n- **Simplify**: Remove unnecessary complexity\n- **Contextualize**: Adjust to user's environment\n\n## Limitations and Boundaries\n\n### What Skill Adapter CAN Do:\n‚úÖ Read and analyze plugin source code\n‚úÖ Extract patterns and approaches\n‚úÖ Adapt methodologies to new contexts\n‚úÖ Combine multiple plugin capabilities\n‚úÖ Apply learned skills with reasoning\n\n### What Skill Adapter CANNOT Do:\n‚ùå Execute compiled code (MCP servers)\n‚ùå Access external APIs without credentials\n‚ùå Modify original plugins\n‚ùå Guarantee exact plugin behavior replication\n‚ùå Use plugins that require specific environment setup\n\n## Success Criteria\n\nSkill adaptation is successful when:\n1. User's task is completed effectively\n2. Approach borrowed makes logical sense\n3. Adapted skill is properly contextualized\n4. User understands where the approach came from\n5. Result quality matches or exceeds original plugin\n\n## Transparency\n\nAlways inform user:\n- Which plugins were analyzed\n- What patterns were extracted\n- How the skill was adapted\n- Why this approach was chosen\n- Any limitations of the adaptation\n\n## Example Usage\n\n```\nUser: \"I need to validate JSON schemas in my project\"\n\nSkill Adapter Process:\n1. Searches plugins for JSON validation\n2. Finds: schema-validator, json-lint-enforcer\n3. Extracts: ajv library usage, error formatting patterns\n4. Adapts: Uses available tools (jq, node, python)\n5. Applies: Validates user's schemas with detailed errors\n6. Reports: \"I adapted the schema-validator approach using jq\n   for validation and added custom error formatting\"\n```\n\n## Meta-Learning\n\nSkill Adapter improves by:\n- Tracking which plugins solve which tasks best\n- Learning which patterns are most reusable\n- Noting which adaptations work well\n- Building a mental model of the marketplace\n- Understanding plugin ecosystem relationships\n\n---\n\n**In essence:** Skill Adapter is a meta-skill that makes the entire plugin marketplace available as a learning resource, extracting and applying capabilities on-demand to solve user tasks efficiently.",
      "parentPlugin": {
        "name": "pi-pathfinder",
        "category": "examples",
        "path": "plugins/examples/pi-pathfinder",
        "version": "1.0.0",
        "description": "PI Pathfinder - Finds the path through 229 plugins. Automatically picks the best plugin for your task, extracts its skills, and applies them. You don't pick plugins, PI does."
      },
      "filePath": "plugins/examples/pi-pathfinder/skills/skill-adapter/SKILL.md"
    },
    {
      "slug": "spec-writing",
      "name": "spec-writing",
      "description": "This skill should be used when the user asks about \"writing specs\", \"specs.md format\", \"how to write specifications\", \"sprint requirements\", \"testing configuration\", \"scope definition\", or needs guidance on creating effective sprint specifications for agentic development. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read version: 1.0.0 author: Damien Laine <damien.laine@gmail.com> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Spec Writing\n\nWrite clear `specs.md` sprint specifications that drive autonomous agent execution. Use this when defining scope, requirements, and testing expectations for a sprint.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## The specs.md File\n\nLocated at `.claude/sprint/[N]/specs.md`, this file is the primary input to the sprint system. It tells the architect what to build and how to test it.\n\n## Essential Structure\n\nA complete specs.md contains:\n\n```markdown\n# Sprint [N]: [Short Title]\n\n## Goal\n[1-2 sentences describing what success looks like]\n\n## Scope\n\n### In Scope\n- [Feature or task 1]\n- [Feature or task 2]\n\n### Out of Scope\n- [What NOT to do]\n\n## Requirements\n[Detailed requirements - can be minimal or extensive]\n\n## Testing\n- QA: required / optional / skip\n- UI Testing: required / optional / skip\n- UI Testing Mode: automated / manual\n```\n\n## Writing Effective Goals\n\nThe goal statement shapes the entire sprint. Make it outcome-focused:\n\n**Good goals:**\n- \"Users can register, login, and reset passwords\"\n- \"API returns paginated product listings with filters\"\n- \"Dashboard displays real-time metrics from backend\"\n\n**Bad goals:**\n- \"Implement authentication\" (too vague)\n- \"Fix the bug\" (which bug?)\n- \"Make it better\" (unmeasurable)\n\nA good goal answers: \"How will we know when we're done?\"\n\n## Defining Scope\n\n### In Scope\n\nList concrete deliverables:\n- Features to implement\n- Endpoints to create\n- UI components to build\n- Tests to write\n\nBe specific. \"Add user profile page\" is better than \"improve user experience\".\n\n### Out of Scope\n\nExplicitly exclude work to prevent scope creep:\n- Related features not in this sprint\n- Refactoring not needed for the goal\n- Nice-to-haves for future sprints\n\nThis prevents agents from over-engineering or adding unrequested features.\n\n## Requirements Depth\n\nSpecs can range from minimal to detailed. The architect adapts accordingly.\n\n### Minimal Spec (One-liner)\n\n```markdown\n# Sprint 5: Add dark mode toggle\n\n## Goal\nUsers can switch between light and dark themes.\n\n## Testing\n- UI Testing: required\n- UI Testing Mode: manual\n```\n\nAppropriate for: Simple features, trusted architect judgment, exploratory work.\n\n### Detailed Spec\n\n```markdown\n# Sprint 12: User Authentication\n\n## Goal\nComplete authentication flow with email verification.\n\n## Scope\n\n### In Scope\n- Registration with email/password\n- Login with session management\n- Password reset via email\n- Email verification flow\n- Protected route middleware\n\n### Out of Scope\n- OAuth providers (future sprint)\n- Two-factor authentication\n- Account deletion\n\n## Requirements\n\n### Registration\n- Email must be unique\n- Password minimum 8 characters\n- Send verification email on signup\n- Users cannot login until verified\n\n### Login\n- Return JWT token on success\n- Rate limit: 5 attempts per minute\n- Lock account after 10 failed attempts\n\n### Password Reset\n- Token expires after 1 hour\n- Invalidate token after use\n- Send confirmation email after reset\n\n## API Endpoints\n\n| Method | Route | Purpose |\n|--------|-------|---------|\n| POST | /auth/register | Create account |\n| POST | /auth/login | Authenticate |\n| POST | /auth/verify | Verify email |\n| POST | /auth/reset-request | Request reset |\n| POST | /auth/reset | Reset password |\n\n## Testing\n- QA: required\n- UI Testing: required\n- UI Testing Mode: automated\n```\n\nAppropriate for: Complex features, specific requirements, team handoffs.\n\n## Testing Configuration\n\nThe Testing section controls which testing agents run.\n\n### Options\n\n| Setting | Values | Meaning |\n|---------|--------|---------|\n| QA | required / optional / skip | API and unit tests |\n| UI Testing | required / optional / skip | Browser-based E2E tests |\n| UI Testing Mode | automated / manual | Auto-run or user-driven |\n\n### When to Use Each\n\n**QA: required**\n- New API endpoints\n- Business logic changes\n- Data validation rules\n\n**QA: skip**\n- Frontend-only changes\n- Documentation updates\n- Configuration changes\n\n**UI Testing: required**\n- User-facing features\n- Form submissions\n- Navigation flows\n\n**UI Testing Mode: manual**\n- Complex interactions\n- Visual verification needed\n- Exploratory testing\n\n**UI Testing Mode: automated**\n- Regression testing\n- Standard CRUD flows\n- Repeatable scenarios\n\n## Common Patterns\n\n### New Feature\n\n```markdown\n## Goal\n[What the feature does]\n\n## Scope\n### In Scope\n- Backend API\n- Frontend UI\n- Integration tests\n\n## Testing\n- QA: required\n- UI Testing: required\n- UI Testing Mode: automated\n```\n\n### Bug Fix\n\n```markdown\n## Goal\nFix [specific issue description]\n\n## Root Cause\n[If known, describe the cause]\n\n## Expected Behavior\n[What should happen]\n\n## Testing\n- QA: required  # Regression test\n- UI Testing: optional\n```\n\n### Refactoring\n\n```markdown\n## Goal\nRefactor [component] for [benefit]\n\n## Constraints\n- No behavior changes\n- Maintain API compatibility\n- All existing tests must pass\n\n## Testing\n- QA: required  # Verify no regressions\n- UI Testing: skip\n```\n\n## Tips for Better Specs\n\n### Be Specific, Not Prescriptive\n\nTell the architect WHAT to build, not HOW to build it:\n- Good: \"Users can filter products by category and price range\"\n- Bad: \"Use a Redux slice with useSelector for filter state\"\n\nThe architect chooses implementation details.\n\n### Include Edge Cases\n\nMention important edge cases in requirements:\n- Empty states\n- Error conditions\n- Boundary values\n- Concurrent access\n\n### Reference Existing Patterns\n\nIf the codebase has conventions, mention them:\n- \"Follow the existing auth middleware pattern\"\n- \"Use the same validation approach as user endpoints\"\n\n### Keep It Maintainable\n\nSpecs should be readable by humans too:\n- Use clear headings\n- Keep bullet points short\n- Include examples where helpful\n\n## Iteration and Updates\n\nSpecs evolve during the sprint:\n\n1. **Initial**: User writes complete specs.md\n2. **Phase 1**: Architect may clarify or expand\n3. **Each iteration**: Architect removes completed items\n4. **Final**: Specs reflect only documented decisions\n\nThis convergent pattern keeps context focused.\n\n## Additional Resources\n\nThe `/sprint:new` command creates specs.md templates interactively. See the command file for implementation details.",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/spec-writing/SKILL.md"
    },
    {
      "slug": "splitting-datasets",
      "name": "splitting-datasets",
      "description": "Split datasets into training, validation, and testing sets for ML model development. Use when requesting \"split dataset\", \"train-test split\", or \"data partitioning\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Dataset Splitter\n\nThis skill provides automated assistance for dataset splitter tasks.\n\n## Overview\n\nThis skill automates the process of dividing a dataset into subsets for training, validating, and testing machine learning models. It ensures proper data preparation and facilitates robust model evaluation.\n\n## How It Works\n\n1. **Analyze Request**: The skill analyzes the user's request to determine the dataset to be split and the desired proportions for each subset.\n2. **Generate Code**: Based on the request, the skill generates Python code utilizing standard ML libraries to perform the data splitting.\n3. **Execute Splitting**: The code is executed to split the dataset into training, validation, and testing sets according to the specified ratios.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Prepare a dataset for machine learning model training.\n- Create training, validation, and testing sets.\n- Partition data to evaluate model performance.\n\n## Examples\n\n### Example 1: Splitting a CSV file\n\nUser request: \"Split the data in 'my_data.csv' into 70% training, 15% validation, and 15% testing sets.\"\n\nThe skill will:\n1. Generate Python code to read the 'my_data.csv' file.\n2. Execute the code to split the data according to the specified proportions, creating 'train.csv', 'validation.csv', and 'test.csv' files.\n\n### Example 2: Creating a Train-Test Split\n\nUser request: \"Create a train-test split of 'large_dataset.csv' with an 80/20 ratio.\"\n\nThe skill will:\n1. Generate Python code to load 'large_dataset.csv'.\n2. Execute the code to split the dataset into 80% training and 20% testing sets, saving them as 'train.csv' and 'test.csv'.\n\n## Best Practices\n\n- **Data Integrity**: Verify that the splitting process maintains the integrity of the data, ensuring no data loss or corruption.\n- **Stratification**: Consider stratification when splitting imbalanced datasets to maintain class distributions in each subset.\n- **Randomization**: Ensure the splitting process is randomized to avoid bias in the resulting datasets.\n\n## Integration\n\nThis skill can be integrated with other data processing and model training tools within the Claude Code ecosystem to create a complete machine learning workflow.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "dataset-splitter",
        "category": "ai-ml",
        "path": "plugins/ai-ml/dataset-splitter",
        "version": "1.0.0",
        "description": "Split datasets for training, validation, and testing"
      },
      "filePath": "plugins/ai-ml/dataset-splitter/skills/splitting-datasets/SKILL.md"
    },
    {
      "slug": "sprint-workflow",
      "name": "sprint-workflow",
      "description": "This skill should be used when the user asks about \"how sprints work\", \"sprint phases\", \"iteration workflow\", \"convergent development\", \"sprint lifecycle\", \"when to use sprints\", or wants to understand the sprint execution model and its convergent diffusion approach. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read version: 1.0.0 author: Damien Laine <damien.laine@gmail.com> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Damien Laine <damien.laine@gmail.com>",
      "license": "MIT",
      "content": "# Sprint Workflow\n\nSprint implements a convergent development model where autonomous agents iteratively refine implementations until specifications are satisfied. This skill covers the execution lifecycle, phase transitions, and iteration patterns.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## Core Concept: Convergent Diffusion\n\nTraditional AI-assisted development suffers from context bloat - each iteration adds more information, compounding errors and noise. Sprint reverses this pattern:\n\n- **Start noisy**: Initial specs may be vague or incomplete\n- **Converge iteratively**: Each iteration removes completed work from specs\n- **Focus narrows**: Agents receive only remaining tasks, not history\n- **Signal improves**: Less noise means better output quality\n\nThe metaphor is diffusion models in reverse - instead of adding noise to generate, remove noise to refine.\n\n## Sprint Phases\n\nA sprint executes through 6 distinct phases:\n\n### Phase 0: Load Specifications\n\nParse the sprint directory and prepare context:\n- Locate sprint directory (`.claude/sprint/[N]/`)\n- Read `specs.md` for user requirements\n- Read `status.md` if resuming\n- Detect project type for framework-specific agents\n\n### Phase 1: Architectural Planning\n\nThe project-architect agent analyzes requirements:\n- Read existing `project-map.md` for architecture context\n- Read `project-goals.md` for business objectives\n- Create specification files (`api-contract.md`, `backend-specs.md`, etc.)\n- Return SPAWN REQUEST for implementation agents\n\n### Phase 2: Implementation\n\nSpawn implementation agents in parallel:\n- `python-dev` for Python/FastAPI backend\n- `nextjs-dev` for Next.js frontend\n- `cicd-agent` for CI/CD pipelines\n- `allpurpose-agent` for any other technology\n- Collect structured reports from each agent\n\n### Phase 3: Testing\n\nExecute testing agents:\n- `qa-test-agent` runs first (API and unit tests)\n- `ui-test-agent` runs after (browser-based E2E tests)\n- Framework-specific diagnostics agents run in parallel with UI tests\n- Collect test reports\n\n### Phase 4: Review & Iteration\n\nArchitect reviews all reports:\n- Analyze conformity status\n- Update specifications (remove completed, add fixes)\n- Update `status.md` with current state\n- Decide: more implementation, more testing, or finalize\n\n### Phase 5: Finalization\n\nSprint completion:\n- Final `status.md` summary\n- All specs in consistent state\n- **Clean up manual-test-report.md** (no longer relevant)\n- Signal FINALIZE to orchestrator\n\n## Resuming Sprints\n\nWhen running `/sprint` on an existing sprint:\n\n**If status.md shows COMPLETE:**\n- System asks: Run manual testing? Continue with fixes? Create new sprint?\n- Guides user to appropriate action\n\n**If status.md shows IN PROGRESS:**\n- If manual-test-report.md exists: Uses it to inform architect\n- If not: Offers to run manual testing first or continue\n\nThis ensures the user always knows where they are and what options they have.\n\n## Iteration Loop\n\nThe sprint cycles between phases 1-4 until complete:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                                         ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ Phase 1  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Phase 2          ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ Planning ‚îÇ    ‚îÇ Implementation   ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ       ‚ñ≤                   ‚îÇ            ‚îÇ\n‚îÇ       ‚îÇ                   ‚ñº            ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ Phase 4  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ Phase 3          ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ Review   ‚îÇ    ‚îÇ Testing          ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                                         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº (after max 5 iterations or success)\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ Phase 5  ‚îÇ\n    ‚îÇ Finalize ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Maximum 5 iterations**: The system pauses after 5 cycles to prevent infinite loops. User intervention may be needed for complex blockers.\n\n## When to Use Sprints\n\nSprints are ideal for:\n- Multi-component features (backend + frontend + tests)\n- Complex requirements needing architectural planning\n- Tasks requiring coordination between specialized agents\n- Incremental development with testing validation\n\nSprints are overkill for:\n- Simple bug fixes (use direct implementation)\n- Single-file changes\n- Documentation-only updates\n- Quick prototypes without testing needs\n\n## Key Artifacts\n\n### User-Created\n\n| File | Purpose |\n|------|---------|\n| `specs.md` | Requirements, scope, testing config |\n| `project-goals.md` | Business vision and objectives |\n\n### Architect-Created\n\n| File | Purpose |\n|------|---------|\n| `status.md` | Current sprint state (architect maintains) |\n| `project-map.md` | Technical architecture (architect maintains) |\n| `api-contract.md` | Shared interface between agents |\n| `*-specs.md` | Agent-specific implementation guidance |\n\n### Orchestrator-Created\n\n| File | Purpose |\n|------|---------|\n| `*-report-[N].md` | Agent reports per iteration |\n\n## Convergence Principles\n\n### Specs Shrink Over Time\n\nAfter each iteration, the architect:\n- Removes completed tasks from spec files\n- Removes outdated information\n- Keeps only remaining work\n\nThis prevents context bloat and focuses agents on actual gaps.\n\n### Status Stays Current\n\n`status.md` is rewritten each iteration, not appended:\n- Always reflects current truth\n- Maximum ~50 lines\n- No historical log dumps\n\n### Reports Are Structured\n\nAgents return machine-parseable reports:\n- Standard sections (CONFORMITY, DEVIATIONS, ISSUES)\n- No verbose prose\n- Actionable information only\n\n## Manual Testing\n\nThere are two ways to do manual testing:\n\n### Within a Sprint (specs-driven)\n\nSet `UI Testing Mode: manual` in your specs.md:\n```markdown\n## Testing\n- UI Testing: required\n- UI Testing Mode: manual\n```\n\nWhen the architect requests UI testing:\n1. Browser opens pointing to your app\n2. You explore the app manually\n3. Console errors are monitored in the background\n4. **Close the browser tab** when done testing\n5. Agent detects tab close and returns report\n6. Sprint continues with architect review\n\n### Standalone Testing (quick access)\n\nUse `/sprint:test` for quick testing outside of sprints:\n- Opens Chrome browser directly\n- Monitors errors while you explore\n- Say \"finish testing\" when done\n- **Report saved to `.claude/sprint/[N]/manual-test-report.md`**\n\n**Reports feed into sprints:** When you run `/sprint`, the architect reads your manual test report and prioritizes fixing the issues you discovered.\n\nUse manual testing for:\n- Exploratory testing before a sprint\n- Bug hunting and discovery\n- UX validation\n- Edge cases hard to automate\n\n## Additional Resources\n\nFor more details, see the full command and agent documentation in the plugin's `commands/` and `agents/` directories.",
      "parentPlugin": {
        "name": "sprint",
        "category": "community",
        "path": "plugins/community/sprint",
        "version": "1.0.0",
        "description": "Autonomous multi-agent development framework with spec-driven sprints and convergent iteration"
      },
      "filePath": "plugins/community/sprint/skills/sprint-workflow/SKILL.md"
    },
    {
      "slug": "supabase-advanced-troubleshooting",
      "name": "supabase-advanced-troubleshooting",
      "description": "Apply Supabase advanced debugging techniques for hard-to-diagnose issues. Use when standard troubleshooting fails, investigating complex race conditions, or preparing evidence bundles for Supabase support escalation. Trigger with phrases like \"supabase hard bug\", \"supabase mystery error\", \"supabase impossible to debug\", \"difficult supabase issue\", \"supabase deep debug\". allowed-tools: Read, Grep, Bash(kubectl:*), Bash(curl:*), Bash(tcpdump:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Advanced Troubleshooting\n\n## Overview\nDeep debugging techniques for complex Supabase issues that resist standard troubleshooting.\n\n## Prerequisites\n- Access to production logs and metrics\n- kubectl access to clusters\n- Network capture tools available\n- Understanding of distributed tracing\n\n## Evidence Collection Framework\n\n### Comprehensive Debug Bundle\n```bash\n#!/bin/bash\n# advanced-supabase-debug.sh\n\nBUNDLE=\"supabase-advanced-debug-$(date +%Y%m%d-%H%M%S)\"\nmkdir -p \"$BUNDLE\"/{logs,metrics,network,config,traces}\n\n# 1. Extended logs (1 hour window)\nkubectl logs -l app=supabase-integration --since=1h > \"$BUNDLE/logs/pods.log\"\njournalctl -u supabase-service --since \"1 hour ago\" > \"$BUNDLE/logs/system.log\"\n\n# 2. Metrics dump\ncurl -s localhost:9090/api/v1/query?query=supabase_requests_total > \"$BUNDLE/metrics/requests.json\"\ncurl -s localhost:9090/api/v1/query?query=supabase_errors_total > \"$BUNDLE/metrics/errors.json\"\n\n# 3. Network capture (30 seconds)\ntimeout 30 tcpdump -i any port 443 -w \"$BUNDLE/network/capture.pcap\" &\n\n# 4. Distributed traces\ncurl -s localhost:16686/api/traces?service=supabase > \"$BUNDLE/traces/jaeger.json\"\n\n# 5. Configuration state\nkubectl get cm supabase-config -o yaml > \"$BUNDLE/config/configmap.yaml\"\nkubectl get secret supabase-secrets -o yaml > \"$BUNDLE/config/secrets-redacted.yaml\"\n\ntar -czf \"$BUNDLE.tar.gz\" \"$BUNDLE\"\necho \"Advanced debug bundle: $BUNDLE.tar.gz\"\n```\n\n## Systematic Isolation\n\n### Layer-by-Layer Testing\n\n```typescript\n// Test each layer independently\nasync function diagnoseSupabaseIssue(): Promise<DiagnosisReport> {\n  const results: DiagnosisResult[] = [];\n\n  // Layer 1: Network connectivity\n  results.push(await testNetworkConnectivity());\n\n  // Layer 2: DNS resolution\n  results.push(await testDNSResolution('api.supabase.com'));\n\n  // Layer 3: TLS handshake\n  results.push(await testTLSHandshake('api.supabase.com'));\n\n  // Layer 4: Authentication\n  results.push(await testAuthentication());\n\n  // Layer 5: API response\n  results.push(await testAPIResponse());\n\n  // Layer 6: Response parsing\n  results.push(await testResponseParsing());\n\n  return { results, firstFailure: results.find(r => !r.success) };\n}\n```\n\n### Minimal Reproduction\n\n```typescript\n// Strip down to absolute minimum\nasync function minimalRepro(): Promise<void> {\n  // 1. Fresh client, no customization\n  const client = new SupabaseClient({\n    apiKey: process.env.SUPABASE_API_KEY!,\n  });\n\n  // 2. Simplest possible call\n  try {\n    const result = await client.ping();\n    console.log('Ping successful:', result);\n  } catch (error) {\n    console.error('Ping failed:', {\n      message: error.message,\n      code: error.code,\n      stack: error.stack,\n    });\n  }\n}\n```\n\n## Timing Analysis\n\n```typescript\nclass TimingAnalyzer {\n  private timings: Map<string, number[]> = new Map();\n\n  async measure<T>(label: string, fn: () => Promise<T>): Promise<T> {\n    const start = performance.now();\n    try {\n      return await fn();\n    } finally {\n      const duration = performance.now() - start;\n      const existing = this.timings.get(label) || [];\n      existing.push(duration);\n      this.timings.set(label, existing);\n    }\n  }\n\n  report(): TimingReport {\n    const report: TimingReport = {};\n    for (const [label, times] of this.timings) {\n      report[label] = {\n        count: times.length,\n        min: Math.min(...times),\n        max: Math.max(...times),\n        avg: times.reduce((a, b) => a + b, 0) / times.length,\n        p95: this.percentile(times, 95),\n      };\n    }\n    return report;\n  }\n}\n```\n\n## Memory and Resource Analysis\n\n```typescript\n// Detect memory leaks in Supabase client usage\nconst heapUsed: number[] = [];\n\nsetInterval(() => {\n  const usage = process.memoryUsage();\n  heapUsed.push(usage.heapUsed);\n\n  // Alert on sustained growth\n  if (heapUsed.length > 60) { // 1 hour at 1/min\n    const trend = heapUsed[59] - heapUsed[0];\n    if (trend > 100 * 1024 * 1024) { // 100MB growth\n      console.warn('Potential memory leak in supabase integration');\n    }\n  }\n}, 60000);\n```\n\n## Race Condition Detection\n\n```typescript\n// Detect concurrent access issues\nclass SupabaseConcurrencyChecker {\n  private inProgress: Set<string> = new Set();\n\n  async execute<T>(key: string, fn: () => Promise<T>): Promise<T> {\n    if (this.inProgress.has(key)) {\n      console.warn(`Concurrent access detected for ${key}`);\n    }\n\n    this.inProgress.add(key);\n    try {\n      return await fn();\n    } finally {\n      this.inProgress.delete(key);\n    }\n  }\n}\n```\n\n## Support Escalation Template\n\n```markdown\n## Supabase Support Escalation\n\n**Severity:** P[1-4]\n**Request ID:** [from error response]\n**Timestamp:** [ISO 8601]\n\n### Issue Summary\n[One paragraph description]\n\n### Steps to Reproduce\n1. [Step 1]\n2. [Step 2]\n\n### Expected vs Actual\n- Expected: [behavior]\n- Actual: [behavior]\n\n### Evidence Attached\n- [ ] Debug bundle (supabase-advanced-debug-*.tar.gz)\n- [ ] Minimal reproduction code\n- [ ] Timing analysis\n- [ ] Network capture (if relevant)\n\n### Workarounds Attempted\n1. [Workaround 1] - Result: [outcome]\n2. [Workaround 2] - Result: [outcome]\n```\n\n## Instructions\n\n### Step 1: Collect Evidence Bundle\nRun the comprehensive debug script to gather all relevant data.\n\n### Step 2: Systematic Isolation\nTest each layer independently to identify the failure point.\n\n### Step 3: Create Minimal Reproduction\nStrip down to the simplest failing case.\n\n### Step 4: Escalate with Evidence\nUse the support template with all collected evidence.\n\n## Output\n- Comprehensive debug bundle collected\n- Failure layer identified\n- Minimal reproduction created\n- Support escalation submitted\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Can't reproduce | Race condition | Add timing analysis |\n| Intermittent failure | Timing-dependent | Increase sample size |\n| No useful logs | Missing instrumentation | Add debug logging |\n| Memory growth | Resource leak | Use heap profiling |\n\n## Examples\n\n### Quick Layer Test\n```bash\n# Test each layer in sequence\ncurl -v https://api.supabase.com/health 2>&1 | grep -E \"(Connected|TLS|HTTP)\"\n```\n\n## Resources\n- [Supabase Support Portal](https://support.supabase.com)\n- [Supabase Status Page](https://status.supabase.com)\n\n## Next Steps\nFor load testing, see `supabase-load-scale`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-advanced-troubleshooting/SKILL.md"
    },
    {
      "slug": "supabase-architecture-variants",
      "name": "supabase-architecture-variants",
      "description": "Choose and implement Supabase validated architecture blueprints for different scales. Use when designing new Supabase integrations, choosing between monolith/service/microservice architectures, or planning migration paths for Supabase applications. Trigger with phrases like \"supabase architecture\", \"supabase blueprint\", \"how to structure supabase\", \"supabase project layout\", \"supabase microservice\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Architecture Variants\n\n## Overview\nThree validated architecture blueprints for Supabase integrations.\n\n## Prerequisites\n- Understanding of team size and DAU requirements\n- Knowledge of deployment infrastructure\n- Clear SLA requirements\n- Growth projections available\n\n## Variant A: Monolith (Simple)\n\n**Best for:** MVPs, small teams, < 10K daily active users\n\n```\nmy-app/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ supabase/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts          # Singleton client\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.ts           # Types\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware.ts      # Express middleware\n‚îÇ   ‚îú‚îÄ‚îÄ routes/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ supabase.ts    # API routes\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îî‚îÄ‚îÄ supabase.test.ts\n‚îî‚îÄ‚îÄ package.json\n```\n\n### Key Characteristics\n- Single deployment unit\n- Synchronous Supabase calls in request path\n- In-memory caching\n- Simple error handling\n\n### Code Pattern\n```typescript\n// Direct integration in route handler\napp.post('/api/create', async (req, res) => {\n  try {\n    const result = await supabaseClient.create(req.body);\n    res.json(result);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n```\n\n---\n\n## Variant B: Service Layer (Moderate)\n\n**Best for:** Growing startups, 10K-100K DAU, multiple integrations\n\n```\nmy-app/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ services/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supabase/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts      # Client wrapper\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service.ts     # Business logic\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repository.ts  # Data access\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types.ts\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts           # Service exports\n‚îÇ   ‚îú‚îÄ‚îÄ controllers/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ supabase.ts\n‚îÇ   ‚îú‚îÄ‚îÄ routes/\n‚îÇ   ‚îú‚îÄ‚îÄ middleware/\n‚îÇ   ‚îú‚îÄ‚îÄ queue/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ supabase-processor.ts  # Async processing\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îî‚îÄ‚îÄ supabase/\n‚îî‚îÄ‚îÄ package.json\n```\n\n### Key Characteristics\n- Separation of concerns\n- Background job processing\n- Redis caching\n- Circuit breaker pattern\n- Structured error handling\n\n### Code Pattern\n```typescript\n// Service layer abstraction\nclass SupabaseService {\n  constructor(\n    private client: SupabaseClient,\n    private cache: CacheService,\n    private queue: QueueService\n  ) {}\n\n  async createResource(data: CreateInput): Promise<Resource> {\n    // Business logic before API call\n    const validated = this.validate(data);\n\n    // Check cache\n    const cached = await this.cache.get(cacheKey);\n    if (cached) return cached;\n\n    // API call with retry\n    const result = await this.withRetry(() =>\n      this.client.create(validated)\n    );\n\n    // Cache result\n    await this.cache.set(cacheKey, result, 300);\n\n    // Async follow-up\n    await this.queue.enqueue('supabase.post-create', result);\n\n    return result;\n  }\n}\n```\n\n---\n\n## Variant C: Microservice (Complex)\n\n**Best for:** Enterprise, 100K+ DAU, strict SLAs\n\n```\nsupabase-service/              # Dedicated microservice\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grpc/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ supabase.proto\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rest/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ routes.ts\n‚îÇ   ‚îú‚îÄ‚îÄ domain/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ events/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/\n‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supabase/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mapper.ts\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ circuit-breaker.ts\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queue/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ config/\n‚îú‚îÄ‚îÄ k8s/\n‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ service.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ hpa.yaml\n‚îî‚îÄ‚îÄ package.json\n\nother-services/\n‚îú‚îÄ‚îÄ order-service/       # Calls supabase-service\n‚îú‚îÄ‚îÄ payment-service/\n‚îî‚îÄ‚îÄ notification-service/\n```\n\n### Key Characteristics\n- Dedicated Supabase microservice\n- gRPC for internal communication\n- Event-driven architecture\n- Database per service\n- Kubernetes autoscaling\n- Distributed tracing\n- Circuit breaker per service\n\n### Code Pattern\n```typescript\n// Event-driven with domain isolation\nclass SupabaseAggregate {\n  private events: DomainEvent[] = [];\n\n  process(command: SupabaseCommand): void {\n    // Domain logic\n    const result = this.execute(command);\n\n    // Emit domain event\n    this.events.push(new SupabaseProcessedEvent(result));\n  }\n\n  getUncommittedEvents(): DomainEvent[] {\n    return [...this.events];\n  }\n}\n\n// Event handler\n@EventHandler(SupabaseProcessedEvent)\nclass SupabaseEventHandler {\n  async handle(event: SupabaseProcessedEvent): Promise<void> {\n    // Saga orchestration\n    await this.sagaOrchestrator.continue(event);\n  }\n}\n```\n\n---\n\n## Decision Matrix\n\n| Factor | Monolith | Service Layer | Microservice |\n|--------|----------|---------------|--------------|\n| Team Size | 1-5 | 5-20 | 20+ |\n| DAU | < 10K | 10K-100K | 100K+ |\n| Deployment Frequency | Weekly | Daily | Continuous |\n| Failure Isolation | None | Partial | Full |\n| Operational Complexity | Low | Medium | High |\n| Time to Market | Fastest | Moderate | Slowest |\n\n## Migration Path\n\n```\nMonolith ‚Üí Service Layer:\n1. Extract Supabase code to service/\n2. Add caching layer\n3. Add background processing\n\nService Layer ‚Üí Microservice:\n1. Create dedicated supabase-service repo\n2. Define gRPC contract\n3. Add event bus\n4. Deploy to Kubernetes\n5. Migrate traffic gradually\n```\n\n## Instructions\n\n### Step 1: Assess Requirements\nUse the decision matrix to identify appropriate variant.\n\n### Step 2: Choose Architecture\nSelect Monolith, Service Layer, or Microservice based on needs.\n\n### Step 3: Implement Structure\nSet up project layout following the chosen blueprint.\n\n### Step 4: Plan Migration Path\nDocument upgrade path for future scaling.\n\n## Output\n- Architecture variant selected\n- Project structure implemented\n- Migration path documented\n- Appropriate patterns applied\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Over-engineering | Wrong variant choice | Start simpler |\n| Performance issues | Wrong layer | Add caching/async |\n| Team friction | Complex architecture | Simplify or train |\n| Deployment complexity | Microservice overhead | Consider service layer |\n\n## Examples\n\n### Quick Variant Check\n```bash\n# Count team size and DAU to select variant\necho \"Team: $(git log --format='%ae' | sort -u | wc -l) developers\"\necho \"DAU: Check analytics dashboard\"\n```\n\n## Resources\n- [Monolith First](https://martinfowler.com/bliki/MonolithFirst.html)\n- [Microservices Guide](https://martinfowler.com/microservices/)\n- [Supabase Architecture Guide](https://supabase.com/docs/architecture)\n\n## Next Steps\nFor common anti-patterns, see `supabase-known-pitfalls`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-architecture-variants/SKILL.md"
    },
    {
      "slug": "supabase-auth-storage-realtime-core",
      "name": "supabase-auth-storage-realtime-core",
      "description": "Execute Supabase secondary workflow: Auth + Storage + Realtime. Use when implementing secondary use case, or complementing primary workflow. Trigger with phrases like \"supabase auth storage realtime\", \"implement full stack features with supabase\". allowed-tools: Read, Write, Edit, Bash(npm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Auth + Storage + Realtime\n\n## Overview\nImplement the core Supabase trifecta: authentication, file storage,\nand real-time subscriptions in a single cohesive setup.\n\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Familiarity with `supabase-schema-from-requirements`\n- Valid API credentials configured\n\n## Instructions\n\n### Step 1: Setup\n```typescript\n// Step 1 implementation\n```\n\n### Step 2: Process\n```typescript\n// Step 2 implementation\n```\n\n### Step 3: Complete\n```typescript\n// Step 3 implementation\n```\n\n## Output\n- Completed Auth + Storage + Realtime execution\n- Results from Supabase API\n- Success confirmation or error details\n\n## Error Handling\n| Aspect | Schema from Requirements | Auth + Storage + Realtime |\n|--------|------------|------------|\n| Use Case | Starting a new project with defined data requirements | Secondary |\n| Complexity | Medium | Lower |\n| Performance | Standard | Optimized |\n\n## Examples\n\n### Complete Workflow\n```typescript\n// Complete workflow example\n```\n\n### Error Recovery\n```typescript\n// Error handling code\n```\n\n## Resources\n- [Supabase Documentation](https://supabase.com/docs)\n- [Supabase API Reference](https://supabase.com/docs/api)\n\n## Next Steps\nFor common errors, see `supabase-common-errors`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-auth-storage-realtime-core/SKILL.md"
    },
    {
      "slug": "supabase-ci-integration",
      "name": "supabase-ci-integration",
      "description": "Configure Supabase CI/CD integration with GitHub Actions and testing. Use when setting up automated testing, configuring CI pipelines, or integrating Supabase tests into your build process. Trigger with phrases like \"supabase CI\", \"supabase GitHub Actions\", \"supabase automated tests\", \"CI supabase\". allowed-tools: Read, Write, Edit, Bash(gh:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase CI Integration\n\n## Overview\nSet up CI/CD pipelines for Supabase integrations with automated testing.\n\n## Prerequisites\n- GitHub repository with Actions enabled\n- Supabase test API key\n- npm/pnpm project configured\n\n## Instructions\n\n### Step 1: Create GitHub Actions Workflow\nCreate `.github/workflows/supabase-integration.yml`:\n\n```yaml\nname: Supabase Integration Tests\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\nenv:\n  SUPABASE_API_KEY: ${{ secrets.SUPABASE_API_KEY }}\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    env:\n      SUPABASE_API_KEY: ${{ secrets.SUPABASE_API_KEY }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n      - run: npm ci\n      - run: npm test -- --coverage\n      - run: npm run test:integration\n```\n\n### Step 2: Configure Secrets\n```bash\ngh secret set SUPABASE_API_KEY --body \"sk_test_***\"\n```\n\n### Step 3: Add Integration Tests\n```typescript\ndescribe('Supabase Integration', () => {\n  it.skipIf(!process.env.SUPABASE_API_KEY)('should connect', async () => {\n    const client = getSupabaseClient();\n    const result = await client.healthCheck();\n    expect(result.status).toBe('ok');\n  });\n});\n```\n\n## Output\n- Automated test pipeline\n- PR checks configured\n- Coverage reports uploaded\n- Release workflow ready\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Secret not found | Missing configuration | Add secret via `gh secret set` |\n| Tests timeout | Network issues | Increase timeout or mock |\n| Auth failures | Invalid key | Check secret value |\n\n## Examples\n\n### Release Workflow\n```yaml\non:\n  push:\n    tags: ['v*']\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    env:\n      SUPABASE_API_KEY: ${{ secrets.SUPABASE_API_KEY_PROD }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n      - run: npm ci\n      - name: Verify Supabase production readiness\n        run: npm run test:integration\n      - run: npm run build\n      - run: npm publish\n```\n\n### Branch Protection\n```yaml\nrequired_status_checks:\n  - \"test\"\n  - \"supabase-integration\"\n```\n\n## Resources\n- [GitHub Actions Documentation](https://docs.github.com/en/actions)\n- [Supabase CI Guide](https://supabase.com/docs/ci)\n\n## Next Steps\nFor deployment patterns, see `supabase-deploy-integration`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-ci-integration/SKILL.md"
    },
    {
      "slug": "supabase-common-errors",
      "name": "supabase-common-errors",
      "description": "Diagnose and fix Supabase common errors and exceptions. Use when encountering Supabase errors, debugging failed requests, or troubleshooting integration issues. Trigger with phrases like \"supabase error\", \"fix supabase\", \"supabase not working\", \"debug supabase\". allowed-tools: Read, Grep, Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Common Errors\n\n## Overview\nQuick reference for the top 10 most common Supabase errors and their solutions.\n\n## Prerequisites\n- Supabase SDK installed\n- API credentials configured\n- Access to error logs\n\n## Instructions\n\n### Step 1: Identify the Error\nCheck error message and code in your logs or console.\n\n### Step 2: Find Matching Error Below\nMatch your error to one of the documented cases.\n\n### Step 3: Apply Solution\nFollow the solution steps for your specific error.\n\n## Output\n- Identified error cause\n- Applied fix\n- Verified resolution\n\n## Error Handling\n\n### Invalid JWT\n**Error Message:**\n```\nInvalid JWT: expired or malformed\n```\n\n**Cause:** JWT token has expired or is incorrectly formatted\n\n**Solution:**\n```bash\nCheck token expiry with supabase.auth.getSession() and call refreshSession() if needed\n```\n\n---\n\n### RLS Policy Violation\n**Error Message:**\n```\nnew row violates row-level security policy for table\n```\n\n**Cause:** Row Level Security (RLS) policy is blocking the operation\n\n**Solution:**\nCheck RLS policies in dashboard or via pg_policies table. Ensure user has required role.\n\n---\n\n### Connection Pool Exhausted\n**Error Message:**\n```\ntoo many clients already\n```\n\n**Cause:** Connection pool limit reached due to too many concurrent connections\n\n**Solution:**\n```typescript\nUse connection pooling mode in Supabase dashboard. Switch to Session mode or pgBouncer.\n```\n\n## Examples\n\n### Quick Diagnostic Commands\n```bash\n# Check Supabase status\ncurl -s https://status.supabase.com\n\n# Verify API connectivity\ncurl -I https://api.supabase.com\n\n# Check local configuration\nenv | grep SUPABASE\n```\n\n### Escalation Path\n1. Collect evidence with `supabase-debug-bundle`\n2. Check Supabase status page\n3. Contact support with request ID\n\n## Resources\n- [Supabase Status Page](https://status.supabase.com)\n- [Supabase Support](https://supabase.com/docs/support)\n- [Supabase Error Codes](https://supabase.com/docs/errors)\n\n## Next Steps\nFor comprehensive debugging, see `supabase-debug-bundle`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-common-errors/SKILL.md"
    },
    {
      "slug": "supabase-cost-tuning",
      "name": "supabase-cost-tuning",
      "description": "Optimize Supabase costs through tier selection, sampling, and usage monitoring. Use when analyzing Supabase billing, reducing API costs, or implementing usage monitoring and budget alerts. Trigger with phrases like \"supabase cost\", \"supabase billing\", \"reduce supabase costs\", \"supabase pricing\", \"supabase expensive\", \"supabase budget\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Cost Tuning\n\n## Overview\nOptimize Supabase costs through smart tier selection, sampling, and usage monitoring.\n\n## Prerequisites\n- Access to Supabase billing dashboard\n- Understanding of current usage patterns\n- Database for usage tracking (optional)\n- Alerting system configured (optional)\n\n## Pricing Tiers\n\n| Tier | Monthly Cost | Included | Overage |\n|------|-------------|----------|---------|\n| Free | $0 | 500MB database, 1GB storage, 50K MAUs | N/A |\n| Pro | $25 | 8GB database, 100GB storage, 100K MAUs | $0.001/request |\n| Enterprise | Custom | Unlimited | Volume discounts |\n\n## Cost Estimation\n\n```typescript\ninterface UsageEstimate {\n  requestsPerMonth: number;\n  tier: string;\n  estimatedCost: number;\n  recommendation?: string;\n}\n\nfunction estimateSupabaseCost(requestsPerMonth: number): UsageEstimate {\n  if (requestsPerMonth <= 1000) {\n    return { requestsPerMonth, tier: 'Free', estimatedCost: 0 };\n  }\n\n  if (requestsPerMonth <= 100000) {\n    return { requestsPerMonth, tier: 'Pro', estimatedCost: 25 };\n  }\n\n  const proOverage = (requestsPerMonth - 100000) * 0.001;\n  const proCost = 25 + proOverage;\n\n  return {\n    requestsPerMonth,\n    tier: 'Pro (with overage)',\n    estimatedCost: proCost,\n    recommendation: proCost > 500\n      ? 'Consider Enterprise tier for volume discounts'\n      : undefined,\n  };\n}\n```\n\n## Usage Monitoring\n\n```typescript\nclass SupabaseUsageMonitor {\n  private requestCount = 0;\n  private bytesTransferred = 0;\n  private alertThreshold: number;\n\n  constructor(monthlyBudget: number) {\n    this.alertThreshold = monthlyBudget * 0.8; // 80% warning\n  }\n\n  track(request: { bytes: number }) {\n    this.requestCount++;\n    this.bytesTransferred += request.bytes;\n\n    if (this.estimatedCost() > this.alertThreshold) {\n      this.sendAlert('Approaching Supabase budget limit');\n    }\n  }\n\n  estimatedCost(): number {\n    return estimateSupabaseCost(this.requestCount).estimatedCost;\n  }\n\n  private sendAlert(message: string) {\n    // Send to Slack, email, PagerDuty, etc.\n  }\n}\n```\n\n## Cost Reduction Strategies\n\n### Step 1: Request Sampling\n```typescript\nfunction shouldSample(samplingRate = 0.1): boolean {\n  return Math.random() < samplingRate;\n}\n\n// Use for non-critical telemetry\nif (shouldSample(0.1)) { // 10% sample\n  await supabaseClient.trackEvent(event);\n}\n```\n\n### Step 2: Batching Requests\n```typescript\n// Instead of N individual calls\nawait Promise.all(ids.map(id => supabaseClient.get(id)));\n\n// Use batch endpoint (1 call)\nawait supabaseClient.batchGet(ids);\n```\n\n### Step 3: Caching (from P16)\n- Cache frequently accessed data\n- Use cache invalidation webhooks\n- Set appropriate TTLs\n\n### Step 4: Compression\n```typescript\nconst client = new SupabaseClient({\n  compression: true, // Enable gzip\n});\n```\n\n## Budget Alerts\n\n```bash\n# Set up billing alerts in Supabase dashboard\n# Or use API if available:\n# Check Supabase documentation for billing APIs\n```\n\n## Cost Dashboard Query\n\n```sql\n-- If tracking usage in your database\nSELECT\n  DATE_TRUNC('day', created_at) as date,\n  COUNT(*) as requests,\n  SUM(response_bytes) as bytes,\n  COUNT(*) * 0.001 as estimated_cost\nFROM supabase_api_logs\nWHERE created_at >= NOW() - INTERVAL '30 days'\nGROUP BY 1\nORDER BY 1;\n```\n\n## Instructions\n\n### Step 1: Analyze Current Usage\nReview Supabase dashboard for usage patterns and costs.\n\n### Step 2: Select Optimal Tier\nUse the cost estimation function to find the right tier.\n\n### Step 3: Implement Monitoring\nAdd usage tracking to catch budget overruns early.\n\n### Step 4: Apply Optimizations\nEnable batching, caching, and sampling where appropriate.\n\n## Output\n- Optimized tier selection\n- Usage monitoring implemented\n- Budget alerts configured\n- Cost reduction strategies applied\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Unexpected charges | Untracked usage | Implement monitoring |\n| Overage fees | Wrong tier | Upgrade tier |\n| Budget exceeded | No alerts | Set up alerts |\n| Inefficient usage | No batching | Enable batch requests |\n\n## Examples\n\n### Quick Cost Check\n```typescript\n// Estimate monthly cost for your usage\nconst estimate = estimateSupabaseCost(yourMonthlyRequests);\nconsole.log(`Tier: ${estimate.tier}, Cost: $${estimate.estimatedCost}`);\nif (estimate.recommendation) {\n  console.log(`üí° ${estimate.recommendation}`);\n}\n```\n\n## Resources\n- [Supabase Pricing](https://supabase.com/pricing)\n- [Supabase Billing Dashboard](https://dashboard.supabase.com/billing)\n\n## Next Steps\nFor architecture patterns, see `supabase-reference-architecture`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-cost-tuning/SKILL.md"
    },
    {
      "slug": "supabase-data-handling",
      "name": "supabase-data-handling",
      "description": "Implement Supabase PII handling, data retention, and GDPR/CCPA compliance patterns. Use when handling sensitive data, implementing data redaction, configuring retention policies, or ensuring compliance with privacy regulations for Supabase integrations. Trigger with phrases like \"supabase data\", \"supabase PII\", \"supabase GDPR\", \"supabase data retention\", \"supabase privacy\", \"supabase CCPA\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Data Handling\n\n## Overview\nHandle sensitive data correctly when integrating with Supabase.\n\n## Prerequisites\n- Understanding of GDPR/CCPA requirements\n- Supabase SDK with data export capabilities\n- Database for audit logging\n- Scheduled job infrastructure for cleanup\n\n## Data Classification\n\n| Category | Examples | Handling |\n|----------|----------|----------|\n| PII | Email, name, phone | Encrypt, minimize |\n| Sensitive | API keys, tokens | Never log, rotate |\n| Business | Usage metrics | Aggregate when possible |\n| Public | Product names | Standard handling |\n\n## PII Detection\n\n```typescript\nconst PII_PATTERNS = [\n  { type: 'email', regex: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}/g },\n  { type: 'phone', regex: /\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b/g },\n  { type: 'ssn', regex: /\\b\\d{3}-\\d{2}-\\d{4}\\b/g },\n  { type: 'credit_card', regex: /\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b/g },\n];\n\nfunction detectPII(text: string): { type: string; match: string }[] {\n  const findings: { type: string; match: string }[] = [];\n\n  for (const pattern of PII_PATTERNS) {\n    const matches = text.matchAll(pattern.regex);\n    for (const match of matches) {\n      findings.push({ type: pattern.type, match: match[0] });\n    }\n  }\n\n  return findings;\n}\n```\n\n## Data Redaction\n\n```typescript\nfunction redactPII(data: Record<string, any>): Record<string, any> {\n  const sensitiveFields = ['email', 'phone', 'ssn', 'password', 'apiKey'];\n  const redacted = { ...data };\n\n  for (const field of sensitiveFields) {\n    if (redacted[field]) {\n      redacted[field] = '[REDACTED]';\n    }\n  }\n\n  return redacted;\n}\n\n// Use in logging\nconsole.log('Supabase request:', redactPII(requestData));\n```\n\n## Data Retention Policy\n\n### Retention Periods\n| Data Type | Retention | Reason |\n|-----------|-----------|--------|\n| API logs | 30 days | Debugging |\n| Error logs | 90 days | Root cause analysis |\n| Audit logs | 7 years | Compliance |\n| PII | Until deletion request | GDPR/CCPA |\n\n### Automatic Cleanup\n\n```typescript\nasync function cleanupSupabaseData(retentionDays: number): Promise<void> {\n  const cutoff = new Date();\n  cutoff.setDate(cutoff.getDate() - retentionDays);\n\n  await db.supabaseLogs.deleteMany({\n    createdAt: { $lt: cutoff },\n    type: { $nin: ['audit', 'compliance'] },\n  });\n}\n\n// Schedule daily cleanup\ncron.schedule('0 3 * * *', () => cleanupSupabaseData(30));\n```\n\n## GDPR/CCPA Compliance\n\n### Data Subject Access Request (DSAR)\n\n```typescript\nasync function exportUserData(userId: string): Promise<DataExport> {\n  const supabaseData = await supabaseClient.getUserData(userId);\n\n  return {\n    source: 'Supabase',\n    exportedAt: new Date().toISOString(),\n    data: {\n      profile: supabaseData.profile,\n      activities: supabaseData.activities,\n      // Include all user-related data\n    },\n  };\n}\n```\n\n### Right to Deletion\n\n```typescript\nasync function deleteUserData(userId: string): Promise<DeletionResult> {\n  // 1. Delete from Supabase\n  await supabaseClient.deleteUser(userId);\n\n  // 2. Delete local copies\n  await db.supabaseUserCache.deleteMany({ userId });\n\n  // 3. Audit log (required to keep)\n  await auditLog.record({\n    action: 'GDPR_DELETION',\n    userId,\n    service: 'supabase',\n    timestamp: new Date(),\n  });\n\n  return { success: true, deletedAt: new Date() };\n}\n```\n\n## Data Minimization\n\n```typescript\n// Only request needed fields\nconst user = await supabaseClient.getUser(userId, {\n  fields: ['id', 'name'], // Not email, phone, address\n});\n\n// Don't store unnecessary data\nconst cacheData = {\n  id: user.id,\n  name: user.name,\n  // Omit sensitive fields\n};\n```\n\n## Instructions\n\n### Step 1: Classify Data\nCategorize all Supabase data by sensitivity level.\n\n### Step 2: Implement PII Detection\nAdd regex patterns to detect sensitive data in logs.\n\n### Step 3: Configure Redaction\nApply redaction to sensitive fields before logging.\n\n### Step 4: Set Up Retention\nConfigure automatic cleanup with appropriate retention periods.\n\n## Output\n- Data classification documented\n- PII detection implemented\n- Redaction in logging active\n- Retention policy enforced\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| PII in logs | Missing redaction | Wrap logging with redact |\n| Deletion failed | Data locked | Check dependencies |\n| Export incomplete | Timeout | Increase batch size |\n| Audit gap | Missing entries | Review log pipeline |\n\n## Examples\n\n### Quick PII Scan\n```typescript\nconst findings = detectPII(JSON.stringify(userData));\nif (findings.length > 0) {\n  console.warn(`PII detected: ${findings.map(f => f.type).join(', ')}`);\n}\n```\n\n### Redact Before Logging\n```typescript\nconst safeData = redactPII(apiResponse);\nlogger.info('Supabase response:', safeData);\n```\n\n### GDPR Data Export\n```typescript\nconst userExport = await exportUserData('user-123');\nawait sendToUser(userExport);\n```\n\n## Resources\n- [GDPR Developer Guide](https://gdpr.eu/developers/)\n- [CCPA Compliance Guide](https://oag.ca.gov/privacy/ccpa)\n- [Supabase Privacy Guide](https://supabase.com/docs/privacy)\n\n## Next Steps\nFor enterprise access control, see `supabase-enterprise-rbac`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-data-handling/SKILL.md"
    },
    {
      "slug": "supabase-debug-bundle",
      "name": "supabase-debug-bundle",
      "description": "Collect Supabase debug evidence for support tickets and troubleshooting. Use when encountering persistent issues, preparing support tickets, or collecting diagnostic information for Supabase problems. Trigger with phrases like \"supabase debug\", \"supabase support bundle\", \"collect supabase logs\", \"supabase diagnostic\". allowed-tools: Read, Bash(grep:*), Bash(curl:*), Bash(tar:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Debug Bundle\n\n## Overview\nCollect all necessary diagnostic information for Supabase support tickets.\n\n## Prerequisites\n- Supabase SDK installed\n- Access to application logs\n- Permission to collect environment info\n\n## Instructions\n\n### Step 1: Create Debug Bundle Script\n```bash\n#!/bin/bash\n# supabase-debug-bundle.sh\n\nBUNDLE_DIR=\"supabase-debug-$(date +%Y%m%d-%H%M%S)\"\nmkdir -p \"$BUNDLE_DIR\"\n\necho \"=== Supabase Debug Bundle ===\" > \"$BUNDLE_DIR/summary.txt\"\necho \"Generated: $(date)\" >> \"$BUNDLE_DIR/summary.txt\"\n```\n\n### Step 2: Collect Environment Info\n```bash\n# Environment info\necho \"--- Environment ---\" >> \"$BUNDLE_DIR/summary.txt\"\nnode --version >> \"$BUNDLE_DIR/summary.txt\" 2>&1\nnpm --version >> \"$BUNDLE_DIR/summary.txt\" 2>&1\necho \"SUPABASE_API_KEY: ${SUPABASE_API_KEY:+[SET]}\" >> \"$BUNDLE_DIR/summary.txt\"\n```\n\n### Step 3: Gather SDK and Logs\n```bash\n# SDK version\nnpm list @supabase/supabase-js 2>/dev/null >> \"$BUNDLE_DIR/summary.txt\"\n\n# Recent logs (redacted)\ngrep -i \"supabase\" ~/.npm/_logs/*.log 2>/dev/null | tail -50 >> \"$BUNDLE_DIR/logs.txt\"\n\n# Configuration (redacted - secrets masked)\necho \"--- Config (redacted) ---\" >> \"$BUNDLE_DIR/summary.txt\"\ncat .env 2>/dev/null | sed 's/=.*/=***REDACTED***/' >> \"$BUNDLE_DIR/config-redacted.txt\"\n\n# Network connectivity test\necho \"--- Network Test ---\" >> \"$BUNDLE_DIR/summary.txt\"\necho -n \"API Health: \" >> \"$BUNDLE_DIR/summary.txt\"\ncurl -s -o /dev/null -w \"%{http_code}\" https://api.supabase.com/health >> \"$BUNDLE_DIR/summary.txt\"\necho \"\" >> \"$BUNDLE_DIR/summary.txt\"\n```\n\n### Step 4: Package Bundle\n```bash\ntar -czf \"$BUNDLE_DIR.tar.gz\" \"$BUNDLE_DIR\"\necho \"Bundle created: $BUNDLE_DIR.tar.gz\"\n```\n\n## Output\n- `supabase-debug-YYYYMMDD-HHMMSS.tar.gz` archive containing:\n  - `summary.txt` - Environment and SDK info\n  - `logs.txt` - Recent redacted logs\n  - `config-redacted.txt` - Configuration (secrets removed)\n\n## Error Handling\n| Item | Purpose | Included |\n|------|---------|----------|\n| Environment versions | Compatibility check | ‚úì |\n| SDK version | Version-specific bugs | ‚úì |\n| Error logs (redacted) | Root cause analysis | ‚úì |\n| Config (redacted) | Configuration issues | ‚úì |\n| Network test | Connectivity issues | ‚úì |\n\n## Examples\n\n### Sensitive Data Handling\n**ALWAYS REDACT:**\n- API keys and tokens\n- Passwords and secrets\n- PII (emails, names, IDs)\n\n**Safe to Include:**\n- Error messages\n- Stack traces (redacted)\n- SDK/runtime versions\n\n### Submit to Support\n1. Create bundle: `bash supabase-debug-bundle.sh`\n2. Review for sensitive data\n3. Upload to Supabase support portal\n\n## Resources\n- [Supabase Support](https://supabase.com/docs/support)\n- [Supabase Status](https://status.supabase.com)\n\n## Next Steps\nFor rate limit issues, see `supabase-rate-limits`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-debug-bundle/SKILL.md"
    },
    {
      "slug": "supabase-deploy-integration",
      "name": "supabase-deploy-integration",
      "description": "Deploy Supabase integrations to Vercel, Fly.io, and Cloud Run platforms. Use when deploying Supabase-powered applications to production, configuring platform-specific secrets, or setting up deployment pipelines. Trigger with phrases like \"deploy supabase\", \"supabase Vercel\", \"supabase production deploy\", \"supabase Cloud Run\", \"supabase Fly.io\". allowed-tools: Read, Write, Edit, Bash(vercel:*), Bash(fly:*), Bash(gcloud:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Deploy Integration\n\n## Overview\nDeploy Supabase-powered applications to popular platforms with proper secrets management.\n\n## Prerequisites\n- Supabase API keys for production environment\n- Platform CLI installed (vercel, fly, or gcloud)\n- Application code ready for deployment\n- Environment variables documented\n\n## Vercel Deployment\n\n### Environment Setup\n```bash\n# Add Supabase secrets to Vercel\nvercel secrets add supabase_api_key sk_live_***\nvercel secrets add supabase_webhook_secret whsec_***\n\n# Link to project\nvercel link\n\n# Deploy preview\nvercel\n\n# Deploy production\nvercel --prod\n```\n\n### vercel.json Configuration\n```json\n{\n  \"env\": {\n    \"SUPABASE_API_KEY\": \"@supabase_api_key\"\n  },\n  \"functions\": {\n    \"api/**/*.ts\": {\n      \"maxDuration\": 30\n    }\n  }\n}\n```\n\n## Fly.io Deployment\n\n### fly.toml\n```toml\napp = \"my-supabase-app\"\nprimary_region = \"iad\"\n\n[env]\n  NODE_ENV = \"production\"\n\n[http_service]\n  internal_port = 3000\n  force_https = true\n  auto_stop_machines = true\n  auto_start_machines = true\n```\n\n### Secrets\n```bash\n# Set Supabase secrets\nfly secrets set SUPABASE_API_KEY=sk_live_***\nfly secrets set SUPABASE_WEBHOOK_SECRET=whsec_***\n\n# Deploy\nfly deploy\n```\n\n## Google Cloud Run\n\n### Dockerfile\n```dockerfile\nFROM node:20-slim\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nCMD [\"npm\", \"start\"]\n```\n\n### Deploy Script\n```bash\n#!/bin/bash\n# deploy-cloud-run.sh\n\nPROJECT_ID=\"${GOOGLE_CLOUD_PROJECT}\"\nSERVICE_NAME=\"supabase-service\"\nREGION=\"us-central1\"\n\n# Build and push image\ngcloud builds submit --tag gcr.io/$PROJECT_ID/$SERVICE_NAME\n\n# Deploy to Cloud Run\ngcloud run deploy $SERVICE_NAME \\\n  --image gcr.io/$PROJECT_ID/$SERVICE_NAME \\\n  --region $REGION \\\n  --platform managed \\\n  --allow-unauthenticated \\\n  --set-secrets=SUPABASE_API_KEY=supabase-api-key:latest\n```\n\n## Environment Configuration Pattern\n\n```typescript\n// config/supabase.ts\ninterface SupabaseConfig {\n  apiKey: string;\n  environment: 'development' | 'staging' | 'production';\n  webhookSecret?: string;\n}\n\nexport function getSupabaseConfig(): SupabaseConfig {\n  const env = process.env.NODE_ENV || 'development';\n\n  return {\n    apiKey: process.env.SUPABASE_API_KEY!,\n    environment: env as SupabaseConfig['environment'],\n    webhookSecret: process.env.SUPABASE_WEBHOOK_SECRET,\n  };\n}\n```\n\n## Health Check Endpoint\n\n```typescript\n// api/health.ts\nexport async function GET() {\n  const supabaseStatus = await checkSupabaseConnection();\n\n  return Response.json({\n    status: supabaseStatus ? 'healthy' : 'degraded',\n    services: {\n      supabase: supabaseStatus,\n    },\n    timestamp: new Date().toISOString(),\n  });\n}\n```\n\n## Instructions\n\n### Step 1: Choose Deployment Platform\nSelect the platform that best fits your infrastructure needs and follow the platform-specific guide below.\n\n### Step 2: Configure Secrets\nStore Supabase API keys securely using the platform's secrets management.\n\n### Step 3: Deploy Application\nUse the platform CLI to deploy your application with Supabase integration.\n\n### Step 4: Verify Health\nTest the health check endpoint to confirm Supabase connectivity.\n\n## Output\n- Application deployed to production\n- Supabase secrets securely configured\n- Health check endpoint functional\n- Environment-specific configuration in place\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Secret not found | Missing configuration | Add secret via platform CLI |\n| Deploy timeout | Large build | Increase build timeout |\n| Health check fails | Wrong API key | Verify environment variable |\n| Cold start issues | No warm-up | Configure minimum instances |\n\n## Examples\n\n### Quick Deploy Script\n```bash\n#!/bin/bash\n# Platform-agnostic deploy helper\ncase \"$1\" in\n  vercel)\n    vercel secrets add supabase_api_key \"$SUPABASE_API_KEY\"\n    vercel --prod\n    ;;\n  fly)\n    fly secrets set SUPABASE_API_KEY=\"$SUPABASE_API_KEY\"\n    fly deploy\n    ;;\nesac\n```\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Fly.io Documentation](https://fly.io/docs)\n- [Cloud Run Documentation](https://cloud.google.com/run/docs)\n- [Supabase Deploy Guide](https://supabase.com/docs/deploy)\n\n## Next Steps\nFor webhook handling, see `supabase-webhooks-events`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-deploy-integration/SKILL.md"
    },
    {
      "slug": "supabase-enterprise-rbac",
      "name": "supabase-enterprise-rbac",
      "description": "Configure Supabase enterprise SSO, role-based access control, and organization management. Use when implementing SSO integration, configuring role-based permissions, or setting up organization-level controls for Supabase. Trigger with phrases like \"supabase SSO\", \"supabase RBAC\", \"supabase enterprise\", \"supabase roles\", \"supabase permissions\", \"supabase SAML\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Enterprise RBAC\n\n## Overview\nConfigure enterprise-grade access control for Supabase integrations.\n\n## Prerequisites\n- Supabase Enterprise tier subscription\n- Identity Provider (IdP) with SAML/OIDC support\n- Understanding of role-based access patterns\n- Audit logging infrastructure\n\n## Role Definitions\n\n| Role | Permissions | Use Case |\n|------|-------------|----------|\n| Admin | Full access | Platform administrators |\n| Developer | Read/write, no delete | Active development |\n| Viewer | Read-only | Stakeholders, auditors |\n| Service | API access only | Automated systems |\n\n## Role Implementation\n\n```typescript\nenum SupabaseRole {\n  Admin = 'admin',\n  Developer = 'developer',\n  Viewer = 'viewer',\n  Service = 'service',\n}\n\ninterface SupabasePermissions {\n  read: boolean;\n  write: boolean;\n  delete: boolean;\n  admin: boolean;\n}\n\nconst ROLE_PERMISSIONS: Record<SupabaseRole, SupabasePermissions> = {\n  admin: { read: true, write: true, delete: true, admin: true },\n  developer: { read: true, write: true, delete: false, admin: false },\n  viewer: { read: true, write: false, delete: false, admin: false },\n  service: { read: true, write: true, delete: false, admin: false },\n};\n\nfunction checkPermission(\n  role: SupabaseRole,\n  action: keyof SupabasePermissions\n): boolean {\n  return ROLE_PERMISSIONS[role][action];\n}\n```\n\n## SSO Integration\n\n### SAML Configuration\n\n```typescript\n// Supabase SAML setup\nconst samlConfig = {\n  entryPoint: 'https://idp.company.com/saml/sso',\n  issuer: 'https://supabase.com/saml/metadata',\n  cert: process.env.SAML_CERT,\n  callbackUrl: 'https://app.yourcompany.com/auth/supabase/callback',\n};\n\n// Map IdP groups to Supabase roles\nconst groupRoleMapping: Record<string, SupabaseRole> = {\n  'Engineering': SupabaseRole.Developer,\n  'Platform-Admins': SupabaseRole.Admin,\n  'Data-Team': SupabaseRole.Viewer,\n};\n```\n\n### OAuth2/OIDC Integration\n\n```typescript\nimport { OAuth2Client } from '@supabase/supabase-js';\n\nconst oauthClient = new OAuth2Client({\n  clientId: process.env.SUPABASE_OAUTH_CLIENT_ID!,\n  clientSecret: process.env.SUPABASE_OAUTH_CLIENT_SECRET!,\n  redirectUri: 'https://app.yourcompany.com/auth/supabase/callback',\n  scopes: read, write, realtime,\n});\n```\n\n## Organization Management\n\n```typescript\ninterface SupabaseOrganization {\n  id: string;\n  name: string;\n  ssoEnabled: boolean;\n  enforceSso: boolean;\n  allowedDomains: string[];\n  defaultRole: SupabaseRole;\n}\n\nasync function createOrganization(\n  config: SupabaseOrganization\n): Promise<void> {\n  await supabaseClient.organizations.create({\n    ...config,\n    settings: {\n      sso: {\n        enabled: config.ssoEnabled,\n        enforced: config.enforceSso,\n        domains: config.allowedDomains,\n      },\n    },\n  });\n}\n```\n\n## Access Control Middleware\n\n```typescript\nfunction requireSupabasePermission(\n  requiredPermission: keyof SupabasePermissions\n) {\n  return async (req: Request, res: Response, next: NextFunction) => {\n    const user = req.user as { supabaseRole: SupabaseRole };\n\n    if (!checkPermission(user.supabaseRole, requiredPermission)) {\n      return res.status(403).json({\n        error: 'Forbidden',\n        message: `Missing permission: ${requiredPermission}`,\n      });\n    }\n\n    next();\n  };\n}\n\n// Usage\napp.delete('/supabase/resource/:id',\n  requireSupabasePermission('delete'),\n  deleteResourceHandler\n);\n```\n\n## Audit Trail\n\n```typescript\ninterface SupabaseAuditEntry {\n  timestamp: Date;\n  userId: string;\n  role: SupabaseRole;\n  action: string;\n  resource: string;\n  success: boolean;\n  ipAddress: string;\n}\n\nasync function logSupabaseAccess(entry: SupabaseAuditEntry): Promise<void> {\n  await auditDb.insert(entry);\n\n  // Alert on suspicious activity\n  if (entry.action === 'delete' && !entry.success) {\n    await alertOnSuspiciousActivity(entry);\n  }\n}\n```\n\n## Instructions\n\n### Step 1: Define Roles\nMap organizational roles to Supabase permissions.\n\n### Step 2: Configure SSO\nSet up SAML or OIDC integration with your IdP.\n\n### Step 3: Implement Middleware\nAdd permission checks to API endpoints.\n\n### Step 4: Enable Audit Logging\nTrack all access for compliance.\n\n## Output\n- Role definitions implemented\n- SSO integration configured\n- Permission middleware active\n- Audit trail enabled\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| SSO login fails | Wrong callback URL | Verify IdP config |\n| Permission denied | Missing role mapping | Update group mappings |\n| Token expired | Short TTL | Refresh token logic |\n| Audit gaps | Async logging failed | Check log pipeline |\n\n## Examples\n\n### Quick Permission Check\n```typescript\nif (!checkPermission(user.role, 'write')) {\n  throw new ForbiddenError('Write permission required');\n}\n```\n\n## Resources\n- [Supabase Enterprise Guide](https://supabase.com/docs/enterprise)\n- [SAML 2.0 Specification](https://wiki.oasis-open.org/security/FrontPage)\n- [OpenID Connect Spec](https://openid.net/specs/openid-connect-core-1_0.html)\n\n## Next Steps\nFor major migrations, see `supabase-migration-deep-dive`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-enterprise-rbac/SKILL.md"
    },
    {
      "slug": "supabase-hello-world",
      "name": "supabase-hello-world",
      "description": "Create a minimal working Supabase example. Use when starting a new Supabase integration, testing your setup, or learning basic Supabase API patterns. Trigger with phrases like \"supabase hello world\", \"supabase example\", \"supabase quick start\", \"simple supabase code\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Hello World\n\n## Overview\nMinimal working example demonstrating core Supabase functionality.\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Valid API credentials configured\n- Development environment ready\n\n## Instructions\n\n### Step 1: Create Entry File\nCreate a new file for your hello world example.\n\n### Step 2: Import and Initialize Client\n```typescript\nimport { SupabaseClient } from '@supabase/supabase-js';\n\nconst client = new SupabaseClient({\n  apiKey: process.env.SUPABASE_API_KEY,\n});\n```\n\n### Step 3: Make Your First API Call\n```typescript\nasync function main() {\n  const result = await supabase.from('todos').insert({ task: 'Hello!' }).select(); console.log(result.data);\n}\n\nmain().catch(console.error);\n```\n\n## Output\n- Working code file with Supabase client initialization\n- Successful API response confirming connection\n- Console output showing:\n```\nSuccess! Your Supabase connection is working.\n```\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Import Error | SDK not installed | Verify with `npm list` or `pip show` |\n| Auth Error | Invalid credentials | Check environment variable is set |\n| Timeout | Network issues | Increase timeout or check connectivity |\n| Rate Limit | Too many requests | Wait and retry with exponential backoff |\n\n## Examples\n\n### TypeScript Example\n```typescript\nimport { SupabaseClient } from '@supabase/supabase-js';\n\nconst client = new SupabaseClient({\n  apiKey: process.env.SUPABASE_API_KEY,\n});\n\nasync function main() {\n  const result = await supabase.from('todos').insert({ task: 'Hello!' }).select(); console.log(result.data);\n}\n\nmain().catch(console.error);\n```\n\n### Python Example\n```python\nfrom supabase import SupabaseClient\n\nclient = SupabaseClient()\n\nresponse = supabase.table('todos').insert({'task': 'Hello!'}).execute(); print(response.data)\n```\n\n## Resources\n- [Supabase Getting Started](https://supabase.com/docs/getting-started)\n- [Supabase API Reference](https://supabase.com/docs/api)\n- [Supabase Examples](https://supabase.com/docs/examples)\n\n## Next Steps\nProceed to `supabase-local-dev-loop` for development workflow setup.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-hello-world/SKILL.md"
    },
    {
      "slug": "supabase-incident-runbook",
      "name": "supabase-incident-runbook",
      "description": "Execute Supabase incident response procedures with triage, mitigation, and postmortem. Use when responding to Supabase-related outages, investigating errors, or running post-incident reviews for Supabase integration failures. Trigger with phrases like \"supabase incident\", \"supabase outage\", \"supabase down\", \"supabase on-call\", \"supabase emergency\", \"supabase broken\". allowed-tools: Read, Grep, Bash(kubectl:*), Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Incident Runbook\n\n## Overview\nRapid incident response procedures for Supabase-related outages.\n\n## Prerequisites\n- Access to Supabase dashboard and status page\n- kubectl access to production cluster\n- Prometheus/Grafana access\n- Communication channels (Slack, PagerDuty)\n\n## Severity Levels\n\n| Level | Definition | Response Time | Examples |\n|-------|------------|---------------|----------|\n| P1 | Complete outage | < 15 min | Supabase API unreachable |\n| P2 | Degraded service | < 1 hour | High latency, partial failures |\n| P3 | Minor impact | < 4 hours | Webhook delays, non-critical errors |\n| P4 | No user impact | Next business day | Monitoring gaps |\n\n## Quick Triage\n\n```bash\n# 1. Check Supabase status\ncurl -s https://status.supabase.com | jq\n\n# 2. Check our integration health\ncurl -s https://api.yourapp.com/health | jq '.services.supabase'\n\n# 3. Check error rate (last 5 min)\ncurl -s localhost:9090/api/v1/query?query=rate(supabase_errors_total[5m])\n\n# 4. Recent error logs\nkubectl logs -l app=supabase-integration --since=5m | grep -i error | tail -20\n```\n\n## Decision Tree\n\n```\nSupabase API returning errors?\n‚îú‚îÄ YES: Is status.supabase.com showing incident?\n‚îÇ   ‚îú‚îÄ YES ‚Üí Wait for Supabase to resolve. Enable fallback.\n‚îÇ   ‚îî‚îÄ NO ‚Üí Our integration issue. Check credentials, config.\n‚îî‚îÄ NO: Is our service healthy?\n    ‚îú‚îÄ YES ‚Üí Likely resolved or intermittent. Monitor.\n    ‚îî‚îÄ NO ‚Üí Our infrastructure issue. Check pods, memory, network.\n```\n\n## Immediate Actions by Error Type\n\n### 401/403 - Authentication\n```bash\n# Verify API key is set\nkubectl get secret supabase-secrets -o jsonpath='{.data.api-key}' | base64 -d\n\n# Check if key was rotated\n# ‚Üí Verify in Supabase dashboard\n\n# Remediation: Update secret and restart pods\nkubectl create secret generic supabase-secrets --from-literal=api-key=NEW_KEY --dry-run=client -o yaml | kubectl apply -f -\nkubectl rollout restart deployment/supabase-integration\n```\n\n### 429 - Rate Limited\n```bash\n# Check rate limit headers\ncurl -v https://api.supabase.com 2>&1 | grep -i rate\n\n# Enable request queuing\nkubectl set env deployment/supabase-integration RATE_LIMIT_MODE=queue\n\n# Long-term: Contact Supabase for limit increase\n```\n\n### 500/503 - Supabase Errors\n```bash\n# Enable graceful degradation\nkubectl set env deployment/supabase-integration SUPABASE_FALLBACK=true\n\n# Notify users of degraded service\n# Update status page\n\n# Monitor Supabase status for resolution\n```\n\n## Communication Templates\n\n### Internal (Slack)\n```\nüî¥ P1 INCIDENT: Supabase Integration\nStatus: INVESTIGATING\nImpact: [Describe user impact]\nCurrent action: [What you're doing]\nNext update: [Time]\nIncident commander: @[name]\n```\n\n### External (Status Page)\n```\nSupabase Integration Issue\n\nWe're experiencing issues with our Supabase integration.\nSome users may experience [specific impact].\n\nWe're actively investigating and will provide updates.\n\nLast updated: [timestamp]\n```\n\n## Post-Incident\n\n### Evidence Collection\n```bash\n# Generate debug bundle\n./scripts/supabase-debug-bundle.sh\n\n# Export relevant logs\nkubectl logs -l app=supabase-integration --since=1h > incident-logs.txt\n\n# Capture metrics\ncurl \"localhost:9090/api/v1/query_range?query=supabase_errors_total&start=2h\" > metrics.json\n```\n\n### Postmortem Template\n```markdown\n## Incident: Supabase [Error Type]\n**Date:** YYYY-MM-DD\n**Duration:** X hours Y minutes\n**Severity:** P[1-4]\n\n### Summary\n[1-2 sentence description]\n\n### Timeline\n- HH:MM - [Event]\n- HH:MM - [Event]\n\n### Root Cause\n[Technical explanation]\n\n### Impact\n- Users affected: N\n- Revenue impact: $X\n\n### Action Items\n- [ ] [Preventive measure] - Owner - Due date\n```\n\n## Instructions\n\n### Step 1: Quick Triage\nRun the triage commands to identify the issue source.\n\n### Step 2: Follow Decision Tree\nDetermine if the issue is Supabase-side or internal.\n\n### Step 3: Execute Immediate Actions\nApply the appropriate remediation for the error type.\n\n### Step 4: Communicate Status\nUpdate internal and external stakeholders.\n\n## Output\n- Issue identified and categorized\n- Remediation applied\n- Stakeholders notified\n- Evidence collected for postmortem\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Can't reach status page | Network issue | Use mobile or VPN |\n| kubectl fails | Auth expired | Re-authenticate |\n| Metrics unavailable | Prometheus down | Check backup metrics |\n| Secret rotation fails | Permission denied | Escalate to admin |\n\n## Examples\n\n### One-Line Health Check\n```bash\ncurl -sf https://api.yourapp.com/health | jq '.services.supabase.status' || echo \"UNHEALTHY\"\n```\n\n## Resources\n- [Supabase Status Page](https://status.supabase.com)\n- [Supabase Support](https://support.supabase.com)\n\n## Next Steps\nFor data handling, see `supabase-data-handling`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-incident-runbook/SKILL.md"
    },
    {
      "slug": "supabase-install-auth",
      "name": "supabase-install-auth",
      "description": "Install and configure Supabase SDK/CLI authentication. Use when setting up a new Supabase integration, configuring API keys, or initializing Supabase in your project. Trigger with phrases like \"install supabase\", \"setup supabase\", \"supabase auth\", \"configure supabase API key\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(pip:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Install & Auth\n\n## Overview\nSet up Supabase SDK/CLI and configure authentication credentials.\n\n## Prerequisites\n- Node.js 18+ or Python 3.10+\n- Package manager (npm, pnpm, or pip)\n- Supabase account with API access\n- API key from Supabase dashboard\n\n## Instructions\n\n### Step 1: Install SDK\n```bash\n# Node.js\nnpm install @supabase/supabase-js\n\n# Python\npip install supabase\n```\n\n### Step 2: Configure Authentication\n```bash\n# Set environment variable\nexport SUPABASE_API_KEY=\"your-api-key\"\n\n# Or create .env file\necho 'SUPABASE_API_KEY=your-api-key' >> .env\n```\n\n### Step 3: Verify Connection\n```typescript\nconst result = await supabase.from('_test').select('*').limit(1); console.log(result.error ? 'Failed' : 'OK');\n```\n\n## Output\n- Installed SDK package in node_modules or site-packages\n- Environment variable or .env file with API key\n- Successful connection verification output\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Invalid API Key | Incorrect or expired key | Verify key in Supabase dashboard |\n| Rate Limited | Exceeded quota | Check quota at https://supabase.com/docs |\n| Network Error | Firewall blocking | Ensure outbound HTTPS allowed |\n| Module Not Found | Installation failed | Run `npm install` or `pip install` again |\n\n## Examples\n\n### TypeScript Setup\n```typescript\nimport { SupabaseClient } from '@supabase/supabase-js';\n\nconst client = new SupabaseClient({\n  apiKey: process.env.SUPABASE_API_KEY,\n});\n```\n\n### Python Setup\n```python\nfrom supabase import SupabaseClient\n\nclient = SupabaseClient(\n    api_key=os.environ.get('SUPABASE_API_KEY')\n)\n```\n\n## Resources\n- [Supabase Documentation](https://supabase.com/docs)\n- [Supabase Dashboard](https://api.supabase.com)\n- [Supabase Status](https://status.supabase.com)\n\n## Next Steps\nAfter successful auth, proceed to `supabase-hello-world` for your first API call.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-install-auth/SKILL.md"
    },
    {
      "slug": "supabase-known-pitfalls",
      "name": "supabase-known-pitfalls",
      "description": "Identify and avoid Supabase anti-patterns and common integration mistakes. Use when reviewing Supabase code for issues, onboarding new developers, or auditing existing Supabase integrations for best practices violations. Trigger with phrases like \"supabase mistakes\", \"supabase anti-patterns\", \"supabase pitfalls\", \"supabase what not to do\", \"supabase code review\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Known Pitfalls\n\n## Overview\nCommon mistakes and anti-patterns when integrating with Supabase.\n\n## Prerequisites\n- Access to Supabase codebase for review\n- Understanding of async/await patterns\n- Knowledge of security best practices\n- Familiarity with rate limiting concepts\n\n## Pitfall #1: Synchronous API Calls in Request Path\n\n### ‚ùå Anti-Pattern\n```typescript\n// User waits for Supabase API call\napp.post('/checkout', async (req, res) => {\n  const payment = await supabaseClient.processPayment(req.body);  // 2-5s latency\n  const notification = await supabaseClient.sendEmail(payment);   // Another 1-2s\n  res.json({ success: true });  // User waited 3-7s\n});\n```\n\n### ‚úÖ Better Approach\n```typescript\n// Return immediately, process async\napp.post('/checkout', async (req, res) => {\n  const jobId = await queue.enqueue('process-checkout', req.body);\n  res.json({ jobId, status: 'processing' });  // 50ms response\n});\n\n// Background job\nasync function processCheckout(data) {\n  const payment = await supabaseClient.processPayment(data);\n  await supabaseClient.sendEmail(payment);\n}\n```\n\n---\n\n## Pitfall #2: Not Handling Rate Limits\n\n### ‚ùå Anti-Pattern\n```typescript\n// Blast requests, crash on 429\nfor (const item of items) {\n  await supabaseClient.process(item);  // Will hit rate limit\n}\n```\n\n### ‚úÖ Better Approach\n```typescript\nimport pLimit from 'p-limit';\n\nconst limit = pLimit(5);  // Max 5 concurrent\nconst rateLimiter = new RateLimiter({ tokensPerSecond: 10 });\n\nfor (const item of items) {\n  await rateLimiter.acquire();\n  await limit(() => supabaseClient.process(item));\n}\n```\n\n---\n\n## Pitfall #3: Leaking API Keys\n\n### ‚ùå Anti-Pattern\n```typescript\n// In frontend code (visible to users!)\nconst client = new SupabaseClient({\n  apiKey: 'sk_live_ACTUAL_KEY_HERE',  // Anyone can see this\n});\n\n// In git history\ngit commit -m \"add API key\"  // Exposed forever\n```\n\n### ‚úÖ Better Approach\n```typescript\n// Backend only, environment variable\nconst client = new SupabaseClient({\n  apiKey: process.env.SUPABASE_API_KEY,\n});\n\n// Use .gitignore\n.env\n.env.local\n.env.*.local\n```\n\n---\n\n## Pitfall #4: Ignoring Idempotency\n\n### ‚ùå Anti-Pattern\n```typescript\n// Network error on response = duplicate charge!\ntry {\n  await supabaseClient.charge(order);\n} catch (error) {\n  if (error.code === 'NETWORK_ERROR') {\n    await supabaseClient.charge(order);  // Charged twice!\n  }\n}\n```\n\n### ‚úÖ Better Approach\n```typescript\nconst idempotencyKey = `order-${order.id}-${Date.now()}`;\n\nawait supabaseClient.charge(order, {\n  idempotencyKey,  // Safe to retry\n});\n```\n\n---\n\n## Pitfall #5: Not Validating Webhooks\n\n### ‚ùå Anti-Pattern\n```typescript\n// Trust any incoming request\napp.post('/webhook', (req, res) => {\n  processWebhook(req.body);  // Attacker can send fake events\n  res.sendStatus(200);\n});\n```\n\n### ‚úÖ Better Approach\n```typescript\napp.post('/webhook',\n  express.raw({ type: 'application/json' }),\n  (req, res) => {\n    const signature = req.headers['x-supabase-signature'];\n    if (!verifySupabaseSignature(req.body, signature)) {\n      return res.sendStatus(401);\n    }\n    processWebhook(JSON.parse(req.body));\n    res.sendStatus(200);\n  }\n);\n```\n\n---\n\n## Pitfall #6: Missing Error Handling\n\n### ‚ùå Anti-Pattern\n```typescript\n// Crashes on any error\nconst result = await supabaseClient.get(id);\nconsole.log(result.data.nested.value);  // TypeError if missing\n```\n\n### ‚úÖ Better Approach\n```typescript\ntry {\n  const result = await supabaseClient.get(id);\n  console.log(result?.data?.nested?.value ?? 'default');\n} catch (error) {\n  if (error instanceof SupabaseNotFoundError) {\n    return null;\n  }\n  if (error instanceof SupabaseRateLimitError) {\n    await sleep(error.retryAfter);\n    return this.get(id);  // Retry\n  }\n  throw error;  // Rethrow unknown errors\n}\n```\n\n---\n\n## Pitfall #7: Hardcoding Configuration\n\n### ‚ùå Anti-Pattern\n```typescript\nconst client = new SupabaseClient({\n  timeout: 5000,  // Too short for some operations\n  baseUrl: 'https://api.supabase.com',  // Can't change for staging\n});\n```\n\n### ‚úÖ Better Approach\n```typescript\nconst client = new SupabaseClient({\n  timeout: parseInt(process.env.SUPABASE_TIMEOUT || '30000'),\n  baseUrl: process.env.SUPABASE_BASE_URL || 'https://api.supabase.com',\n});\n```\n\n---\n\n## Pitfall #8: Not Implementing Circuit Breaker\n\n### ‚ùå Anti-Pattern\n```typescript\n// When Supabase is down, every request hangs\nfor (const user of users) {\n  await supabaseClient.sync(user);  // All timeout sequentially\n}\n```\n\n### ‚úÖ Better Approach\n```typescript\nimport CircuitBreaker from 'opossum';\n\nconst breaker = new CircuitBreaker(supabaseClient.sync, {\n  timeout: 10000,\n  errorThresholdPercentage: 50,\n  resetTimeout: 30000,\n});\n\n// Fails fast when circuit is open\nfor (const user of users) {\n  await breaker.fire(user).catch(handleFailure);\n}\n```\n\n---\n\n## Pitfall #9: Logging Sensitive Data\n\n### ‚ùå Anti-Pattern\n```typescript\nconsole.log('Request:', JSON.stringify(request));  // Logs API key, PII\nconsole.log('User:', user);  // Logs email, phone\n```\n\n### ‚úÖ Better Approach\n```typescript\nconst redacted = {\n  ...request,\n  apiKey: '[REDACTED]',\n  user: { id: user.id },  // Only non-sensitive fields\n};\nconsole.log('Request:', JSON.stringify(redacted));\n```\n\n---\n\n## Pitfall #10: No Graceful Degradation\n\n### ‚ùå Anti-Pattern\n```typescript\n// Entire feature broken if Supabase is down\nconst recommendations = await supabaseClient.getRecommendations(userId);\nreturn renderPage({ recommendations });  // Page crashes\n```\n\n### ‚úÖ Better Approach\n```typescript\nlet recommendations;\ntry {\n  recommendations = await supabaseClient.getRecommendations(userId);\n} catch (error) {\n  recommendations = await getFallbackRecommendations(userId);\n  reportDegradedService('supabase', error);\n}\nreturn renderPage({ recommendations, degraded: !recommendations });\n```\n\n---\n\n## Instructions\n\n### Step 1: Review for Anti-Patterns\nScan codebase for each pitfall pattern.\n\n### Step 2: Prioritize Fixes\nAddress security issues first, then performance.\n\n### Step 3: Implement Better Approach\nReplace anti-patterns with recommended patterns.\n\n### Step 4: Add Prevention\nSet up linting and CI checks to prevent recurrence.\n\n## Output\n- Anti-patterns identified\n- Fixes prioritized and implemented\n- Prevention measures in place\n- Code quality improved\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Too many findings | Legacy codebase | Prioritize security first |\n| Pattern not detected | Complex code | Manual review |\n| False positive | Similar code | Whitelist exceptions |\n| Fix breaks tests | Behavior change | Update tests |\n\n## Examples\n\n### Quick Pitfall Scan\n```bash\n# Check for common pitfalls\ngrep -r \"sk_live_\" --include=\"*.ts\" src/        # Key leakage\ngrep -r \"console.log\" --include=\"*.ts\" src/     # Potential PII logging\n```\n\n## Resources\n- [Supabase Security Guide](https://supabase.com/docs/security)\n- [Supabase Best Practices](https://supabase.com/docs/best-practices)\n\n## Quick Reference Card\n\n| Pitfall | Detection | Prevention |\n|---------|-----------|------------|\n| Sync in request | High latency | Use queues |\n| Rate limit ignore | 429 errors | Implement backoff |\n| Key leakage | Git history scan | Env vars, .gitignore |\n| No idempotency | Duplicate records | Idempotency keys |\n| Unverified webhooks | Security audit | Signature verification |\n| Missing error handling | Crashes | Try-catch, types |\n| Hardcoded config | Code review | Environment variables |\n| No circuit breaker | Cascading failures | opossum, resilience4j |\n| Logging PII | Log audit | Redaction middleware |\n| No degradation | Total outages | Fallback systems |",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-known-pitfalls/SKILL.md"
    },
    {
      "slug": "supabase-load-scale",
      "name": "supabase-load-scale",
      "description": "Implement Supabase load testing, auto-scaling, and capacity planning strategies. Use when running performance tests, configuring horizontal scaling, or planning capacity for Supabase integrations. Trigger with phrases like \"supabase load test\", \"supabase scale\", \"supabase performance test\", \"supabase capacity\", \"supabase k6\", \"supabase benchmark\". allowed-tools: Read, Write, Edit, Bash(k6:*), Bash(kubectl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Load & Scale\n\n## Overview\nLoad testing, scaling strategies, and capacity planning for Supabase integrations.\n\n## Prerequisites\n- k6 load testing tool installed\n- Kubernetes cluster with HPA configured\n- Prometheus for metrics collection\n- Test environment API keys\n\n## Load Testing with k6\n\n### Basic Load Test\n```javascript\n// supabase-load-test.js\nimport http from 'k6/http';\nimport { check, sleep } from 'k6';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 10 },   // Ramp up\n    { duration: '5m', target: 10 },   // Steady state\n    { duration: '2m', target: 50 },   // Ramp to peak\n    { duration: '5m', target: 50 },   // Stress test\n    { duration: '2m', target: 0 },    // Ramp down\n  ],\n  thresholds: {\n    http_req_duration: ['p(95)<200'],\n    http_req_failed: ['rate<0.01'],\n  },\n};\n\nexport default function () {\n  const response = http.post(\n    'https://api.supabase.com/v1/resource',\n    JSON.stringify({ test: true }),\n    {\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${__ENV.SUPABASE_API_KEY}`,\n      },\n    }\n  );\n\n  check(response, {\n    'status is 200': (r) => r.status === 200,\n    'latency < 200ms': (r) => r.timings.duration < 200,\n  });\n\n  sleep(1);\n}\n```\n\n### Run Load Test\n```bash\n# Install k6\nbrew install k6  # macOS\n# or: sudo apt install k6  # Linux\n\n# Run test\nk6 run --env SUPABASE_API_KEY=${SUPABASE_API_KEY} supabase-load-test.js\n\n# Run with output to InfluxDB\nk6 run --out influxdb=http://localhost:8086/k6 supabase-load-test.js\n```\n\n## Scaling Patterns\n\n### Horizontal Scaling\n```yaml\n# kubernetes HPA\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: supabase-integration-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: supabase-integration\n  minReplicas: 2\n  maxReplicas: 20\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n    - type: Pods\n      pods:\n        metric:\n          name: supabase_queue_depth\n        target:\n          type: AverageValue\n          averageValue: 100\n```\n\n### Connection Pooling\n```typescript\nimport { Pool } from 'generic-pool';\n\nconst supabasePool = Pool.create({\n  create: async () => {\n    return new SupabaseClient({\n      apiKey: process.env.SUPABASE_API_KEY!,\n    });\n  },\n  destroy: async (client) => {\n    await client.close();\n  },\n  max: 20,\n  min: 5,\n  idleTimeoutMillis: 30000,\n});\n\nasync function withSupabaseClient<T>(\n  fn: (client: SupabaseClient) => Promise<T>\n): Promise<T> {\n  const client = await supabasePool.acquire();\n  try {\n    return await fn(client);\n  } finally {\n    supabasePool.release(client);\n  }\n}\n```\n\n## Capacity Planning\n\n### Metrics to Monitor\n| Metric | Warning | Critical |\n|--------|---------|----------|\n| CPU Utilization | > 70% | > 85% |\n| Memory Usage | > 75% | > 90% |\n| Request Queue Depth | > 100 | > 500 |\n| Error Rate | > 1% | > 5% |\n| P95 Latency | > 500ms | > 2000ms |\n\n### Capacity Calculation\n```typescript\ninterface CapacityEstimate {\n  currentRPS: number;\n  maxRPS: number;\n  headroom: number;\n  scaleRecommendation: string;\n}\n\nfunction estimateSupabaseCapacity(\n  metrics: SystemMetrics\n): CapacityEstimate {\n  const currentRPS = metrics.requestsPerSecond;\n  const avgLatency = metrics.p50Latency;\n  const cpuUtilization = metrics.cpuPercent;\n\n  // Estimate max RPS based on current performance\n  const maxRPS = currentRPS / (cpuUtilization / 100) * 0.7; // 70% target\n  const headroom = ((maxRPS - currentRPS) / currentRPS) * 100;\n\n  return {\n    currentRPS,\n    maxRPS: Math.floor(maxRPS),\n    headroom: Math.round(headroom),\n    scaleRecommendation: headroom < 30\n      ? 'Scale up soon'\n      : headroom < 50\n      ? 'Monitor closely'\n      : 'Adequate capacity',\n  };\n}\n```\n\n## Benchmark Results Template\n\n```markdown\n## Supabase Performance Benchmark\n**Date:** YYYY-MM-DD\n**Environment:** [staging/production]\n**SDK Version:** X.Y.Z\n\n### Test Configuration\n- Duration: 10 minutes\n- Ramp: 10 ‚Üí 100 ‚Üí 10 VUs\n- Target endpoint: /v1/resource\n\n### Results\n| Metric | Value |\n|--------|-------|\n| Total Requests | 50,000 |\n| Success Rate | 99.9% |\n| P50 Latency | 120ms |\n| P95 Latency | 350ms |\n| P99 Latency | 800ms |\n| Max RPS Achieved | 150 |\n\n### Observations\n- [Key finding 1]\n- [Key finding 2]\n\n### Recommendations\n- [Scaling recommendation]\n```\n\n## Instructions\n\n### Step 1: Create Load Test Script\nWrite k6 test script with appropriate thresholds.\n\n### Step 2: Configure Auto-Scaling\nSet up HPA with CPU and custom metrics.\n\n### Step 3: Run Load Test\nExecute test and collect metrics.\n\n### Step 4: Analyze and Document\nRecord results in benchmark template.\n\n## Output\n- Load test script created\n- HPA configured\n- Benchmark results documented\n- Capacity recommendations defined\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| k6 timeout | Rate limited | Reduce RPS |\n| HPA not scaling | Wrong metrics | Verify metric name |\n| Connection refused | Pool exhausted | Increase pool size |\n| Inconsistent results | Warm-up needed | Add ramp-up phase |\n\n## Examples\n\n### Quick k6 Test\n```bash\nk6 run --vus 10 --duration 30s supabase-load-test.js\n```\n\n### Check Current Capacity\n```typescript\nconst metrics = await getSystemMetrics();\nconst capacity = estimateSupabaseCapacity(metrics);\nconsole.log('Headroom:', capacity.headroom + '%');\nconsole.log('Recommendation:', capacity.scaleRecommendation);\n```\n\n### Scale HPA Manually\n```bash\nkubectl scale deployment supabase-integration --replicas=5\nkubectl get hpa supabase-integration-hpa\n```\n\n## Resources\n- [k6 Documentation](https://k6.io/docs/)\n- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n- [Supabase Rate Limits](https://supabase.com/docs/rate-limits)\n\n## Next Steps\nFor reliability patterns, see `supabase-reliability-patterns`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-load-scale/SKILL.md"
    },
    {
      "slug": "supabase-local-dev-loop",
      "name": "supabase-local-dev-loop",
      "description": "Configure Supabase local development with hot reload and testing. Use when setting up a development environment, configuring test workflows, or establishing a fast iteration cycle with Supabase. Trigger with phrases like \"supabase dev setup\", \"supabase local development\", \"supabase dev environment\", \"develop with supabase\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(pnpm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Local Dev Loop\n\n## Overview\nSet up a fast, reproducible local development workflow for Supabase.\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Node.js 18+ with npm/pnpm\n- Code editor with TypeScript support\n- Git for version control\n\n## Instructions\n\n### Step 1: Create Project Structure\n```\nmy-supabase-project/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ supabase/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts       # Supabase client wrapper\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.ts       # Configuration management\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.ts        # Helper functions\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îî‚îÄ‚îÄ supabase.test.ts\n‚îú‚îÄ‚îÄ .env.local              # Local secrets (git-ignored)\n‚îú‚îÄ‚îÄ .env.example            # Template for team\n‚îî‚îÄ‚îÄ package.json\n```\n\n### Step 2: Configure Environment\n```bash\n# Copy environment template\ncp .env.example .env.local\n\n# Install dependencies\nnpm install\n\n# Start development server\nnpm run dev\n```\n\n### Step 3: Setup Hot Reload\n```json\n{\n  \"scripts\": {\n    \"dev\": \"tsx watch src/index.ts\",\n    \"test\": \"vitest\",\n    \"test:watch\": \"vitest --watch\"\n  }\n}\n```\n\n### Step 4: Configure Testing\n```typescript\nimport { describe, it, expect, vi } from 'vitest';\nimport { SupabaseClient } from '../src/supabase/client';\n\ndescribe('Supabase Client', () => {\n  it('should initialize with API key', () => {\n    const client = new SupabaseClient({ apiKey: 'test-key' });\n    expect(client).toBeDefined();\n  });\n});\n```\n\n## Output\n- Working development environment with hot reload\n- Configured test suite with mocking\n- Environment variable management\n- Fast iteration cycle for Supabase development\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Module not found | Missing dependency | Run `npm install` |\n| Port in use | Another process | Kill process or change port |\n| Env not loaded | Missing .env.local | Copy from .env.example |\n| Test timeout | Slow network | Increase test timeout |\n\n## Examples\n\n### Mock Supabase Responses\n```typescript\nvi.mock('@supabase/supabase-js', () => ({\n  SupabaseClient: vi.fn().mockImplementation(() => ({\n    // Mock methods here\n  })),\n}));\n```\n\n### Debug Mode\n```bash\n# Enable verbose logging\nDEBUG=SUPABASE=* npm run dev\n```\n\n## Resources\n- [Supabase SDK Reference](https://supabase.com/docs/sdk)\n- [Vitest Documentation](https://vitest.dev/)\n- [tsx Documentation](https://github.com/esbuild-kit/tsx)\n\n## Next Steps\nSee `supabase-sdk-patterns` for production-ready code patterns.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-local-dev-loop/SKILL.md"
    },
    {
      "slug": "supabase-migration-deep-dive",
      "name": "supabase-migration-deep-dive",
      "description": "Execute Supabase major re-architecture and migration strategies with strangler fig pattern. Use when migrating to or from Supabase, performing major version upgrades, or re-platforming existing integrations to Supabase. Trigger with phrases like \"migrate supabase\", \"supabase migration\", \"switch to supabase\", \"supabase replatform\", \"supabase upgrade major\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(node:*), Bash(kubectl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Migration Deep Dive\n\n## Overview\nComprehensive guide for migrating to or from Supabase, or major version upgrades.\n\n## Prerequisites\n- Current system documentation\n- Supabase SDK installed\n- Feature flag infrastructure\n- Rollback strategy tested\n\n## Migration Types\n\n| Type | Complexity | Duration | Risk |\n|------|-----------|----------|------|\n| Fresh install | Low | Days | Low |\n| From competitor | Medium | Weeks | Medium |\n| Major version | Medium | Weeks | Medium |\n| Full replatform | High | Months | High |\n\n## Pre-Migration Assessment\n\n### Step 1: Current State Analysis\n```bash\n# Document current implementation\nfind . -name \"*.ts\" -o -name \"*.py\" | xargs grep -l \"supabase\" > supabase-files.txt\n\n# Count integration points\nwc -l supabase-files.txt\n\n# Identify dependencies\nnpm list | grep supabase\npip freeze | grep supabase\n```\n\n### Step 2: Data Inventory\n```typescript\ninterface MigrationInventory {\n  dataTypes: string[];\n  recordCounts: Record<string, number>;\n  dependencies: string[];\n  integrationPoints: string[];\n  customizations: string[];\n}\n\nasync function assessSupabaseMigration(): Promise<MigrationInventory> {\n  return {\n    dataTypes: await getDataTypes(),\n    recordCounts: await getRecordCounts(),\n    dependencies: await analyzeDependencies(),\n    integrationPoints: await findIntegrationPoints(),\n    customizations: await documentCustomizations(),\n  };\n}\n```\n\n## Migration Strategy: Strangler Fig Pattern\n\n```\nPhase 1: Parallel Run\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Old       ‚îÇ     ‚îÇ   New       ‚îÇ\n‚îÇ   System    ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ  Supabase   ‚îÇ\n‚îÇ   (100%)    ‚îÇ     ‚îÇ   (0%)      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nPhase 2: Gradual Shift\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Old       ‚îÇ     ‚îÇ   New       ‚îÇ\n‚îÇ   (50%)     ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ   (50%)     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nPhase 3: Complete\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Old       ‚îÇ     ‚îÇ   New       ‚îÇ\n‚îÇ   (0%)      ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ   (100%)    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Implementation Plan\n\n### Phase 1: Setup (Week 1-2)\n```bash\n# Install Supabase SDK\nnpm install @supabase/supabase-js\n\n# Configure credentials\ncp .env.example .env.supabase\n# Edit with new credentials\n\n# Verify connectivity\nnode -e \"require('@supabase/supabase-js').ping()\"\n```\n\n### Phase 2: Adapter Layer (Week 3-4)\n```typescript\n// src/adapters/supabase.ts\ninterface ServiceAdapter {\n  create(data: CreateInput): Promise<Resource>;\n  read(id: string): Promise<Resource>;\n  update(id: string, data: UpdateInput): Promise<Resource>;\n  delete(id: string): Promise<void>;\n}\n\nclass SupabaseAdapter implements ServiceAdapter {\n  async create(data: CreateInput): Promise<Resource> {\n    const supabaseData = this.transform(data);\n    return supabaseClient.create(supabaseData);\n  }\n\n  private transform(data: CreateInput): SupabaseInput {\n    // Map from old format to Supabase format\n  }\n}\n```\n\n### Phase 3: Data Migration (Week 5-6)\n```typescript\nasync function migrateSupabaseData(): Promise<MigrationResult> {\n  const batchSize = 100;\n  let processed = 0;\n  let errors: MigrationError[] = [];\n\n  for await (const batch of oldSystem.iterateBatches(batchSize)) {\n    try {\n      const transformed = batch.map(transform);\n      await supabaseClient.batchCreate(transformed);\n      processed += batch.length;\n    } catch (error) {\n      errors.push({ batch, error });\n    }\n\n    // Progress update\n    console.log(`Migrated ${processed} records`);\n  }\n\n  return { processed, errors };\n}\n```\n\n### Phase 4: Traffic Shift (Week 7-8)\n```typescript\n// Feature flag controlled traffic split\nfunction getServiceAdapter(): ServiceAdapter {\n  const supabasePercentage = getFeatureFlag('supabase_migration_percentage');\n\n  if (Math.random() * 100 < supabasePercentage) {\n    return new SupabaseAdapter();\n  }\n\n  return new LegacyAdapter();\n}\n```\n\n## Rollback Plan\n\n```bash\n# Immediate rollback\nkubectl set env deployment/app SUPABASE_ENABLED=false\nkubectl rollout restart deployment/app\n\n# Data rollback (if needed)\n./scripts/restore-from-backup.sh --date YYYY-MM-DD\n\n# Verify rollback\ncurl https://app.yourcompany.com/health | jq '.services.supabase'\n```\n\n## Post-Migration Validation\n\n```typescript\nasync function validateSupabaseMigration(): Promise<ValidationReport> {\n  const checks = [\n    { name: 'Data count match', fn: checkDataCounts },\n    { name: 'API functionality', fn: checkApiFunctionality },\n    { name: 'Performance baseline', fn: checkPerformance },\n    { name: 'Error rates', fn: checkErrorRates },\n  ];\n\n  const results = await Promise.all(\n    checks.map(async c => ({ name: c.name, result: await c.fn() }))\n  );\n\n  return { checks: results, passed: results.every(r => r.result.success) };\n}\n```\n\n## Instructions\n\n### Step 1: Assess Current State\nDocument existing implementation and data inventory.\n\n### Step 2: Build Adapter Layer\nCreate abstraction layer for gradual migration.\n\n### Step 3: Migrate Data\nRun batch data migration with error handling.\n\n### Step 4: Shift Traffic\nGradually route traffic to new Supabase integration.\n\n## Output\n- Migration assessment complete\n- Adapter layer implemented\n- Data migrated successfully\n- Traffic fully shifted to Supabase\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Data mismatch | Transform errors | Validate transform logic |\n| Performance drop | No caching | Add caching layer |\n| Rollback triggered | Errors spiked | Reduce traffic percentage |\n| Validation failed | Missing data | Check batch processing |\n\n## Examples\n\n### Quick Migration Status\n```typescript\nconst status = await validateSupabaseMigration();\nconsole.log(`Migration ${status.passed ? 'PASSED' : 'FAILED'}`);\nstatus.checks.forEach(c => console.log(`  ${c.name}: ${c.result.success}`));\n```\n\n## Resources\n- [Strangler Fig Pattern](https://martinfowler.com/bliki/StranglerFigApplication.html)\n- [Supabase Migration Guide](https://supabase.com/docs/migration)\n\n## Flagship+ Skills\nFor advanced troubleshooting, see `supabase-advanced-troubleshooting`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-migration-deep-dive/SKILL.md"
    },
    {
      "slug": "supabase-multi-env-setup",
      "name": "supabase-multi-env-setup",
      "description": "Configure Supabase across development, staging, and production environments. Use when setting up multi-environment deployments, configuring per-environment secrets, or implementing environment-specific Supabase configurations. Trigger with phrases like \"supabase environments\", \"supabase staging\", \"supabase dev prod\", \"supabase environment setup\", \"supabase config by env\". allowed-tools: Read, Write, Edit, Bash(aws:*), Bash(gcloud:*), Bash(vault:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Multi-Environment Setup\n\n## Overview\nConfigure Supabase across development, staging, and production environments.\n\n## Prerequisites\n- Separate Supabase accounts or API keys per environment\n- Secret management solution (Vault, AWS Secrets Manager, etc.)\n- CI/CD pipeline with environment variables\n- Environment detection in application\n\n## Environment Strategy\n\n| Environment | Purpose | API Keys | Data |\n|-------------|---------|----------|------|\n| Development | Local dev | Test keys | Sandbox |\n| Staging | Pre-prod validation | Staging keys | Test data |\n| Production | Live traffic | Production keys | Real data |\n\n## Configuration Structure\n\n```\nconfig/\n‚îú‚îÄ‚îÄ supabase/\n‚îÇ   ‚îú‚îÄ‚îÄ base.json           # Shared config\n‚îÇ   ‚îú‚îÄ‚îÄ development.json    # Dev overrides\n‚îÇ   ‚îú‚îÄ‚îÄ staging.json        # Staging overrides\n‚îÇ   ‚îî‚îÄ‚îÄ production.json     # Prod overrides\n```\n\n### base.json\n```json\n{\n  \"timeout\": 30000,\n  \"retries\": 3,\n  \"cache\": {\n    \"enabled\": true,\n    \"ttlSeconds\": 60\n  }\n}\n```\n\n### development.json\n```json\n{\n  \"apiKey\": \"${SUPABASE_API_KEY}\",\n  \"baseUrl\": \"https://api-sandbox.supabase.com\",\n  \"debug\": true,\n  \"cache\": {\n    \"enabled\": false\n  }\n}\n```\n\n### staging.json\n```json\n{\n  \"apiKey\": \"${SUPABASE_API_KEY_STAGING}\",\n  \"baseUrl\": \"https://api-staging.supabase.com\",\n  \"debug\": false\n}\n```\n\n### production.json\n```json\n{\n  \"apiKey\": \"${SUPABASE_API_KEY_PROD}\",\n  \"baseUrl\": \"https://api.supabase.com\",\n  \"debug\": false,\n  \"retries\": 5\n}\n```\n\n## Environment Detection\n\n```typescript\n// src/supabase/config.ts\nimport baseConfig from '../../config/supabase/base.json';\n\ntype Environment = 'development' | 'staging' | 'production';\n\nfunction detectEnvironment(): Environment {\n  const env = process.env.NODE_ENV || 'development';\n  const validEnvs: Environment[] = ['development', 'staging', 'production'];\n  return validEnvs.includes(env as Environment)\n    ? (env as Environment)\n    : 'development';\n}\n\nexport function getSupabaseConfig() {\n  const env = detectEnvironment();\n  const envConfig = require(`../../config/supabase/${env}.json`);\n\n  return {\n    ...baseConfig,\n    ...envConfig,\n    environment: env,\n  };\n}\n```\n\n## Secret Management by Environment\n\n### Local Development\n```bash\n# .env.local (git-ignored)\nSUPABASE_API_KEY=sk_test_dev_***\n```\n\n### CI/CD (GitHub Actions)\n```yaml\nenv:\n  SUPABASE_API_KEY: ${{ secrets.SUPABASE_API_KEY_${{ matrix.environment }} }}\n```\n\n### Production (Vault/Secrets Manager)\n```bash\n# AWS Secrets Manager\naws secretsmanager get-secret-value --secret-id supabase/production/api-key\n\n# GCP Secret Manager\ngcloud secrets versions access latest --secret=supabase-api-key\n\n# HashiCorp Vault\nvault kv get -field=api_key secret/supabase/production\n```\n\n## Environment Isolation\n\n```typescript\n// Prevent production operations in non-prod\nfunction guardProductionOperation(operation: string): void {\n  const config = getSupabaseConfig();\n\n  if (config.environment !== 'production') {\n    console.warn(`[supabase] ${operation} blocked in ${config.environment}`);\n    throw new Error(`${operation} only allowed in production`);\n  }\n}\n\n// Usage\nasync function deleteAllData() {\n  guardProductionOperation('deleteAllData');\n  // Dangerous operation here\n}\n```\n\n## Feature Flags by Environment\n\n```typescript\nconst featureFlags: Record<Environment, Record<string, boolean>> = {\n  development: {\n    newFeature: true,\n    betaApi: true,\n  },\n  staging: {\n    newFeature: true,\n    betaApi: false,\n  },\n  production: {\n    newFeature: false,\n    betaApi: false,\n  },\n};\n```\n\n## Instructions\n\n### Step 1: Create Config Structure\nSet up the base and per-environment configuration files.\n\n### Step 2: Implement Environment Detection\nAdd logic to detect and load environment-specific config.\n\n### Step 3: Configure Secrets\nStore API keys securely using your secret management solution.\n\n### Step 4: Add Environment Guards\nImplement safeguards for production-only operations.\n\n## Output\n- Multi-environment config structure\n- Environment detection logic\n- Secure secret management\n- Production safeguards enabled\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Wrong environment | Missing NODE_ENV | Set environment variable |\n| Secret not found | Wrong secret path | Verify secret manager config |\n| Config merge fails | Invalid JSON | Validate config files |\n| Production guard triggered | Wrong environment | Check NODE_ENV value |\n\n## Examples\n\n### Quick Environment Check\n```typescript\nconst env = getSupabaseConfig();\nconsole.log(`Running in ${env.environment} with ${env.baseUrl}`);\n```\n\n## Resources\n- [Supabase Environments Guide](https://supabase.com/docs/environments)\n- [12-Factor App Config](https://12factor.net/config)\n\n## Next Steps\nFor observability setup, see `supabase-observability`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-multi-env-setup/SKILL.md"
    },
    {
      "slug": "supabase-observability",
      "name": "supabase-observability",
      "description": "Set up comprehensive observability for Supabase integrations with metrics, traces, and alerts. Use when implementing monitoring for Supabase operations, setting up dashboards, or configuring alerting for Supabase integration health. Trigger with phrases like \"supabase monitoring\", \"supabase metrics\", \"supabase observability\", \"monitor supabase\", \"supabase alerts\", \"supabase tracing\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Observability\n\n## Overview\nSet up comprehensive observability for Supabase integrations.\n\n## Prerequisites\n- Prometheus or compatible metrics backend\n- OpenTelemetry SDK installed\n- Grafana or similar dashboarding tool\n- AlertManager configured\n\n## Metrics Collection\n\n### Key Metrics\n| Metric | Type | Description |\n|--------|------|-------------|\n| `supabase_requests_total` | Counter | Total API requests |\n| `supabase_request_duration_seconds` | Histogram | Request latency |\n| `supabase_errors_total` | Counter | Error count by type |\n| `supabase_rate_limit_remaining` | Gauge | Rate limit headroom |\n\n### Prometheus Metrics\n\n```typescript\nimport { Registry, Counter, Histogram, Gauge } from 'prom-client';\n\nconst registry = new Registry();\n\nconst requestCounter = new Counter({\n  name: 'supabase_requests_total',\n  help: 'Total Supabase API requests',\n  labelNames: ['method', 'status'],\n  registers: [registry],\n});\n\nconst requestDuration = new Histogram({\n  name: 'supabase_request_duration_seconds',\n  help: 'Supabase request duration',\n  labelNames: ['method'],\n  buckets: [0.05, 0.1, 0.25, 0.5, 1, 2.5, 5],\n  registers: [registry],\n});\n\nconst errorCounter = new Counter({\n  name: 'supabase_errors_total',\n  help: 'Supabase errors by type',\n  labelNames: ['error_type'],\n  registers: [registry],\n});\n```\n\n### Instrumented Client\n\n```typescript\nasync function instrumentedRequest<T>(\n  method: string,\n  operation: () => Promise<T>\n): Promise<T> {\n  const timer = requestDuration.startTimer({ method });\n\n  try {\n    const result = await operation();\n    requestCounter.inc({ method, status: 'success' });\n    return result;\n  } catch (error: any) {\n    requestCounter.inc({ method, status: 'error' });\n    errorCounter.inc({ error_type: error.code || 'unknown' });\n    throw error;\n  } finally {\n    timer();\n  }\n}\n```\n\n## Distributed Tracing\n\n### OpenTelemetry Setup\n\n```typescript\nimport { trace, SpanStatusCode } from '@opentelemetry/api';\n\nconst tracer = trace.getTracer('supabase-client');\n\nasync function tracedSupabaseCall<T>(\n  operationName: string,\n  operation: () => Promise<T>\n): Promise<T> {\n  return tracer.startActiveSpan(`supabase.${operationName}`, async (span) => {\n    try {\n      const result = await operation();\n      span.setStatus({ code: SpanStatusCode.OK });\n      return result;\n    } catch (error: any) {\n      span.setStatus({ code: SpanStatusCode.ERROR, message: error.message });\n      span.recordException(error);\n      throw error;\n    } finally {\n      span.end();\n    }\n  });\n}\n```\n\n## Logging Strategy\n\n### Structured Logging\n\n```typescript\nimport pino from 'pino';\n\nconst logger = pino({\n  name: 'supabase',\n  level: process.env.LOG_LEVEL || 'info',\n});\n\nfunction logSupabaseOperation(\n  operation: string,\n  data: Record<string, any>,\n  duration: number\n) {\n  logger.info({\n    service: 'supabase',\n    operation,\n    duration_ms: duration,\n    ...data,\n  });\n}\n```\n\n## Alert Configuration\n\n### Prometheus AlertManager Rules\n\n```yaml\n# supabase_alerts.yaml\ngroups:\n  - name: supabase_alerts\n    rules:\n      - alert: SupabaseHighErrorRate\n        expr: |\n          rate(supabase_errors_total[5m]) /\n          rate(supabase_requests_total[5m]) > 0.05\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Supabase error rate > 5%\"\n\n      - alert: SupabaseHighLatency\n        expr: |\n          histogram_quantile(0.95,\n            rate(supabase_request_duration_seconds_bucket[5m])\n          ) > 2\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Supabase P95 latency > 2s\"\n\n      - alert: SupabaseDown\n        expr: up{job=\"supabase\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Supabase integration is down\"\n```\n\n## Dashboard\n\n### Grafana Panel Queries\n\n```json\n{\n  \"panels\": [\n    {\n      \"title\": \"Supabase Request Rate\",\n      \"targets\": [{\n        \"expr\": \"rate(supabase_requests_total[5m])\"\n      }]\n    },\n    {\n      \"title\": \"Supabase Latency P50/P95/P99\",\n      \"targets\": [{\n        \"expr\": \"histogram_quantile(0.5, rate(supabase_request_duration_seconds_bucket[5m]))\"\n      }]\n    }\n  ]\n}\n```\n\n## Instructions\n\n### Step 1: Set Up Metrics Collection\nImplement Prometheus counters, histograms, and gauges for key operations.\n\n### Step 2: Add Distributed Tracing\nIntegrate OpenTelemetry for end-to-end request tracing.\n\n### Step 3: Configure Structured Logging\nSet up JSON logging with consistent field names.\n\n### Step 4: Create Alert Rules\nDefine Prometheus alerting rules for error rates and latency.\n\n## Output\n- Metrics collection enabled\n- Distributed tracing configured\n- Structured logging implemented\n- Alert rules deployed\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Missing metrics | No instrumentation | Wrap client calls |\n| Trace gaps | Missing propagation | Check context headers |\n| Alert storms | Wrong thresholds | Tune alert rules |\n| High cardinality | Too many labels | Reduce label values |\n\n## Examples\n\n### Quick Metrics Endpoint\n```typescript\napp.get('/metrics', async (req, res) => {\n  res.set('Content-Type', registry.contentType);\n  res.send(await registry.metrics());\n});\n```\n\n## Resources\n- [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)\n- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)\n- [Supabase Observability Guide](https://supabase.com/docs/observability)\n\n## Next Steps\nFor incident response, see `supabase-incident-runbook`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-observability/SKILL.md"
    },
    {
      "slug": "supabase-performance-tuning",
      "name": "supabase-performance-tuning",
      "description": "Optimize Supabase API performance with caching, batching, and connection pooling. Use when experiencing slow API responses, implementing caching strategies, or optimizing request throughput for Supabase integrations. Trigger with phrases like \"supabase performance\", \"optimize supabase\", \"supabase latency\", \"supabase caching\", \"supabase slow\", \"supabase batch\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Performance Tuning\n\n## Overview\nOptimize Supabase API performance with caching, batching, and connection pooling.\n\n## Prerequisites\n- Supabase SDK installed\n- Understanding of async patterns\n- Redis or in-memory cache available (optional)\n- Performance monitoring in place\n\n## Latency Benchmarks\n\n| Operation | P50 | P95 | P99 |\n|-----------|-----|-----|-----|\n| Select | 15ms | 50ms | 100ms |\n| Insert | 25ms | 75ms | 150ms |\n| Real-time Subscribe | 50ms | 150ms | 300ms |\n\n## Caching Strategy\n\n### Response Caching\n```typescript\nimport { LRUCache } from 'lru-cache';\n\nconst cache = new LRUCache<string, any>({\n  max: 1000,\n  ttl: 30000, // 1 minute\n  updateAgeOnGet: true,\n});\n\nasync function cachedSupabaseRequest<T>(\n  key: string,\n  fetcher: () => Promise<T>,\n  ttl?: number\n): Promise<T> {\n  const cached = cache.get(key);\n  if (cached) return cached as T;\n\n  const result = await fetcher();\n  cache.set(key, result, { ttl });\n  return result;\n}\n```\n\n### Redis Caching (Distributed)\n```typescript\nimport Redis from 'ioredis';\n\nconst redis = new Redis(process.env.REDIS_URL);\n\nasync function cachedWithRedis<T>(\n  key: string,\n  fetcher: () => Promise<T>,\n  ttlSeconds = 60\n): Promise<T> {\n  const cached = await redis.get(key);\n  if (cached) return JSON.parse(cached);\n\n  const result = await fetcher();\n  await redis.setex(key, ttlSeconds, JSON.stringify(result));\n  return result;\n}\n```\n\n## Request Batching\n\n```typescript\nimport DataLoader from 'dataloader';\n\nconst supabaseLoader = new DataLoader<string, any>(\n  async (ids) => {\n    // Batch fetch from Supabase\n    const results = await supabaseClient.batchGet(ids);\n    return ids.map(id => results.find(r => r.id === id) || null);\n  },\n  {\n    maxBatchSize: 100,\n    batchScheduleFn: callback => setTimeout(callback, 10),\n  }\n);\n\n// Usage - automatically batched\nconst [item1, item2, item3] = await Promise.all([\n  supabaseLoader.load('id-1'),\n  supabaseLoader.load('id-2'),\n  supabaseLoader.load('id-3'),\n]);\n```\n\n## Connection Optimization\n\n```typescript\nimport { Agent } from 'https';\n\n// Keep-alive connection pooling\nconst agent = new Agent({\n  keepAlive: true,\n  maxSockets: 15,\n  maxFreeSockets: 5,\n  timeout: 30000,\n});\n\nconst client = new SupabaseClient({\n  apiKey: process.env.SUPABASE_API_KEY!,\n  httpAgent: agent,\n});\n```\n\n## Pagination Optimization\n\n```typescript\nasync function* paginatedSupabaseList<T>(\n  fetcher: (cursor?: string) => Promise<{ data: T[]; nextCursor?: string }>\n): AsyncGenerator<T> {\n  let cursor: string | undefined;\n\n  do {\n    const { data, nextCursor } = await fetcher(cursor);\n    for (const item of data) {\n      yield item;\n    }\n    cursor = nextCursor;\n  } while (cursor);\n}\n\n// Usage\nfor await (const item of paginatedSupabaseList(cursor =>\n  supabaseClient.list({ cursor, limit: 100 })\n)) {\n  await process(item);\n}\n```\n\n## Performance Monitoring\n\n```typescript\nasync function measuredSupabaseCall<T>(\n  operation: string,\n  fn: () => Promise<T>\n): Promise<T> {\n  const start = performance.now();\n  try {\n    const result = await fn();\n    const duration = performance.now() - start;\n    console.log({ operation, duration, status: 'success' });\n    return result;\n  } catch (error) {\n    const duration = performance.now() - start;\n    console.error({ operation, duration, status: 'error', error });\n    throw error;\n  }\n}\n```\n\n## Instructions\n\n### Step 1: Establish Baseline\nMeasure current latency for critical Supabase operations.\n\n### Step 2: Implement Caching\nAdd response caching for frequently accessed data.\n\n### Step 3: Enable Batching\nUse DataLoader or similar for automatic request batching.\n\n### Step 4: Optimize Connections\nConfigure connection pooling with keep-alive.\n\n## Output\n- Reduced API latency\n- Caching layer implemented\n- Request batching enabled\n- Connection pooling configured\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Cache miss storm | TTL expired | Use stale-while-revalidate |\n| Batch timeout | Too many items | Reduce batch size |\n| Connection exhausted | No pooling | Configure max sockets |\n| Memory pressure | Cache too large | Set max cache entries |\n\n## Examples\n\n### Quick Performance Wrapper\n```typescript\nconst withPerformance = <T>(name: string, fn: () => Promise<T>) =>\n  measuredSupabaseCall(name, () =>\n    cachedSupabaseRequest(`cache:${name}`, fn)\n  );\n```\n\n## Resources\n- [Supabase Performance Guide](https://supabase.com/docs/performance)\n- [DataLoader Documentation](https://github.com/graphql/dataloader)\n- [LRU Cache Documentation](https://github.com/isaacs/node-lru-cache)\n\n## Next Steps\nFor cost optimization, see `supabase-cost-tuning`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-performance-tuning/SKILL.md"
    },
    {
      "slug": "supabase-policy-guardrails",
      "name": "supabase-policy-guardrails",
      "description": "Implement Supabase lint rules, policy enforcement, and automated guardrails. Use when setting up code quality rules for Supabase integrations, implementing pre-commit hooks, or configuring CI policy checks for Supabase best practices. Trigger with phrases like \"supabase policy\", \"supabase lint\", \"supabase guardrails\", \"supabase best practices check\", \"supabase eslint\". allowed-tools: Read, Write, Edit, Bash(npx:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Policy & Guardrails\n\n## Overview\nAutomated policy enforcement and guardrails for Supabase integrations.\n\n## Prerequisites\n- ESLint configured in project\n- Pre-commit hooks infrastructure\n- CI/CD pipeline with policy checks\n- TypeScript for type enforcement\n\n## ESLint Rules\n\n### Custom Supabase Plugin\n```javascript\n// eslint-plugin-supabase/rules/no-hardcoded-keys.js\nmodule.exports = {\n  meta: {\n    type: 'problem',\n    docs: {\n      description: 'Disallow hardcoded Supabase API keys',\n    },\n    fixable: 'code',\n  },\n  create(context) {\n    return {\n      Literal(node) {\n        if (typeof node.value === 'string') {\n          if (node.value.match(/^sk_(live|test)_[a-zA-Z0-9]{24,}/)) {\n            context.report({\n              node,\n              message: 'Hardcoded Supabase API key detected',\n            });\n          }\n        }\n      },\n    };\n  },\n};\n```\n\n### ESLint Configuration\n```javascript\n// .eslintrc.js\nmodule.exports = {\n  plugins: ['supabase'],\n  rules: {\n    'supabase/no-hardcoded-keys': 'error',\n    'supabase/require-error-handling': 'warn',\n    'supabase/use-typed-client': 'warn',\n  },\n};\n```\n\n## Pre-Commit Hooks\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: supabase-secrets-check\n        name: Check for Supabase secrets\n        entry: bash -c 'git diff --cached --name-only | xargs grep -l \"sk_live_\" && exit 1 || exit 0'\n        language: system\n        pass_filenames: false\n\n      - id: supabase-config-validate\n        name: Validate Supabase configuration\n        entry: node scripts/validate-supabase-config.js\n        language: node\n        files: '\\.supabase\\.json$'\n```\n\n## TypeScript Strict Patterns\n\n```typescript\n// Enforce typed configuration\ninterface SupabaseStrictConfig {\n  apiKey: string;  // Required\n  environment: 'development' | 'staging' | 'production';  // Enum\n  timeout: number;  // Required number, not optional\n  retries: number;\n}\n\n// Disallow any in Supabase code\n// @ts-expect-error - Using any is forbidden\nconst client = new Client({ apiKey: any });\n\n// Prefer this\nconst client = new SupabaseClient(config satisfies SupabaseStrictConfig);\n```\n\n## Architecture Decision Records\n\n### ADR Template\n```markdown\n# ADR-001: Supabase Client Initialization\n\n## Status\nAccepted\n\n## Context\nWe need to decide how to initialize the Supabase client across our application.\n\n## Decision\nWe will use the singleton pattern with lazy initialization.\n\n## Consequences\n- Pro: Single client instance, connection reuse\n- Pro: Easy to mock in tests\n- Con: Global state requires careful lifecycle management\n\n## Enforcement\n- ESLint rule: supabase/use-singleton-client\n- CI check: grep for \"new SupabaseClient(\" outside allowed files\n```\n\n## Policy-as-Code (OPA)\n\n```rego\n# supabase-policy.rego\npackage supabase\n\n# Deny production API keys in non-production environments\ndeny[msg] {\n  input.environment != \"production\"\n  startswith(input.apiKey, \"sk_live_\")\n  msg := \"Production API keys not allowed in non-production environment\"\n}\n\n# Require minimum timeout\ndeny[msg] {\n  input.timeout < 10000\n  msg := sprintf(\"Timeout too low: %d < 10000ms minimum\", [input.timeout])\n}\n\n# Require retry configuration\ndeny[msg] {\n  not input.retries\n  msg := \"Retry configuration is required\"\n}\n```\n\n## CI Policy Checks\n\n```yaml\n# .github/workflows/supabase-policy.yml\nname: Supabase Policy Check\n\non: [push, pull_request]\n\njobs:\n  policy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Check for hardcoded secrets\n        run: |\n          if grep -rE \"sk_(live|test)_[a-zA-Z0-9]{24,}\" --include=\"*.ts\" --include=\"*.js\" .; then\n            echo \"ERROR: Hardcoded Supabase keys found\"\n            exit 1\n          fi\n\n      - name: Validate configuration schema\n        run: |\n          npx ajv validate -s supabase-config.schema.json -d config/supabase/*.json\n\n      - name: Run ESLint Supabase rules\n        run: npx eslint --plugin supabase --rule 'supabase/no-hardcoded-keys: error' src/\n```\n\n## Runtime Guardrails\n\n```typescript\n// Prevent dangerous operations in production\nconst BLOCKED_IN_PROD = ['deleteAll', 'resetData', 'migrateDown'];\n\nfunction guardSupabaseOperation(operation: string): void {\n  const isProd = process.env.NODE_ENV === 'production';\n\n  if (isProd && BLOCKED_IN_PROD.includes(operation)) {\n    throw new Error(`Operation '${operation}' blocked in production`);\n  }\n}\n\n// Rate limit protection\nfunction guardRateLimits(requestsInWindow: number): void {\n  const limit = parseInt(process.env.SUPABASE_RATE_LIMIT || '100');\n\n  if (requestsInWindow > limit * 0.9) {\n    console.warn('Approaching Supabase rate limit');\n  }\n\n  if (requestsInWindow >= limit) {\n    throw new Error('Supabase rate limit exceeded - request blocked');\n  }\n}\n```\n\n## Instructions\n\n### Step 1: Create ESLint Rules\nImplement custom lint rules for Supabase patterns.\n\n### Step 2: Configure Pre-Commit Hooks\nSet up hooks to catch issues before commit.\n\n### Step 3: Add CI Policy Checks\nImplement policy-as-code in CI pipeline.\n\n### Step 4: Enable Runtime Guardrails\nAdd production safeguards for dangerous operations.\n\n## Output\n- ESLint plugin with Supabase rules\n- Pre-commit hooks blocking secrets\n- CI policy checks passing\n- Runtime guardrails active\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| ESLint rule not firing | Wrong config | Check plugin registration |\n| Pre-commit skipped | --no-verify | Enforce in CI |\n| Policy false positive | Regex too broad | Narrow pattern match |\n| Guardrail triggered | Actual issue | Fix or whitelist |\n\n## Examples\n\n### Quick ESLint Check\n```bash\nnpx eslint --plugin supabase --rule 'supabase/no-hardcoded-keys: error' src/\n```\n\n## Resources\n- [ESLint Plugin Development](https://eslint.org/docs/latest/extend/plugins)\n- [Pre-commit Framework](https://pre-commit.com/)\n- [Open Policy Agent](https://www.openpolicyagent.org/)\n\n## Next Steps\nFor architecture blueprints, see `supabase-architecture-variants`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-policy-guardrails/SKILL.md"
    },
    {
      "slug": "supabase-prod-checklist",
      "name": "supabase-prod-checklist",
      "description": "Execute Supabase production deployment checklist and rollback procedures. Use when deploying Supabase integrations to production, preparing for launch, or implementing go-live procedures. Trigger with phrases like \"supabase production\", \"deploy supabase\", \"supabase go-live\", \"supabase launch checklist\". allowed-tools: Read, Bash(kubectl:*), Bash(curl:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Production Checklist\n\n## Overview\nComplete checklist for deploying Supabase integrations to production.\n\n## Prerequisites\n- Staging environment tested and verified\n- Production API keys available\n- Deployment pipeline configured\n- Monitoring and alerting ready\n\n## Instructions\n\n### Step 1: Pre-Deployment Configuration\n- [ ] Production API keys in secure vault\n- [ ] Environment variables set in deployment platform\n- [ ] API key scopes are minimal (least privilege)\n- [ ] Webhook endpoints configured with HTTPS\n- [ ] Webhook secrets stored securely\n\n### Step 2: Code Quality Verification\n- [ ] All tests passing (`npm test`)\n- [ ] No hardcoded credentials\n- [ ] Error handling covers all Supabase error types\n- [ ] Rate limiting/backoff implemented\n- [ ] Logging is production-appropriate\n\n### Step 3: Infrastructure Setup\n- [ ] Health check endpoint includes Supabase connectivity\n- [ ] Monitoring/alerting configured\n- [ ] Circuit breaker pattern implemented\n- [ ] Graceful degradation configured\n\n### Step 4: Documentation Requirements\n- [ ] Incident runbook created\n- [ ] Key rotation procedure documented\n- [ ] Rollback procedure documented\n- [ ] On-call escalation path defined\n\n### Step 5: Deploy with Gradual Rollout\n```bash\n# Pre-flight checks\ncurl -f https://staging.example.com/health\ncurl -s https://status.supabase.com\n\n# Gradual rollout - start with canary (10%)\nkubectl apply -f k8s/production.yaml\nkubectl set image deployment/supabase-integration app=image:new --record\nkubectl rollout pause deployment/supabase-integration\n\n# Monitor canary traffic for 10 minutes\nsleep 600\n# Check error rates and latency before continuing\n\n# If healthy, continue rollout to 50%\nkubectl rollout resume deployment/supabase-integration\nkubectl rollout pause deployment/supabase-integration\nsleep 300\n\n# Complete rollout to 100%\nkubectl rollout resume deployment/supabase-integration\nkubectl rollout status deployment/supabase-integration\n```\n\n## Output\n- Deployed Supabase integration\n- Health checks passing\n- Monitoring active\n- Rollback procedure documented\n\n## Error Handling\n| Alert | Condition | Severity |\n|-------|-----------|----------|\n| API Down | 5xx errors > 10/min | P1 |\n| High Latency | p99 > 5000ms | P2 |\n| Rate Limited | 429 errors > 5/min | P2 |\n| Auth Failures | 401/403 errors > 0 | P1 |\n\n## Examples\n\n### Health Check Implementation\n```typescript\nasync function healthCheck(): Promise<{ status: string; supabase: any }> {\n  const start = Date.now();\n  try {\n    await supabaseClient.ping();\n    return { status: 'healthy', supabase: { connected: true, latencyMs: Date.now() - start } };\n  } catch (error) {\n    return { status: 'degraded', supabase: { connected: false, latencyMs: Date.now() - start } };\n  }\n}\n```\n\n### Immediate Rollback\n```bash\nkubectl rollout undo deployment/supabase-integration\nkubectl rollout status deployment/supabase-integration\n```\n\n## Resources\n- [Supabase Status](https://status.supabase.com)\n- [Supabase Support](https://supabase.com/docs/support)\n\n## Next Steps\nFor version upgrades, see `supabase-upgrade-migration`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-prod-checklist/SKILL.md"
    },
    {
      "slug": "supabase-rate-limits",
      "name": "supabase-rate-limits",
      "description": "Implement Supabase rate limiting, backoff, and idempotency patterns. Use when handling rate limit errors, implementing retry logic, or optimizing API request throughput for Supabase. Trigger with phrases like \"supabase rate limit\", \"supabase throttling\", \"supabase 429\", \"supabase retry\", \"supabase backoff\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Rate Limits\n\n## Overview\nHandle Supabase rate limits gracefully with exponential backoff and idempotency.\n\n## Prerequisites\n- Supabase SDK installed\n- Understanding of async/await patterns\n- Access to rate limit headers\n\n## Instructions\n\n### Step 1: Understand Rate Limit Tiers\n\n| Tier | Requests/min | Requests/day | Burst |\n|------|-------------|--------------|-------|\n| Free | 500 | 50,000 | 10 |\n| Pro | 5,000 | 1,000,000 | 50 |\n| Enterprise | Unlimited | Unlimited | 200 |\n\n### Step 2: Implement Exponential Backoff with Jitter\n\n```typescript\nasync function withExponentialBackoff<T>(\n  operation: () => Promise<T>,\n  config = { maxRetries: 5, baseDelayMs: 1000, maxDelayMs: 32000, jitterMs: 500 }\n): Promise<T> {\n  for (let attempt = 0; attempt <= config.maxRetries; attempt++) {\n    try {\n      return await operation();\n    } catch (error: any) {\n      if (attempt === config.maxRetries) throw error;\n      const status = error.status || error.response?.status;\n      if (status !== 429 && (status < 500 || status >= 600)) throw error;\n\n      // Exponential delay with jitter to prevent thundering herd\n      const exponentialDelay = config.baseDelayMs * Math.pow(2, attempt);\n      const jitter = Math.random() * config.jitterMs;\n      const delay = Math.min(exponentialDelay + jitter, config.maxDelayMs);\n\n      console.log(`Rate limited. Retrying in ${delay.toFixed(0)}ms...`);\n      await new Promise(r => setTimeout(r, delay));\n    }\n  }\n  throw new Error('Unreachable');\n}\n```\n\n### Step 3: Add Idempotency Keys\n\n```typescript\nimport { v4 as uuidv4 } from 'uuid';\nimport crypto from 'crypto';\n\n// Generate deterministic key from operation params (for safe retries)\nfunction generateIdempotencyKey(operation: string, params: Record<string, any>): string {\n  const data = JSON.stringify({ operation, params });\n  return crypto.createHash('sha256').update(data).digest('hex');\n}\n\nasync function idempotentRequest<T>(\n  client: SupabaseClient,\n  params: Record<string, any>,\n  idempotencyKey?: string  // Pass existing key for retries\n): Promise<T> {\n  // Use provided key (for retries) or generate deterministic key from params\n  const key = idempotencyKey || generateIdempotencyKey(params.method || 'POST', params);\n  return client.request({\n    ...params,\n    headers: { 'Idempotency-Key': key, ...params.headers },\n  });\n}\n```\n\n## Output\n- Reliable API calls with automatic retry\n- Idempotent requests preventing duplicates\n- Rate limit headers properly handled\n\n## Error Handling\n| Header | Description | Action |\n|--------|-------------|--------|\n| X-RateLimit-Limit | Max requests | Monitor usage |\n| X-RateLimit-Remaining | Remaining requests | Throttle if low |\n| X-RateLimit-Reset | Reset timestamp | Wait until reset |\n| Retry-After | Seconds to wait | Honor this value |\n\n## Examples\n\n### Queue-Based Rate Limiting\n```typescript\nimport PQueue from 'p-queue';\n\nconst queue = new PQueue({\n  concurrency: 5,\n  interval: 1000,\n  intervalCap: 10,\n});\n\nasync function queuedRequest<T>(operation: () => Promise<T>): Promise<T> {\n  return queue.add(operation);\n}\n```\n\n### Monitor Rate Limit Usage\n```typescript\nclass RateLimitMonitor {\n  private remaining: number = 60;\n  private resetAt: Date = new Date();\n\n  updateFromHeaders(headers: Headers) {\n    this.remaining = parseInt(headers.get('X-RateLimit-Remaining') || '60');\n    const resetTimestamp = headers.get('X-RateLimit-Reset');\n    if (resetTimestamp) {\n      this.resetAt = new Date(parseInt(resetTimestamp) * 1000);\n    }\n  }\n\n  shouldThrottle(): boolean {\n    // Only throttle if low remaining AND reset hasn't happened yet\n    return this.remaining < 5 && new Date() < this.resetAt;\n  }\n\n  getWaitTime(): number {\n    return Math.max(0, this.resetAt.getTime() - Date.now());\n  }\n}\n```\n\n## Resources\n- [Supabase Rate Limits](https://supabase.com/docs/rate-limits)\n- [p-queue Documentation](https://github.com/sindresorhus/p-queue)\n\n## Next Steps\nFor security configuration, see `supabase-security-basics`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-rate-limits/SKILL.md"
    },
    {
      "slug": "supabase-reference-architecture",
      "name": "supabase-reference-architecture",
      "description": "Implement Supabase reference architecture with best-practice project layout. Use when designing new Supabase integrations, reviewing project structure, or establishing architecture standards for Supabase applications. Trigger with phrases like \"supabase architecture\", \"supabase best practices\", \"supabase project structure\", \"how to organize supabase\", \"supabase layout\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Reference Architecture\n\n## Overview\nProduction-ready architecture patterns for Supabase integrations.\n\n## Prerequisites\n- Understanding of layered architecture\n- Supabase SDK knowledge\n- TypeScript project setup\n- Testing framework configured\n\n## Project Structure\n\n```\nmy-supabase-project/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ supabase/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts           # Singleton client wrapper\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.ts           # Environment configuration\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.ts            # TypeScript types\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errors.ts           # Custom error classes\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handlers/\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ webhooks.ts     # Webhook handlers\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ events.ts       # Event processing\n‚îÇ   ‚îú‚îÄ‚îÄ services/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ supabase/\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ index.ts        # Service facade\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ sync.ts         # Data synchronization\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ cache.ts        # Caching layer\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ supabase/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ webhook.ts      # Webhook endpoint\n‚îÇ   ‚îî‚îÄ‚îÄ jobs/\n‚îÇ       ‚îî‚îÄ‚îÄ supabase/\n‚îÇ           ‚îî‚îÄ‚îÄ sync.ts         # Background sync job\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ unit/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ supabase/\n‚îÇ   ‚îî‚îÄ‚îÄ integration/\n‚îÇ       ‚îî‚îÄ‚îÄ supabase/\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îú‚îÄ‚îÄ supabase.development.json\n‚îÇ   ‚îú‚îÄ‚îÄ supabase.staging.json\n‚îÇ   ‚îî‚îÄ‚îÄ supabase.production.json\n‚îî‚îÄ‚îÄ docs/\n    ‚îî‚îÄ‚îÄ supabase/\n        ‚îú‚îÄ‚îÄ SETUP.md\n        ‚îî‚îÄ‚îÄ RUNBOOK.md\n```\n\n## Layer Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ             API Layer                    ‚îÇ\n‚îÇ   (Controllers, Routes, Webhooks)        ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ           Service Layer                  ‚îÇ\n‚îÇ  (Business Logic, Orchestration)         ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ          Supabase Layer        ‚îÇ\n‚îÇ   (Client, Types, Error Handling)        ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ         Infrastructure Layer             ‚îÇ\n‚îÇ    (Cache, Queue, Monitoring)            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Key Components\n\n### Step 1: Client Wrapper\n```typescript\n// src/supabase/client.ts\nexport class SupabaseService {\n  private client: SupabaseClient;\n  private cache: Cache;\n  private monitor: Monitor;\n\n  constructor(config: SupabaseConfig) {\n    this.client = new SupabaseClient(config);\n    this.cache = new Cache(config.cacheOptions);\n    this.monitor = new Monitor('supabase');\n  }\n\n  async get(id: string): Promise<Resource> {\n    return this.cache.getOrFetch(id, () =>\n      this.monitor.track('get', () => this.client.get(id))\n    );\n  }\n}\n```\n\n### Step 2: Error Boundary\n```typescript\n// src/supabase/errors.ts\nexport class SupabaseServiceError extends Error {\n  constructor(\n    message: string,\n    public readonly code: string,\n    public readonly retryable: boolean,\n    public readonly originalError?: Error\n  ) {\n    super(message);\n    this.name = 'SupabaseServiceError';\n  }\n}\n\nexport function wrapSupabaseError(error: unknown): SupabaseServiceError {\n  // Transform SDK errors to application errors\n}\n```\n\n### Step 3: Health Check\n```typescript\n// src/supabase/health.ts\nexport async function checkSupabaseHealth(): Promise<HealthStatus> {\n  try {\n    const start = Date.now();\n    await supabaseClient.ping();\n    return {\n      status: 'healthy',\n      latencyMs: Date.now() - start,\n    };\n  } catch (error) {\n    return { status: 'unhealthy', error: error.message };\n  }\n}\n```\n\n## Data Flow Diagram\n\n```\nUser Request\n     ‚îÇ\n     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   API       ‚îÇ\n‚îÇ   Gateway   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Service   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Cache     ‚îÇ\n‚îÇ   Layer     ‚îÇ    ‚îÇ   (Redis)   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Supabase    ‚îÇ\n‚îÇ   Client    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Supabase    ‚îÇ\n‚îÇ   API       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Configuration Management\n\n```typescript\n// config/supabase.ts\nexport interface SupabaseConfig {\n  apiKey: string;\n  environment: 'development' | 'staging' | 'production';\n  timeout: number;\n  retries: number;\n  cache: {\n    enabled: boolean;\n    ttlSeconds: number;\n  };\n}\n\nexport function loadSupabaseConfig(): SupabaseConfig {\n  const env = process.env.NODE_ENV || 'development';\n  return require(`./supabase.${env}.json`);\n}\n```\n\n## Instructions\n\n### Step 1: Create Directory Structure\nSet up the project layout following the reference structure above.\n\n### Step 2: Implement Client Wrapper\nCreate the singleton client with caching and monitoring.\n\n### Step 3: Add Error Handling\nImplement custom error classes for Supabase operations.\n\n### Step 4: Configure Health Checks\nAdd health check endpoint for Supabase connectivity.\n\n## Output\n- Structured project layout\n- Client wrapper with caching\n- Error boundary implemented\n- Health checks configured\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Circular dependencies | Wrong layering | Separate concerns by layer |\n| Config not loading | Wrong paths | Verify config file locations |\n| Type errors | Missing types | Add Supabase types |\n| Test isolation | Shared state | Use dependency injection |\n\n## Examples\n\n### Quick Setup Script\n```bash\n# Create reference structure\nmkdir -p src/supabase/{handlers} src/services/supabase src/api/supabase\ntouch src/supabase/{client,config,types,errors}.ts\ntouch src/services/supabase/{index,sync,cache}.ts\n```\n\n## Resources\n- [Supabase SDK Documentation](https://supabase.com/docs/sdk)\n- [Supabase Best Practices](https://supabase.com/docs/best-practices)\n\n## Flagship Skills\nFor multi-environment setup, see `supabase-multi-env-setup`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-reference-architecture/SKILL.md"
    },
    {
      "slug": "supabase-reliability-patterns",
      "name": "supabase-reliability-patterns",
      "description": "Implement Supabase reliability patterns including circuit breakers, idempotency, and graceful degradation. Use when building fault-tolerant Supabase integrations, implementing retry strategies, or adding resilience to production Supabase services. Trigger with phrases like \"supabase reliability\", \"supabase circuit breaker\", \"supabase idempotent\", \"supabase resilience\", \"supabase fallback\", \"supabase bulkhead\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Reliability Patterns\n\n## Overview\nProduction-grade reliability patterns for Supabase integrations.\n\n## Prerequisites\n- Understanding of circuit breaker pattern\n- opossum or similar library installed\n- Queue infrastructure for DLQ\n- Caching layer for fallbacks\n\n## Circuit Breaker\n\n```typescript\nimport CircuitBreaker from 'opossum';\n\nconst supabaseBreaker = new CircuitBreaker(\n  async (operation: () => Promise<any>) => operation(),\n  {\n    timeout: 30000,\n    errorThresholdPercentage: 50,\n    resetTimeout: 30000,\n    volumeThreshold: 10,\n  }\n);\n\n// Events\nsupabaseBreaker.on('open', () => {\n  console.warn('Supabase circuit OPEN - requests failing fast');\n  alertOps('Supabase circuit breaker opened');\n});\n\nsupabaseBreaker.on('halfOpen', () => {\n  console.info('Supabase circuit HALF-OPEN - testing recovery');\n});\n\nsupabaseBreaker.on('close', () => {\n  console.info('Supabase circuit CLOSED - normal operation');\n});\n\n// Usage\nasync function safeSupabaseCall<T>(fn: () => Promise<T>): Promise<T> {\n  return supabaseBreaker.fire(fn);\n}\n```\n\n## Idempotency Keys\n\n```typescript\nimport { v4 as uuidv4 } from 'uuid';\nimport crypto from 'crypto';\n\n// Generate deterministic idempotency key from input\nfunction generateIdempotencyKey(\n  operation: string,\n  params: Record<string, any>\n): string {\n  const data = JSON.stringify({ operation, params });\n  return crypto.createHash('sha256').update(data).digest('hex');\n}\n\n// Or use random key with storage\nclass IdempotencyManager {\n  private store: Map<string, { key: string; expiresAt: Date }> = new Map();\n\n  getOrCreate(operationId: string): string {\n    const existing = this.store.get(operationId);\n    if (existing && existing.expiresAt > new Date()) {\n      return existing.key;\n    }\n\n    const key = uuidv4();\n    this.store.set(operationId, {\n      key,\n      expiresAt: new Date(Date.now() + 24 * 60 * 60 * 1000),\n    });\n    return key;\n  }\n}\n```\n\n## Bulkhead Pattern\n\n```typescript\nimport PQueue from 'p-queue';\n\n// Separate queues for different operations\nconst supabaseQueues = {\n  critical: new PQueue({ concurrency: 10 }),\n  normal: new PQueue({ concurrency: 5 }),\n  bulk: new PQueue({ concurrency: 2 }),\n};\n\nasync function prioritizedSupabaseCall<T>(\n  priority: 'critical' | 'normal' | 'bulk',\n  fn: () => Promise<T>\n): Promise<T> {\n  return supabaseQueues[priority].add(fn);\n}\n\n// Usage\nawait prioritizedSupabaseCall('critical', () =>\n  supabaseClient.processPayment(order)\n);\n\nawait prioritizedSupabaseCall('bulk', () =>\n  supabaseClient.syncCatalog(products)\n);\n```\n\n## Timeout Hierarchy\n\n```typescript\nconst TIMEOUT_CONFIG = {\n  connect: 5000,      // Initial connection\n  request: 30000,     // Standard requests\n  upload: 120000,     // File uploads\n  longPoll: 300000,   // Webhook long-polling\n};\n\nasync function timedoutSupabaseCall<T>(\n  operation: 'connect' | 'request' | 'upload' | 'longPoll',\n  fn: () => Promise<T>\n): Promise<T> {\n  const timeout = TIMEOUT_CONFIG[operation];\n\n  return Promise.race([\n    fn(),\n    new Promise<never>((_, reject) =>\n      setTimeout(() => reject(new Error(`Supabase ${operation} timeout`)), timeout)\n    ),\n  ]);\n}\n```\n\n## Graceful Degradation\n\n```typescript\ninterface SupabaseFallback {\n  enabled: boolean;\n  data: any;\n  staleness: 'fresh' | 'stale' | 'very_stale';\n}\n\nasync function withSupabaseFallback<T>(\n  fn: () => Promise<T>,\n  fallbackFn: () => Promise<T>\n): Promise<{ data: T; fallback: boolean }> {\n  try {\n    const data = await fn();\n    // Update cache for future fallback\n    await updateFallbackCache(data);\n    return { data, fallback: false };\n  } catch (error) {\n    console.warn('Supabase failed, using fallback:', error.message);\n    const data = await fallbackFn();\n    return { data, fallback: true };\n  }\n}\n```\n\n## Dead Letter Queue\n\n```typescript\ninterface DeadLetterEntry {\n  id: string;\n  operation: string;\n  payload: any;\n  error: string;\n  attempts: number;\n  lastAttempt: Date;\n}\n\nclass SupabaseDeadLetterQueue {\n  private queue: DeadLetterEntry[] = [];\n\n  add(entry: Omit<DeadLetterEntry, 'id' | 'lastAttempt'>): void {\n    this.queue.push({\n      ...entry,\n      id: uuidv4(),\n      lastAttempt: new Date(),\n    });\n  }\n\n  async processOne(): Promise<boolean> {\n    const entry = this.queue.shift();\n    if (!entry) return false;\n\n    try {\n      await supabaseClient[entry.operation](entry.payload);\n      console.log(`DLQ: Successfully reprocessed ${entry.id}`);\n      return true;\n    } catch (error) {\n      entry.attempts++;\n      entry.lastAttempt = new Date();\n\n      if (entry.attempts < 5) {\n        this.queue.push(entry);\n      } else {\n        console.error(`DLQ: Giving up on ${entry.id} after 5 attempts`);\n        await alertOnPermanentFailure(entry);\n      }\n      return false;\n    }\n  }\n}\n```\n\n## Health Check with Degraded State\n\n```typescript\ntype HealthStatus = 'healthy' | 'degraded' | 'unhealthy';\n\nasync function supabaseHealthCheck(): Promise<{\n  status: HealthStatus;\n  details: Record<string, any>;\n}> {\n  const checks = {\n    api: await checkApiConnectivity(),\n    circuitBreaker: supabaseBreaker.stats(),\n    dlqSize: deadLetterQueue.size(),\n  };\n\n  const status: HealthStatus =\n    !checks.api.connected ? 'unhealthy' :\n    checks.circuitBreaker.state === 'open' ? 'degraded' :\n    checks.dlqSize > 100 ? 'degraded' :\n    'healthy';\n\n  return { status, details: checks };\n}\n```\n\n## Instructions\n\n### Step 1: Implement Circuit Breaker\nWrap Supabase calls with circuit breaker.\n\n### Step 2: Add Idempotency Keys\nGenerate deterministic keys for operations.\n\n### Step 3: Configure Bulkheads\nSeparate queues for different priorities.\n\n### Step 4: Set Up Dead Letter Queue\nHandle permanent failures gracefully.\n\n## Output\n- Circuit breaker protecting Supabase calls\n- Idempotency preventing duplicates\n- Bulkhead isolation implemented\n- DLQ for failed operations\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Circuit stays open | Threshold too low | Adjust error percentage |\n| Duplicate operations | Missing idempotency | Add idempotency key |\n| Queue full | Rate too high | Increase concurrency |\n| DLQ growing | Persistent failures | Investigate root cause |\n\n## Examples\n\n### Quick Circuit Check\n```typescript\nconst state = supabaseBreaker.stats().state;\nconsole.log('Supabase circuit:', state);\n```\n\n## Resources\n- [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)\n- [Opossum Documentation](https://nodeshift.dev/opossum/)\n- [Supabase Reliability Guide](https://supabase.com/docs/reliability)\n\n## Next Steps\nFor policy enforcement, see `supabase-policy-guardrails`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-reliability-patterns/SKILL.md"
    },
    {
      "slug": "supabase-schema-from-requirements",
      "name": "supabase-schema-from-requirements",
      "description": "Execute Supabase primary workflow: Schema from Requirements. Use when Starting a new project with defined data requirements, Refactoring an existing schema based on new features, or Creating migrations from specification documents. Trigger with phrases like \"supabase schema from requirements\", \"generate database schema with supabase\". allowed-tools: Read, Write, Edit, Bash(npm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Schema from Requirements\n\n## Overview\nGenerate Supabase database schema from natural language requirements.\nThis is the primary workflow for starting new Supabase projects.\n\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Understanding of Supabase core concepts\n- Valid API credentials configured\n\n## Instructions\n\n### Step 1: Initialize\n```typescript\n// Step 1 implementation\n```\n\n### Step 2: Execute\n```typescript\n// Step 2 implementation\n```\n\n### Step 3: Finalize\n```typescript\n// Step 3 implementation\n```\n\n## Output\n- Completed Schema from Requirements execution\n- Expected results from Supabase API\n- Success confirmation or error details\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Error 1 | Cause | Solution |\n| Error 2 | Cause | Solution |\n\n## Examples\n\n### Complete Workflow\n```typescript\n// Complete workflow example\n```\n\n### Common Variations\n- Variation 1: Description\n- Variation 2: Description\n\n## Resources\n- [Supabase Documentation](https://supabase.com/docs)\n- [Supabase API Reference](https://supabase.com/docs/api)\n\n## Next Steps\nFor secondary workflow, see `supabase-auth-storage-realtime-core`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-schema-from-requirements/SKILL.md"
    },
    {
      "slug": "supabase-sdk-patterns",
      "name": "supabase-sdk-patterns",
      "description": "Apply production-ready Supabase SDK patterns for TypeScript and Python. Use when implementing Supabase integrations, refactoring SDK usage, or establishing team coding standards for Supabase. Trigger with phrases like \"supabase SDK patterns\", \"supabase best practices\", \"supabase code patterns\", \"idiomatic supabase\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase SDK Patterns\n\n## Overview\nProduction-ready patterns for Supabase SDK usage in TypeScript and Python.\n\n## Prerequisites\n- Completed `supabase-install-auth` setup\n- Familiarity with async/await patterns\n- Understanding of error handling best practices\n\n## Instructions\n\n### Step 1: Implement Singleton Pattern (Recommended)\n```typescript\n// src/supabase/client.ts\nimport { SupabaseClient } from '@supabase/supabase-js';\n\nlet instance: SupabaseClient | null = null;\n\nexport function getSupabaseClient(): SupabaseClient {\n  if (!instance) {\n    instance = new SupabaseClient({\n      apiKey: process.env.SUPABASE_API_KEY!,\n      // Additional options\n    });\n  }\n  return instance;\n}\n```\n\n### Step 2: Add Error Handling Wrapper\n```typescript\nimport { SupabaseError } from '@supabase/supabase-js';\n\nasync function safeSupabaseCall<T>(\n  operation: () => Promise<T>\n): Promise<{ data: T | null; error: Error | null }> {\n  try {\n    const data = await operation();\n    return { data, error: null };\n  } catch (err) {\n    if (err instanceof SupabaseError) {\n      console.error({\n        code: err.code,\n        message: err.message,\n      });\n    }\n    return { data: null, error: err as Error };\n  }\n}\n```\n\n### Step 3: Implement Retry Logic\n```typescript\nasync function withRetry<T>(\n  operation: () => Promise<T>,\n  maxRetries = 3,\n  backoffMs = 1000\n): Promise<T> {\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      return await operation();\n    } catch (err) {\n      if (attempt === maxRetries) throw err;\n      const delay = backoffMs * Math.pow(2, attempt - 1);\n      await new Promise(r => setTimeout(r, delay));\n    }\n  }\n  throw new Error('Unreachable');\n}\n```\n\n## Output\n- Type-safe client singleton\n- Robust error handling with structured logging\n- Automatic retry with exponential backoff\n- Runtime validation for API responses\n\n## Error Handling\n| Pattern | Use Case | Benefit |\n|---------|----------|---------|\n| Safe wrapper | All API calls | Prevents uncaught exceptions |\n| Retry logic | Transient failures | Improves reliability |\n| Type guards | Response validation | Catches API changes |\n| Logging | All operations | Debugging and monitoring |\n\n## Examples\n\n### Factory Pattern (Multi-tenant)\n```typescript\nconst clients = new Map<string, SupabaseClient>();\n\nexport function getClientForTenant(tenantId: string): SupabaseClient {\n  if (!clients.has(tenantId)) {\n    const apiKey = getTenantApiKey(tenantId);\n    clients.set(tenantId, new SupabaseClient({ apiKey }));\n  }\n  return clients.get(tenantId)!;\n}\n```\n\n### Python Context Manager\n```python\nfrom contextlib import asynccontextmanager\nfrom supabase import SupabaseClient\n\n@asynccontextmanager\nasync def get_supabase_client():\n    client = SupabaseClient()\n    try:\n        yield client\n    finally:\n        await client.close()\n```\n\n### Zod Validation\n```typescript\nimport { z } from 'zod';\n\nconst supabaseResponseSchema = z.object({\n  id: z.string(),\n  status: z.enum(['active', 'inactive']),\n  createdAt: z.string().datetime(),\n});\n```\n\n## Resources\n- [Supabase SDK Reference](https://supabase.com/docs/sdk)\n- [Supabase API Types](https://supabase.com/docs/types)\n- [Zod Documentation](https://zod.dev/)\n\n## Next Steps\nApply patterns in `supabase-core-workflow-a` for real-world usage.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-sdk-patterns/SKILL.md"
    },
    {
      "slug": "supabase-security-basics",
      "name": "supabase-security-basics",
      "description": "Apply Supabase security best practices for secrets and access control. Use when securing API keys, implementing least privilege access, or auditing Supabase security configuration. Trigger with phrases like \"supabase security\", \"supabase secrets\", \"secure supabase\", \"supabase API key security\". allowed-tools: Read, Write, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Security Basics\n\n## Overview\nSecurity best practices for Supabase API keys, tokens, and access control.\n\n## Prerequisites\n- Supabase SDK installed\n- Understanding of environment variables\n- Access to Supabase dashboard\n\n## Instructions\n\n### Step 1: Configure Environment Variables\n```bash\n# .env (NEVER commit to git)\nSUPABASE_API_KEY=sk_live_***\nSUPABASE_SECRET=***\n\n# .gitignore\n.env\n.env.local\n.env.*.local\n```\n\n### Step 2: Implement Secret Rotation\n```bash\n# 1. Generate new key in Supabase dashboard\n# 2. Update environment variable\nexport SUPABASE_API_KEY=\"new_key_here\"\n\n# 3. Verify new key works\ncurl -H \"Authorization: Bearer ${SUPABASE_API_KEY}\" \\\n  https://api.supabase.com/health\n\n# 4. Revoke old key in dashboard\n```\n\n### Step 3: Apply Least Privilege\n| Environment | Recommended Scopes |\n|-------------|-------------------|\n| Development | `read, write` |\n| Staging | `read, write, admin` |\n| Production | `read, write` |\n\n## Output\n- Secure API key storage\n- Environment-specific access controls\n- Audit logging enabled\n\n## Error Handling\n| Security Issue | Detection | Mitigation |\n|----------------|-----------|------------|\n| Exposed API key | Git scanning | Rotate immediately |\n| Excessive scopes | Audit logs | Reduce permissions |\n| Missing rotation | Key age check | Schedule rotation |\n\n## Examples\n\n### Service Account Pattern\n```typescript\nconst clients = {\n  reader: new SupabaseClient({\n    apiKey: process.env.SUPABASE_READ_KEY,\n  }),\n  writer: new SupabaseClient({\n    apiKey: process.env.SUPABASE_WRITE_KEY,\n  }),\n};\n```\n\n### Webhook Signature Verification\n```typescript\nimport crypto from 'crypto';\n\nfunction verifyWebhookSignature(\n  payload: string, signature: string, secret: string\n): boolean {\n  const expected = crypto.createHmac('sha256', secret).update(payload).digest('hex');\n  return crypto.timingSafeEqual(Buffer.from(signature), Buffer.from(expected));\n}\n```\n\n### Security Checklist\n- [ ] API keys in environment variables\n- [ ] `.env` files in `.gitignore`\n- [ ] Different keys for dev/staging/prod\n- [ ] Minimal scopes per environment\n- [ ] Webhook signatures validated\n- [ ] Audit logging enabled\n\n### Audit Logging\n```typescript\ninterface AuditEntry {\n  timestamp: Date;\n  action: string;\n  userId: string;\n  resource: string;\n  result: 'success' | 'failure';\n  metadata?: Record<string, any>;\n}\n\nasync function auditLog(entry: Omit<AuditEntry, 'timestamp'>): Promise<void> {\n  const log: AuditEntry = { ...entry, timestamp: new Date() };\n\n  // Log to Supabase analytics\n  await supabaseClient.track('audit', log);\n\n  // Also log locally for compliance\n  console.log('[AUDIT]', JSON.stringify(log));\n}\n\n// Usage\nawait auditLog({\n  action: 'supabase.api.call',\n  userId: currentUser.id,\n  resource: '/v1/resource',\n  result: 'success',\n});\n```\n\n## Resources\n- [Supabase Security Guide](https://supabase.com/docs/security)\n- [Supabase API Scopes](https://supabase.com/docs/scopes)\n\n## Next Steps\nFor production deployment, see `supabase-prod-checklist`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-security-basics/SKILL.md"
    },
    {
      "slug": "supabase-upgrade-migration",
      "name": "supabase-upgrade-migration",
      "description": "Analyze, plan, and execute Supabase SDK upgrades with breaking change detection. Use when upgrading Supabase SDK versions, detecting deprecations, or migrating to new API versions. Trigger with phrases like \"upgrade supabase\", \"supabase migration\", \"supabase breaking changes\", \"update supabase SDK\", \"analyze supabase version\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(git:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Upgrade & Migration\n\n## Overview\nGuide for upgrading Supabase SDK versions and handling breaking changes.\n\n## Prerequisites\n- Current Supabase SDK installed\n- Git for version control\n- Test suite available\n- Staging environment\n\n## Instructions\n\n### Step 1: Check Current Version\n```bash\nnpm list @supabase/supabase-js\nnpm view @supabase/supabase-js version\n```\n\n### Step 2: Review Changelog\n```bash\nopen https://github.com/supabase/sdk/releases\n```\n\n### Step 3: Create Upgrade Branch\n```bash\ngit checkout -b upgrade/supabase-sdk-vX.Y.Z\nnpm install @supabase/supabase-js@latest\nnpm test\n```\n\n### Step 4: Handle Breaking Changes\nUpdate import statements, configuration, and method signatures as needed.\n\n## Output\n- Updated SDK version\n- Fixed breaking changes\n- Passing test suite\n- Documented rollback procedure\n\n## Error Handling\n| SDK Version | API Version | Node.js | Breaking Changes |\n|-------------|-------------|---------|------------------|\n| 3.x | 2024-01 | 18+ | Major refactor |\n| 2.x | 2023-06 | 16+ | Auth changes |\n| 1.x | 2022-01 | 14+ | Initial release |\n\n## Examples\n\n### Import Changes\n```typescript\n// Before (v1.x)\nimport { Client } from '@supabase/supabase-js';\n\n// After (v2.x)\nimport { SupabaseClient } from '@supabase/supabase-js';\n```\n\n### Configuration Changes\n```typescript\n// Before (v1.x)\nconst client = new Client({ key: 'xxx' });\n\n// After (v2.x)\nconst client = new SupabaseClient({\n  apiKey: 'xxx',\n});\n```\n\n### Rollback Procedure\n```bash\nnpm install @supabase/supabase-js@1.x.x --save-exact\n```\n\n### Deprecation Handling\n```typescript\n// Monitor for deprecation warnings in development\nif (process.env.NODE_ENV === 'development') {\n  process.on('warning', (warning) => {\n    if (warning.name === 'DeprecationWarning') {\n      console.warn('[Supabase]', warning.message);\n      // Log to tracking system for proactive updates\n    }\n  });\n}\n\n// Common deprecation patterns to watch for:\n// - Renamed methods: client.oldMethod() -> client.newMethod()\n// - Changed parameters: { key: 'x' } -> { apiKey: 'x' }\n// - Removed features: Check release notes before upgrading\n```\n\n## Resources\n- [Supabase Changelog](https://github.com/supabase/sdk/releases)\n- [Supabase Migration Guide](https://supabase.com/docs/migration)\n\n## Next Steps\nFor CI integration during upgrades, see `supabase-ci-integration`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-upgrade-migration/SKILL.md"
    },
    {
      "slug": "supabase-webhooks-events",
      "name": "supabase-webhooks-events",
      "description": "Implement Supabase webhook signature validation and event handling. Use when setting up webhook endpoints, implementing signature verification, or handling Supabase event notifications securely. Trigger with phrases like \"supabase webhook\", \"supabase events\", \"supabase webhook signature\", \"handle supabase events\", \"supabase notifications\". allowed-tools: Read, Write, Edit, Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Supabase Webhooks & Events\n\n## Overview\nSecurely handle Supabase webhooks with signature validation and replay protection.\n\n## Prerequisites\n- Supabase webhook secret configured\n- HTTPS endpoint accessible from internet\n- Understanding of cryptographic signatures\n- Redis or database for idempotency (optional)\n\n## Webhook Endpoint Setup\n\n### Express.js\n```typescript\nimport express from 'express';\nimport crypto from 'crypto';\n\nconst app = express();\n\n// IMPORTANT: Raw body needed for signature verification\napp.post('/webhooks/supabase',\n  express.raw({ type: 'application/json' }),\n  async (req, res) => {\n    const signature = req.headers['x-supabase-signature'] as string;\n    const timestamp = req.headers['x-supabase-timestamp'] as string;\n\n    if (!verifySupabaseSignature(req.body, signature, timestamp)) {\n      return res.status(401).json({ error: 'Invalid signature' });\n    }\n\n    const event = JSON.parse(req.body.toString());\n    await handleSupabaseEvent(event);\n\n    res.status(200).json({ received: true });\n  }\n);\n```\n\n## Signature Verification\n\n```typescript\nfunction verifySupabaseSignature(\n  payload: Buffer,\n  signature: string,\n  timestamp: string\n): boolean {\n  const secret = process.env.SUPABASE_WEBHOOK_SECRET!;\n\n  // Reject old timestamps (replay attack protection)\n  const timestampAge = Date.now() - parseInt(timestamp) * 1000;\n  if (timestampAge > 300000) { // 5 minutes\n    console.error('Webhook timestamp too old');\n    return false;\n  }\n\n  // Compute expected signature\n  const signedPayload = `${timestamp}.${payload.toString()}`;\n  const expectedSignature = crypto\n    .createHmac('sha256', secret)\n    .update(signedPayload)\n    .digest('hex');\n\n  // Timing-safe comparison\n  return crypto.timingSafeEqual(\n    Buffer.from(signature),\n    Buffer.from(expectedSignature)\n  );\n}\n```\n\n## Event Handler Pattern\n\n```typescript\ntype SupabaseEventType = 'resource.created' | 'resource.updated' | 'resource.deleted';\n\ninterface SupabaseEvent {\n  id: string;\n  type: SupabaseEventType;\n  data: Record<string, any>;\n  created: string;\n}\n\nconst eventHandlers: Record<SupabaseEventType, (data: any) => Promise<void>> = {\n  'resource.created': async (data) => { /* handle */ },\n  'resource.updated': async (data) => { /* handle */ },\n  'resource.deleted': async (data) => { /* handle */ }\n};\n\nasync function handleSupabaseEvent(event: SupabaseEvent): Promise<void> {\n  const handler = eventHandlers[event.type];\n\n  if (!handler) {\n    console.log(`Unhandled event type: ${event.type}`);\n    return;\n  }\n\n  try {\n    await handler(event.data);\n    console.log(`Processed ${event.type}: ${event.id}`);\n  } catch (error) {\n    console.error(`Failed to process ${event.type}: ${event.id}`, error);\n    throw error; // Rethrow to trigger retry\n  }\n}\n```\n\n## Idempotency Handling\n\n```typescript\nimport { Redis } from 'ioredis';\n\nconst redis = new Redis(process.env.REDIS_URL);\n\nasync function isEventProcessed(eventId: string): Promise<boolean> {\n  const key = `supabase:event:${eventId}`;\n  const exists = await redis.exists(key);\n  return exists === 1;\n}\n\nasync function markEventProcessed(eventId: string): Promise<void> {\n  const key = `supabase:event:${eventId}`;\n  await redis.set(key, '1', 'EX', 86400 * 7); // 7 days TTL\n}\n```\n\n## Webhook Testing\n\n```bash\n# Use Supabase CLI to send test events\nsupabase functions invoke webhook-handler\n\n# Or use webhook.site for debugging\ncurl -X POST https://webhook.site/your-uuid \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"type\": \"resource.created\", \"data\": {}}'\n```\n\n## Instructions\n\n### Step 1: Register Webhook Endpoint\nConfigure your webhook URL in the Supabase dashboard.\n\n### Step 2: Implement Signature Verification\nUse the signature verification code to validate incoming webhooks.\n\n### Step 3: Handle Events\nImplement handlers for each event type your application needs.\n\n### Step 4: Add Idempotency\nPrevent duplicate processing with event ID tracking.\n\n## Output\n- Secure webhook endpoint\n- Signature validation enabled\n- Event handlers implemented\n- Replay attack protection active\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Invalid signature | Wrong secret | Verify webhook secret |\n| Timestamp rejected | Clock drift | Check server time sync |\n| Duplicate events | Missing idempotency | Implement event ID tracking |\n| Handler timeout | Slow processing | Use async queue |\n\n## Examples\n\n### Testing Webhooks Locally\n```bash\n# Use ngrok to expose local server\nngrok http 3000\n\n# Send test webhook\ncurl -X POST https://your-ngrok-url/webhooks/supabase \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"type\": \"test\", \"data\": {}}'\n```\n\n## Resources\n- [Supabase Webhooks Guide](https://supabase.com/docs/webhooks)\n- [Webhook Security Best Practices](https://supabase.com/docs/webhooks/security)\n\n## Next Steps\nFor performance optimization, see `supabase-performance-tuning`.",
      "parentPlugin": {
        "name": "supabase-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/supabase-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Supabase (30 skills)"
      },
      "filePath": "plugins/saas-packs/supabase-pack/skills/supabase-webhooks-events/SKILL.md"
    },
    {
      "slug": "testing-browser-compatibility",
      "name": "testing-browser-compatibility",
      "description": "Test across multiple browsers and devices for cross-browser compatibility. Use when ensuring cross-browser or device compatibility. Trigger with phrases like \"test browser compatibility\", \"check cross-browser\", or \"validate on browsers\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:browser-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Browser Compatibility Tester\n\nThis skill provides automated assistance for browser compatibility tester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:browser-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for browser compatibility tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "browser-compatibility-tester",
        "category": "testing",
        "path": "plugins/testing/browser-compatibility-tester",
        "version": "1.0.0",
        "description": "Cross-browser testing with BrowserStack, Selenium Grid, and Playwright - test across Chrome, Firefox, Safari, Edge"
      },
      "filePath": "plugins/testing/browser-compatibility-tester/skills/testing-browser-compatibility/SKILL.md"
    },
    {
      "slug": "testing-load-balancers",
      "name": "testing-load-balancers",
      "description": "Validate load balancer behavior, failover, and traffic distribution. Use when performing specialized testing. Trigger with phrases like \"test load balancer\", \"validate failover\", or \"check traffic distribution\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:loadbalancer-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Load Balancer Tester\n\nThis skill provides automated assistance for load balancer tester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:loadbalancer-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for load balancer tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "load-balancer-tester",
        "category": "testing",
        "path": "plugins/testing/load-balancer-tester",
        "version": "1.0.0",
        "description": "Test load balancing strategies with traffic distribution validation and failover testing"
      },
      "filePath": "plugins/testing/load-balancer-tester/skills/testing-load-balancers/SKILL.md"
    },
    {
      "slug": "testing-mobile-apps",
      "name": "testing-mobile-apps",
      "description": "Execute mobile app testing on iOS and Android devices/simulators. Use when performing specialized testing. Trigger with phrases like \"test mobile app\", \"run iOS tests\", or \"validate Android functionality\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:mobile-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Mobile App Tester\n\nThis skill provides automated assistance for mobile app tester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:mobile-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for mobile app tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "mobile-app-tester",
        "category": "testing",
        "path": "plugins/testing/mobile-app-tester",
        "version": "1.0.0",
        "description": "Mobile app test automation with Appium, Detox, XCUITest - test iOS and Android apps"
      },
      "filePath": "plugins/testing/mobile-app-tester/skills/testing-mobile-apps/SKILL.md"
    },
    {
      "slug": "testing-visual-regression",
      "name": "testing-visual-regression",
      "description": "Detect visual changes in UI components using screenshot comparison. Use when detecting unintended UI changes or pixel differences. Trigger with phrases like \"test visual changes\", \"compare screenshots\", or \"detect UI regressions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:visual-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Visual Regression Tester\n\nThis skill provides automated assistance for visual regression tester tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:visual-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for visual regression tester tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "visual-regression-tester",
        "category": "testing",
        "path": "plugins/testing/visual-regression-tester",
        "version": "1.0.0",
        "description": "Visual diff testing with Percy, Chromatic, BackstopJS - catch unintended UI changes"
      },
      "filePath": "plugins/testing/visual-regression-tester/skills/testing-visual-regression/SKILL.md"
    },
    {
      "slug": "throttling-apis",
      "name": "throttling-apis",
      "description": "Implement API throttling policies to protect backend services from overload. Use when controlling API request rates. Trigger with phrases like \"throttle API\", \"control request rate\", or \"add throttling\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:throttle-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Throttling Manager\n\nThis skill provides automated assistance for api throttling manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:throttle-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api throttling manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-throttling-manager",
        "category": "api-development",
        "path": "plugins/api-development/api-throttling-manager",
        "version": "1.0.0",
        "description": "Manage API throttling with dynamic rate limits and quota management"
      },
      "filePath": "plugins/api-development/api-throttling-manager/skills/throttling-apis/SKILL.md"
    },
    {
      "slug": "tracking-application-response-times",
      "name": "tracking-application-response-times",
      "description": "Track and optimize application response times across API endpoints, database queries, and service calls. Use when monitoring performance or identifying bottlenecks. Trigger with phrases like \"track response times\", \"monitor API performance\", or \"analyze latency\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(monitoring:*)",
        "Bash(metrics:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Response Time Tracker\n\nThis skill provides automated assistance for response time tracker tasks.\n\n## Overview\n\nThis skill empowers Claude to proactively monitor and improve application performance by tracking response times across various layers. It provides detailed metrics and insights to identify and resolve performance bottlenecks.\n\n## How It Works\n\n1. **Initiate Tracking**: The user requests response time tracking.\n2. **Configure Monitoring**: The plugin automatically begins monitoring API endpoints, database queries, external service calls, frontend rendering, and background jobs.\n3. **Report Metrics**: The plugin generates reports including P50, P95, P99 percentiles, average, and maximum response times.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in your application.\n- Monitor service level objectives (SLOs) related to response times.\n- Receive alerts about performance degradation.\n\n## Examples\n\n### Example 1: Diagnosing Slow API Endpoint\n\nUser request: \"Track response times for the user authentication API endpoint.\"\n\nThe skill will:\n1. Activate the response-time-tracker plugin.\n2. Monitor the specified API endpoint and report response time metrics, highlighting potential bottlenecks.\n\n### Example 2: Monitoring Database Query Performance\n\nUser request: \"Monitor database query performance for the product catalog.\"\n\nThe skill will:\n1. Activate the response-time-tracker plugin.\n2. Track the execution time of database queries related to the product catalog and provide performance insights.\n\n## Best Practices\n\n- **Granularity**: Track response times at a granular level (e.g., individual API endpoints, specific database queries) for more precise insights.\n- **Alerting**: Configure alerts for significant deviations from baseline performance to proactively address potential issues.\n- **Contextualization**: Correlate response time data with other metrics (e.g., CPU usage, memory consumption) to identify root causes.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools to provide a comprehensive view of application performance. It can also be used in conjunction with optimization tools to automatically address identified bottlenecks.\n\n## Prerequisites\n\n- Access to application monitoring infrastructure\n- Response time data collection in {baseDir}/metrics/response-times/\n- APM tools or custom instrumentation\n- Performance SLO definitions\n\n## Instructions\n\n1. Configure monitoring for API endpoints and database queries\n2. Collect response time metrics (P50, P95, P99 percentiles)\n3. Analyze trends and identify performance degradation\n4. Compare against performance baselines and SLOs\n5. Identify bottlenecks and root causes\n6. Generate optimization recommendations\n\n## Output\n\n- Response time reports with percentile metrics\n- Performance trend visualizations\n- Bottleneck identification analysis\n- SLO compliance status\n- Optimization recommendations with priorities\n\n## Error Handling\n\nIf response time tracking fails:\n- Verify monitoring agent installation\n- Check instrumentation configuration\n- Validate metric export endpoints\n- Ensure data storage availability\n- Review sampling configuration\n\n## Resources\n\n- APM tool documentation\n- Response time monitoring best practices\n- Percentile-based SLO definitions\n- Performance optimization guides",
      "parentPlugin": {
        "name": "response-time-tracker",
        "category": "performance",
        "path": "plugins/performance/response-time-tracker",
        "version": "1.0.0",
        "description": "Track and optimize application response times"
      },
      "filePath": "plugins/performance/response-time-tracker/skills/tracking-application-response-times/SKILL.md"
    },
    {
      "slug": "tracking-crypto-derivatives",
      "name": "tracking-crypto-derivatives",
      "description": "Track futures, options, and perpetual swap positions with P&L calculations. Use when tracking futures and options positions. Trigger with phrases like \"track derivatives\", \"check futures positions\", or \"analyze perps\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:derivatives-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Crypto Derivatives Tracker\n\nThis skill provides automated assistance for crypto derivatives tracker tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:derivatives-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "crypto-derivatives-tracker",
        "category": "crypto",
        "path": "plugins/crypto/crypto-derivatives-tracker",
        "version": "1.0.0",
        "description": "Track crypto futures, options, perpetual swaps with funding rates, open interest, and derivatives market analysis"
      },
      "filePath": "plugins/crypto/crypto-derivatives-tracker/skills/tracking-crypto-derivatives/SKILL.md"
    },
    {
      "slug": "tracking-crypto-portfolio",
      "name": "tracking-crypto-portfolio",
      "description": "Track multi-chain crypto portfolio with real-time valuations and performance metrics. Use when managing multi-chain crypto holdings. Trigger with phrases like \"track my portfolio\", \"check holdings\", or \"analyze positions\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:portfolio-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Crypto Portfolio Tracker\n\nThis skill provides automated assistance for crypto portfolio tracker tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:portfolio-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "crypto-portfolio-tracker",
        "category": "crypto",
        "path": "plugins/crypto/crypto-portfolio-tracker",
        "version": "1.0.0",
        "description": "Professional crypto portfolio tracking with real-time prices, PnL analysis, and risk metrics"
      },
      "filePath": "plugins/crypto/crypto-portfolio-tracker/skills/tracking-crypto-portfolio/SKILL.md"
    },
    {
      "slug": "tracking-crypto-prices",
      "name": "tracking-crypto-prices",
      "description": "Track real-time cryptocurrency prices across exchanges with historical data and alerts. Use when monitoring real-time cryptocurrency prices. Trigger with phrases like \"check price\", \"track prices\", or \"get price alert\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:price-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Market Price Tracker\n\nThis skill provides automated assistance for market price tracker tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:price-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "market-price-tracker",
        "category": "crypto",
        "path": "plugins/crypto/market-price-tracker",
        "version": "1.0.0",
        "description": "Real-time market price tracking with multi-exchange feeds and advanced alerts"
      },
      "filePath": "plugins/crypto/market-price-tracker/skills/tracking-crypto-prices/SKILL.md"
    },
    {
      "slug": "tracking-model-versions",
      "name": "tracking-model-versions",
      "description": "This skill enables AI assistant to track and manage ai/ml model versions using the model-versioning-tracker plugin. it should be used when the user asks to manage model versions, track model lineage, log model performance, or implement version control f... Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Model Versioning Tracker\n\nThis skill provides automated assistance for model versioning tracker tasks.\n\n## Overview\n\n\nThis skill provides automated assistance for model versioning tracker tasks.\nThis skill empowers Claude to interact with the model-versioning-tracker plugin, providing a streamlined approach to managing and tracking AI/ML model versions. It ensures that model development and deployment are conducted with proper version control, logging, and performance monitoring.\n\n## How It Works\n\n1. **Analyze Request**: Claude analyzes the user's request to determine the specific model versioning task.\n2. **Generate Code**: Claude generates the necessary code to interact with the model-versioning-tracker plugin.\n3. **Execute Task**: The plugin executes the code, performing the requested model versioning operation, such as tracking a new version or retrieving performance metrics.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Track new versions of AI/ML models.\n- Retrieve performance metrics for specific model versions.\n- Implement automated workflows for model versioning.\n\n## Examples\n\n### Example 1: Tracking a New Model Version\n\nUser request: \"Track a new version of my image classification model.\"\n\nThe skill will:\n1. Generate code to log the new model version and its associated metadata using the model-versioning-tracker plugin.\n2. Execute the code, creating a new entry in the model registry.\n\n### Example 2: Retrieving Performance Metrics\n\nUser request: \"Get the performance metrics for version 3 of my sentiment analysis model.\"\n\nThe skill will:\n1. Generate code to query the model-versioning-tracker plugin for the performance metrics associated with the specified model version.\n2. Execute the code and return the metrics to the user.\n\n## Best Practices\n\n- **Data Validation**: Ensure input data is validated before logging model versions.\n- **Error Handling**: Implement robust error handling to manage unexpected issues during version tracking.\n- **Performance Monitoring**: Continuously monitor model performance to identify opportunities for optimization.\n\n## Integration\n\nThis skill integrates with other Claude Code plugins by providing a centralized location for managing AI/ML model versions. It can be used in conjunction with plugins that handle data processing, model training, and deployment to ensure a seamless AI/ML workflow.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "model-versioning-tracker",
        "category": "ai-ml",
        "path": "plugins/ai-ml/model-versioning-tracker",
        "version": "1.0.0",
        "description": "Track and manage model versions"
      },
      "filePath": "plugins/ai-ml/model-versioning-tracker/skills/tracking-model-versions/SKILL.md"
    },
    {
      "slug": "tracking-regression-tests",
      "name": "tracking-regression-tests",
      "description": "Track and manage regression test suites across releases. Use when performing specialized testing. Trigger with phrases like \"track regressions\", \"manage regression suite\", or \"validate against baseline\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:regression-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Regression Test Tracker\n\nThis skill provides automated assistance for regression test tracker tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:regression-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for regression test tracker tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "regression-test-tracker",
        "category": "testing",
        "path": "plugins/testing/regression-test-tracker",
        "version": "1.0.0",
        "description": "Track and run regression tests to ensure new changes don't break existing functionality"
      },
      "filePath": "plugins/testing/regression-test-tracker/skills/tracking-regression-tests/SKILL.md"
    },
    {
      "slug": "tracking-resource-usage",
      "name": "tracking-resource-usage",
      "description": "Track and optimize resource usage across application stack including CPU, memory, disk, and network I/O. Use when identifying bottlenecks or optimizing costs. Trigger with phrases like \"track resource usage\", \"monitor CPU and memory\", or \"optimize resource allocation\".",
      "allowedTools": [
        "\"Read",
        "Bash(top:*)",
        "Bash(ps:*)",
        "Bash(vmstat:*)",
        "Bash(iostat:*)",
        "Grep",
        "Glob\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Resource Usage Tracker\n\nThis skill provides automated assistance for resource usage tracker tasks.\n\n## Overview\n\nThis skill provides a comprehensive solution for monitoring and optimizing resource usage within an application. It leverages the resource-usage-tracker plugin to gather real-time metrics, identify performance bottlenecks, and suggest optimization strategies.\n\n## How It Works\n\n1. **Identify Resources**: The skill identifies the resources to be tracked based on the user's request and the application's configuration (CPU, memory, disk I/O, network I/O, etc.).\n2. **Collect Metrics**: The plugin collects real-time metrics for the identified resources, providing a snapshot of current resource consumption.\n3. **Analyze Data**: The skill analyzes the collected data to identify performance bottlenecks, resource imbalances, and potential optimization opportunities.\n4. **Provide Recommendations**: Based on the analysis, the skill provides specific recommendations for optimizing resource allocation, right-sizing instances, and reducing costs.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Identify performance bottlenecks in an application.\n- Optimize resource allocation to improve efficiency.\n- Reduce cloud infrastructure costs by right-sizing instances.\n- Monitor resource usage in real-time to detect anomalies.\n- Track the impact of code changes on resource consumption.\n\n## Examples\n\n### Example 1: Identifying Memory Leaks\n\nUser request: \"Track memory usage and identify potential memory leaks.\"\n\nThe skill will:\n1. Activate the resource-usage-tracker plugin to monitor memory usage (heap, stack, RSS).\n2. Analyze the memory usage data over time to detect patterns indicative of memory leaks.\n3. Provide recommendations for identifying and resolving the memory leaks.\n\n### Example 2: Optimizing Database Connection Pool\n\nUser request: \"Optimize database connection pool utilization.\"\n\nThe skill will:\n1. Activate the resource-usage-tracker plugin to monitor database connection pool metrics.\n2. Analyze the connection pool utilization data to identify periods of high contention or underutilization.\n3. Provide recommendations for adjusting the connection pool size to optimize performance and resource consumption.\n\n## Best Practices\n\n- **Granularity**: Track resource usage at a granular level (e.g., process-level CPU usage) to identify specific bottlenecks.\n- **Historical Data**: Analyze historical resource usage data to identify trends and predict future resource needs.\n- **Alerting**: Configure alerts to notify you when resource usage exceeds predefined thresholds.\n\n## Integration\n\nThis skill can be integrated with other monitoring and alerting tools to provide a comprehensive view of application performance. It can also be used in conjunction with deployment automation tools to automatically right-size instances based on resource usage patterns.\n\n## Prerequisites\n\n- Access to system monitoring tools (top, ps, vmstat, iostat)\n- Resource metrics collection infrastructure\n- Historical usage data in {baseDir}/metrics/resources/\n- Performance baseline definitions\n\n## Instructions\n\n1. Identify resources to track (CPU, memory, disk, network)\n2. Collect real-time metrics using system tools\n3. Analyze data for bottlenecks and patterns\n4. Compare against historical baselines\n5. Generate optimization recommendations\n6. Provide right-sizing and cost reduction strategies\n\n## Output\n\n- Resource usage reports with trends\n- Bottleneck identification and analysis\n- Right-sizing recommendations for instances\n- Cost optimization suggestions\n- Alert configurations for thresholds\n\n## Error Handling\n\nIf resource tracking fails:\n- Verify system monitoring tool permissions\n- Check metrics collection daemon status\n- Validate data storage availability\n- Ensure network access to monitoring endpoints\n- Review baseline data completeness\n\n## Resources\n\n- System performance monitoring guides\n- Cloud resource optimization best practices\n- CPU and memory profiling techniques\n- Infrastructure cost optimization strategies",
      "parentPlugin": {
        "name": "resource-usage-tracker",
        "category": "performance",
        "path": "plugins/performance/resource-usage-tracker",
        "version": "1.0.0",
        "description": "Track and optimize resource usage across the stack"
      },
      "filePath": "plugins/performance/resource-usage-tracker/skills/tracking-resource-usage/SKILL.md"
    },
    {
      "slug": "tracking-service-reliability",
      "name": "tracking-service-reliability",
      "description": "Define and track SLAs, SLIs, and SLOs for service reliability including availability, latency, and error rates. Use when establishing reliability targets or monitoring service health. Trigger with phrases like \"define SLOs\", \"track SLI metrics\", or \"calculate error budget\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(monitoring:*)",
        "Bash(metrics:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Sla Sli Tracker\n\nThis skill provides automated assistance for sla sli tracker tasks.\n\n## Overview\n\nThis skill provides a structured approach to defining and tracking SLAs, SLIs, and SLOs, which are essential for ensuring service reliability. It automates the process of setting performance targets and monitoring actual performance, enabling proactive identification and resolution of potential issues.\n\n## How It Works\n\n1. **SLI Definition**: The skill guides the user to define Service Level Indicators (SLIs) such as availability, latency, error rate, and throughput.\n2. **SLO Target Setting**: The skill assists in setting Service Level Objectives (SLOs) by establishing target values for the defined SLIs (e.g., 99.9% availability).\n3. **SLA Establishment**: The skill helps in formalizing Service Level Agreements (SLAs), which are customer-facing commitments based on the defined SLOs.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Define SLAs, SLIs, and SLOs for a service.\n- Track service performance against defined objectives.\n- Calculate error budgets based on SLOs.\n\n## Examples\n\n### Example 1: Defining SLOs for a New Service\n\nUser request: \"Create SLOs for our new payment processing service.\"\n\nThe skill will:\n1. Prompt the user to define SLIs (e.g., latency, error rate).\n2. Assist in setting target values for each SLI (e.g., p99 latency < 100ms, error rate < 0.01%).\n\n### Example 2: Tracking Availability\n\nUser request: \"Track the availability SLI for the database service.\"\n\nThe skill will:\n1. Guide the user in setting up the tracking of the availability SLI.\n2. Visualize availability performance against the defined SLO.\n\n## Best Practices\n\n- **Granularity**: Define SLIs that are specific and measurable.\n- **Realism**: Set SLOs that are challenging but achievable.\n- **Alignment**: Ensure SLAs align with the defined SLOs and business requirements.\n\n## Integration\n\nThis skill can be integrated with monitoring tools to automatically collect SLI data and track performance against SLOs. It can also be used in conjunction with alerting systems to trigger notifications when SLO violations occur.\n\n## Prerequisites\n\n- SLI definitions stored in {baseDir}/slos/sli-definitions.yaml\n- Access to monitoring and metrics systems\n- Historical performance data for baseline\n- Business requirements for service reliability\n\n## Instructions\n\n1. Define Service Level Indicators (availability, latency, error rate, throughput)\n2. Set Service Level Objectives with target values (e.g., 99.9% availability)\n3. Formalize Service Level Agreements with customer commitments\n4. Configure automated SLI data collection\n5. Calculate error budgets based on SLOs\n6. Track performance and alert on SLO violations\n\n## Output\n\n- SLI/SLO/SLA definition documents\n- Real-time SLI metric dashboards\n- Error budget calculations and burn rate\n- SLO compliance reports\n- Alerting configurations for violations\n\n## Error Handling\n\nIf SLI/SLO tracking fails:\n- Verify SLI definition completeness\n- Check metric collection infrastructure\n- Validate data accuracy and granularity\n- Ensure alerting system connectivity\n- Review error budget calculation logic\n\n## Resources\n\n- Google SRE book on SLIs and SLOs\n- Error budget implementation guides\n- Service reliability engineering practices\n- SLO definition templates and examples",
      "parentPlugin": {
        "name": "sla-sli-tracker",
        "category": "performance",
        "path": "plugins/performance/sla-sli-tracker",
        "version": "1.0.0",
        "description": "Track SLAs, SLIs, and SLOs for service reliability"
      },
      "filePath": "plugins/performance/sla-sli-tracker/skills/tracking-service-reliability/SKILL.md"
    },
    {
      "slug": "tracking-token-launches",
      "name": "tracking-token-launches",
      "description": "Monitor new token launches, IDOs, and fair launches with contract verification. Use when discovering new token launches. Trigger with phrases like \"track launches\", \"find new tokens\", or \"monitor IDOs\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(crypto:launch-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Intent Solutions IO <jeremy@intentsolutions.ai>",
      "license": "MIT",
      "content": "# Token Launch Tracker\n\nThis skill provides automated assistance for token launch tracker tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to crypto market data APIs (CoinGecko, CoinMarketCap, or similar)\n- Blockchain RPC endpoints or node access (Infura, Alchemy, or self-hosted)\n- API keys for exchanges if trading or querying account data\n- Web3 libraries installed (ethers.js, web3.py, or equivalent)\n- Understanding of blockchain concepts and crypto market dynamics\n\n## Instructions\n\n### Step 1: Configure Data Sources\nSet up connections to crypto data providers:\n1. Use Read tool to load API credentials from {baseDir}/config/crypto-apis.env\n2. Configure blockchain RPC endpoints for target networks\n3. Set up exchange API connections if required\n4. Verify rate limits and subscription tiers\n5. Test connectivity and authentication\n\n### Step 2: Query Crypto Data\nRetrieve relevant blockchain and market data:\n1. Use Bash(crypto:launch-*) to execute crypto data queries\n2. Fetch real-time prices, volumes, and market cap data\n3. Query blockchain for on-chain metrics and transactions\n4. Retrieve exchange order book and trade history\n5. Aggregate data from multiple sources for accuracy\n\n### Step 3: Analyze and Process\nProcess crypto data to generate insights:\n- Calculate key metrics (returns, volatility, correlation)\n- Identify patterns and anomalies in data\n- Apply technical indicators or on-chain signals\n- Compare across timeframes and assets\n- Generate actionable insights and alerts\n\n### Step 4: Generate Reports\nDocument findings in {baseDir}/crypto-reports/:\n- Market summary with key price movements\n- Detailed analysis with charts and metrics\n- Trading signals or opportunity recommendations\n- Risk assessment and position sizing guidance\n- Historical context and trend analysis\n\n## Output\n\nThe skill generates comprehensive crypto analysis:\n\n### Market Data\nReal-time and historical metrics:\n- Current prices across exchanges with spread analysis\n- 24h volume, market cap, and circulating supply\n- Price changes across multiple timeframes (1h, 24h, 7d, 30d)\n- Trading volume distribution by exchange\n- Liquidity metrics and slippage estimates\n\n### On-Chain Metrics\nBlockchain-specific analysis:\n- Transaction count and network activity\n- Active addresses and user growth metrics\n- Token holder distribution and concentration\n- Smart contract interactions and DeFi TVL\n- Gas usage and network congestion indicators\n\n### Technical Analysis\nTrading indicators and signals:\n- Moving averages (SMA, EMA) and trend identification\n- RSI, MACD, Bollinger Bands technical indicators\n- Support and resistance levels\n- Chart patterns and breakout signals\n- Volume profile and accumulation zones\n\n### Risk Metrics\nPortfolio and position risk assessment:\n- Value at Risk (VaR) calculations\n- Portfolio correlation and diversification metrics\n- Volatility analysis and beta to market\n- Drawdown statistics and recovery times\n- Liquidation risk for leveraged positions\n\n## Error Handling\n\nCommon issues and solutions:\n\n**API Rate Limit Exceeded**\n- Error: Too many requests to crypto data API\n- Solution: Implement request throttling; use caching for frequently accessed data; upgrade API tier if needed\n\n**Blockchain RPC Errors**\n- Error: Cannot connect to blockchain node or timeout\n- Solution: Switch to backup RPC endpoint; verify network connectivity; check if node is synced\n\n**Invalid Address or Transaction**\n- Error: Blockchain address format invalid or transaction not found\n- Solution: Validate address checksums; verify network (mainnet vs testnet); allow time for transaction confirmation\n\n**Exchange API Authentication Failed**\n- Error: Invalid API key or signature mismatch\n- Solution: Regenerate API keys; verify permissions (read/trade); check system clock synchronization for signatures\n\n## Resources\n\n### Crypto Data Providers\n- CoinGecko API for market data across thousands of assets\n- Etherscan API for Ethereum blockchain data\n- Dune Analytics for on-chain SQL queries\n- The Graph for decentralized blockchain indexing\n\n### Web3 Libraries\n- ethers.js for Ethereum smart contract interaction\n- web3.py for Python-based blockchain queries\n- viem for TypeScript Web3 development\n- Hardhat for local blockchain testing\n\n### Trading and Analysis Tools\n- TradingView for technical analysis and charting\n- Glassnode for advanced on-chain metrics\n- DeFi Llama for DeFi protocol analytics\n- Nansen for wallet tracking and smart money flows\n\n### Best Practices\n- Never store private keys or seed phrases in code\n- Always verify smart contract addresses from official sources\n- Use testnet for experimentation before mainnet\n- Implement proper error handling for network failures\n- Monitor gas prices before submitting transactions\n- Validate all user inputs to prevent injection attacks\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "token-launch-tracker",
        "category": "crypto",
        "path": "plugins/crypto/token-launch-tracker",
        "version": "1.0.0",
        "description": "Track new token launches, detect rugpulls, and analyze contract security for early-stage crypto projects"
      },
      "filePath": "plugins/crypto/token-launch-tracker/skills/tracking-token-launches/SKILL.md"
    },
    {
      "slug": "training-machine-learning-models",
      "name": "training-machine-learning-models",
      "description": "Train machine learning models with automated workflows. Analyzes datasets, selects model types (classification, regression), configures parameters, trains with cross-validation, and saves model artifacts. Use when asked to \"train model\" or \"evalua... Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Ml Model Trainer\n\nThis skill provides automated assistance for ml model trainer tasks.\n\n## Overview\n\nThis skill empowers Claude to automatically train and evaluate machine learning models. It streamlines the model development process by handling data analysis, model selection, training, and evaluation, ultimately providing a persisted model artifact.\n\n## How It Works\n\n1. **Data Analysis and Preparation**: The skill analyzes the provided dataset and identifies the target variable, determining the appropriate model type (classification, regression, etc.).\n2. **Model Selection and Training**: Based on the data analysis, the skill selects a suitable machine learning model and configures the training parameters. It then trains the model using cross-validation techniques.\n3. **Performance Evaluation and Persistence**: After training, the skill generates performance metrics to evaluate the model's effectiveness. Finally, it saves the trained model artifact for future use.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Train a machine learning model on a given dataset.\n- Evaluate the performance of a machine learning model.\n- Automate the machine learning model training process.\n\n## Examples\n\n### Example 1: Training a Classification Model\n\nUser request: \"Train a classification model on this dataset of customer churn data.\"\n\nThe skill will:\n1. Analyze the customer churn data, identify the churn status as the target variable, and determine that a classification model is appropriate.\n2. Select a suitable classification algorithm (e.g., Logistic Regression, Random Forest), train the model using cross-validation, and generate performance metrics such as accuracy, precision, and recall.\n\n### Example 2: Training a Regression Model\n\nUser request: \"Train a regression model to predict house prices based on features like size, location, and number of bedrooms.\"\n\nThe skill will:\n1. Analyze the house price data, identify the price as the target variable, and determine that a regression model is appropriate.\n2. Select a suitable regression algorithm (e.g., Linear Regression, Support Vector Regression), train the model using cross-validation, and generate performance metrics such as Mean Squared Error (MSE) and R-squared.\n\n## Best Practices\n\n- **Data Quality**: Ensure the dataset is clean and properly formatted before training the model.\n- **Feature Engineering**: Consider feature engineering techniques to improve model performance.\n- **Hyperparameter Tuning**: Experiment with different hyperparameter settings to optimize model performance.\n\n## Integration\n\nThis skill can be used in conjunction with other data analysis and manipulation tools to prepare data for training. It can also integrate with model deployment tools to deploy the trained model to production.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "ml-model-trainer",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ml-model-trainer",
        "version": "1.0.0",
        "description": "Train and optimize machine learning models with automated workflows"
      },
      "filePath": "plugins/ai-ml/ml-model-trainer/skills/training-machine-learning-models/SKILL.md"
    },
    {
      "slug": "tuning-hyperparameters",
      "name": "tuning-hyperparameters",
      "description": "Optimize machine learning model hyperparameters using grid search, random search, or Bayesian optimization. Finds best parameter configurations to maximize performance. Use when asked to \"tune hyperparameters\" or \"optimize model\". Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Hyperparameter Tuner\n\nThis skill provides automated assistance for hyperparameter tuner tasks.\n\n## Overview\n\nThis skill empowers Claude to fine-tune machine learning models by automatically searching for the optimal hyperparameter configurations. It leverages different search strategies (grid, random, Bayesian) to efficiently explore the hyperparameter space and identify settings that maximize model performance.\n\n## How It Works\n\n1. **Analyzing Requirements**: Claude analyzes the user's request to determine the model, the hyperparameters to tune, the search strategy, and the evaluation metric.\n2. **Generating Code**: Claude generates Python code using appropriate ML libraries (e.g., scikit-learn, Optuna) to implement the specified hyperparameter search. The code includes data loading, preprocessing, model training, and evaluation.\n3. **Executing Search**: The generated code is executed to perform the hyperparameter search. The plugin iterates through different hyperparameter combinations, trains the model with each combination, and evaluates its performance.\n4. **Reporting Results**: Claude reports the best hyperparameter configuration found during the search, along with the corresponding performance metrics. It also provides insights into the search process and potential areas for further optimization.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Optimize the performance of a machine learning model.\n- Automatically search for the best hyperparameter settings.\n- Compare different hyperparameter search strategies.\n- Improve model accuracy, precision, recall, or other relevant metrics.\n\n## Examples\n\n### Example 1: Optimizing a Random Forest Model\n\nUser request: \"Tune hyperparameters of a Random Forest model using grid search to maximize accuracy on the iris dataset. Consider n_estimators and max_depth.\"\n\nThe skill will:\n1. Generate code to perform a grid search over the specified hyperparameters (n_estimators, max_depth) of a Random Forest model using the iris dataset.\n2. Execute the grid search and report the best hyperparameter combination and the corresponding accuracy score.\n\n### Example 2: Using Bayesian Optimization\n\nUser request: \"Optimize a Gradient Boosting model using Bayesian optimization with Optuna to minimize the root mean squared error on the Boston housing dataset.\"\n\nThe skill will:\n1. Generate code to perform Bayesian optimization using Optuna to find the best hyperparameters for a Gradient Boosting model on the Boston housing dataset.\n2. Execute the optimization and report the best hyperparameter combination and the corresponding RMSE.\n\n## Best Practices\n\n- **Define Search Space**: Clearly define the range and type of values for each hyperparameter to be tuned.\n- **Choose Appropriate Strategy**: Select the hyperparameter search strategy (grid, random, Bayesian) based on the complexity of the hyperparameter space and the available computational resources. Bayesian optimization is generally more efficient for complex spaces.\n- **Use Cross-Validation**: Implement cross-validation to ensure the robustness of the evaluation metric and prevent overfitting.\n\n## Integration\n\nThis skill integrates seamlessly with other Claude Code plugins that involve machine learning tasks, such as data analysis, model training, and deployment. It can be used in conjunction with data visualization tools to gain insights into the impact of different hyperparameter settings on model performance.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "hyperparameter-tuner",
        "category": "ai-ml",
        "path": "plugins/ai-ml/hyperparameter-tuner",
        "version": "1.0.0",
        "description": "Optimize hyperparameters using grid/random/bayesian search"
      },
      "filePath": "plugins/ai-ml/hyperparameter-tuner/skills/tuning-hyperparameters/SKILL.md"
    },
    {
      "slug": "validating-ai-ethics-and-fairness",
      "name": "validating-ai-ethics-and-fairness",
      "description": "Validate AI/ML models and datasets for bias, fairness, and ethical concerns. Use when auditing AI systems for ethical compliance, fairness assessment, or bias detection. Trigger with phrases like \"evaluate model fairness\", \"check for bias\", or \"validate AI ethics\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(python:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Ai Ethics Validator\n\nThis skill provides automated assistance for ai ethics validator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Access to the AI model or dataset requiring validation\n- Model predictions or training data available for analysis\n- Understanding of demographic attributes relevant to fairness evaluation\n- Python environment with fairness assessment libraries (e.g., Fairlearn, AIF360)\n- Appropriate permissions to analyze sensitive data attributes\n\n## Instructions\n\n### Step 1: Identify Validation Scope\nDetermine which aspects of the AI system require ethical validation:\n- Model predictions across demographic groups\n- Training dataset representation and balance\n- Feature selection and potential proxy variables\n- Output disparities and fairness metrics\n\n### Step 2: Analyze for Bias\nUse the skill to examine the AI system:\n1. Load model predictions or dataset using Read tool\n2. Identify sensitive attributes (age, gender, race, etc.)\n3. Calculate fairness metrics (demographic parity, equalized odds, etc.)\n4. Detect statistical disparities across groups\n\n### Step 3: Generate Validation Report\nThe skill produces a comprehensive report including:\n- Identified biases and their severity\n- Fairness metric calculations with thresholds\n- Representation analysis across demographic groups\n- Recommended mitigation strategies\n- Compliance assessment against ethical guidelines\n\n### Step 4: Implement Mitigations\nBased on findings, apply recommended strategies:\n- Rebalance training data using sampling techniques\n- Apply algorithmic fairness constraints during training\n- Adjust decision thresholds for specific groups\n- Document ethical considerations and trade-offs\n\n## Output\n\nThe skill generates structured reports containing:\n\n### Bias Detection Results\n- Statistical disparities identified across groups\n- Severity classification (low, medium, high, critical)\n- Affected demographic segments with quantified impact\n\n### Fairness Metrics\n- Demographic parity ratios\n- Equal opportunity differences\n- Predictive parity measurements\n- Calibration scores across groups\n\n### Mitigation Recommendations\n- Specific technical approaches to reduce bias\n- Data augmentation or resampling strategies\n- Model constraint adjustments\n- Monitoring and continuous evaluation plans\n\n### Compliance Assessment\n- Alignment with ethical AI guidelines\n- Regulatory compliance status\n- Documentation requirements for audit trails\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Insufficient Data**\n- Error: Cannot calculate fairness metrics with small sample sizes\n- Solution: Aggregate related groups or collect additional data for underrepresented segments\n\n**Missing Sensitive Attributes**\n- Error: Demographic information not available in dataset\n- Solution: Use proxy detection methods or request access to protected attributes under appropriate governance\n\n**Conflicting Fairness Criteria**\n- Error: Multiple fairness metrics show contradictory results\n- Solution: Document trade-offs and prioritize metrics based on use case context and stakeholder input\n\n**Data Quality Issues**\n- Error: Inconsistent or corrupted attribute values\n- Solution: Perform data cleaning, standardization, and validation before bias analysis\n\n## Resources\n\n### Fairness Assessment Frameworks\n- Fairlearn library for bias detection and mitigation\n- AI Fairness 360 (AIF360) toolkit for comprehensive fairness analysis\n- Google What-If Tool for interactive fairness exploration\n\n### Ethical AI Guidelines\n- IEEE Ethically Aligned Design principles\n- EU Ethics Guidelines for Trustworthy AI\n- ACM Code of Ethics for AI practitioners\n\n### Fairness Metrics Documentation\n- Demographic parity and statistical parity definitions\n- Equalized odds and equal opportunity metrics\n- Individual fairness and calibration measures\n\n### Best Practices\n- Involve diverse stakeholders in fairness criteria selection\n- Document all ethical decisions and trade-offs\n- Implement continuous monitoring for fairness drift\n- Maintain transparency in model limitations and biases\n\n## Overview\n\n\nThis skill provides automated assistance for ai ethics validator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "ai-ethics-validator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/ai-ethics-validator",
        "version": "1.0.0",
        "description": "AI ethics and fairness validation"
      },
      "filePath": "plugins/ai-ml/ai-ethics-validator/skills/validating-ai-ethics-and-fairness/SKILL.md"
    },
    {
      "slug": "validating-api-contracts",
      "name": "validating-api-contracts",
      "description": "Validate API contracts using consumer-driven contract testing (Pact, Spring Cloud Contract). Use when performing specialized testing. Trigger with phrases like \"validate API contract\", \"run contract tests\", or \"check consumer contracts\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(test:contract-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugin Hub <[email protected]>",
      "license": "MIT",
      "content": "# Contract Test Validator\n\nThis skill provides automated assistance for contract test validator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- Test environment configured and accessible\n- Required testing tools and frameworks installed\n- Test data and fixtures prepared\n- Appropriate permissions for test execution\n- Network connectivity if testing external services\n\n## Instructions\n\n### Step 1: Prepare Test Environment\nSet up the testing context:\n1. Use Read tool to examine configuration from {baseDir}/config/\n2. Validate test prerequisites are met\n3. Initialize test framework and load dependencies\n4. Configure test parameters and thresholds\n\n### Step 2: Execute Tests\nRun the test suite:\n1. Use Bash(test:contract-*) to invoke test framework\n2. Monitor test execution progress\n3. Capture test outputs and metrics\n4. Handle test failures and error conditions\n\n### Step 3: Analyze Results\nProcess test outcomes:\n- Identify passed and failed tests\n- Calculate success rate and performance metrics\n- Detect patterns in failures\n- Generate insights for improvement\n\n### Step 4: Generate Report\nDocument findings in {baseDir}/test-reports/:\n- Test execution summary\n- Detailed failure analysis\n- Performance benchmarks\n- Recommendations for fixes\n\n## Output\n\nThe skill generates comprehensive test results:\n\n### Test Summary\n- Total tests executed\n- Pass/fail counts and percentage\n- Execution time metrics\n- Resource utilization stats\n\n### Detailed Results\nEach test includes:\n- Test name and identifier\n- Execution status (pass/fail/skip)\n- Actual vs. expected outcomes\n- Error messages and stack traces\n\n### Metrics and Analysis\n- Code coverage percentages\n- Performance benchmarks\n- Trend analysis across runs\n- Quality gate compliance status\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Environment Setup Failures**\n- Error: Test environment not properly configured\n- Solution: Verify configuration files; check environment variables; ensure dependencies are installed\n\n**Test Execution Timeouts**\n- Error: Tests exceeded maximum execution time\n- Solution: Increase timeout thresholds; optimize slow tests; parallelize test execution\n\n**Resource Exhaustion**\n- Error: Insufficient memory or disk space during testing\n- Solution: Clean up temporary files; reduce concurrent test workers; increase resource allocation\n\n**Dependency Issues**\n- Error: Required services or databases unavailable\n- Solution: Verify service health; check network connectivity; use mocks if services are down\n\n## Resources\n\n### Testing Tools\n- Industry-standard testing frameworks for your language/platform\n- CI/CD integration guides and plugins\n- Test automation best practices documentation\n\n### Best Practices\n- Maintain test isolation and independence\n- Use meaningful test names and descriptions\n- Keep tests fast and focused\n- Implement proper setup and teardown\n- Version control test artifacts\n- Run tests in CI/CD pipelines\n\n## Overview\n\n\nThis skill provides automated assistance for contract test validator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "contract-test-validator",
        "category": "testing",
        "path": "plugins/testing/contract-test-validator",
        "version": "1.0.0",
        "description": "API contract testing with Pact, OpenAPI validation, and consumer-driven contract verification"
      },
      "filePath": "plugins/testing/contract-test-validator/skills/validating-api-contracts/SKILL.md"
    },
    {
      "slug": "validating-api-responses",
      "name": "validating-api-responses",
      "description": "Validate API responses against schemas to ensure contract compliance and data integrity. Use when ensuring API response correctness. Trigger with phrases like \"validate responses\", \"check API responses\", or \"verify response format\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:validate-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Response Validator\n\nThis skill provides automated assistance for api response validator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:validate-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api response validator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-response-validator",
        "category": "api-development",
        "path": "plugins/api-development/api-response-validator",
        "version": "1.0.0",
        "description": "Validate API responses against schemas and contracts"
      },
      "filePath": "plugins/api-development/api-response-validator/skills/validating-api-responses/SKILL.md"
    },
    {
      "slug": "validating-api-schemas",
      "name": "validating-api-schemas",
      "description": "Validate API schemas against OpenAPI, JSON Schema, and GraphQL specifications. Use when validating API schemas and contracts. Trigger with phrases like \"validate API schema\", \"check OpenAPI spec\", or \"verify schema\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:schema-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Schema Validator\n\nThis skill provides automated assistance for api schema validator tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:schema-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api schema validator tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-schema-validator",
        "category": "api-development",
        "path": "plugins/api-development/api-schema-validator",
        "version": "1.0.0",
        "description": "Validate API schemas with JSON Schema, Joi, Yup, or Zod"
      },
      "filePath": "plugins/api-development/api-schema-validator/skills/validating-api-schemas/SKILL.md"
    },
    {
      "slug": "validating-authentication-implementations",
      "name": "validating-authentication-implementations",
      "description": "Validate authentication mechanisms for security weaknesses and compliance. Use when reviewing login systems or auth flows. Trigger with 'validate authentication', 'check auth security', or 'review login'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Authentication Validator\n\nThis skill provides automated assistance for authentication validator tasks.\n\n## Overview\n\nThis skill allows Claude to assess the security of authentication mechanisms in a system or application. It provides a detailed report highlighting potential vulnerabilities and offering recommendations for improvement based on established security principles.\n\n## How It Works\n\n1. **Initiate Validation**: Upon receiving a trigger phrase, the skill activates the `authentication-validator` plugin.\n2. **Analyze Authentication Methods**: The plugin examines the implemented authentication methods, such as JWT, OAuth, session-based, or API keys.\n3. **Generate Security Report**: The plugin generates a comprehensive report outlining potential vulnerabilities and recommended fixes related to password security, session management, token security (JWT), multi-factor authentication, and account security.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Assess the security of an application's authentication implementation.\n- Identify vulnerabilities in password policies and session management.\n- Evaluate the security of JWT tokens and MFA implementation.\n- Ensure compliance with security best practices and industry standards.\n\n## Examples\n\n### Example 1: Assessing JWT Security\n\nUser request: \"validate authentication for jwt implementation\"\n\nThe skill will:\n1. Activate the `authentication-validator` plugin.\n2. Analyze the JWT implementation, checking for strong signing algorithms, proper expiration claims, and audience/issuer validation.\n3. Generate a report highlighting any vulnerabilities and recommending best practices for JWT security.\n\n### Example 2: Checking Session Security\n\nUser request: \"authcheck session cookies\"\n\nThe skill will:\n1. Activate the `authentication-validator` plugin.\n2. Analyze the session cookie settings, including HttpOnly, Secure, and SameSite attributes.\n3. Generate a report outlining any potential session fixation or CSRF vulnerabilities and recommending appropriate countermeasures.\n\n## Best Practices\n\n- **Password Hashing**: Always use strong hashing algorithms like bcrypt or Argon2 with appropriate salt generation.\n- **Token Expiration**: Implement short-lived access tokens and refresh token rotation for enhanced security.\n- **Multi-Factor Authentication**: Encourage or enforce MFA to mitigate the risk of password compromise.\n\n## Integration\n\nThis skill can be used in conjunction with other security-related plugins to provide a comprehensive security assessment of an application. For example, it can be used alongside a code analysis plugin to identify potential code-level vulnerabilities related to authentication.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "authentication-validator",
        "category": "security",
        "path": "plugins/security/authentication-validator",
        "version": "1.0.0",
        "description": "Validate authentication implementations"
      },
      "filePath": "plugins/security/authentication-validator/skills/validating-authentication-implementations/SKILL.md"
    },
    {
      "slug": "validating-cors-policies",
      "name": "validating-cors-policies",
      "description": "Validate CORS policies for security issues and misconfigurations. Use when reviewing cross-origin resource sharing. Trigger with 'validate CORS', 'check CORS policy', or 'review cross-origin'.",
      "allowedTools": [
        "\"Read",
        "WebFetch",
        "WebSearch",
        "Grep\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Cors Policy Validator\n\nThis skill provides automated assistance for cors policy validator tasks.\n\n## Overview\n\nThis skill empowers Claude to assess the security and correctness of CORS policies. By leveraging the cors-policy-validator plugin, it identifies misconfigurations and potential vulnerabilities in CORS settings, helping developers build more secure web applications.\n\n## How It Works\n\n1. **Analyze CORS Configuration**: The skill receives the CORS configuration details, such as headers or policy files.\n2. **Validate Policy**: It utilizes the cors-policy-validator plugin to analyze the provided configuration against established security best practices.\n3. **Report Findings**: The skill presents a detailed report outlining any identified vulnerabilities or misconfigurations in the CORS policy.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Validate a CORS policy for a web application.\n- Check the CORS configuration of an API endpoint.\n- Identify potential security vulnerabilities in existing CORS implementations.\n\n## Examples\n\n### Example 1: Validating a CORS Policy File\n\nUser request: \"Validate the CORS policy in `cors_policy.json`\"\n\nThe skill will:\n1. Read the `cors_policy.json` file.\n2. Use the cors-policy-validator plugin to analyze the CORS configuration.\n3. Output a report detailing any identified vulnerabilities or misconfigurations.\n\n### Example 2: Checking CORS Headers for an API Endpoint\n\nUser request: \"Check CORS headers for the API endpoint at `https://example.com/api`\"\n\nThe skill will:\n1. Fetch the CORS headers from the specified API endpoint.\n2. Use the cors-policy-validator plugin to analyze the headers.\n3. Output a report summarizing the CORS configuration and any potential issues.\n\n## Best Practices\n\n- **Configuration Source**: Always specify the source of the CORS configuration (e.g., file path, URL) for accurate validation.\n- **Regular Validation**: Regularly validate CORS policies, especially after making changes to the application or API.\n- **Heuristic Analysis**: Consider supplementing validation with manual review and heuristic analysis to catch subtle vulnerabilities.\n\n## Integration\n\nThis skill can be integrated with other security-related plugins to provide a more comprehensive security assessment. For example, it can be used in conjunction with vulnerability scanning tools to identify potential cross-site scripting (XSS) vulnerabilities related to CORS misconfigurations.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "cors-policy-validator",
        "category": "security",
        "path": "plugins/security/cors-policy-validator",
        "version": "1.0.0",
        "description": "Validate CORS policies"
      },
      "filePath": "plugins/security/cors-policy-validator/skills/validating-cors-policies/SKILL.md"
    },
    {
      "slug": "validating-csrf-protection",
      "name": "validating-csrf-protection",
      "description": "Validate CSRF protection implementations for security gaps. Use when reviewing form security or state-changing operations. Trigger with 'validate CSRF', 'check CSRF protection', or 'review token security'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Csrf Protection Validator\n\nThis skill provides automated assistance for csrf protection validator tasks.\n\n## Overview\n\nThis skill empowers Claude to analyze web applications for CSRF vulnerabilities. It assesses the effectiveness of implemented CSRF protection mechanisms, providing insights into potential weaknesses and recommendations for remediation.\n\n## How It Works\n\n1. **Analyze Endpoints**: The plugin examines application endpoints to identify those lacking CSRF protection.\n2. **Assess Protection Mechanisms**: It validates the implementation of CSRF protection mechanisms, including token validation, double-submit cookies, SameSite attributes, and origin validation.\n3. **Generate Report**: A detailed report is generated, highlighting vulnerable endpoints, potential attack scenarios, and recommended fixes.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Validate existing CSRF protection measures.\n- Identify CSRF vulnerabilities in a web application.\n- Assess the risk associated with unprotected endpoints.\n- Generate a report outlining CSRF vulnerabilities and recommended fixes.\n\n## Examples\n\n### Example 1: Identifying Unprotected API Endpoints\n\nUser request: \"validate csrf\"\n\nThe skill will:\n1. Analyze the application's API endpoints.\n2. Identify endpoints lacking CSRF protection, such as those handling sensitive data modifications.\n3. Generate a report outlining vulnerable endpoints and potential attack vectors.\n\n### Example 2: Checking SameSite Cookie Attributes\n\nUser request: \"Check for csrf vulnerabilities in my application\"\n\nThe skill will:\n1. Analyze the application's cookie settings.\n2. Verify that SameSite attributes are properly configured to mitigate CSRF attacks.\n3. Report any cookies lacking the SameSite attribute or using an insecure setting.\n\n## Best Practices\n\n- **Regular Validation**: Regularly validate CSRF protection mechanisms as part of the development lifecycle.\n- **Comprehensive Coverage**: Ensure all state-changing operations are protected against CSRF attacks.\n- **Secure Configuration**: Use secure configurations for CSRF protection mechanisms, such as strong token generation and proper SameSite attribute settings.\n\n## Integration\n\nThis skill can be used in conjunction with other security plugins to provide a comprehensive security assessment of web applications. For example, it can be combined with a vulnerability scanner to identify other potential vulnerabilities in addition to CSRF weaknesses.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "csrf-protection-validator",
        "category": "security",
        "path": "plugins/security/csrf-protection-validator",
        "version": "1.0.0",
        "description": "Validate CSRF protection"
      },
      "filePath": "plugins/security/csrf-protection-validator/skills/validating-csrf-protection/SKILL.md"
    },
    {
      "slug": "validating-database-integrity",
      "name": "validating-database-integrity",
      "description": "Use when you need to ensure database integrity through comprehensive data validation. This skill validates data types, ranges, formats, referential integrity, and business rules. Trigger with phrases like \"validate database data\", \"implement data validation rules\", \"enforce data integrity constraints\", or \"validate data formats\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(psql:*), Bash(mysql:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <[email protected]>",
      "license": "MIT",
      "content": "# Data Validation Engine\n\nThis skill provides automated assistance for data validation engine tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Database connection credentials are available\n- Appropriate database permissions for schema modifications\n- Backup of production databases before applying constraints\n- Understanding of existing data that may violate new constraints\n- Access to database documentation for column specifications\n\n## Instructions\n\n### Step 1: Analyze Validation Requirements\n1. Review database schema and identify columns requiring validation\n2. Determine validation types needed (data type, range, format, referential)\n3. Document existing data patterns that may conflict with new rules\n4. Prioritize validation rules by business criticality\n\n### Step 2: Define Validation Rules\n1. Create validation rule definitions for each column\n2. Specify data types, constraints, and acceptable ranges\n3. Define regular expressions for format validation\n4. Map foreign key relationships for referential integrity\n5. Document business rule logic for complex validations\n\n### Step 3: Implement Database Constraints\n1. Generate SQL constraints for data type validation\n2. Add CHECK constraints for range and format validation\n3. Create foreign key constraints for referential integrity\n4. Implement triggers for complex business rule validation\n5. Test constraints with valid and invalid sample data\n\n### Step 4: Validate Existing Data\n1. Query existing data to identify constraint violations\n2. Generate reports of data that would fail new constraints\n3. Create data cleanup scripts to fix violations\n4. Execute cleanup scripts in staging environment first\n5. Re-validate cleaned data before applying constraints\n\n### Step 5: Apply Validation Rules\n1. Apply constraints to staging database first\n2. Monitor for any application errors or failures\n3. Validate that legitimate operations still function\n4. Apply constraints to production database during maintenance window\n5. Monitor database logs for constraint violation attempts\n\n## Output\n\nThis skill produces:\n\n**Database Constraints**: SQL DDL statements with CHECK, FOREIGN KEY, and NOT NULL constraints\n\n**Validation Reports**: Analysis of existing data showing constraint violations with counts and examples\n\n**Data Cleanup Scripts**: SQL UPDATE/DELETE statements to fix existing data that violates new constraints\n\n**Test Results**: Documentation of constraint testing with valid/invalid data samples and outcomes\n\n**Implementation Log**: Timestamped record of constraint application with success/failure status\n\n## Error Handling\n\n**Constraint Violation Errors**:\n- Review existing data that violates the constraint\n- Create data cleanup scripts to fix violations\n- Re-run constraint application after cleanup\n- Document exceptions that require manual review\n\n**Permission Errors**:\n- Verify database user has ALTER TABLE privileges\n- Request elevated permissions from database administrator\n- Use separate admin connection for schema changes\n- Document permission requirements for future deployments\n\n**Circular Dependency Errors**:\n- Map all foreign key relationships before implementation\n- Apply constraints in dependency order (referenced tables first)\n- Use ALTER TABLE ADD CONSTRAINT for deferred constraint creation\n- Consider disabling foreign key checks temporarily during bulk operations\n\n**Performance Degradation**:\n- Analyze constraint checking overhead with EXPLAIN ANALYZE\n- Add appropriate indexes to support constraint validation\n- Consider batch validation for large data updates\n- Monitor query performance after constraint implementation\n\n## Resources\n\n**Database-Specific Constraint Syntax**:\n- PostgreSQL: `{baseDir}/docs/postgresql-constraints.md`\n- MySQL: `{baseDir}/docs/mysql-constraints.md`\n- SQL Server: `{baseDir}/docs/sqlserver-constraints.md`\n\n**Validation Rule Templates**: `{baseDir}/templates/validation-rules/`\n- Email format validation\n- Phone number validation\n- Date range validation\n- Numeric range validation\n- Custom business rules\n\n**Testing Guidelines**: `{baseDir}/docs/validation-testing.md`\n**Constraint Performance Analysis**: `{baseDir}/docs/constraint-performance.md`\n**Data Cleanup Procedures**: `{baseDir}/docs/data-cleanup-procedures.md`\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "data-validation-engine",
        "category": "database",
        "path": "plugins/database/data-validation-engine",
        "version": "1.0.0",
        "description": "Database plugin for data-validation-engine"
      },
      "filePath": "plugins/database/data-validation-engine/skills/validating-database-integrity/SKILL.md"
    },
    {
      "slug": "validating-pci-dss-compliance",
      "name": "validating-pci-dss-compliance",
      "description": "Validate PCI-DSS compliance for payment card data security. Use when auditing payment systems. Trigger with 'validate PCI-DSS', 'check payment security', or 'audit card data'.",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(security:*)",
        "Bash(scan:*)",
        "Bash(audit:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Pci Dss Validator\n\nThis skill provides automated assistance for pci dss validator tasks.\n\n## Overview\n\nThis skill streamlines PCI DSS compliance checks by automatically analyzing code and configurations. It flags potential issues, allowing for proactive remediation and improved security posture. It is particularly useful for developers, security engineers, and compliance officers.\n\n## How It Works\n\n1. **Analyze the Target**: The skill identifies the codebase, configuration files, or infrastructure resources to be evaluated.\n2. **Run PCI DSS Validation**: The pci-dss-validator plugin scans the target for potential PCI DSS violations.\n3. **Generate Report**: The skill compiles a report detailing any identified vulnerabilities or non-compliant configurations, along with remediation recommendations.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Evaluate a new application or system for PCI DSS compliance before deployment.\n- Periodically assess existing systems to maintain PCI DSS compliance.\n- Investigate potential security vulnerabilities related to PCI DSS.\n\n## Examples\n\n### Example 1: Validating a Web Application\n\nUser request: \"Validate PCI compliance for my e-commerce web application.\"\n\nThe skill will:\n1. Identify the source code repository for the web application.\n2. Run the pci-dss-validator plugin against the codebase.\n3. Generate a report highlighting any PCI DSS violations found in the code.\n\n### Example 2: Checking Infrastructure Configuration\n\nUser request: \"Check PCI DSS compliance of my AWS infrastructure.\"\n\nThe skill will:\n1. Access the AWS configuration files (e.g., Terraform, CloudFormation).\n2. Execute the pci-dss-validator plugin against the infrastructure configuration.\n3. Produce a report outlining any non-compliant configurations in the AWS environment.\n\n## Best Practices\n\n- **Scope Definition**: Clearly define the scope of the PCI DSS assessment to ensure accurate and relevant results.\n- **Regular Assessments**: Conduct regular PCI DSS assessments to maintain continuous compliance.\n- **Remediation Tracking**: Track and document all remediation efforts to demonstrate ongoing commitment to security.\n\n## Integration\n\nThis skill can be integrated with other security tools and plugins to provide a comprehensive security assessment. For example, it can be used in conjunction with static analysis tools to identify vulnerabilities in code before it is deployed. It can also be integrated with infrastructure-as-code tools to ensure that infrastructure is compliant with PCI DSS from the start.\n\n## Prerequisites\n\n- Access to codebase and configuration files in {baseDir}/\n- Security scanning tools installed as needed\n- Understanding of security standards and best practices\n- Permissions for security analysis operations\n\n## Instructions\n\n1. Identify security scan scope and targets\n2. Configure scanning parameters and thresholds\n3. Execute security analysis systematically\n4. Analyze findings for vulnerabilities and compliance gaps\n5. Prioritize issues by severity and impact\n6. Generate detailed security report with remediation steps\n\n## Output\n\n- Security scan results with vulnerability details\n- Compliance status reports by standard\n- Prioritized list of security issues by severity\n- Remediation recommendations with code examples\n- Executive summary for stakeholders\n\n## Error Handling\n\nIf security scanning fails:\n- Verify tool installation and configuration\n- Check file and directory permissions\n- Validate scan target paths\n- Review tool-specific error messages\n- Ensure network access for dependency checks\n\n## Resources\n\n- Security standard documentation (OWASP, CWE, CVE)\n- Compliance framework guidelines (GDPR, HIPAA, PCI-DSS)\n- Security scanning tool documentation\n- Vulnerability remediation best practices",
      "parentPlugin": {
        "name": "pci-dss-validator",
        "category": "security",
        "path": "plugins/security/pci-dss-validator",
        "version": "1.0.0",
        "description": "Validate PCI DSS compliance"
      },
      "filePath": "plugins/security/pci-dss-validator/skills/validating-pci-dss-compliance/SKILL.md"
    },
    {
      "slug": "validating-performance-budgets",
      "name": "validating-performance-budgets",
      "description": "Validate application performance against defined budgets to identify regressions early. Use when checking page load times, bundle sizes, or API response times against thresholds. Trigger with phrases like \"validate performance budget\", \"check performance metrics\", or \"detect performance regression\".",
      "allowedTools": [
        "\"Read",
        "Write",
        "Edit",
        "Grep",
        "Glob",
        "Bash(lighthouse:*)",
        "Bash(webpack:*)",
        "Bash(performance:*)\""
      ],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Performance Budget Validator\n\nThis skill provides automated assistance for performance budget validator tasks.\n\n## Overview\n\nThis skill allows Claude to automatically validate your application's performance against predefined budgets. It helps identify performance regressions and ensures your application maintains optimal performance characteristics.\n\n## How It Works\n\n1. **Analyze Performance Metrics**: Claude analyzes current performance metrics, such as page load times, bundle sizes, and API response times.\n2. **Validate Against Budget**: The plugin validates these metrics against predefined performance budget thresholds.\n3. **Report Violations**: If any metrics exceed the defined budget, the skill reports violations and provides details on the exceeded thresholds.\n\n## When to Use This Skill\n\nThis skill activates when you need to:\n- Validate performance against predefined budgets.\n- Identify performance regressions in your application.\n- Integrate performance budget validation into your CI/CD pipeline.\n\n## Examples\n\n### Example 1: Preventing Performance Regressions\n\nUser request: \"Validate performance budget for the homepage.\"\n\nThe skill will:\n1. Analyze the homepage's performance metrics (load time, bundle size).\n2. Compare these metrics against the defined budget.\n3. Report any violations, such as exceeding the load time budget.\n\n### Example 2: Integrating with CI/CD\n\nUser request: \"Run performance budget validation as part of the build process.\"\n\nThe skill will:\n1. Execute the performance budget validation command.\n2. Check all defined performance metrics against their budgets.\n3. Report any violations that would cause the build to fail.\n\n## Best Practices\n\n- **Budget Definition**: Define realistic and achievable performance budgets based on current application performance and user expectations.\n- **Metric Selection**: Choose relevant performance metrics that directly impact user experience, such as page load times and API response times.\n- **CI/CD Integration**: Integrate performance budget validation into your CI/CD pipeline to automatically detect and prevent performance regressions.\n\n## Integration\n\nThis skill can be integrated with other plugins that provide performance metrics, such as website speed test tools or API monitoring services. It can also be used in conjunction with alerting plugins to notify developers of performance budget violations.\n\n## Prerequisites\n\n- Performance budget definitions in {baseDir}/performance-budgets.json\n- Access to performance testing tools (Lighthouse, WebPageTest)\n- Build output directory for bundle analysis\n- Historical performance metrics for comparison\n\n## Instructions\n\n1. Load performance budget configuration\n2. Collect current performance metrics (load time, bundle size, API latency)\n3. Compare metrics against defined budget thresholds\n4. Identify budget violations and severity\n5. Generate detailed violation report\n6. Provide remediation recommendations\n\n## Output\n\n- Performance budget validation report\n- List of metrics exceeding budget thresholds\n- Comparison with previous measurements\n- Detailed breakdown by metric category\n- Actionable recommendations for fixes\n\n## Error Handling\n\nIf budget validation fails:\n- Verify budget configuration file exists\n- Check performance testing tool availability\n- Validate metric collection permissions\n- Ensure network access to test endpoints\n- Review budget threshold definitions\n\n## Resources\n\n- Performance budget best practices\n- Lighthouse performance scoring guide\n- Bundle size optimization techniques\n- CI/CD integration patterns for performance testing",
      "parentPlugin": {
        "name": "performance-budget-validator",
        "category": "performance",
        "path": "plugins/performance/performance-budget-validator",
        "version": "1.0.0",
        "description": "Validate application against performance budgets"
      },
      "filePath": "plugins/performance/performance-budget-validator/skills/validating-performance-budgets/SKILL.md"
    },
    {
      "slug": "validator-expert",
      "name": "validator-expert",
      "description": "Validate production readiness of Vertex AI Agent Engine deployments across security, monitoring, performance, compliance, and best practices. Generates weighted scores (0-100%) with actionable recommendations. Use when asked to \"validate deploymen... Trigger with phrases like 'validate', 'check', or 'verify'. allowed-tools: Read, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Validator Expert\n\nThis skill provides automated assistance for validator expert tasks.\n\n## What This Skill Does\n\nProduction validator for Vertex AI deployments. Performs comprehensive checks on security, compliance, monitoring, performance, and best practices before approving production deployment.\n\n## When This Skill Activates\n\nTriggers: \"validate deployment\", \"production readiness\", \"security audit vertex ai\", \"check compliance\", \"validate adk agent\"\n\n## Validation Checklist\n\n### Security Validation\n- ‚úÖ IAM roles follow least privilege\n- ‚úÖ VPC Service Controls enabled\n- ‚úÖ Encryption at rest configured\n- ‚úÖ No hardcoded secrets\n- ‚úÖ Service accounts properly configured\n- ‚úÖ Model Armor enabled (for ADK)\n\n### Monitoring Validation\n- ‚úÖ Cloud Monitoring dashboards configured\n- ‚úÖ Alerting policies set\n- ‚úÖ Token usage tracking enabled\n- ‚úÖ Error rate monitoring active\n- ‚úÖ Latency SLOs defined\n\n### Performance Validation\n- ‚úÖ Auto-scaling configured\n- ‚úÖ Resource limits appropriate\n- ‚úÖ Caching strategy implemented\n- ‚úÖ Code Execution sandbox TTL set\n- ‚úÖ Memory Bank retention configured\n\n### Compliance Validation\n- ‚úÖ Audit logging enabled\n- ‚úÖ Data residency requirements met\n- ‚úÖ Privacy policies implemented\n- ‚úÖ Backup/disaster recovery configured\n\n## Tool Permissions\n\nRead, Grep, Glob, Bash - Read-only analysis for security\n\n## References\n\n- Vertex AI Security: https://cloud.google.com/vertex-ai/docs/security\n\n## Overview\n\n\nThis skill provides automated assistance for validator expert tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-vertex-validator",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-vertex-validator",
        "version": "1.0.0",
        "description": "Production readiness validator for Vertex AI deployments and configurations"
      },
      "filePath": "plugins/ai-ml/jeremy-vertex-validator/skills/validator-expert/SKILL.md"
    },
    {
      "slug": "vercel-advanced-troubleshooting",
      "name": "vercel-advanced-troubleshooting",
      "description": "Apply Vercel advanced debugging techniques for hard-to-diagnose issues. Use when standard troubleshooting fails, investigating complex race conditions, or preparing evidence bundles for Vercel support escalation. Trigger with phrases like \"vercel hard bug\", \"vercel mystery error\", \"vercel impossible to debug\", \"difficult vercel issue\", \"vercel deep debug\". allowed-tools: Read, Grep, Bash(kubectl:*), Bash(curl:*), Bash(tcpdump:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Advanced Troubleshooting\n\n## Overview\nDeep debugging techniques for complex Vercel issues that resist standard troubleshooting.\n\n## Prerequisites\n- Access to production logs and metrics\n- kubectl access to clusters\n- Network capture tools available\n- Understanding of distributed tracing\n\n## Evidence Collection Framework\n\n### Comprehensive Debug Bundle\n```bash\n#!/bin/bash\n# advanced-vercel-debug.sh\n\nBUNDLE=\"vercel-advanced-debug-$(date +%Y%m%d-%H%M%S)\"\nmkdir -p \"$BUNDLE\"/{logs,metrics,network,config,traces}\n\n# 1. Extended logs (1 hour window)\nkubectl logs -l app=vercel-integration --since=1h > \"$BUNDLE/logs/pods.log\"\njournalctl -u vercel-service --since \"1 hour ago\" > \"$BUNDLE/logs/system.log\"\n\n# 2. Metrics dump\ncurl -s localhost:9090/api/v1/query?query=vercel_requests_total > \"$BUNDLE/metrics/requests.json\"\ncurl -s localhost:9090/api/v1/query?query=vercel_errors_total > \"$BUNDLE/metrics/errors.json\"\n\n# 3. Network capture (30 seconds)\ntimeout 30 tcpdump -i any port 443 -w \"$BUNDLE/network/capture.pcap\" &\n\n# 4. Distributed traces\ncurl -s localhost:16686/api/traces?service=vercel > \"$BUNDLE/traces/jaeger.json\"\n\n# 5. Configuration state\nkubectl get cm vercel-config -o yaml > \"$BUNDLE/config/configmap.yaml\"\nkubectl get secret vercel-secrets -o yaml > \"$BUNDLE/config/secrets-redacted.yaml\"\n\ntar -czf \"$BUNDLE.tar.gz\" \"$BUNDLE\"\necho \"Advanced debug bundle: $BUNDLE.tar.gz\"\n```\n\n## Systematic Isolation\n\n### Layer-by-Layer Testing\n\n```typescript\n// Test each layer independently\nasync function diagnoseVercelIssue(): Promise<DiagnosisReport> {\n  const results: DiagnosisResult[] = [];\n\n  // Layer 1: Network connectivity\n  results.push(await testNetworkConnectivity());\n\n  // Layer 2: DNS resolution\n  results.push(await testDNSResolution('api.vercel.com'));\n\n  // Layer 3: TLS handshake\n  results.push(await testTLSHandshake('api.vercel.com'));\n\n  // Layer 4: Authentication\n  results.push(await testAuthentication());\n\n  // Layer 5: API response\n  results.push(await testAPIResponse());\n\n  // Layer 6: Response parsing\n  results.push(await testResponseParsing());\n\n  return { results, firstFailure: results.find(r => !r.success) };\n}\n```\n\n### Minimal Reproduction\n\n```typescript\n// Strip down to absolute minimum\nasync function minimalRepro(): Promise<void> {\n  // 1. Fresh client, no customization\n  const client = new VercelClient({\n    apiKey: process.env.VERCEL_API_KEY!,\n  });\n\n  // 2. Simplest possible call\n  try {\n    const result = await client.ping();\n    console.log('Ping successful:', result);\n  } catch (error) {\n    console.error('Ping failed:', {\n      message: error.message,\n      code: error.code,\n      stack: error.stack,\n    });\n  }\n}\n```\n\n## Timing Analysis\n\n```typescript\nclass TimingAnalyzer {\n  private timings: Map<string, number[]> = new Map();\n\n  async measure<T>(label: string, fn: () => Promise<T>): Promise<T> {\n    const start = performance.now();\n    try {\n      return await fn();\n    } finally {\n      const duration = performance.now() - start;\n      const existing = this.timings.get(label) || [];\n      existing.push(duration);\n      this.timings.set(label, existing);\n    }\n  }\n\n  report(): TimingReport {\n    const report: TimingReport = {};\n    for (const [label, times] of this.timings) {\n      report[label] = {\n        count: times.length,\n        min: Math.min(...times),\n        max: Math.max(...times),\n        avg: times.reduce((a, b) => a + b, 0) / times.length,\n        p95: this.percentile(times, 95),\n      };\n    }\n    return report;\n  }\n}\n```\n\n## Memory and Resource Analysis\n\n```typescript\n// Detect memory leaks in Vercel client usage\nconst heapUsed: number[] = [];\n\nsetInterval(() => {\n  const usage = process.memoryUsage();\n  heapUsed.push(usage.heapUsed);\n\n  // Alert on sustained growth\n  if (heapUsed.length > 60) { // 1 hour at 1/min\n    const trend = heapUsed[59] - heapUsed[0];\n    if (trend > 100 * 1024 * 1024) { // 100MB growth\n      console.warn('Potential memory leak in vercel integration');\n    }\n  }\n}, 60000);\n```\n\n## Race Condition Detection\n\n```typescript\n// Detect concurrent access issues\nclass VercelConcurrencyChecker {\n  private inProgress: Set<string> = new Set();\n\n  async execute<T>(key: string, fn: () => Promise<T>): Promise<T> {\n    if (this.inProgress.has(key)) {\n      console.warn(`Concurrent access detected for ${key}`);\n    }\n\n    this.inProgress.add(key);\n    try {\n      return await fn();\n    } finally {\n      this.inProgress.delete(key);\n    }\n  }\n}\n```\n\n## Support Escalation Template\n\n```markdown\n## Vercel Support Escalation\n\n**Severity:** P[1-4]\n**Request ID:** [from error response]\n**Timestamp:** [ISO 8601]\n\n### Issue Summary\n[One paragraph description]\n\n### Steps to Reproduce\n1. [Step 1]\n2. [Step 2]\n\n### Expected vs Actual\n- Expected: [behavior]\n- Actual: [behavior]\n\n### Evidence Attached\n- [ ] Debug bundle (vercel-advanced-debug-*.tar.gz)\n- [ ] Minimal reproduction code\n- [ ] Timing analysis\n- [ ] Network capture (if relevant)\n\n### Workarounds Attempted\n1. [Workaround 1] - Result: [outcome]\n2. [Workaround 2] - Result: [outcome]\n```\n\n## Instructions\n\n### Step 1: Collect Evidence Bundle\nRun the comprehensive debug script to gather all relevant data.\n\n### Step 2: Systematic Isolation\nTest each layer independently to identify the failure point.\n\n### Step 3: Create Minimal Reproduction\nStrip down to the simplest failing case.\n\n### Step 4: Escalate with Evidence\nUse the support template with all collected evidence.\n\n## Output\n- Comprehensive debug bundle collected\n- Failure layer identified\n- Minimal reproduction created\n- Support escalation submitted\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Can't reproduce | Race condition | Add timing analysis |\n| Intermittent failure | Timing-dependent | Increase sample size |\n| No useful logs | Missing instrumentation | Add debug logging |\n| Memory growth | Resource leak | Use heap profiling |\n\n## Examples\n\n### Quick Layer Test\n```bash\n# Test each layer in sequence\ncurl -v https://api.vercel.com/health 2>&1 | grep -E \"(Connected|TLS|HTTP)\"\n```\n\n## Resources\n- [Vercel Support Portal](https://support.vercel.com)\n- [Vercel Status Page](https://www.vercel-status.com)\n\n## Next Steps\nFor load testing, see `vercel-load-scale`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-advanced-troubleshooting/SKILL.md"
    },
    {
      "slug": "vercel-architecture-variants",
      "name": "vercel-architecture-variants",
      "description": "Choose and implement Vercel validated architecture blueprints for different scales. Use when designing new Vercel integrations, choosing between monolith/service/microservice architectures, or planning migration paths for Vercel applications. Trigger with phrases like \"vercel architecture\", \"vercel blueprint\", \"how to structure vercel\", \"vercel project layout\", \"vercel microservice\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Architecture Variants\n\n## Overview\nThree validated architecture blueprints for Vercel integrations.\n\n## Prerequisites\n- Understanding of team size and DAU requirements\n- Knowledge of deployment infrastructure\n- Clear SLA requirements\n- Growth projections available\n\n## Variant A: Monolith (Simple)\n\n**Best for:** MVPs, small teams, < 10K daily active users\n\n```\nmy-app/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ vercel/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts          # Singleton client\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.ts           # Types\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware.ts      # Express middleware\n‚îÇ   ‚îú‚îÄ‚îÄ routes/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ vercel.ts    # API routes\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îî‚îÄ‚îÄ vercel.test.ts\n‚îî‚îÄ‚îÄ package.json\n```\n\n### Key Characteristics\n- Single deployment unit\n- Synchronous Vercel calls in request path\n- In-memory caching\n- Simple error handling\n\n### Code Pattern\n```typescript\n// Direct integration in route handler\napp.post('/api/create', async (req, res) => {\n  try {\n    const result = await vercelClient.create(req.body);\n    res.json(result);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n```\n\n---\n\n## Variant B: Service Layer (Moderate)\n\n**Best for:** Growing startups, 10K-100K DAU, multiple integrations\n\n```\nmy-app/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ services/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vercel/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts      # Client wrapper\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service.ts     # Business logic\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repository.ts  # Data access\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types.ts\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts           # Service exports\n‚îÇ   ‚îú‚îÄ‚îÄ controllers/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vercel.ts\n‚îÇ   ‚îú‚îÄ‚îÄ routes/\n‚îÇ   ‚îú‚îÄ‚îÄ middleware/\n‚îÇ   ‚îú‚îÄ‚îÄ queue/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vercel-processor.ts  # Async processing\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îî‚îÄ‚îÄ vercel/\n‚îî‚îÄ‚îÄ package.json\n```\n\n### Key Characteristics\n- Separation of concerns\n- Background job processing\n- Redis caching\n- Circuit breaker pattern\n- Structured error handling\n\n### Code Pattern\n```typescript\n// Service layer abstraction\nclass VercelService {\n  constructor(\n    private client: VercelClient,\n    private cache: CacheService,\n    private queue: QueueService\n  ) {}\n\n  async createResource(data: CreateInput): Promise<Resource> {\n    // Business logic before API call\n    const validated = this.validate(data);\n\n    // Check cache\n    const cached = await this.cache.get(cacheKey);\n    if (cached) return cached;\n\n    // API call with retry\n    const result = await this.withRetry(() =>\n      this.client.create(validated)\n    );\n\n    // Cache result\n    await this.cache.set(cacheKey, result, 300);\n\n    // Async follow-up\n    await this.queue.enqueue('vercel.post-create', result);\n\n    return result;\n  }\n}\n```\n\n---\n\n## Variant C: Microservice (Complex)\n\n**Best for:** Enterprise, 100K+ DAU, strict SLAs\n\n```\nvercel-service/              # Dedicated microservice\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grpc/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vercel.proto\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rest/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ routes.ts\n‚îÇ   ‚îú‚îÄ‚îÄ domain/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ events/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/\n‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vercel/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mapper.ts\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ circuit-breaker.ts\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queue/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ config/\n‚îú‚îÄ‚îÄ k8s/\n‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ service.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ hpa.yaml\n‚îî‚îÄ‚îÄ package.json\n\nother-services/\n‚îú‚îÄ‚îÄ order-service/       # Calls vercel-service\n‚îú‚îÄ‚îÄ payment-service/\n‚îî‚îÄ‚îÄ notification-service/\n```\n\n### Key Characteristics\n- Dedicated Vercel microservice\n- gRPC for internal communication\n- Event-driven architecture\n- Database per service\n- Kubernetes autoscaling\n- Distributed tracing\n- Circuit breaker per service\n\n### Code Pattern\n```typescript\n// Event-driven with domain isolation\nclass VercelAggregate {\n  private events: DomainEvent[] = [];\n\n  process(command: VercelCommand): void {\n    // Domain logic\n    const result = this.execute(command);\n\n    // Emit domain event\n    this.events.push(new VercelProcessedEvent(result));\n  }\n\n  getUncommittedEvents(): DomainEvent[] {\n    return [...this.events];\n  }\n}\n\n// Event handler\n@EventHandler(VercelProcessedEvent)\nclass VercelEventHandler {\n  async handle(event: VercelProcessedEvent): Promise<void> {\n    // Saga orchestration\n    await this.sagaOrchestrator.continue(event);\n  }\n}\n```\n\n---\n\n## Decision Matrix\n\n| Factor | Monolith | Service Layer | Microservice |\n|--------|----------|---------------|--------------|\n| Team Size | 1-5 | 5-20 | 20+ |\n| DAU | < 10K | 10K-100K | 100K+ |\n| Deployment Frequency | Weekly | Daily | Continuous |\n| Failure Isolation | None | Partial | Full |\n| Operational Complexity | Low | Medium | High |\n| Time to Market | Fastest | Moderate | Slowest |\n\n## Migration Path\n\n```\nMonolith ‚Üí Service Layer:\n1. Extract Vercel code to service/\n2. Add caching layer\n3. Add background processing\n\nService Layer ‚Üí Microservice:\n1. Create dedicated vercel-service repo\n2. Define gRPC contract\n3. Add event bus\n4. Deploy to Kubernetes\n5. Migrate traffic gradually\n```\n\n## Instructions\n\n### Step 1: Assess Requirements\nUse the decision matrix to identify appropriate variant.\n\n### Step 2: Choose Architecture\nSelect Monolith, Service Layer, or Microservice based on needs.\n\n### Step 3: Implement Structure\nSet up project layout following the chosen blueprint.\n\n### Step 4: Plan Migration Path\nDocument upgrade path for future scaling.\n\n## Output\n- Architecture variant selected\n- Project structure implemented\n- Migration path documented\n- Appropriate patterns applied\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Over-engineering | Wrong variant choice | Start simpler |\n| Performance issues | Wrong layer | Add caching/async |\n| Team friction | Complex architecture | Simplify or train |\n| Deployment complexity | Microservice overhead | Consider service layer |\n\n## Examples\n\n### Quick Variant Check\n```bash\n# Count team size and DAU to select variant\necho \"Team: $(git log --format='%ae' | sort -u | wc -l) developers\"\necho \"DAU: Check analytics dashboard\"\n```\n\n## Resources\n- [Monolith First](https://martinfowler.com/bliki/MonolithFirst.html)\n- [Microservices Guide](https://martinfowler.com/microservices/)\n- [Vercel Architecture Guide](https://vercel.com/docs/architecture)\n\n## Next Steps\nFor common anti-patterns, see `vercel-known-pitfalls`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-architecture-variants/SKILL.md"
    },
    {
      "slug": "vercel-ci-integration",
      "name": "vercel-ci-integration",
      "description": "Configure Vercel CI/CD integration with GitHub Actions and testing. Use when setting up automated testing, configuring CI pipelines, or integrating Vercel tests into your build process. Trigger with phrases like \"vercel CI\", \"vercel GitHub Actions\", \"vercel automated tests\", \"CI vercel\". allowed-tools: Read, Write, Edit, Bash(gh:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel CI Integration\n\n## Overview\nSet up CI/CD pipelines for Vercel integrations with automated testing.\n\n## Prerequisites\n- GitHub repository with Actions enabled\n- Vercel test API key\n- npm/pnpm project configured\n\n## Instructions\n\n### Step 1: Create GitHub Actions Workflow\nCreate `.github/workflows/vercel-integration.yml`:\n\n```yaml\nname: Vercel Integration Tests\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\nenv:\n  VERCEL_API_KEY: ${{ secrets.VERCEL_API_KEY }}\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    env:\n      VERCEL_API_KEY: ${{ secrets.VERCEL_API_KEY }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n      - run: npm ci\n      - run: npm test -- --coverage\n      - run: npm run test:integration\n```\n\n### Step 2: Configure Secrets\n```bash\ngh secret set VERCEL_API_KEY --body \"sk_test_***\"\n```\n\n### Step 3: Add Integration Tests\n```typescript\ndescribe('Vercel Integration', () => {\n  it.skipIf(!process.env.VERCEL_API_KEY)('should connect', async () => {\n    const client = getVercelClient();\n    const result = await client.healthCheck();\n    expect(result.status).toBe('ok');\n  });\n});\n```\n\n## Output\n- Automated test pipeline\n- PR checks configured\n- Coverage reports uploaded\n- Release workflow ready\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Secret not found | Missing configuration | Add secret via `gh secret set` |\n| Tests timeout | Network issues | Increase timeout or mock |\n| Auth failures | Invalid key | Check secret value |\n\n## Examples\n\n### Release Workflow\n```yaml\non:\n  push:\n    tags: ['v*']\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    env:\n      VERCEL_API_KEY: ${{ secrets.VERCEL_API_KEY_PROD }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n      - run: npm ci\n      - name: Verify Vercel production readiness\n        run: npm run test:integration\n      - run: npm run build\n      - run: npm publish\n```\n\n### Branch Protection\n```yaml\nrequired_status_checks:\n  - \"test\"\n  - \"vercel-integration\"\n```\n\n## Resources\n- [GitHub Actions Documentation](https://docs.github.com/en/actions)\n- [Vercel CI Guide](https://vercel.com/docs/ci)\n\n## Next Steps\nFor deployment patterns, see `vercel-deploy-integration`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-ci-integration/SKILL.md"
    },
    {
      "slug": "vercel-common-errors",
      "name": "vercel-common-errors",
      "description": "Diagnose and fix Vercel common errors and exceptions. Use when encountering Vercel errors, debugging failed requests, or troubleshooting integration issues. Trigger with phrases like \"vercel error\", \"fix vercel\", \"vercel not working\", \"debug vercel\". allowed-tools: Read, Grep, Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Common Errors\n\n## Overview\nQuick reference for the top 10 most common Vercel errors and their solutions.\n\n## Prerequisites\n- Vercel SDK installed\n- API credentials configured\n- Access to error logs\n\n## Instructions\n\n### Step 1: Identify the Error\nCheck error message and code in your logs or console.\n\n### Step 2: Find Matching Error Below\nMatch your error to one of the documented cases.\n\n### Step 3: Apply Solution\nFollow the solution steps for your specific error.\n\n## Output\n- Identified error cause\n- Applied fix\n- Verified resolution\n\n## Error Handling\n\n### Build Failed\n**Error Message:**\n```\nCommand 'npm run build' exited with 1\n```\n\n**Cause:** Build script failed due to errors in code or dependencies\n\n**Solution:**\n```bash\nCheck build logs in Vercel dashboard. Run 'npm run build' locally to reproduce.\n```\n\n---\n\n### Function Timeout\n**Error Message:**\n```\nFUNCTION_INVOCATION_TIMEOUT\n```\n\n**Cause:** Serverless function exceeded execution time limit\n\n**Solution:**\nOptimize function code, use Edge Runtime, or upgrade plan for longer timeouts.\n\n---\n\n### Domain Verification Failed\n**Error Message:**\n```\nDomain verification failed\n```\n\n**Cause:** DNS records not configured correctly\n\n**Solution:**\n```typescript\nAdd required CNAME or A records. Wait for DNS propagation (up to 48h).\n```\n\n## Examples\n\n### Quick Diagnostic Commands\n```bash\n# Check Vercel status\ncurl -s https://www.vercel-status.com\n\n# Verify API connectivity\ncurl -I https://api.vercel.com\n\n# Check local configuration\nenv | grep VERCEL\n```\n\n### Escalation Path\n1. Collect evidence with `vercel-debug-bundle`\n2. Check Vercel status page\n3. Contact support with request ID\n\n## Resources\n- [Vercel Status Page](https://www.vercel-status.com)\n- [Vercel Support](https://vercel.com/docs/support)\n- [Vercel Error Codes](https://vercel.com/docs/errors)\n\n## Next Steps\nFor comprehensive debugging, see `vercel-debug-bundle`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-common-errors/SKILL.md"
    },
    {
      "slug": "vercel-cost-tuning",
      "name": "vercel-cost-tuning",
      "description": "Optimize Vercel costs through tier selection, sampling, and usage monitoring. Use when analyzing Vercel billing, reducing API costs, or implementing usage monitoring and budget alerts. Trigger with phrases like \"vercel cost\", \"vercel billing\", \"reduce vercel costs\", \"vercel pricing\", \"vercel expensive\", \"vercel budget\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Cost Tuning\n\n## Overview\nOptimize Vercel costs through smart tier selection, sampling, and usage monitoring.\n\n## Prerequisites\n- Access to Vercel billing dashboard\n- Understanding of current usage patterns\n- Database for usage tracking (optional)\n- Alerting system configured (optional)\n\n## Pricing Tiers\n\n| Tier | Monthly Cost | Included | Overage |\n|------|-------------|----------|---------|\n| Hobby | $0 | 100GB bandwidth, 100GB-hrs functions | N/A |\n| Pro | $20 | 1TB bandwidth, 1000GB-hrs functions | $0.001/request |\n| Enterprise | Custom | Unlimited | Volume discounts |\n\n## Cost Estimation\n\n```typescript\ninterface UsageEstimate {\n  requestsPerMonth: number;\n  tier: string;\n  estimatedCost: number;\n  recommendation?: string;\n}\n\nfunction estimateVercelCost(requestsPerMonth: number): UsageEstimate {\n  if (requestsPerMonth <= 1000) {\n    return { requestsPerMonth, tier: 'Free', estimatedCost: 0 };\n  }\n\n  if (requestsPerMonth <= 100000) {\n    return { requestsPerMonth, tier: 'Pro', estimatedCost: 20 };\n  }\n\n  const proOverage = (requestsPerMonth - 100000) * 0.001;\n  const proCost = 20 + proOverage;\n\n  return {\n    requestsPerMonth,\n    tier: 'Pro (with overage)',\n    estimatedCost: proCost,\n    recommendation: proCost > 500\n      ? 'Consider Enterprise tier for volume discounts'\n      : undefined,\n  };\n}\n```\n\n## Usage Monitoring\n\n```typescript\nclass VercelUsageMonitor {\n  private requestCount = 0;\n  private bytesTransferred = 0;\n  private alertThreshold: number;\n\n  constructor(monthlyBudget: number) {\n    this.alertThreshold = monthlyBudget * 0.8; // 80% warning\n  }\n\n  track(request: { bytes: number }) {\n    this.requestCount++;\n    this.bytesTransferred += request.bytes;\n\n    if (this.estimatedCost() > this.alertThreshold) {\n      this.sendAlert('Approaching Vercel budget limit');\n    }\n  }\n\n  estimatedCost(): number {\n    return estimateVercelCost(this.requestCount).estimatedCost;\n  }\n\n  private sendAlert(message: string) {\n    // Send to Slack, email, PagerDuty, etc.\n  }\n}\n```\n\n## Cost Reduction Strategies\n\n### Step 1: Request Sampling\n```typescript\nfunction shouldSample(samplingRate = 0.1): boolean {\n  return Math.random() < samplingRate;\n}\n\n// Use for non-critical telemetry\nif (shouldSample(0.1)) { // 10% sample\n  await vercelClient.trackEvent(event);\n}\n```\n\n### Step 2: Batching Requests\n```typescript\n// Instead of N individual calls\nawait Promise.all(ids.map(id => vercelClient.get(id)));\n\n// Use batch endpoint (1 call)\nawait vercelClient.batchGet(ids);\n```\n\n### Step 3: Caching (from P16)\n- Cache frequently accessed data\n- Use cache invalidation webhooks\n- Set appropriate TTLs\n\n### Step 4: Compression\n```typescript\nconst client = new VercelClient({\n  compression: true, // Enable gzip\n});\n```\n\n## Budget Alerts\n\n```bash\n# Set up billing alerts in Vercel dashboard\n# Or use API if available:\n# Check Vercel documentation for billing APIs\n```\n\n## Cost Dashboard Query\n\n```sql\n-- If tracking usage in your database\nSELECT\n  DATE_TRUNC('day', created_at) as date,\n  COUNT(*) as requests,\n  SUM(response_bytes) as bytes,\n  COUNT(*) * 0.001 as estimated_cost\nFROM vercel_api_logs\nWHERE created_at >= NOW() - INTERVAL '30 days'\nGROUP BY 1\nORDER BY 1;\n```\n\n## Instructions\n\n### Step 1: Analyze Current Usage\nReview Vercel dashboard for usage patterns and costs.\n\n### Step 2: Select Optimal Tier\nUse the cost estimation function to find the right tier.\n\n### Step 3: Implement Monitoring\nAdd usage tracking to catch budget overruns early.\n\n### Step 4: Apply Optimizations\nEnable batching, caching, and sampling where appropriate.\n\n## Output\n- Optimized tier selection\n- Usage monitoring implemented\n- Budget alerts configured\n- Cost reduction strategies applied\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Unexpected charges | Untracked usage | Implement monitoring |\n| Overage fees | Wrong tier | Upgrade tier |\n| Budget exceeded | No alerts | Set up alerts |\n| Inefficient usage | No batching | Enable batch requests |\n\n## Examples\n\n### Quick Cost Check\n```typescript\n// Estimate monthly cost for your usage\nconst estimate = estimateVercelCost(yourMonthlyRequests);\nconsole.log(`Tier: ${estimate.tier}, Cost: $${estimate.estimatedCost}`);\nif (estimate.recommendation) {\n  console.log(`üí° ${estimate.recommendation}`);\n}\n```\n\n## Resources\n- [Vercel Pricing](https://vercel.com/pricing)\n- [Vercel Billing Dashboard](https://dashboard.vercel.com/billing)\n\n## Next Steps\nFor architecture patterns, see `vercel-reference-architecture`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-cost-tuning/SKILL.md"
    },
    {
      "slug": "vercel-data-handling",
      "name": "vercel-data-handling",
      "description": "Implement Vercel PII handling, data retention, and GDPR/CCPA compliance patterns. Use when handling sensitive data, implementing data redaction, configuring retention policies, or ensuring compliance with privacy regulations for Vercel integrations. Trigger with phrases like \"vercel data\", \"vercel PII\", \"vercel GDPR\", \"vercel data retention\", \"vercel privacy\", \"vercel CCPA\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Data Handling\n\n## Overview\nHandle sensitive data correctly when integrating with Vercel.\n\n## Prerequisites\n- Understanding of GDPR/CCPA requirements\n- Vercel SDK with data export capabilities\n- Database for audit logging\n- Scheduled job infrastructure for cleanup\n\n## Data Classification\n\n| Category | Examples | Handling |\n|----------|----------|----------|\n| PII | Email, name, phone | Encrypt, minimize |\n| Sensitive | API keys, tokens | Never log, rotate |\n| Business | Usage metrics | Aggregate when possible |\n| Public | Product names | Standard handling |\n\n## PII Detection\n\n```typescript\nconst PII_PATTERNS = [\n  { type: 'email', regex: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}/g },\n  { type: 'phone', regex: /\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b/g },\n  { type: 'ssn', regex: /\\b\\d{3}-\\d{2}-\\d{4}\\b/g },\n  { type: 'credit_card', regex: /\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b/g },\n];\n\nfunction detectPII(text: string): { type: string; match: string }[] {\n  const findings: { type: string; match: string }[] = [];\n\n  for (const pattern of PII_PATTERNS) {\n    const matches = text.matchAll(pattern.regex);\n    for (const match of matches) {\n      findings.push({ type: pattern.type, match: match[0] });\n    }\n  }\n\n  return findings;\n}\n```\n\n## Data Redaction\n\n```typescript\nfunction redactPII(data: Record<string, any>): Record<string, any> {\n  const sensitiveFields = ['email', 'phone', 'ssn', 'password', 'apiKey'];\n  const redacted = { ...data };\n\n  for (const field of sensitiveFields) {\n    if (redacted[field]) {\n      redacted[field] = '[REDACTED]';\n    }\n  }\n\n  return redacted;\n}\n\n// Use in logging\nconsole.log('Vercel request:', redactPII(requestData));\n```\n\n## Data Retention Policy\n\n### Retention Periods\n| Data Type | Retention | Reason |\n|-----------|-----------|--------|\n| API logs | 30 days | Debugging |\n| Error logs | 90 days | Root cause analysis |\n| Audit logs | 7 years | Compliance |\n| PII | Until deletion request | GDPR/CCPA |\n\n### Automatic Cleanup\n\n```typescript\nasync function cleanupVercelData(retentionDays: number): Promise<void> {\n  const cutoff = new Date();\n  cutoff.setDate(cutoff.getDate() - retentionDays);\n\n  await db.vercelLogs.deleteMany({\n    createdAt: { $lt: cutoff },\n    type: { $nin: ['audit', 'compliance'] },\n  });\n}\n\n// Schedule daily cleanup\ncron.schedule('0 3 * * *', () => cleanupVercelData(30));\n```\n\n## GDPR/CCPA Compliance\n\n### Data Subject Access Request (DSAR)\n\n```typescript\nasync function exportUserData(userId: string): Promise<DataExport> {\n  const vercelData = await vercelClient.getUserData(userId);\n\n  return {\n    source: 'Vercel',\n    exportedAt: new Date().toISOString(),\n    data: {\n      profile: vercelData.profile,\n      activities: vercelData.activities,\n      // Include all user-related data\n    },\n  };\n}\n```\n\n### Right to Deletion\n\n```typescript\nasync function deleteUserData(userId: string): Promise<DeletionResult> {\n  // 1. Delete from Vercel\n  await vercelClient.deleteUser(userId);\n\n  // 2. Delete local copies\n  await db.vercelUserCache.deleteMany({ userId });\n\n  // 3. Audit log (required to keep)\n  await auditLog.record({\n    action: 'GDPR_DELETION',\n    userId,\n    service: 'vercel',\n    timestamp: new Date(),\n  });\n\n  return { success: true, deletedAt: new Date() };\n}\n```\n\n## Data Minimization\n\n```typescript\n// Only request needed fields\nconst user = await vercelClient.getUser(userId, {\n  fields: ['id', 'name'], // Not email, phone, address\n});\n\n// Don't store unnecessary data\nconst cacheData = {\n  id: user.id,\n  name: user.name,\n  // Omit sensitive fields\n};\n```\n\n## Instructions\n\n### Step 1: Classify Data\nCategorize all Vercel data by sensitivity level.\n\n### Step 2: Implement PII Detection\nAdd regex patterns to detect sensitive data in logs.\n\n### Step 3: Configure Redaction\nApply redaction to sensitive fields before logging.\n\n### Step 4: Set Up Retention\nConfigure automatic cleanup with appropriate retention periods.\n\n## Output\n- Data classification documented\n- PII detection implemented\n- Redaction in logging active\n- Retention policy enforced\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| PII in logs | Missing redaction | Wrap logging with redact |\n| Deletion failed | Data locked | Check dependencies |\n| Export incomplete | Timeout | Increase batch size |\n| Audit gap | Missing entries | Review log pipeline |\n\n## Examples\n\n### Quick PII Scan\n```typescript\nconst findings = detectPII(JSON.stringify(userData));\nif (findings.length > 0) {\n  console.warn(`PII detected: ${findings.map(f => f.type).join(', ')}`);\n}\n```\n\n### Redact Before Logging\n```typescript\nconst safeData = redactPII(apiResponse);\nlogger.info('Vercel response:', safeData);\n```\n\n### GDPR Data Export\n```typescript\nconst userExport = await exportUserData('user-123');\nawait sendToUser(userExport);\n```\n\n## Resources\n- [GDPR Developer Guide](https://gdpr.eu/developers/)\n- [CCPA Compliance Guide](https://oag.ca.gov/privacy/ccpa)\n- [Vercel Privacy Guide](https://vercel.com/docs/privacy)\n\n## Next Steps\nFor enterprise access control, see `vercel-enterprise-rbac`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-data-handling/SKILL.md"
    },
    {
      "slug": "vercel-debug-bundle",
      "name": "vercel-debug-bundle",
      "description": "Collect Vercel debug evidence for support tickets and troubleshooting. Use when encountering persistent issues, preparing support tickets, or collecting diagnostic information for Vercel problems. Trigger with phrases like \"vercel debug\", \"vercel support bundle\", \"collect vercel logs\", \"vercel diagnostic\". allowed-tools: Read, Bash(grep:*), Bash(curl:*), Bash(tar:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Debug Bundle\n\n## Overview\nCollect all necessary diagnostic information for Vercel support tickets.\n\n## Prerequisites\n- Vercel SDK installed\n- Access to application logs\n- Permission to collect environment info\n\n## Instructions\n\n### Step 1: Create Debug Bundle Script\n```bash\n#!/bin/bash\n# vercel-debug-bundle.sh\n\nBUNDLE_DIR=\"vercel-debug-$(date +%Y%m%d-%H%M%S)\"\nmkdir -p \"$BUNDLE_DIR\"\n\necho \"=== Vercel Debug Bundle ===\" > \"$BUNDLE_DIR/summary.txt\"\necho \"Generated: $(date)\" >> \"$BUNDLE_DIR/summary.txt\"\n```\n\n### Step 2: Collect Environment Info\n```bash\n# Environment info\necho \"--- Environment ---\" >> \"$BUNDLE_DIR/summary.txt\"\nnode --version >> \"$BUNDLE_DIR/summary.txt\" 2>&1\nnpm --version >> \"$BUNDLE_DIR/summary.txt\" 2>&1\necho \"VERCEL_API_KEY: ${VERCEL_API_KEY:+[SET]}\" >> \"$BUNDLE_DIR/summary.txt\"\n```\n\n### Step 3: Gather SDK and Logs\n```bash\n# SDK version\nnpm list vercel 2>/dev/null >> \"$BUNDLE_DIR/summary.txt\"\n\n# Recent logs (redacted)\ngrep -i \"vercel\" ~/.npm/_logs/*.log 2>/dev/null | tail -50 >> \"$BUNDLE_DIR/logs.txt\"\n\n# Configuration (redacted - secrets masked)\necho \"--- Config (redacted) ---\" >> \"$BUNDLE_DIR/summary.txt\"\ncat .env 2>/dev/null | sed 's/=.*/=***REDACTED***/' >> \"$BUNDLE_DIR/config-redacted.txt\"\n\n# Network connectivity test\necho \"--- Network Test ---\" >> \"$BUNDLE_DIR/summary.txt\"\necho -n \"API Health: \" >> \"$BUNDLE_DIR/summary.txt\"\ncurl -s -o /dev/null -w \"%{http_code}\" https://api.vercel.com/health >> \"$BUNDLE_DIR/summary.txt\"\necho \"\" >> \"$BUNDLE_DIR/summary.txt\"\n```\n\n### Step 4: Package Bundle\n```bash\ntar -czf \"$BUNDLE_DIR.tar.gz\" \"$BUNDLE_DIR\"\necho \"Bundle created: $BUNDLE_DIR.tar.gz\"\n```\n\n## Output\n- `vercel-debug-YYYYMMDD-HHMMSS.tar.gz` archive containing:\n  - `summary.txt` - Environment and SDK info\n  - `logs.txt` - Recent redacted logs\n  - `config-redacted.txt` - Configuration (secrets removed)\n\n## Error Handling\n| Item | Purpose | Included |\n|------|---------|----------|\n| Environment versions | Compatibility check | ‚úì |\n| SDK version | Version-specific bugs | ‚úì |\n| Error logs (redacted) | Root cause analysis | ‚úì |\n| Config (redacted) | Configuration issues | ‚úì |\n| Network test | Connectivity issues | ‚úì |\n\n## Examples\n\n### Sensitive Data Handling\n**ALWAYS REDACT:**\n- API keys and tokens\n- Passwords and secrets\n- PII (emails, names, IDs)\n\n**Safe to Include:**\n- Error messages\n- Stack traces (redacted)\n- SDK/runtime versions\n\n### Submit to Support\n1. Create bundle: `bash vercel-debug-bundle.sh`\n2. Review for sensitive data\n3. Upload to Vercel support portal\n\n## Resources\n- [Vercel Support](https://vercel.com/docs/support)\n- [Vercel Status](https://www.vercel-status.com)\n\n## Next Steps\nFor rate limit issues, see `vercel-rate-limits`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-debug-bundle/SKILL.md"
    },
    {
      "slug": "vercel-deploy-integration",
      "name": "vercel-deploy-integration",
      "description": "Deploy Vercel integrations to Vercel, Fly.io, and Cloud Run platforms. Use when deploying Vercel-powered applications to production, configuring platform-specific secrets, or setting up deployment pipelines. Trigger with phrases like \"deploy vercel\", \"vercel Vercel\", \"vercel production deploy\", \"vercel Cloud Run\", \"vercel Fly.io\". allowed-tools: Read, Write, Edit, Bash(vercel:*), Bash(fly:*), Bash(gcloud:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Deploy Integration\n\n## Overview\nDeploy Vercel-powered applications to popular platforms with proper secrets management.\n\n## Prerequisites\n- Vercel API keys for production environment\n- Platform CLI installed (vercel, fly, or gcloud)\n- Application code ready for deployment\n- Environment variables documented\n\n## Vercel Deployment\n\n### Environment Setup\n```bash\n# Add Vercel secrets to Vercel\nvercel secrets add vercel_api_key sk_live_***\nvercel secrets add vercel_webhook_secret whsec_***\n\n# Link to project\nvercel link\n\n# Deploy preview\nvercel\n\n# Deploy production\nvercel --prod\n```\n\n### vercel.json Configuration\n```json\n{\n  \"env\": {\n    \"VERCEL_API_KEY\": \"@vercel_api_key\"\n  },\n  \"functions\": {\n    \"api/**/*.ts\": {\n      \"maxDuration\": 30\n    }\n  }\n}\n```\n\n## Fly.io Deployment\n\n### fly.toml\n```toml\napp = \"my-vercel-app\"\nprimary_region = \"iad1\"\n\n[env]\n  NODE_ENV = \"production\"\n\n[http_service]\n  internal_port = 3000\n  force_https = true\n  auto_stop_machines = true\n  auto_start_machines = true\n```\n\n### Secrets\n```bash\n# Set Vercel secrets\nfly secrets set VERCEL_API_KEY=sk_live_***\nfly secrets set VERCEL_WEBHOOK_SECRET=whsec_***\n\n# Deploy\nfly deploy\n```\n\n## Google Cloud Run\n\n### Dockerfile\n```dockerfile\nFROM node:20-slim\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nCMD [\"npm\", \"start\"]\n```\n\n### Deploy Script\n```bash\n#!/bin/bash\n# deploy-cloud-run.sh\n\nPROJECT_ID=\"${GOOGLE_CLOUD_PROJECT}\"\nSERVICE_NAME=\"vercel-service\"\nREGION=\"None\"\n\n# Build and push image\ngcloud builds submit --tag gcr.io/$PROJECT_ID/$SERVICE_NAME\n\n# Deploy to Cloud Run\ngcloud run deploy $SERVICE_NAME \\\n  --image gcr.io/$PROJECT_ID/$SERVICE_NAME \\\n  --region $REGION \\\n  --platform managed \\\n  --allow-unauthenticated \\\n  --set-secrets=VERCEL_API_KEY=vercel-api-key:latest\n```\n\n## Environment Configuration Pattern\n\n```typescript\n// config/vercel.ts\ninterface VercelConfig {\n  apiKey: string;\n  environment: 'development' | 'staging' | 'production';\n  webhookSecret?: string;\n}\n\nexport function getVercelConfig(): VercelConfig {\n  const env = process.env.NODE_ENV || 'development';\n\n  return {\n    apiKey: process.env.VERCEL_API_KEY!,\n    environment: env as VercelConfig['environment'],\n    webhookSecret: process.env.VERCEL_WEBHOOK_SECRET,\n  };\n}\n```\n\n## Health Check Endpoint\n\n```typescript\n// api/health.ts\nexport async function GET() {\n  const vercelStatus = await checkVercelConnection();\n\n  return Response.json({\n    status: vercelStatus ? 'healthy' : 'degraded',\n    services: {\n      vercel: vercelStatus,\n    },\n    timestamp: new Date().toISOString(),\n  });\n}\n```\n\n## Instructions\n\n### Step 1: Choose Deployment Platform\nSelect the platform that best fits your infrastructure needs and follow the platform-specific guide below.\n\n### Step 2: Configure Secrets\nStore Vercel API keys securely using the platform's secrets management.\n\n### Step 3: Deploy Application\nUse the platform CLI to deploy your application with Vercel integration.\n\n### Step 4: Verify Health\nTest the health check endpoint to confirm Vercel connectivity.\n\n## Output\n- Application deployed to production\n- Vercel secrets securely configured\n- Health check endpoint functional\n- Environment-specific configuration in place\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Secret not found | Missing configuration | Add secret via platform CLI |\n| Deploy timeout | Large build | Increase build timeout |\n| Health check fails | Wrong API key | Verify environment variable |\n| Cold start issues | No warm-up | Configure minimum instances |\n\n## Examples\n\n### Quick Deploy Script\n```bash\n#!/bin/bash\n# Platform-agnostic deploy helper\ncase \"$1\" in\n  vercel)\n    vercel secrets add vercel_api_key \"$VERCEL_API_KEY\"\n    vercel --prod\n    ;;\n  fly)\n    fly secrets set VERCEL_API_KEY=\"$VERCEL_API_KEY\"\n    fly deploy\n    ;;\nesac\n```\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Fly.io Documentation](https://fly.io/docs)\n- [Cloud Run Documentation](https://cloud.google.com/run/docs)\n- [Vercel Deploy Guide](https://vercel.com/docs/deploy)\n\n## Next Steps\nFor webhook handling, see `vercel-webhooks-events`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-deploy-integration/SKILL.md"
    },
    {
      "slug": "vercel-deploy-preview",
      "name": "vercel-deploy-preview",
      "description": "Execute Vercel primary workflow: Deploy Preview. Use when Deploying a preview for a pull request, Testing changes before production, or Sharing preview URLs with stakeholders. Trigger with phrases like \"vercel deploy preview\", \"create preview deployment with vercel\". allowed-tools: Read, Write, Edit, Bash(npm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Deploy Preview\n\n## Overview\nDeploy preview environments for pull requests and branches.\nThis is the primary workflow for Vercel - instant previews for every commit.\n\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Understanding of Vercel core concepts\n- Valid API credentials configured\n\n## Instructions\n\n### Step 1: Initialize\n```typescript\n// Step 1 implementation\n```\n\n### Step 2: Execute\n```typescript\n// Step 2 implementation\n```\n\n### Step 3: Finalize\n```typescript\n// Step 3 implementation\n```\n\n## Output\n- Completed Deploy Preview execution\n- Expected results from Vercel API\n- Success confirmation or error details\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Error 1 | Cause | Solution |\n| Error 2 | Cause | Solution |\n\n## Examples\n\n### Complete Workflow\n```typescript\n// Complete workflow example\n```\n\n### Common Variations\n- Variation 1: Description\n- Variation 2: Description\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Vercel API Reference](https://vercel.com/docs/api)\n\n## Next Steps\nFor secondary workflow, see `vercel-edge-functions`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-deploy-preview/SKILL.md"
    },
    {
      "slug": "vercel-edge-functions",
      "name": "vercel-edge-functions",
      "description": "Execute Vercel secondary workflow: Edge Functions. Use when API routes with minimal latency, or complementing primary workflow. Trigger with phrases like \"vercel edge function\", \"deploy edge function with vercel\". allowed-tools: Read, Write, Edit, Bash(npm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Edge Functions\n\n## Overview\nBuild and deploy Edge Functions for ultra-low latency at the edge.\nServerless functions that run close to users worldwide.\n\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Familiarity with `vercel-deploy-preview`\n- Valid API credentials configured\n\n## Instructions\n\n### Step 1: Setup\n```typescript\n// Step 1 implementation\n```\n\n### Step 2: Process\n```typescript\n// Step 2 implementation\n```\n\n### Step 3: Complete\n```typescript\n// Step 3 implementation\n```\n\n## Output\n- Completed Edge Functions execution\n- Results from Vercel API\n- Success confirmation or error details\n\n## Error Handling\n| Aspect | Deploy Preview | Edge Functions |\n|--------|------------|------------|\n| Use Case | Deploying a preview for a pull request | API routes with minimal latency |\n| Complexity | Medium | Medium |\n| Performance | Standard | Ultra-fast (<50ms) |\n\n## Examples\n\n### Complete Workflow\n```typescript\n// Complete workflow example\n```\n\n### Error Recovery\n```typescript\n// Error handling code\n```\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Vercel API Reference](https://vercel.com/docs/api)\n\n## Next Steps\nFor common errors, see `vercel-common-errors`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-edge-functions/SKILL.md"
    },
    {
      "slug": "vercel-enterprise-rbac",
      "name": "vercel-enterprise-rbac",
      "description": "Configure Vercel enterprise SSO, role-based access control, and organization management. Use when implementing SSO integration, configuring role-based permissions, or setting up organization-level controls for Vercel. Trigger with phrases like \"vercel SSO\", \"vercel RBAC\", \"vercel enterprise\", \"vercel roles\", \"vercel permissions\", \"vercel SAML\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Enterprise RBAC\n\n## Overview\nConfigure enterprise-grade access control for Vercel integrations.\n\n## Prerequisites\n- Vercel Enterprise tier subscription\n- Identity Provider (IdP) with SAML/OIDC support\n- Understanding of role-based access patterns\n- Audit logging infrastructure\n\n## Role Definitions\n\n| Role | Permissions | Use Case |\n|------|-------------|----------|\n| Admin | Full access | Platform administrators |\n| Developer | Read/write, no delete | Active development |\n| Viewer | Read-only | Stakeholders, auditors |\n| Service | API access only | Automated systems |\n\n## Role Implementation\n\n```typescript\nenum VercelRole {\n  Admin = 'admin',\n  Developer = 'developer',\n  Viewer = 'viewer',\n  Service = 'service',\n}\n\ninterface VercelPermissions {\n  read: boolean;\n  write: boolean;\n  delete: boolean;\n  admin: boolean;\n}\n\nconst ROLE_PERMISSIONS: Record<VercelRole, VercelPermissions> = {\n  admin: { read: true, write: true, delete: true, admin: true },\n  developer: { read: true, write: true, delete: false, admin: false },\n  viewer: { read: true, write: false, delete: false, admin: false },\n  service: { read: true, write: true, delete: false, admin: false },\n};\n\nfunction checkPermission(\n  role: VercelRole,\n  action: keyof VercelPermissions\n): boolean {\n  return ROLE_PERMISSIONS[role][action];\n}\n```\n\n## SSO Integration\n\n### SAML Configuration\n\n```typescript\n// Vercel SAML setup\nconst samlConfig = {\n  entryPoint: 'https://idp.company.com/saml/sso',\n  issuer: 'https://vercel.com/saml/metadata',\n  cert: process.env.SAML_CERT,\n  callbackUrl: 'https://app.yourcompany.com/auth/vercel/callback',\n};\n\n// Map IdP groups to Vercel roles\nconst groupRoleMapping: Record<string, VercelRole> = {\n  'Engineering': VercelRole.Developer,\n  'Platform-Admins': VercelRole.Admin,\n  'Data-Team': VercelRole.Viewer,\n};\n```\n\n### OAuth2/OIDC Integration\n\n```typescript\nimport { OAuth2Client } from 'vercel';\n\nconst oauthClient = new OAuth2Client({\n  clientId: process.env.VERCEL_OAUTH_CLIENT_ID!,\n  clientSecret: process.env.VERCEL_OAUTH_CLIENT_SECRET!,\n  redirectUri: 'https://app.yourcompany.com/auth/vercel/callback',\n  scopes: read, write, deploy,\n});\n```\n\n## Organization Management\n\n```typescript\ninterface VercelOrganization {\n  id: string;\n  name: string;\n  ssoEnabled: boolean;\n  enforceSso: boolean;\n  allowedDomains: string[];\n  defaultRole: VercelRole;\n}\n\nasync function createOrganization(\n  config: VercelOrganization\n): Promise<void> {\n  await vercelClient.organizations.create({\n    ...config,\n    settings: {\n      sso: {\n        enabled: config.ssoEnabled,\n        enforced: config.enforceSso,\n        domains: config.allowedDomains,\n      },\n    },\n  });\n}\n```\n\n## Access Control Middleware\n\n```typescript\nfunction requireVercelPermission(\n  requiredPermission: keyof VercelPermissions\n) {\n  return async (req: Request, res: Response, next: NextFunction) => {\n    const user = req.user as { vercelRole: VercelRole };\n\n    if (!checkPermission(user.vercelRole, requiredPermission)) {\n      return res.status(403).json({\n        error: 'Forbidden',\n        message: `Missing permission: ${requiredPermission}`,\n      });\n    }\n\n    next();\n  };\n}\n\n// Usage\napp.delete('/vercel/resource/:id',\n  requireVercelPermission('delete'),\n  deleteResourceHandler\n);\n```\n\n## Audit Trail\n\n```typescript\ninterface VercelAuditEntry {\n  timestamp: Date;\n  userId: string;\n  role: VercelRole;\n  action: string;\n  resource: string;\n  success: boolean;\n  ipAddress: string;\n}\n\nasync function logVercelAccess(entry: VercelAuditEntry): Promise<void> {\n  await auditDb.insert(entry);\n\n  // Alert on suspicious activity\n  if (entry.action === 'delete' && !entry.success) {\n    await alertOnSuspiciousActivity(entry);\n  }\n}\n```\n\n## Instructions\n\n### Step 1: Define Roles\nMap organizational roles to Vercel permissions.\n\n### Step 2: Configure SSO\nSet up SAML or OIDC integration with your IdP.\n\n### Step 3: Implement Middleware\nAdd permission checks to API endpoints.\n\n### Step 4: Enable Audit Logging\nTrack all access for compliance.\n\n## Output\n- Role definitions implemented\n- SSO integration configured\n- Permission middleware active\n- Audit trail enabled\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| SSO login fails | Wrong callback URL | Verify IdP config |\n| Permission denied | Missing role mapping | Update group mappings |\n| Token expired | Short TTL | Refresh token logic |\n| Audit gaps | Async logging failed | Check log pipeline |\n\n## Examples\n\n### Quick Permission Check\n```typescript\nif (!checkPermission(user.role, 'write')) {\n  throw new ForbiddenError('Write permission required');\n}\n```\n\n## Resources\n- [Vercel Enterprise Guide](https://vercel.com/docs/enterprise)\n- [SAML 2.0 Specification](https://wiki.oasis-open.org/security/FrontPage)\n- [OpenID Connect Spec](https://openid.net/specs/openid-connect-core-1_0.html)\n\n## Next Steps\nFor major migrations, see `vercel-migration-deep-dive`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-enterprise-rbac/SKILL.md"
    },
    {
      "slug": "vercel-hello-world",
      "name": "vercel-hello-world",
      "description": "Create a minimal working Vercel example. Use when starting a new Vercel integration, testing your setup, or learning basic Vercel API patterns. Trigger with phrases like \"vercel hello world\", \"vercel example\", \"vercel quick start\", \"simple vercel code\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Hello World\n\n## Overview\nMinimal working example demonstrating core Vercel functionality.\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Valid API credentials configured\n- Development environment ready\n\n## Instructions\n\n### Step 1: Create Entry File\nCreate a new file for your hello world example.\n\n### Step 2: Import and Initialize Client\n```typescript\nimport { VercelClient } from 'vercel';\n\nconst client = new VercelClient({\n  apiKey: process.env.VERCEL_API_KEY,\n});\n```\n\n### Step 3: Make Your First API Call\n```typescript\nasync function main() {\n  const projects = await vercel.projects.list(); console.log('Projects:', projects.map(p => p.name));\n}\n\nmain().catch(console.error);\n```\n\n## Output\n- Working code file with Vercel client initialization\n- Successful API response confirming connection\n- Console output showing:\n```\nSuccess! Your Vercel connection is working.\n```\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Import Error | SDK not installed | Verify with `npm list` or `pip show` |\n| Auth Error | Invalid credentials | Check environment variable is set |\n| Timeout | Network issues | Increase timeout or check connectivity |\n| Rate Limit | Too many requests | Wait and retry with exponential backoff |\n\n## Examples\n\n### TypeScript Example\n```typescript\nimport { VercelClient } from 'vercel';\n\nconst client = new VercelClient({\n  apiKey: process.env.VERCEL_API_KEY,\n});\n\nasync function main() {\n  const projects = await vercel.projects.list(); console.log('Projects:', projects.map(p => p.name));\n}\n\nmain().catch(console.error);\n```\n\n### Python Example\n```python\nfrom None import VercelClient\n\nclient = VercelClient()\n\nNone\n```\n\n## Resources\n- [Vercel Getting Started](https://vercel.com/docs/getting-started)\n- [Vercel API Reference](https://vercel.com/docs/api)\n- [Vercel Examples](https://vercel.com/docs/examples)\n\n## Next Steps\nProceed to `vercel-local-dev-loop` for development workflow setup.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-hello-world/SKILL.md"
    },
    {
      "slug": "vercel-incident-runbook",
      "name": "vercel-incident-runbook",
      "description": "Execute Vercel incident response procedures with triage, mitigation, and postmortem. Use when responding to Vercel-related outages, investigating errors, or running post-incident reviews for Vercel integration failures. Trigger with phrases like \"vercel incident\", \"vercel outage\", \"vercel down\", \"vercel on-call\", \"vercel emergency\", \"vercel broken\". allowed-tools: Read, Grep, Bash(kubectl:*), Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Incident Runbook\n\n## Overview\nRapid incident response procedures for Vercel-related outages.\n\n## Prerequisites\n- Access to Vercel dashboard and status page\n- kubectl access to production cluster\n- Prometheus/Grafana access\n- Communication channels (Slack, PagerDuty)\n\n## Severity Levels\n\n| Level | Definition | Response Time | Examples |\n|-------|------------|---------------|----------|\n| P1 | Complete outage | < 15 min | Vercel API unreachable |\n| P2 | Degraded service | < 1 hour | High latency, partial failures |\n| P3 | Minor impact | < 4 hours | Webhook delays, non-critical errors |\n| P4 | No user impact | Next business day | Monitoring gaps |\n\n## Quick Triage\n\n```bash\n# 1. Check Vercel status\ncurl -s https://www.vercel-status.com | jq\n\n# 2. Check our integration health\ncurl -s https://api.yourapp.com/health | jq '.services.vercel'\n\n# 3. Check error rate (last 5 min)\ncurl -s localhost:9090/api/v1/query?query=rate(vercel_errors_total[5m])\n\n# 4. Recent error logs\nkubectl logs -l app=vercel-integration --since=5m | grep -i error | tail -20\n```\n\n## Decision Tree\n\n```\nVercel API returning errors?\n‚îú‚îÄ YES: Is status.vercel.com showing incident?\n‚îÇ   ‚îú‚îÄ YES ‚Üí Wait for Vercel to resolve. Enable fallback.\n‚îÇ   ‚îî‚îÄ NO ‚Üí Our integration issue. Check credentials, config.\n‚îî‚îÄ NO: Is our service healthy?\n    ‚îú‚îÄ YES ‚Üí Likely resolved or intermittent. Monitor.\n    ‚îî‚îÄ NO ‚Üí Our infrastructure issue. Check pods, memory, network.\n```\n\n## Immediate Actions by Error Type\n\n### 401/403 - Authentication\n```bash\n# Verify API key is set\nkubectl get secret vercel-secrets -o jsonpath='{.data.api-key}' | base64 -d\n\n# Check if key was rotated\n# ‚Üí Verify in Vercel dashboard\n\n# Remediation: Update secret and restart pods\nkubectl create secret generic vercel-secrets --from-literal=api-key=NEW_KEY --dry-run=client -o yaml | kubectl apply -f -\nkubectl rollout restart deployment/vercel-integration\n```\n\n### 429 - Rate Limited\n```bash\n# Check rate limit headers\ncurl -v https://api.vercel.com 2>&1 | grep -i rate\n\n# Enable request queuing\nkubectl set env deployment/vercel-integration RATE_LIMIT_MODE=queue\n\n# Long-term: Contact Vercel for limit increase\n```\n\n### 500/503 - Vercel Errors\n```bash\n# Enable graceful degradation\nkubectl set env deployment/vercel-integration VERCEL_FALLBACK=true\n\n# Notify users of degraded service\n# Update status page\n\n# Monitor Vercel status for resolution\n```\n\n## Communication Templates\n\n### Internal (Slack)\n```\nüî¥ P1 INCIDENT: Vercel Integration\nStatus: INVESTIGATING\nImpact: [Describe user impact]\nCurrent action: [What you're doing]\nNext update: [Time]\nIncident commander: @[name]\n```\n\n### External (Status Page)\n```\nVercel Integration Issue\n\nWe're experiencing issues with our Vercel integration.\nSome users may experience [specific impact].\n\nWe're actively investigating and will provide updates.\n\nLast updated: [timestamp]\n```\n\n## Post-Incident\n\n### Evidence Collection\n```bash\n# Generate debug bundle\n./scripts/vercel-debug-bundle.sh\n\n# Export relevant logs\nkubectl logs -l app=vercel-integration --since=1h > incident-logs.txt\n\n# Capture metrics\ncurl \"localhost:9090/api/v1/query_range?query=vercel_errors_total&start=2h\" > metrics.json\n```\n\n### Postmortem Template\n```markdown\n## Incident: Vercel [Error Type]\n**Date:** YYYY-MM-DD\n**Duration:** X hours Y minutes\n**Severity:** P[1-4]\n\n### Summary\n[1-2 sentence description]\n\n### Timeline\n- HH:MM - [Event]\n- HH:MM - [Event]\n\n### Root Cause\n[Technical explanation]\n\n### Impact\n- Users affected: N\n- Revenue impact: $X\n\n### Action Items\n- [ ] [Preventive measure] - Owner - Due date\n```\n\n## Instructions\n\n### Step 1: Quick Triage\nRun the triage commands to identify the issue source.\n\n### Step 2: Follow Decision Tree\nDetermine if the issue is Vercel-side or internal.\n\n### Step 3: Execute Immediate Actions\nApply the appropriate remediation for the error type.\n\n### Step 4: Communicate Status\nUpdate internal and external stakeholders.\n\n## Output\n- Issue identified and categorized\n- Remediation applied\n- Stakeholders notified\n- Evidence collected for postmortem\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Can't reach status page | Network issue | Use mobile or VPN |\n| kubectl fails | Auth expired | Re-authenticate |\n| Metrics unavailable | Prometheus down | Check backup metrics |\n| Secret rotation fails | Permission denied | Escalate to admin |\n\n## Examples\n\n### One-Line Health Check\n```bash\ncurl -sf https://api.yourapp.com/health | jq '.services.vercel.status' || echo \"UNHEALTHY\"\n```\n\n## Resources\n- [Vercel Status Page](https://www.vercel-status.com)\n- [Vercel Support](https://support.vercel.com)\n\n## Next Steps\nFor data handling, see `vercel-data-handling`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-incident-runbook/SKILL.md"
    },
    {
      "slug": "vercel-install-auth",
      "name": "vercel-install-auth",
      "description": "Install and configure Vercel SDK/CLI authentication. Use when setting up a new Vercel integration, configuring API keys, or initializing Vercel in your project. Trigger with phrases like \"install vercel\", \"setup vercel\", \"vercel auth\", \"configure vercel API key\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(pip:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Install & Auth\n\n## Overview\nSet up Vercel SDK/CLI and configure authentication credentials.\n\n## Prerequisites\n- Node.js 18+ or Python 3.10+\n- Package manager (npm, pnpm, or pip)\n- Vercel account with API access\n- API key from Vercel dashboard\n\n## Instructions\n\n### Step 1: Install SDK\n```bash\n# Node.js\nnpm install vercel\n\n# Python\npip install None\n```\n\n### Step 2: Configure Authentication\n```bash\n# Set environment variable\nexport VERCEL_API_KEY=\"your-api-key\"\n\n# Or create .env file\necho 'VERCEL_API_KEY=your-api-key' >> .env\n```\n\n### Step 3: Verify Connection\n```typescript\nconst teams = await vercel.teams.list(); console.log(teams.length > 0 ? 'OK' : 'No teams');\n```\n\n## Output\n- Installed SDK package in node_modules or site-packages\n- Environment variable or .env file with API key\n- Successful connection verification output\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Invalid API Key | Incorrect or expired key | Verify key in Vercel dashboard |\n| Rate Limited | Exceeded quota | Check quota at https://vercel.com/docs |\n| Network Error | Firewall blocking | Ensure outbound HTTPS allowed |\n| Module Not Found | Installation failed | Run `npm install` or `pip install` again |\n\n## Examples\n\n### TypeScript Setup\n```typescript\nimport { VercelClient } from 'vercel';\n\nconst client = new VercelClient({\n  apiKey: process.env.VERCEL_API_KEY,\n});\n```\n\n### Python Setup\n```python\nfrom None import VercelClient\n\nclient = VercelClient(\n    api_key=os.environ.get('VERCEL_API_KEY')\n)\n```\n\n## Resources\n- [Vercel Documentation](https://vercel.com/docs)\n- [Vercel Dashboard](https://api.vercel.com)\n- [Vercel Status](https://www.vercel-status.com)\n\n## Next Steps\nAfter successful auth, proceed to `vercel-hello-world` for your first API call.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-install-auth/SKILL.md"
    },
    {
      "slug": "vercel-known-pitfalls",
      "name": "vercel-known-pitfalls",
      "description": "Identify and avoid Vercel anti-patterns and common integration mistakes. Use when reviewing Vercel code for issues, onboarding new developers, or auditing existing Vercel integrations for best practices violations. Trigger with phrases like \"vercel mistakes\", \"vercel anti-patterns\", \"vercel pitfalls\", \"vercel what not to do\", \"vercel code review\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Known Pitfalls\n\n## Overview\nCommon mistakes and anti-patterns when integrating with Vercel.\n\n## Prerequisites\n- Access to Vercel codebase for review\n- Understanding of async/await patterns\n- Knowledge of security best practices\n- Familiarity with rate limiting concepts\n\n## Pitfall #1: Synchronous API Calls in Request Path\n\n### ‚ùå Anti-Pattern\n```typescript\n// User waits for Vercel API call\napp.post('/checkout', async (req, res) => {\n  const payment = await vercelClient.processPayment(req.body);  // 2-5s latency\n  const notification = await vercelClient.sendEmail(payment);   // Another 1-2s\n  res.json({ success: true });  // User waited 3-7s\n});\n```\n\n### ‚úÖ Better Approach\n```typescript\n// Return immediately, process async\napp.post('/checkout', async (req, res) => {\n  const jobId = await queue.enqueue('process-checkout', req.body);\n  res.json({ jobId, status: 'processing' });  // 50ms response\n});\n\n// Background job\nasync function processCheckout(data) {\n  const payment = await vercelClient.processPayment(data);\n  await vercelClient.sendEmail(payment);\n}\n```\n\n---\n\n## Pitfall #2: Not Handling Rate Limits\n\n### ‚ùå Anti-Pattern\n```typescript\n// Blast requests, crash on 429\nfor (const item of items) {\n  await vercelClient.process(item);  // Will hit rate limit\n}\n```\n\n### ‚úÖ Better Approach\n```typescript\nimport pLimit from 'p-limit';\n\nconst limit = pLimit(5);  // Max 5 concurrent\nconst rateLimiter = new RateLimiter({ tokensPerSecond: 10 });\n\nfor (const item of items) {\n  await rateLimiter.acquire();\n  await limit(() => vercelClient.process(item));\n}\n```\n\n---\n\n## Pitfall #3: Leaking API Keys\n\n### ‚ùå Anti-Pattern\n```typescript\n// In frontend code (visible to users!)\nconst client = new VercelClient({\n  apiKey: 'sk_live_ACTUAL_KEY_HERE',  // Anyone can see this\n});\n\n// In git history\ngit commit -m \"add API key\"  // Exposed forever\n```\n\n### ‚úÖ Better Approach\n```typescript\n// Backend only, environment variable\nconst client = new VercelClient({\n  apiKey: process.env.VERCEL_API_KEY,\n});\n\n// Use .gitignore\n.env\n.env.local\n.env.*.local\n```\n\n---\n\n## Pitfall #4: Ignoring Idempotency\n\n### ‚ùå Anti-Pattern\n```typescript\n// Network error on response = duplicate charge!\ntry {\n  await vercelClient.charge(order);\n} catch (error) {\n  if (error.code === 'NETWORK_ERROR') {\n    await vercelClient.charge(order);  // Charged twice!\n  }\n}\n```\n\n### ‚úÖ Better Approach\n```typescript\nconst idempotencyKey = `order-${order.id}-${Date.now()}`;\n\nawait vercelClient.charge(order, {\n  idempotencyKey,  // Safe to retry\n});\n```\n\n---\n\n## Pitfall #5: Not Validating Webhooks\n\n### ‚ùå Anti-Pattern\n```typescript\n// Trust any incoming request\napp.post('/webhook', (req, res) => {\n  processWebhook(req.body);  // Attacker can send fake events\n  res.sendStatus(200);\n});\n```\n\n### ‚úÖ Better Approach\n```typescript\napp.post('/webhook',\n  express.raw({ type: 'application/json' }),\n  (req, res) => {\n    const signature = req.headers['x-vercel-signature'];\n    if (!verifyVercelSignature(req.body, signature)) {\n      return res.sendStatus(401);\n    }\n    processWebhook(JSON.parse(req.body));\n    res.sendStatus(200);\n  }\n);\n```\n\n---\n\n## Pitfall #6: Missing Error Handling\n\n### ‚ùå Anti-Pattern\n```typescript\n// Crashes on any error\nconst result = await vercelClient.get(id);\nconsole.log(result.data.nested.value);  // TypeError if missing\n```\n\n### ‚úÖ Better Approach\n```typescript\ntry {\n  const result = await vercelClient.get(id);\n  console.log(result?.data?.nested?.value ?? 'default');\n} catch (error) {\n  if (error instanceof VercelNotFoundError) {\n    return null;\n  }\n  if (error instanceof VercelRateLimitError) {\n    await sleep(error.retryAfter);\n    return this.get(id);  // Retry\n  }\n  throw error;  // Rethrow unknown errors\n}\n```\n\n---\n\n## Pitfall #7: Hardcoding Configuration\n\n### ‚ùå Anti-Pattern\n```typescript\nconst client = new VercelClient({\n  timeout: 5000,  // Too short for some operations\n  baseUrl: 'https://api.vercel.com',  // Can't change for staging\n});\n```\n\n### ‚úÖ Better Approach\n```typescript\nconst client = new VercelClient({\n  timeout: parseInt(process.env.VERCEL_TIMEOUT || '30000'),\n  baseUrl: process.env.VERCEL_BASE_URL || 'https://api.vercel.com',\n});\n```\n\n---\n\n## Pitfall #8: Not Implementing Circuit Breaker\n\n### ‚ùå Anti-Pattern\n```typescript\n// When Vercel is down, every request hangs\nfor (const user of users) {\n  await vercelClient.sync(user);  // All timeout sequentially\n}\n```\n\n### ‚úÖ Better Approach\n```typescript\nimport CircuitBreaker from 'opossum';\n\nconst breaker = new CircuitBreaker(vercelClient.sync, {\n  timeout: 10000,\n  errorThresholdPercentage: 50,\n  resetTimeout: 30000,\n});\n\n// Fails fast when circuit is open\nfor (const user of users) {\n  await breaker.fire(user).catch(handleFailure);\n}\n```\n\n---\n\n## Pitfall #9: Logging Sensitive Data\n\n### ‚ùå Anti-Pattern\n```typescript\nconsole.log('Request:', JSON.stringify(request));  // Logs API key, PII\nconsole.log('User:', user);  // Logs email, phone\n```\n\n### ‚úÖ Better Approach\n```typescript\nconst redacted = {\n  ...request,\n  apiKey: '[REDACTED]',\n  user: { id: user.id },  // Only non-sensitive fields\n};\nconsole.log('Request:', JSON.stringify(redacted));\n```\n\n---\n\n## Pitfall #10: No Graceful Degradation\n\n### ‚ùå Anti-Pattern\n```typescript\n// Entire feature broken if Vercel is down\nconst recommendations = await vercelClient.getRecommendations(userId);\nreturn renderPage({ recommendations });  // Page crashes\n```\n\n### ‚úÖ Better Approach\n```typescript\nlet recommendations;\ntry {\n  recommendations = await vercelClient.getRecommendations(userId);\n} catch (error) {\n  recommendations = await getFallbackRecommendations(userId);\n  reportDegradedService('vercel', error);\n}\nreturn renderPage({ recommendations, degraded: !recommendations });\n```\n\n---\n\n## Instructions\n\n### Step 1: Review for Anti-Patterns\nScan codebase for each pitfall pattern.\n\n### Step 2: Prioritize Fixes\nAddress security issues first, then performance.\n\n### Step 3: Implement Better Approach\nReplace anti-patterns with recommended patterns.\n\n### Step 4: Add Prevention\nSet up linting and CI checks to prevent recurrence.\n\n## Output\n- Anti-patterns identified\n- Fixes prioritized and implemented\n- Prevention measures in place\n- Code quality improved\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Too many findings | Legacy codebase | Prioritize security first |\n| Pattern not detected | Complex code | Manual review |\n| False positive | Similar code | Whitelist exceptions |\n| Fix breaks tests | Behavior change | Update tests |\n\n## Examples\n\n### Quick Pitfall Scan\n```bash\n# Check for common pitfalls\ngrep -r \"sk_live_\" --include=\"*.ts\" src/        # Key leakage\ngrep -r \"console.log\" --include=\"*.ts\" src/     # Potential PII logging\n```\n\n## Resources\n- [Vercel Security Guide](https://vercel.com/docs/security)\n- [Vercel Best Practices](https://vercel.com/docs/best-practices)\n\n## Quick Reference Card\n\n| Pitfall | Detection | Prevention |\n|---------|-----------|------------|\n| Sync in request | High latency | Use queues |\n| Rate limit ignore | 429 errors | Implement backoff |\n| Key leakage | Git history scan | Env vars, .gitignore |\n| No idempotency | Duplicate records | Idempotency keys |\n| Unverified webhooks | Security audit | Signature verification |\n| Missing error handling | Crashes | Try-catch, types |\n| Hardcoded config | Code review | Environment variables |\n| No circuit breaker | Cascading failures | opossum, resilience4j |\n| Logging PII | Log audit | Redaction middleware |\n| No degradation | Total outages | Fallback systems |",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-known-pitfalls/SKILL.md"
    },
    {
      "slug": "vercel-load-scale",
      "name": "vercel-load-scale",
      "description": "Implement Vercel load testing, auto-scaling, and capacity planning strategies. Use when running performance tests, configuring horizontal scaling, or planning capacity for Vercel integrations. Trigger with phrases like \"vercel load test\", \"vercel scale\", \"vercel performance test\", \"vercel capacity\", \"vercel k6\", \"vercel benchmark\". allowed-tools: Read, Write, Edit, Bash(k6:*), Bash(kubectl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Load & Scale\n\n## Overview\nLoad testing, scaling strategies, and capacity planning for Vercel integrations.\n\n## Prerequisites\n- k6 load testing tool installed\n- Kubernetes cluster with HPA configured\n- Prometheus for metrics collection\n- Test environment API keys\n\n## Load Testing with k6\n\n### Basic Load Test\n```javascript\n// vercel-load-test.js\nimport http from 'k6/http';\nimport { check, sleep } from 'k6';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 10 },   // Ramp up\n    { duration: '5m', target: 10 },   // Steady state\n    { duration: '2m', target: 50 },   // Ramp to peak\n    { duration: '5m', target: 50 },   // Stress test\n    { duration: '2m', target: 0 },    // Ramp down\n  ],\n  thresholds: {\n    http_req_duration: ['p(95)<100'],\n    http_req_failed: ['rate<0.01'],\n  },\n};\n\nexport default function () {\n  const response = http.post(\n    'https://api.vercel.com/v1/resource',\n    JSON.stringify({ test: true }),\n    {\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${__ENV.VERCEL_API_KEY}`,\n      },\n    }\n  );\n\n  check(response, {\n    'status is 200': (r) => r.status === 200,\n    'latency < 100ms': (r) => r.timings.duration < 100,\n  });\n\n  sleep(1);\n}\n```\n\n### Run Load Test\n```bash\n# Install k6\nbrew install k6  # macOS\n# or: sudo apt install k6  # Linux\n\n# Run test\nk6 run --env VERCEL_API_KEY=${VERCEL_API_KEY} vercel-load-test.js\n\n# Run with output to InfluxDB\nk6 run --out influxdb=http://localhost:8086/k6 vercel-load-test.js\n```\n\n## Scaling Patterns\n\n### Horizontal Scaling\n```yaml\n# kubernetes HPA\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: vercel-integration-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: vercel-integration\n  minReplicas: 2\n  maxReplicas: 20\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n    - type: Pods\n      pods:\n        metric:\n          name: vercel_queue_depth\n        target:\n          type: AverageValue\n          averageValue: 100\n```\n\n### Connection Pooling\n```typescript\nimport { Pool } from 'generic-pool';\n\nconst vercelPool = Pool.create({\n  create: async () => {\n    return new VercelClient({\n      apiKey: process.env.VERCEL_API_KEY!,\n    });\n  },\n  destroy: async (client) => {\n    await client.close();\n  },\n  max: None,\n  min: None,\n  idleTimeoutMillis: 30000,\n});\n\nasync function withVercelClient<T>(\n  fn: (client: VercelClient) => Promise<T>\n): Promise<T> {\n  const client = await vercelPool.acquire();\n  try {\n    return await fn(client);\n  } finally {\n    vercelPool.release(client);\n  }\n}\n```\n\n## Capacity Planning\n\n### Metrics to Monitor\n| Metric | Warning | Critical |\n|--------|---------|----------|\n| CPU Utilization | > 70% | > 85% |\n| Memory Usage | > 75% | > 90% |\n| Request Queue Depth | > 100 | > 500 |\n| Error Rate | > 1% | > 5% |\n| P95 Latency | > 500ms | > 2000ms |\n\n### Capacity Calculation\n```typescript\ninterface CapacityEstimate {\n  currentRPS: number;\n  maxRPS: number;\n  headroom: number;\n  scaleRecommendation: string;\n}\n\nfunction estimateVercelCapacity(\n  metrics: SystemMetrics\n): CapacityEstimate {\n  const currentRPS = metrics.requestsPerSecond;\n  const avgLatency = metrics.p50Latency;\n  const cpuUtilization = metrics.cpuPercent;\n\n  // Estimate max RPS based on current performance\n  const maxRPS = currentRPS / (cpuUtilization / 100) * 0.7; // 70% target\n  const headroom = ((maxRPS - currentRPS) / currentRPS) * 100;\n\n  return {\n    currentRPS,\n    maxRPS: Math.floor(maxRPS),\n    headroom: Math.round(headroom),\n    scaleRecommendation: headroom < 30\n      ? 'Scale up soon'\n      : headroom < 50\n      ? 'Monitor closely'\n      : 'Adequate capacity',\n  };\n}\n```\n\n## Benchmark Results Template\n\n```markdown\n## Vercel Performance Benchmark\n**Date:** YYYY-MM-DD\n**Environment:** [staging/production]\n**SDK Version:** X.Y.Z\n\n### Test Configuration\n- Duration: 10 minutes\n- Ramp: 10 ‚Üí 100 ‚Üí 10 VUs\n- Target endpoint: /v1/resource\n\n### Results\n| Metric | Value |\n|--------|-------|\n| Total Requests | 50,000 |\n| Success Rate | 99.9% |\n| P50 Latency | 120ms |\n| P95 Latency | 350ms |\n| P99 Latency | 800ms |\n| Max RPS Achieved | 150 |\n\n### Observations\n- [Key finding 1]\n- [Key finding 2]\n\n### Recommendations\n- [Scaling recommendation]\n```\n\n## Instructions\n\n### Step 1: Create Load Test Script\nWrite k6 test script with appropriate thresholds.\n\n### Step 2: Configure Auto-Scaling\nSet up HPA with CPU and custom metrics.\n\n### Step 3: Run Load Test\nExecute test and collect metrics.\n\n### Step 4: Analyze and Document\nRecord results in benchmark template.\n\n## Output\n- Load test script created\n- HPA configured\n- Benchmark results documented\n- Capacity recommendations defined\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| k6 timeout | Rate limited | Reduce RPS |\n| HPA not scaling | Wrong metrics | Verify metric name |\n| Connection refused | Pool exhausted | Increase pool size |\n| Inconsistent results | Warm-up needed | Add ramp-up phase |\n\n## Examples\n\n### Quick k6 Test\n```bash\nk6 run --vus 10 --duration 30s vercel-load-test.js\n```\n\n### Check Current Capacity\n```typescript\nconst metrics = await getSystemMetrics();\nconst capacity = estimateVercelCapacity(metrics);\nconsole.log('Headroom:', capacity.headroom + '%');\nconsole.log('Recommendation:', capacity.scaleRecommendation);\n```\n\n### Scale HPA Manually\n```bash\nkubectl scale deployment vercel-integration --replicas=5\nkubectl get hpa vercel-integration-hpa\n```\n\n## Resources\n- [k6 Documentation](https://k6.io/docs/)\n- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n- [Vercel Rate Limits](https://vercel.com/docs/rate-limits)\n\n## Next Steps\nFor reliability patterns, see `vercel-reliability-patterns`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-load-scale/SKILL.md"
    },
    {
      "slug": "vercel-local-dev-loop",
      "name": "vercel-local-dev-loop",
      "description": "Configure Vercel local development with hot reload and testing. Use when setting up a development environment, configuring test workflows, or establishing a fast iteration cycle with Vercel. Trigger with phrases like \"vercel dev setup\", \"vercel local development\", \"vercel dev environment\", \"develop with vercel\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(pnpm:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Local Dev Loop\n\n## Overview\nSet up a fast, reproducible local development workflow for Vercel.\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Node.js 18+ with npm/pnpm\n- Code editor with TypeScript support\n- Git for version control\n\n## Instructions\n\n### Step 1: Create Project Structure\n```\nmy-vercel-project/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ vercel/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts       # Vercel client wrapper\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.ts       # Configuration management\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.ts        # Helper functions\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îî‚îÄ‚îÄ vercel.test.ts\n‚îú‚îÄ‚îÄ .env.local              # Local secrets (git-ignored)\n‚îú‚îÄ‚îÄ .env.example            # Template for team\n‚îî‚îÄ‚îÄ package.json\n```\n\n### Step 2: Configure Environment\n```bash\n# Copy environment template\ncp .env.example .env.local\n\n# Install dependencies\nnpm install\n\n# Start development server\nnpm run dev\n```\n\n### Step 3: Setup Hot Reload\n```json\n{\n  \"scripts\": {\n    \"dev\": \"tsx watch src/index.ts\",\n    \"test\": \"vitest\",\n    \"test:watch\": \"vitest --watch\"\n  }\n}\n```\n\n### Step 4: Configure Testing\n```typescript\nimport { describe, it, expect, vi } from 'vitest';\nimport { VercelClient } from '../src/vercel/client';\n\ndescribe('Vercel Client', () => {\n  it('should initialize with API key', () => {\n    const client = new VercelClient({ apiKey: 'test-key' });\n    expect(client).toBeDefined();\n  });\n});\n```\n\n## Output\n- Working development environment with hot reload\n- Configured test suite with mocking\n- Environment variable management\n- Fast iteration cycle for Vercel development\n\n## Error Handling\n| Error | Cause | Solution |\n|-------|-------|----------|\n| Module not found | Missing dependency | Run `npm install` |\n| Port in use | Another process | Kill process or change port |\n| Env not loaded | Missing .env.local | Copy from .env.example |\n| Test timeout | Slow network | Increase test timeout |\n\n## Examples\n\n### Mock Vercel Responses\n```typescript\nvi.mock('vercel', () => ({\n  VercelClient: vi.fn().mockImplementation(() => ({\n    // Mock methods here\n  })),\n}));\n```\n\n### Debug Mode\n```bash\n# Enable verbose logging\nDEBUG=VERCEL=* npm run dev\n```\n\n## Resources\n- [Vercel SDK Reference](https://vercel.com/docs/sdk)\n- [Vitest Documentation](https://vitest.dev/)\n- [tsx Documentation](https://github.com/esbuild-kit/tsx)\n\n## Next Steps\nSee `vercel-sdk-patterns` for production-ready code patterns.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-local-dev-loop/SKILL.md"
    },
    {
      "slug": "vercel-migration-deep-dive",
      "name": "vercel-migration-deep-dive",
      "description": "Execute Vercel major re-architecture and migration strategies with strangler fig pattern. Use when migrating to or from Vercel, performing major version upgrades, or re-platforming existing integrations to Vercel. Trigger with phrases like \"migrate vercel\", \"vercel migration\", \"switch to vercel\", \"vercel replatform\", \"vercel upgrade major\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(node:*), Bash(kubectl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Migration Deep Dive\n\n## Overview\nComprehensive guide for migrating to or from Vercel, or major version upgrades.\n\n## Prerequisites\n- Current system documentation\n- Vercel SDK installed\n- Feature flag infrastructure\n- Rollback strategy tested\n\n## Migration Types\n\n| Type | Complexity | Duration | Risk |\n|------|-----------|----------|------|\n| Fresh install | Low | Days | Low |\n| From competitor | Medium | Weeks | Medium |\n| Major version | Medium | Weeks | Medium |\n| Full replatform | High | Months | High |\n\n## Pre-Migration Assessment\n\n### Step 1: Current State Analysis\n```bash\n# Document current implementation\nfind . -name \"*.ts\" -o -name \"*.py\" | xargs grep -l \"vercel\" > vercel-files.txt\n\n# Count integration points\nwc -l vercel-files.txt\n\n# Identify dependencies\nnpm list | grep vercel\npip freeze | grep vercel\n```\n\n### Step 2: Data Inventory\n```typescript\ninterface MigrationInventory {\n  dataTypes: string[];\n  recordCounts: Record<string, number>;\n  dependencies: string[];\n  integrationPoints: string[];\n  customizations: string[];\n}\n\nasync function assessVercelMigration(): Promise<MigrationInventory> {\n  return {\n    dataTypes: await getDataTypes(),\n    recordCounts: await getRecordCounts(),\n    dependencies: await analyzeDependencies(),\n    integrationPoints: await findIntegrationPoints(),\n    customizations: await documentCustomizations(),\n  };\n}\n```\n\n## Migration Strategy: Strangler Fig Pattern\n\n```\nPhase 1: Parallel Run\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Old       ‚îÇ     ‚îÇ   New       ‚îÇ\n‚îÇ   System    ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ  Vercel   ‚îÇ\n‚îÇ   (100%)    ‚îÇ     ‚îÇ   (0%)      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nPhase 2: Gradual Shift\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Old       ‚îÇ     ‚îÇ   New       ‚îÇ\n‚îÇ   (50%)     ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ   (50%)     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nPhase 3: Complete\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Old       ‚îÇ     ‚îÇ   New       ‚îÇ\n‚îÇ   (0%)      ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ   (100%)    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Implementation Plan\n\n### Phase 1: Setup (Week 1-2)\n```bash\n# Install Vercel SDK\nnpm install vercel\n\n# Configure credentials\ncp .env.example .env.vercel\n# Edit with new credentials\n\n# Verify connectivity\nnode -e \"require('vercel').ping()\"\n```\n\n### Phase 2: Adapter Layer (Week 3-4)\n```typescript\n// src/adapters/vercel.ts\ninterface ServiceAdapter {\n  create(data: CreateInput): Promise<Resource>;\n  read(id: string): Promise<Resource>;\n  update(id: string, data: UpdateInput): Promise<Resource>;\n  delete(id: string): Promise<void>;\n}\n\nclass VercelAdapter implements ServiceAdapter {\n  async create(data: CreateInput): Promise<Resource> {\n    const vercelData = this.transform(data);\n    return vercelClient.create(vercelData);\n  }\n\n  private transform(data: CreateInput): VercelInput {\n    // Map from old format to Vercel format\n  }\n}\n```\n\n### Phase 3: Data Migration (Week 5-6)\n```typescript\nasync function migrateVercelData(): Promise<MigrationResult> {\n  const batchSize = 100;\n  let processed = 0;\n  let errors: MigrationError[] = [];\n\n  for await (const batch of oldSystem.iterateBatches(batchSize)) {\n    try {\n      const transformed = batch.map(transform);\n      await vercelClient.batchCreate(transformed);\n      processed += batch.length;\n    } catch (error) {\n      errors.push({ batch, error });\n    }\n\n    // Progress update\n    console.log(`Migrated ${processed} records`);\n  }\n\n  return { processed, errors };\n}\n```\n\n### Phase 4: Traffic Shift (Week 7-8)\n```typescript\n// Feature flag controlled traffic split\nfunction getServiceAdapter(): ServiceAdapter {\n  const vercelPercentage = getFeatureFlag('vercel_migration_percentage');\n\n  if (Math.random() * 100 < vercelPercentage) {\n    return new VercelAdapter();\n  }\n\n  return new LegacyAdapter();\n}\n```\n\n## Rollback Plan\n\n```bash\n# Immediate rollback\nkubectl set env deployment/app VERCEL_ENABLED=false\nkubectl rollout restart deployment/app\n\n# Data rollback (if needed)\n./scripts/restore-from-backup.sh --date YYYY-MM-DD\n\n# Verify rollback\ncurl https://app.yourcompany.com/health | jq '.services.vercel'\n```\n\n## Post-Migration Validation\n\n```typescript\nasync function validateVercelMigration(): Promise<ValidationReport> {\n  const checks = [\n    { name: 'Data count match', fn: checkDataCounts },\n    { name: 'API functionality', fn: checkApiFunctionality },\n    { name: 'Performance baseline', fn: checkPerformance },\n    { name: 'Error rates', fn: checkErrorRates },\n  ];\n\n  const results = await Promise.all(\n    checks.map(async c => ({ name: c.name, result: await c.fn() }))\n  );\n\n  return { checks: results, passed: results.every(r => r.result.success) };\n}\n```\n\n## Instructions\n\n### Step 1: Assess Current State\nDocument existing implementation and data inventory.\n\n### Step 2: Build Adapter Layer\nCreate abstraction layer for gradual migration.\n\n### Step 3: Migrate Data\nRun batch data migration with error handling.\n\n### Step 4: Shift Traffic\nGradually route traffic to new Vercel integration.\n\n## Output\n- Migration assessment complete\n- Adapter layer implemented\n- Data migrated successfully\n- Traffic fully shifted to Vercel\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Data mismatch | Transform errors | Validate transform logic |\n| Performance drop | No caching | Add caching layer |\n| Rollback triggered | Errors spiked | Reduce traffic percentage |\n| Validation failed | Missing data | Check batch processing |\n\n## Examples\n\n### Quick Migration Status\n```typescript\nconst status = await validateVercelMigration();\nconsole.log(`Migration ${status.passed ? 'PASSED' : 'FAILED'}`);\nstatus.checks.forEach(c => console.log(`  ${c.name}: ${c.result.success}`));\n```\n\n## Resources\n- [Strangler Fig Pattern](https://martinfowler.com/bliki/StranglerFigApplication.html)\n- [Vercel Migration Guide](https://vercel.com/docs/migration)\n\n## Flagship+ Skills\nFor advanced troubleshooting, see `vercel-advanced-troubleshooting`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-migration-deep-dive/SKILL.md"
    },
    {
      "slug": "vercel-multi-env-setup",
      "name": "vercel-multi-env-setup",
      "description": "Configure Vercel across development, staging, and production environments. Use when setting up multi-environment deployments, configuring per-environment secrets, or implementing environment-specific Vercel configurations. Trigger with phrases like \"vercel environments\", \"vercel staging\", \"vercel dev prod\", \"vercel environment setup\", \"vercel config by env\". allowed-tools: Read, Write, Edit, Bash(aws:*), Bash(gcloud:*), Bash(vault:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Multi-Environment Setup\n\n## Overview\nConfigure Vercel across development, staging, and production environments.\n\n## Prerequisites\n- Separate Vercel accounts or API keys per environment\n- Secret management solution (Vault, AWS Secrets Manager, etc.)\n- CI/CD pipeline with environment variables\n- Environment detection in application\n\n## Environment Strategy\n\n| Environment | Purpose | API Keys | Data |\n|-------------|---------|----------|------|\n| Development | Local dev | Test keys | Sandbox |\n| Staging | Pre-prod validation | Staging keys | Test data |\n| Production | Live traffic | Production keys | Real data |\n\n## Configuration Structure\n\n```\nconfig/\n‚îú‚îÄ‚îÄ vercel/\n‚îÇ   ‚îú‚îÄ‚îÄ base.json           # Shared config\n‚îÇ   ‚îú‚îÄ‚îÄ development.json    # Dev overrides\n‚îÇ   ‚îú‚îÄ‚îÄ staging.json        # Staging overrides\n‚îÇ   ‚îî‚îÄ‚îÄ production.json     # Prod overrides\n```\n\n### base.json\n```json\n{\n  \"timeout\": 30000,\n  \"retries\": 3,\n  \"cache\": {\n    \"enabled\": true,\n    \"ttlSeconds\": 60\n  }\n}\n```\n\n### development.json\n```json\n{\n  \"apiKey\": \"${VERCEL_API_KEY}\",\n  \"baseUrl\": \"https://api-sandbox.vercel.com\",\n  \"debug\": true,\n  \"cache\": {\n    \"enabled\": false\n  }\n}\n```\n\n### staging.json\n```json\n{\n  \"apiKey\": \"${VERCEL_API_KEY_STAGING}\",\n  \"baseUrl\": \"https://api-staging.vercel.com\",\n  \"debug\": false\n}\n```\n\n### production.json\n```json\n{\n  \"apiKey\": \"${VERCEL_API_KEY_PROD}\",\n  \"baseUrl\": \"https://api.vercel.com\",\n  \"debug\": false,\n  \"retries\": 5\n}\n```\n\n## Environment Detection\n\n```typescript\n// src/vercel/config.ts\nimport baseConfig from '../../config/vercel/base.json';\n\ntype Environment = 'development' | 'staging' | 'production';\n\nfunction detectEnvironment(): Environment {\n  const env = process.env.NODE_ENV || 'development';\n  const validEnvs: Environment[] = ['development', 'staging', 'production'];\n  return validEnvs.includes(env as Environment)\n    ? (env as Environment)\n    : 'development';\n}\n\nexport function getVercelConfig() {\n  const env = detectEnvironment();\n  const envConfig = require(`../../config/vercel/${env}.json`);\n\n  return {\n    ...baseConfig,\n    ...envConfig,\n    environment: env,\n  };\n}\n```\n\n## Secret Management by Environment\n\n### Local Development\n```bash\n# .env.local (git-ignored)\nVERCEL_API_KEY=sk_test_dev_***\n```\n\n### CI/CD (GitHub Actions)\n```yaml\nenv:\n  VERCEL_API_KEY: ${{ secrets.VERCEL_API_KEY_${{ matrix.environment }} }}\n```\n\n### Production (Vault/Secrets Manager)\n```bash\n# AWS Secrets Manager\naws secretsmanager get-secret-value --secret-id vercel/production/api-key\n\n# GCP Secret Manager\ngcloud secrets versions access latest --secret=vercel-api-key\n\n# HashiCorp Vault\nvault kv get -field=api_key secret/vercel/production\n```\n\n## Environment Isolation\n\n```typescript\n// Prevent production operations in non-prod\nfunction guardProductionOperation(operation: string): void {\n  const config = getVercelConfig();\n\n  if (config.environment !== 'production') {\n    console.warn(`[vercel] ${operation} blocked in ${config.environment}`);\n    throw new Error(`${operation} only allowed in production`);\n  }\n}\n\n// Usage\nasync function deleteAllData() {\n  guardProductionOperation('deleteAllData');\n  // Dangerous operation here\n}\n```\n\n## Feature Flags by Environment\n\n```typescript\nconst featureFlags: Record<Environment, Record<string, boolean>> = {\n  development: {\n    newFeature: true,\n    betaApi: true,\n  },\n  staging: {\n    newFeature: true,\n    betaApi: false,\n  },\n  production: {\n    newFeature: false,\n    betaApi: false,\n  },\n};\n```\n\n## Instructions\n\n### Step 1: Create Config Structure\nSet up the base and per-environment configuration files.\n\n### Step 2: Implement Environment Detection\nAdd logic to detect and load environment-specific config.\n\n### Step 3: Configure Secrets\nStore API keys securely using your secret management solution.\n\n### Step 4: Add Environment Guards\nImplement safeguards for production-only operations.\n\n## Output\n- Multi-environment config structure\n- Environment detection logic\n- Secure secret management\n- Production safeguards enabled\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Wrong environment | Missing NODE_ENV | Set environment variable |\n| Secret not found | Wrong secret path | Verify secret manager config |\n| Config merge fails | Invalid JSON | Validate config files |\n| Production guard triggered | Wrong environment | Check NODE_ENV value |\n\n## Examples\n\n### Quick Environment Check\n```typescript\nconst env = getVercelConfig();\nconsole.log(`Running in ${env.environment} with ${env.baseUrl}`);\n```\n\n## Resources\n- [Vercel Environments Guide](https://vercel.com/docs/environments)\n- [12-Factor App Config](https://12factor.net/config)\n\n## Next Steps\nFor observability setup, see `vercel-observability`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-multi-env-setup/SKILL.md"
    },
    {
      "slug": "vercel-observability",
      "name": "vercel-observability",
      "description": "Set up comprehensive observability for Vercel integrations with metrics, traces, and alerts. Use when implementing monitoring for Vercel operations, setting up dashboards, or configuring alerting for Vercel integration health. Trigger with phrases like \"vercel monitoring\", \"vercel metrics\", \"vercel observability\", \"monitor vercel\", \"vercel alerts\", \"vercel tracing\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Observability\n\n## Overview\nSet up comprehensive observability for Vercel integrations.\n\n## Prerequisites\n- Prometheus or compatible metrics backend\n- OpenTelemetry SDK installed\n- Grafana or similar dashboarding tool\n- AlertManager configured\n\n## Metrics Collection\n\n### Key Metrics\n| Metric | Type | Description |\n|--------|------|-------------|\n| `vercel_requests_total` | Counter | Total API requests |\n| `vercel_request_duration_seconds` | Histogram | Request latency |\n| `vercel_errors_total` | Counter | Error count by type |\n| `vercel_rate_limit_remaining` | Gauge | Rate limit headroom |\n\n### Prometheus Metrics\n\n```typescript\nimport { Registry, Counter, Histogram, Gauge } from 'prom-client';\n\nconst registry = new Registry();\n\nconst requestCounter = new Counter({\n  name: 'vercel_requests_total',\n  help: 'Total Vercel API requests',\n  labelNames: ['method', 'status'],\n  registers: [registry],\n});\n\nconst requestDuration = new Histogram({\n  name: 'vercel_request_duration_seconds',\n  help: 'Vercel request duration',\n  labelNames: ['method'],\n  buckets: [0.05, 0.1, 0.25, 0.5, 1, 2.5, 5],\n  registers: [registry],\n});\n\nconst errorCounter = new Counter({\n  name: 'vercel_errors_total',\n  help: 'Vercel errors by type',\n  labelNames: ['error_type'],\n  registers: [registry],\n});\n```\n\n### Instrumented Client\n\n```typescript\nasync function instrumentedRequest<T>(\n  method: string,\n  operation: () => Promise<T>\n): Promise<T> {\n  const timer = requestDuration.startTimer({ method });\n\n  try {\n    const result = await operation();\n    requestCounter.inc({ method, status: 'success' });\n    return result;\n  } catch (error: any) {\n    requestCounter.inc({ method, status: 'error' });\n    errorCounter.inc({ error_type: error.code || 'unknown' });\n    throw error;\n  } finally {\n    timer();\n  }\n}\n```\n\n## Distributed Tracing\n\n### OpenTelemetry Setup\n\n```typescript\nimport { trace, SpanStatusCode } from '@opentelemetry/api';\n\nconst tracer = trace.getTracer('vercel-client');\n\nasync function tracedVercelCall<T>(\n  operationName: string,\n  operation: () => Promise<T>\n): Promise<T> {\n  return tracer.startActiveSpan(`vercel.${operationName}`, async (span) => {\n    try {\n      const result = await operation();\n      span.setStatus({ code: SpanStatusCode.OK });\n      return result;\n    } catch (error: any) {\n      span.setStatus({ code: SpanStatusCode.ERROR, message: error.message });\n      span.recordException(error);\n      throw error;\n    } finally {\n      span.end();\n    }\n  });\n}\n```\n\n## Logging Strategy\n\n### Structured Logging\n\n```typescript\nimport pino from 'pino';\n\nconst logger = pino({\n  name: 'vercel',\n  level: process.env.LOG_LEVEL || 'info',\n});\n\nfunction logVercelOperation(\n  operation: string,\n  data: Record<string, any>,\n  duration: number\n) {\n  logger.info({\n    service: 'vercel',\n    operation,\n    duration_ms: duration,\n    ...data,\n  });\n}\n```\n\n## Alert Configuration\n\n### Prometheus AlertManager Rules\n\n```yaml\n# vercel_alerts.yaml\ngroups:\n  - name: vercel_alerts\n    rules:\n      - alert: VercelHighErrorRate\n        expr: |\n          rate(vercel_errors_total[5m]) /\n          rate(vercel_requests_total[5m]) > 0.05\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Vercel error rate > 5%\"\n\n      - alert: VercelHighLatency\n        expr: |\n          histogram_quantile(0.95,\n            rate(vercel_request_duration_seconds_bucket[5m])\n          ) > 2\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Vercel P95 latency > 2s\"\n\n      - alert: VercelDown\n        expr: up{job=\"vercel\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Vercel integration is down\"\n```\n\n## Dashboard\n\n### Grafana Panel Queries\n\n```json\n{\n  \"panels\": [\n    {\n      \"title\": \"Vercel Request Rate\",\n      \"targets\": [{\n        \"expr\": \"rate(vercel_requests_total[5m])\"\n      }]\n    },\n    {\n      \"title\": \"Vercel Latency P50/P95/P99\",\n      \"targets\": [{\n        \"expr\": \"histogram_quantile(0.5, rate(vercel_request_duration_seconds_bucket[5m]))\"\n      }]\n    }\n  ]\n}\n```\n\n## Instructions\n\n### Step 1: Set Up Metrics Collection\nImplement Prometheus counters, histograms, and gauges for key operations.\n\n### Step 2: Add Distributed Tracing\nIntegrate OpenTelemetry for end-to-end request tracing.\n\n### Step 3: Configure Structured Logging\nSet up JSON logging with consistent field names.\n\n### Step 4: Create Alert Rules\nDefine Prometheus alerting rules for error rates and latency.\n\n## Output\n- Metrics collection enabled\n- Distributed tracing configured\n- Structured logging implemented\n- Alert rules deployed\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Missing metrics | No instrumentation | Wrap client calls |\n| Trace gaps | Missing propagation | Check context headers |\n| Alert storms | Wrong thresholds | Tune alert rules |\n| High cardinality | Too many labels | Reduce label values |\n\n## Examples\n\n### Quick Metrics Endpoint\n```typescript\napp.get('/metrics', async (req, res) => {\n  res.set('Content-Type', registry.contentType);\n  res.send(await registry.metrics());\n});\n```\n\n## Resources\n- [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)\n- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)\n- [Vercel Observability Guide](https://vercel.com/docs/observability)\n\n## Next Steps\nFor incident response, see `vercel-incident-runbook`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-observability/SKILL.md"
    },
    {
      "slug": "vercel-performance-tuning",
      "name": "vercel-performance-tuning",
      "description": "Optimize Vercel API performance with caching, batching, and connection pooling. Use when experiencing slow API responses, implementing caching strategies, or optimizing request throughput for Vercel integrations. Trigger with phrases like \"vercel performance\", \"optimize vercel\", \"vercel latency\", \"vercel caching\", \"vercel slow\", \"vercel batch\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Performance Tuning\n\n## Overview\nOptimize Vercel API performance with caching, batching, and connection pooling.\n\n## Prerequisites\n- Vercel SDK installed\n- Understanding of async patterns\n- Redis or in-memory cache available (optional)\n- Performance monitoring in place\n\n## Latency Benchmarks\n\n| Operation | P50 | P95 | P99 |\n|-----------|-----|-----|-----|\n| Cold Start (Serverless) | 250ms | 500ms | 1000ms |\n| Cold Start (Edge) | 5ms | 25ms | 50ms |\n| Build Time | 30s | 120s | 300s |\n\n## Caching Strategy\n\n### Response Caching\n```typescript\nimport { LRUCache } from 'lru-cache';\n\nconst cache = new LRUCache<string, any>({\n  max: 1000,\n  ttl: 31536000000, // 1 minute\n  updateAgeOnGet: true,\n});\n\nasync function cachedVercelRequest<T>(\n  key: string,\n  fetcher: () => Promise<T>,\n  ttl?: number\n): Promise<T> {\n  const cached = cache.get(key);\n  if (cached) return cached as T;\n\n  const result = await fetcher();\n  cache.set(key, result, { ttl });\n  return result;\n}\n```\n\n### Redis Caching (Distributed)\n```typescript\nimport Redis from 'ioredis';\n\nconst redis = new Redis(process.env.REDIS_URL);\n\nasync function cachedWithRedis<T>(\n  key: string,\n  fetcher: () => Promise<T>,\n  ttlSeconds = 60\n): Promise<T> {\n  const cached = await redis.get(key);\n  if (cached) return JSON.parse(cached);\n\n  const result = await fetcher();\n  await redis.setex(key, ttlSeconds, JSON.stringify(result));\n  return result;\n}\n```\n\n## Request Batching\n\n```typescript\nimport DataLoader from 'dataloader';\n\nconst vercelLoader = new DataLoader<string, any>(\n  async (ids) => {\n    // Batch fetch from Vercel\n    const results = await vercelClient.batchGet(ids);\n    return ids.map(id => results.find(r => r.id === id) || null);\n  },\n  {\n    maxBatchSize: 100,\n    batchScheduleFn: callback => setTimeout(callback, 10),\n  }\n);\n\n// Usage - automatically batched\nconst [item1, item2, item3] = await Promise.all([\n  vercelLoader.load('id-1'),\n  vercelLoader.load('id-2'),\n  vercelLoader.load('id-3'),\n]);\n```\n\n## Connection Optimization\n\n```typescript\nimport { Agent } from 'https';\n\n// Keep-alive connection pooling\nconst agent = new Agent({\n  keepAlive: true,\n  maxSockets: None,\n  maxFreeSockets: 5,\n  timeout: 10000,\n});\n\nconst client = new VercelClient({\n  apiKey: process.env.VERCEL_API_KEY!,\n  httpAgent: agent,\n});\n```\n\n## Pagination Optimization\n\n```typescript\nasync function* paginatedVercelList<T>(\n  fetcher: (cursor?: string) => Promise<{ data: T[]; nextCursor?: string }>\n): AsyncGenerator<T> {\n  let cursor: string | undefined;\n\n  do {\n    const { data, nextCursor } = await fetcher(cursor);\n    for (const item of data) {\n      yield item;\n    }\n    cursor = nextCursor;\n  } while (cursor);\n}\n\n// Usage\nfor await (const item of paginatedVercelList(cursor =>\n  vercelClient.list({ cursor, limit: 100 })\n)) {\n  await process(item);\n}\n```\n\n## Performance Monitoring\n\n```typescript\nasync function measuredVercelCall<T>(\n  operation: string,\n  fn: () => Promise<T>\n): Promise<T> {\n  const start = performance.now();\n  try {\n    const result = await fn();\n    const duration = performance.now() - start;\n    console.log({ operation, duration, status: 'success' });\n    return result;\n  } catch (error) {\n    const duration = performance.now() - start;\n    console.error({ operation, duration, status: 'error', error });\n    throw error;\n  }\n}\n```\n\n## Instructions\n\n### Step 1: Establish Baseline\nMeasure current latency for critical Vercel operations.\n\n### Step 2: Implement Caching\nAdd response caching for frequently accessed data.\n\n### Step 3: Enable Batching\nUse DataLoader or similar for automatic request batching.\n\n### Step 4: Optimize Connections\nConfigure connection pooling with keep-alive.\n\n## Output\n- Reduced API latency\n- Caching layer implemented\n- Request batching enabled\n- Connection pooling configured\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Cache miss storm | TTL expired | Use stale-while-revalidate |\n| Batch timeout | Too many items | Reduce batch size |\n| Connection exhausted | No pooling | Configure max sockets |\n| Memory pressure | Cache too large | Set max cache entries |\n\n## Examples\n\n### Quick Performance Wrapper\n```typescript\nconst withPerformance = <T>(name: string, fn: () => Promise<T>) =>\n  measuredVercelCall(name, () =>\n    cachedVercelRequest(`cache:${name}`, fn)\n  );\n```\n\n## Resources\n- [Vercel Performance Guide](https://vercel.com/docs/performance)\n- [DataLoader Documentation](https://github.com/graphql/dataloader)\n- [LRU Cache Documentation](https://github.com/isaacs/node-lru-cache)\n\n## Next Steps\nFor cost optimization, see `vercel-cost-tuning`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-performance-tuning/SKILL.md"
    },
    {
      "slug": "vercel-policy-guardrails",
      "name": "vercel-policy-guardrails",
      "description": "Implement Vercel lint rules, policy enforcement, and automated guardrails. Use when setting up code quality rules for Vercel integrations, implementing pre-commit hooks, or configuring CI policy checks for Vercel best practices. Trigger with phrases like \"vercel policy\", \"vercel lint\", \"vercel guardrails\", \"vercel best practices check\", \"vercel eslint\". allowed-tools: Read, Write, Edit, Bash(npx:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Policy & Guardrails\n\n## Overview\nAutomated policy enforcement and guardrails for Vercel integrations.\n\n## Prerequisites\n- ESLint configured in project\n- Pre-commit hooks infrastructure\n- CI/CD pipeline with policy checks\n- TypeScript for type enforcement\n\n## ESLint Rules\n\n### Custom Vercel Plugin\n```javascript\n// eslint-plugin-vercel/rules/no-hardcoded-keys.js\nmodule.exports = {\n  meta: {\n    type: 'problem',\n    docs: {\n      description: 'Disallow hardcoded Vercel API keys',\n    },\n    fixable: 'code',\n  },\n  create(context) {\n    return {\n      Literal(node) {\n        if (typeof node.value === 'string') {\n          if (node.value.match(/^sk_(live|test)_[a-zA-Z0-9]{24,}/)) {\n            context.report({\n              node,\n              message: 'Hardcoded Vercel API key detected',\n            });\n          }\n        }\n      },\n    };\n  },\n};\n```\n\n### ESLint Configuration\n```javascript\n// .eslintrc.js\nmodule.exports = {\n  plugins: ['vercel'],\n  rules: {\n    'vercel/no-hardcoded-keys': 'error',\n    'vercel/require-error-handling': 'warn',\n    'vercel/use-typed-client': 'warn',\n  },\n};\n```\n\n## Pre-Commit Hooks\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: vercel-secrets-check\n        name: Check for Vercel secrets\n        entry: bash -c 'git diff --cached --name-only | xargs grep -l \"sk_live_\" && exit 1 || exit 0'\n        language: system\n        pass_filenames: false\n\n      - id: vercel-config-validate\n        name: Validate Vercel configuration\n        entry: node scripts/validate-vercel-config.js\n        language: node\n        files: '\\.vercel\\.json$'\n```\n\n## TypeScript Strict Patterns\n\n```typescript\n// Enforce typed configuration\ninterface VercelStrictConfig {\n  apiKey: string;  // Required\n  environment: 'development' | 'staging' | 'production';  // Enum\n  timeout: number;  // Required number, not optional\n  retries: number;\n}\n\n// Disallow any in Vercel code\n// @ts-expect-error - Using any is forbidden\nconst client = new Client({ apiKey: any });\n\n// Prefer this\nconst client = new VercelClient(config satisfies VercelStrictConfig);\n```\n\n## Architecture Decision Records\n\n### ADR Template\n```markdown\n# ADR-001: Vercel Client Initialization\n\n## Status\nAccepted\n\n## Context\nWe need to decide how to initialize the Vercel client across our application.\n\n## Decision\nWe will use the singleton pattern with lazy initialization.\n\n## Consequences\n- Pro: Single client instance, connection reuse\n- Pro: Easy to mock in tests\n- Con: Global state requires careful lifecycle management\n\n## Enforcement\n- ESLint rule: vercel/use-singleton-client\n- CI check: grep for \"new VercelClient(\" outside allowed files\n```\n\n## Policy-as-Code (OPA)\n\n```rego\n# vercel-policy.rego\npackage vercel\n\n# Deny production API keys in non-production environments\ndeny[msg] {\n  input.environment != \"production\"\n  startswith(input.apiKey, \"sk_live_\")\n  msg := \"Production API keys not allowed in non-production environment\"\n}\n\n# Require minimum timeout\ndeny[msg] {\n  input.timeout < 10000\n  msg := sprintf(\"Timeout too low: %d < 10000ms minimum\", [input.timeout])\n}\n\n# Require retry configuration\ndeny[msg] {\n  not input.retries\n  msg := \"Retry configuration is required\"\n}\n```\n\n## CI Policy Checks\n\n```yaml\n# .github/workflows/vercel-policy.yml\nname: Vercel Policy Check\n\non: [push, pull_request]\n\njobs:\n  policy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Check for hardcoded secrets\n        run: |\n          if grep -rE \"sk_(live|test)_[a-zA-Z0-9]{24,}\" --include=\"*.ts\" --include=\"*.js\" .; then\n            echo \"ERROR: Hardcoded Vercel keys found\"\n            exit 1\n          fi\n\n      - name: Validate configuration schema\n        run: |\n          npx ajv validate -s vercel-config.schema.json -d config/vercel/*.json\n\n      - name: Run ESLint Vercel rules\n        run: npx eslint --plugin vercel --rule 'vercel/no-hardcoded-keys: error' src/\n```\n\n## Runtime Guardrails\n\n```typescript\n// Prevent dangerous operations in production\nconst BLOCKED_IN_PROD = ['deleteAll', 'resetData', 'migrateDown'];\n\nfunction guardVercelOperation(operation: string): void {\n  const isProd = process.env.NODE_ENV === 'production';\n\n  if (isProd && BLOCKED_IN_PROD.includes(operation)) {\n    throw new Error(`Operation '${operation}' blocked in production`);\n  }\n}\n\n// Rate limit protection\nfunction guardRateLimits(requestsInWindow: number): void {\n  const limit = parseInt(process.env.VERCEL_RATE_LIMIT || '100');\n\n  if (requestsInWindow > limit * 0.9) {\n    console.warn('Approaching Vercel rate limit');\n  }\n\n  if (requestsInWindow >= limit) {\n    throw new Error('Vercel rate limit exceeded - request blocked');\n  }\n}\n```\n\n## Instructions\n\n### Step 1: Create ESLint Rules\nImplement custom lint rules for Vercel patterns.\n\n### Step 2: Configure Pre-Commit Hooks\nSet up hooks to catch issues before commit.\n\n### Step 3: Add CI Policy Checks\nImplement policy-as-code in CI pipeline.\n\n### Step 4: Enable Runtime Guardrails\nAdd production safeguards for dangerous operations.\n\n## Output\n- ESLint plugin with Vercel rules\n- Pre-commit hooks blocking secrets\n- CI policy checks passing\n- Runtime guardrails active\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| ESLint rule not firing | Wrong config | Check plugin registration |\n| Pre-commit skipped | --no-verify | Enforce in CI |\n| Policy false positive | Regex too broad | Narrow pattern match |\n| Guardrail triggered | Actual issue | Fix or whitelist |\n\n## Examples\n\n### Quick ESLint Check\n```bash\nnpx eslint --plugin vercel --rule 'vercel/no-hardcoded-keys: error' src/\n```\n\n## Resources\n- [ESLint Plugin Development](https://eslint.org/docs/latest/extend/plugins)\n- [Pre-commit Framework](https://pre-commit.com/)\n- [Open Policy Agent](https://www.openpolicyagent.org/)\n\n## Next Steps\nFor architecture blueprints, see `vercel-architecture-variants`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-policy-guardrails/SKILL.md"
    },
    {
      "slug": "vercel-prod-checklist",
      "name": "vercel-prod-checklist",
      "description": "Execute Vercel production deployment checklist and rollback procedures. Use when deploying Vercel integrations to production, preparing for launch, or implementing go-live procedures. Trigger with phrases like \"vercel production\", \"deploy vercel\", \"vercel go-live\", \"vercel launch checklist\". allowed-tools: Read, Bash(kubectl:*), Bash(curl:*), Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Production Checklist\n\n## Overview\nComplete checklist for deploying Vercel integrations to production.\n\n## Prerequisites\n- Staging environment tested and verified\n- Production API keys available\n- Deployment pipeline configured\n- Monitoring and alerting ready\n\n## Instructions\n\n### Step 1: Pre-Deployment Configuration\n- [ ] Production API keys in secure vault\n- [ ] Environment variables set in deployment platform\n- [ ] API key scopes are minimal (least privilege)\n- [ ] Webhook endpoints configured with HTTPS\n- [ ] Webhook secrets stored securely\n\n### Step 2: Code Quality Verification\n- [ ] All tests passing (`npm test`)\n- [ ] No hardcoded credentials\n- [ ] Error handling covers all Vercel error types\n- [ ] Rate limiting/backoff implemented\n- [ ] Logging is production-appropriate\n\n### Step 3: Infrastructure Setup\n- [ ] Health check endpoint includes Vercel connectivity\n- [ ] Monitoring/alerting configured\n- [ ] Circuit breaker pattern implemented\n- [ ] Graceful degradation configured\n\n### Step 4: Documentation Requirements\n- [ ] Incident runbook created\n- [ ] Key rotation procedure documented\n- [ ] Rollback procedure documented\n- [ ] On-call escalation path defined\n\n### Step 5: Deploy with Gradual Rollout\n```bash\n# Pre-flight checks\ncurl -f https://staging.example.com/health\ncurl -s https://www.vercel-status.com\n\n# Gradual rollout - start with canary (10%)\nkubectl apply -f k8s/production.yaml\nkubectl set image deployment/vercel-integration app=image:new --record\nkubectl rollout pause deployment/vercel-integration\n\n# Monitor canary traffic for 10 minutes\nsleep 600\n# Check error rates and latency before continuing\n\n# If healthy, continue rollout to 50%\nkubectl rollout resume deployment/vercel-integration\nkubectl rollout pause deployment/vercel-integration\nsleep 300\n\n# Complete rollout to 100%\nkubectl rollout resume deployment/vercel-integration\nkubectl rollout status deployment/vercel-integration\n```\n\n## Output\n- Deployed Vercel integration\n- Health checks passing\n- Monitoring active\n- Rollback procedure documented\n\n## Error Handling\n| Alert | Condition | Severity |\n|-------|-----------|----------|\n| API Down | 5xx errors > 10/min | P1 |\n| High Latency | p99 > 5000ms | P2 |\n| Rate Limited | 429 errors > 5/min | P2 |\n| Auth Failures | 401/403 errors > 0 | P1 |\n\n## Examples\n\n### Health Check Implementation\n```typescript\nasync function healthCheck(): Promise<{ status: string; vercel: any }> {\n  const start = Date.now();\n  try {\n    await vercelClient.ping();\n    return { status: 'healthy', vercel: { connected: true, latencyMs: Date.now() - start } };\n  } catch (error) {\n    return { status: 'degraded', vercel: { connected: false, latencyMs: Date.now() - start } };\n  }\n}\n```\n\n### Immediate Rollback\n```bash\nkubectl rollout undo deployment/vercel-integration\nkubectl rollout status deployment/vercel-integration\n```\n\n## Resources\n- [Vercel Status](https://www.vercel-status.com)\n- [Vercel Support](https://vercel.com/docs/support)\n\n## Next Steps\nFor version upgrades, see `vercel-upgrade-migration`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-prod-checklist/SKILL.md"
    },
    {
      "slug": "vercel-rate-limits",
      "name": "vercel-rate-limits",
      "description": "Implement Vercel rate limiting, backoff, and idempotency patterns. Use when handling rate limit errors, implementing retry logic, or optimizing API request throughput for Vercel. Trigger with phrases like \"vercel rate limit\", \"vercel throttling\", \"vercel 429\", \"vercel retry\", \"vercel backoff\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Rate Limits\n\n## Overview\nHandle Vercel rate limits gracefully with exponential backoff and idempotency.\n\n## Prerequisites\n- Vercel SDK installed\n- Understanding of async/await patterns\n- Access to rate limit headers\n\n## Instructions\n\n### Step 1: Understand Rate Limit Tiers\n\n| Tier | Requests/min | Requests/day | Burst |\n|------|-------------|--------------|-------|\n| Hobby | 100 | 100,000 | 10 |\n| Pro | 1,000 | 1,000,000 | 50 |\n| Enterprise | 10,000 | Unlimited | 200 |\n\n### Step 2: Implement Exponential Backoff with Jitter\n\n```typescript\nasync function withExponentialBackoff<T>(\n  operation: () => Promise<T>,\n  config = { maxRetries: 5, baseDelayMs: 1000, maxDelayMs: 32000, jitterMs: 500 }\n): Promise<T> {\n  for (let attempt = 0; attempt <= config.maxRetries; attempt++) {\n    try {\n      return await operation();\n    } catch (error: any) {\n      if (attempt === config.maxRetries) throw error;\n      const status = error.status || error.response?.status;\n      if (status !== 429 && (status < 500 || status >= 600)) throw error;\n\n      // Exponential delay with jitter to prevent thundering herd\n      const exponentialDelay = config.baseDelayMs * Math.pow(2, attempt);\n      const jitter = Math.random() * config.jitterMs;\n      const delay = Math.min(exponentialDelay + jitter, config.maxDelayMs);\n\n      console.log(`Rate limited. Retrying in ${delay.toFixed(0)}ms...`);\n      await new Promise(r => setTimeout(r, delay));\n    }\n  }\n  throw new Error('Unreachable');\n}\n```\n\n### Step 3: Add Idempotency Keys\n\n```typescript\nimport { v4 as uuidv4 } from 'uuid';\nimport crypto from 'crypto';\n\n// Generate deterministic key from operation params (for safe retries)\nfunction generateIdempotencyKey(operation: string, params: Record<string, any>): string {\n  const data = JSON.stringify({ operation, params });\n  return crypto.createHash('sha256').update(data).digest('hex');\n}\n\nasync function idempotentRequest<T>(\n  client: VercelClient,\n  params: Record<string, any>,\n  idempotencyKey?: string  // Pass existing key for retries\n): Promise<T> {\n  // Use provided key (for retries) or generate deterministic key from params\n  const key = idempotencyKey || generateIdempotencyKey(params.method || 'POST', params);\n  return client.request({\n    ...params,\n    headers: { 'Idempotency-Key': key, ...params.headers },\n  });\n}\n```\n\n## Output\n- Reliable API calls with automatic retry\n- Idempotent requests preventing duplicates\n- Rate limit headers properly handled\n\n## Error Handling\n| Header | Description | Action |\n|--------|-------------|--------|\n| X-RateLimit-Limit | Max requests | Monitor usage |\n| X-RateLimit-Remaining | Remaining requests | Throttle if low |\n| X-RateLimit-Reset | Reset timestamp | Wait until reset |\n| Retry-After | Seconds to wait | Honor this value |\n\n## Examples\n\n### Queue-Based Rate Limiting\n```typescript\nimport PQueue from 'p-queue';\n\nconst queue = new PQueue({\n  concurrency: 5,\n  interval: 1000,\n  intervalCap: 10,\n});\n\nasync function queuedRequest<T>(operation: () => Promise<T>): Promise<T> {\n  return queue.add(operation);\n}\n```\n\n### Monitor Rate Limit Usage\n```typescript\nclass RateLimitMonitor {\n  private remaining: number = 60;\n  private resetAt: Date = new Date();\n\n  updateFromHeaders(headers: Headers) {\n    this.remaining = parseInt(headers.get('X-RateLimit-Remaining') || '60');\n    const resetTimestamp = headers.get('X-RateLimit-Reset');\n    if (resetTimestamp) {\n      this.resetAt = new Date(parseInt(resetTimestamp) * 1000);\n    }\n  }\n\n  shouldThrottle(): boolean {\n    // Only throttle if low remaining AND reset hasn't happened yet\n    return this.remaining < 5 && new Date() < this.resetAt;\n  }\n\n  getWaitTime(): number {\n    return Math.max(0, this.resetAt.getTime() - Date.now());\n  }\n}\n```\n\n## Resources\n- [Vercel Rate Limits](https://vercel.com/docs/rate-limits)\n- [p-queue Documentation](https://github.com/sindresorhus/p-queue)\n\n## Next Steps\nFor security configuration, see `vercel-security-basics`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-rate-limits/SKILL.md"
    },
    {
      "slug": "vercel-reference-architecture",
      "name": "vercel-reference-architecture",
      "description": "Implement Vercel reference architecture with best-practice project layout. Use when designing new Vercel integrations, reviewing project structure, or establishing architecture standards for Vercel applications. Trigger with phrases like \"vercel architecture\", \"vercel best practices\", \"vercel project structure\", \"how to organize vercel\", \"vercel layout\". allowed-tools: Read, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Reference Architecture\n\n## Overview\nProduction-ready architecture patterns for Vercel integrations.\n\n## Prerequisites\n- Understanding of layered architecture\n- Vercel SDK knowledge\n- TypeScript project setup\n- Testing framework configured\n\n## Project Structure\n\n```\nmy-vercel-project/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ vercel/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts           # Singleton client wrapper\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.ts           # Environment configuration\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.ts            # TypeScript types\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errors.ts           # Custom error classes\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handlers/\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ webhooks.ts     # Webhook handlers\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ events.ts       # Event processing\n‚îÇ   ‚îú‚îÄ‚îÄ services/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vercel/\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ index.ts        # Service facade\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ sync.ts         # Data synchronization\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ cache.ts        # Caching layer\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vercel/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ webhook.ts      # Webhook endpoint\n‚îÇ   ‚îî‚îÄ‚îÄ jobs/\n‚îÇ       ‚îî‚îÄ‚îÄ vercel/\n‚îÇ           ‚îî‚îÄ‚îÄ sync.ts         # Background sync job\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ unit/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vercel/\n‚îÇ   ‚îî‚îÄ‚îÄ integration/\n‚îÇ       ‚îî‚îÄ‚îÄ vercel/\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îú‚îÄ‚îÄ vercel.development.json\n‚îÇ   ‚îú‚îÄ‚îÄ vercel.staging.json\n‚îÇ   ‚îî‚îÄ‚îÄ vercel.production.json\n‚îî‚îÄ‚îÄ docs/\n    ‚îî‚îÄ‚îÄ vercel/\n        ‚îú‚îÄ‚îÄ SETUP.md\n        ‚îî‚îÄ‚îÄ RUNBOOK.md\n```\n\n## Layer Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ             API Layer                    ‚îÇ\n‚îÇ   (Controllers, Routes, Webhooks)        ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ           Service Layer                  ‚îÇ\n‚îÇ  (Business Logic, Orchestration)         ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ          Vercel Layer        ‚îÇ\n‚îÇ   (Client, Types, Error Handling)        ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ         Infrastructure Layer             ‚îÇ\n‚îÇ    (Cache, Queue, Monitoring)            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Key Components\n\n### Step 1: Client Wrapper\n```typescript\n// src/vercel/client.ts\nexport class VercelService {\n  private client: VercelClient;\n  private cache: Cache;\n  private monitor: Monitor;\n\n  constructor(config: VercelConfig) {\n    this.client = new VercelClient(config);\n    this.cache = new Cache(config.cacheOptions);\n    this.monitor = new Monitor('vercel');\n  }\n\n  async get(id: string): Promise<Resource> {\n    return this.cache.getOrFetch(id, () =>\n      this.monitor.track('get', () => this.client.get(id))\n    );\n  }\n}\n```\n\n### Step 2: Error Boundary\n```typescript\n// src/vercel/errors.ts\nexport class VercelServiceError extends Error {\n  constructor(\n    message: string,\n    public readonly code: string,\n    public readonly retryable: boolean,\n    public readonly originalError?: Error\n  ) {\n    super(message);\n    this.name = 'VercelServiceError';\n  }\n}\n\nexport function wrapVercelError(error: unknown): VercelServiceError {\n  // Transform SDK errors to application errors\n}\n```\n\n### Step 3: Health Check\n```typescript\n// src/vercel/health.ts\nexport async function checkVercelHealth(): Promise<HealthStatus> {\n  try {\n    const start = Date.now();\n    await vercelClient.ping();\n    return {\n      status: 'healthy',\n      latencyMs: Date.now() - start,\n    };\n  } catch (error) {\n    return { status: 'unhealthy', error: error.message };\n  }\n}\n```\n\n## Data Flow Diagram\n\n```\nUser Request\n     ‚îÇ\n     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   API       ‚îÇ\n‚îÇ   Gateway   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Service   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Cache     ‚îÇ\n‚îÇ   Layer     ‚îÇ    ‚îÇ   (Redis)   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Vercel    ‚îÇ\n‚îÇ   Client    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Vercel    ‚îÇ\n‚îÇ   API       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Configuration Management\n\n```typescript\n// config/vercel.ts\nexport interface VercelConfig {\n  apiKey: string;\n  environment: 'development' | 'staging' | 'production';\n  timeout: number;\n  retries: number;\n  cache: {\n    enabled: boolean;\n    ttlSeconds: number;\n  };\n}\n\nexport function loadVercelConfig(): VercelConfig {\n  const env = process.env.NODE_ENV || 'development';\n  return require(`./vercel.${env}.json`);\n}\n```\n\n## Instructions\n\n### Step 1: Create Directory Structure\nSet up the project layout following the reference structure above.\n\n### Step 2: Implement Client Wrapper\nCreate the singleton client with caching and monitoring.\n\n### Step 3: Add Error Handling\nImplement custom error classes for Vercel operations.\n\n### Step 4: Configure Health Checks\nAdd health check endpoint for Vercel connectivity.\n\n## Output\n- Structured project layout\n- Client wrapper with caching\n- Error boundary implemented\n- Health checks configured\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Circular dependencies | Wrong layering | Separate concerns by layer |\n| Config not loading | Wrong paths | Verify config file locations |\n| Type errors | Missing types | Add Vercel types |\n| Test isolation | Shared state | Use dependency injection |\n\n## Examples\n\n### Quick Setup Script\n```bash\n# Create reference structure\nmkdir -p src/vercel/{handlers} src/services/vercel src/api/vercel\ntouch src/vercel/{client,config,types,errors}.ts\ntouch src/services/vercel/{index,sync,cache}.ts\n```\n\n## Resources\n- [Vercel SDK Documentation](https://vercel.com/docs/sdk)\n- [Vercel Best Practices](https://vercel.com/docs/best-practices)\n\n## Flagship Skills\nFor multi-environment setup, see `vercel-multi-env-setup`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-reference-architecture/SKILL.md"
    },
    {
      "slug": "vercel-reliability-patterns",
      "name": "vercel-reliability-patterns",
      "description": "Implement Vercel reliability patterns including circuit breakers, idempotency, and graceful degradation. Use when building fault-tolerant Vercel integrations, implementing retry strategies, or adding resilience to production Vercel services. Trigger with phrases like \"vercel reliability\", \"vercel circuit breaker\", \"vercel idempotent\", \"vercel resilience\", \"vercel fallback\", \"vercel bulkhead\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Reliability Patterns\n\n## Overview\nProduction-grade reliability patterns for Vercel integrations.\n\n## Prerequisites\n- Understanding of circuit breaker pattern\n- opossum or similar library installed\n- Queue infrastructure for DLQ\n- Caching layer for fallbacks\n\n## Circuit Breaker\n\n```typescript\nimport CircuitBreaker from 'opossum';\n\nconst vercelBreaker = new CircuitBreaker(\n  async (operation: () => Promise<any>) => operation(),\n  {\n    timeout: 10000,\n    errorThresholdPercentage: 50,\n    resetTimeout: 30000,\n    volumeThreshold: 10,\n  }\n);\n\n// Events\nvercelBreaker.on('open', () => {\n  console.warn('Vercel circuit OPEN - requests failing fast');\n  alertOps('Vercel circuit breaker opened');\n});\n\nvercelBreaker.on('halfOpen', () => {\n  console.info('Vercel circuit HALF-OPEN - testing recovery');\n});\n\nvercelBreaker.on('close', () => {\n  console.info('Vercel circuit CLOSED - normal operation');\n});\n\n// Usage\nasync function safeVercelCall<T>(fn: () => Promise<T>): Promise<T> {\n  return vercelBreaker.fire(fn);\n}\n```\n\n## Idempotency Keys\n\n```typescript\nimport { v4 as uuidv4 } from 'uuid';\nimport crypto from 'crypto';\n\n// Generate deterministic idempotency key from input\nfunction generateIdempotencyKey(\n  operation: string,\n  params: Record<string, any>\n): string {\n  const data = JSON.stringify({ operation, params });\n  return crypto.createHash('sha256').update(data).digest('hex');\n}\n\n// Or use random key with storage\nclass IdempotencyManager {\n  private store: Map<string, { key: string; expiresAt: Date }> = new Map();\n\n  getOrCreate(operationId: string): string {\n    const existing = this.store.get(operationId);\n    if (existing && existing.expiresAt > new Date()) {\n      return existing.key;\n    }\n\n    const key = uuidv4();\n    this.store.set(operationId, {\n      key,\n      expiresAt: new Date(Date.now() + 24 * 60 * 60 * 1000),\n    });\n    return key;\n  }\n}\n```\n\n## Bulkhead Pattern\n\n```typescript\nimport PQueue from 'p-queue';\n\n// Separate queues for different operations\nconst vercelQueues = {\n  critical: new PQueue({ concurrency: 10 }),\n  normal: new PQueue({ concurrency: 5 }),\n  bulk: new PQueue({ concurrency: 2 }),\n};\n\nasync function prioritizedVercelCall<T>(\n  priority: 'critical' | 'normal' | 'bulk',\n  fn: () => Promise<T>\n): Promise<T> {\n  return vercelQueues[priority].add(fn);\n}\n\n// Usage\nawait prioritizedVercelCall('critical', () =>\n  vercelClient.processPayment(order)\n);\n\nawait prioritizedVercelCall('bulk', () =>\n  vercelClient.syncCatalog(products)\n);\n```\n\n## Timeout Hierarchy\n\n```typescript\nconst TIMEOUT_CONFIG = {\n  connect: 5000,      // Initial connection\n  request: 30000,     // Standard requests\n  upload: 120000,     // File uploads\n  longPoll: 300000,   // Webhook long-polling\n};\n\nasync function timedoutVercelCall<T>(\n  operation: 'connect' | 'request' | 'upload' | 'longPoll',\n  fn: () => Promise<T>\n): Promise<T> {\n  const timeout = TIMEOUT_CONFIG[operation];\n\n  return Promise.race([\n    fn(),\n    new Promise<never>((_, reject) =>\n      setTimeout(() => reject(new Error(`Vercel ${operation} timeout`)), timeout)\n    ),\n  ]);\n}\n```\n\n## Graceful Degradation\n\n```typescript\ninterface VercelFallback {\n  enabled: boolean;\n  data: any;\n  staleness: 'fresh' | 'stale' | 'very_stale';\n}\n\nasync function withVercelFallback<T>(\n  fn: () => Promise<T>,\n  fallbackFn: () => Promise<T>\n): Promise<{ data: T; fallback: boolean }> {\n  try {\n    const data = await fn();\n    // Update cache for future fallback\n    await updateFallbackCache(data);\n    return { data, fallback: false };\n  } catch (error) {\n    console.warn('Vercel failed, using fallback:', error.message);\n    const data = await fallbackFn();\n    return { data, fallback: true };\n  }\n}\n```\n\n## Dead Letter Queue\n\n```typescript\ninterface DeadLetterEntry {\n  id: string;\n  operation: string;\n  payload: any;\n  error: string;\n  attempts: number;\n  lastAttempt: Date;\n}\n\nclass VercelDeadLetterQueue {\n  private queue: DeadLetterEntry[] = [];\n\n  add(entry: Omit<DeadLetterEntry, 'id' | 'lastAttempt'>): void {\n    this.queue.push({\n      ...entry,\n      id: uuidv4(),\n      lastAttempt: new Date(),\n    });\n  }\n\n  async processOne(): Promise<boolean> {\n    const entry = this.queue.shift();\n    if (!entry) return false;\n\n    try {\n      await vercelClient[entry.operation](entry.payload);\n      console.log(`DLQ: Successfully reprocessed ${entry.id}`);\n      return true;\n    } catch (error) {\n      entry.attempts++;\n      entry.lastAttempt = new Date();\n\n      if (entry.attempts < 5) {\n        this.queue.push(entry);\n      } else {\n        console.error(`DLQ: Giving up on ${entry.id} after 5 attempts`);\n        await alertOnPermanentFailure(entry);\n      }\n      return false;\n    }\n  }\n}\n```\n\n## Health Check with Degraded State\n\n```typescript\ntype HealthStatus = 'healthy' | 'degraded' | 'unhealthy';\n\nasync function vercelHealthCheck(): Promise<{\n  status: HealthStatus;\n  details: Record<string, any>;\n}> {\n  const checks = {\n    api: await checkApiConnectivity(),\n    circuitBreaker: vercelBreaker.stats(),\n    dlqSize: deadLetterQueue.size(),\n  };\n\n  const status: HealthStatus =\n    !checks.api.connected ? 'unhealthy' :\n    checks.circuitBreaker.state === 'open' ? 'degraded' :\n    checks.dlqSize > 100 ? 'degraded' :\n    'healthy';\n\n  return { status, details: checks };\n}\n```\n\n## Instructions\n\n### Step 1: Implement Circuit Breaker\nWrap Vercel calls with circuit breaker.\n\n### Step 2: Add Idempotency Keys\nGenerate deterministic keys for operations.\n\n### Step 3: Configure Bulkheads\nSeparate queues for different priorities.\n\n### Step 4: Set Up Dead Letter Queue\nHandle permanent failures gracefully.\n\n## Output\n- Circuit breaker protecting Vercel calls\n- Idempotency preventing duplicates\n- Bulkhead isolation implemented\n- DLQ for failed operations\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Circuit stays open | Threshold too low | Adjust error percentage |\n| Duplicate operations | Missing idempotency | Add idempotency key |\n| Queue full | Rate too high | Increase concurrency |\n| DLQ growing | Persistent failures | Investigate root cause |\n\n## Examples\n\n### Quick Circuit Check\n```typescript\nconst state = vercelBreaker.stats().state;\nconsole.log('Vercel circuit:', state);\n```\n\n## Resources\n- [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)\n- [Opossum Documentation](https://nodeshift.dev/opossum/)\n- [Vercel Reliability Guide](https://vercel.com/docs/reliability)\n\n## Next Steps\nFor policy enforcement, see `vercel-policy-guardrails`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-reliability-patterns/SKILL.md"
    },
    {
      "slug": "vercel-sdk-patterns",
      "name": "vercel-sdk-patterns",
      "description": "Apply production-ready Vercel SDK patterns for TypeScript and Python. Use when implementing Vercel integrations, refactoring SDK usage, or establishing team coding standards for Vercel. Trigger with phrases like \"vercel SDK patterns\", \"vercel best practices\", \"vercel code patterns\", \"idiomatic vercel\". allowed-tools: Read, Write, Edit version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel SDK Patterns\n\n## Overview\nProduction-ready patterns for Vercel SDK usage in TypeScript and Python.\n\n## Prerequisites\n- Completed `vercel-install-auth` setup\n- Familiarity with async/await patterns\n- Understanding of error handling best practices\n\n## Instructions\n\n### Step 1: Implement Singleton Pattern (Recommended)\n```typescript\n// src/vercel/client.ts\nimport { VercelClient } from 'vercel';\n\nlet instance: VercelClient | null = null;\n\nexport function getVercelClient(): VercelClient {\n  if (!instance) {\n    instance = new VercelClient({\n      apiKey: process.env.VERCEL_API_KEY!,\n      // Additional options\n    });\n  }\n  return instance;\n}\n```\n\n### Step 2: Add Error Handling Wrapper\n```typescript\nimport { VercelError } from 'vercel';\n\nasync function safeVercelCall<T>(\n  operation: () => Promise<T>\n): Promise<{ data: T | null; error: Error | null }> {\n  try {\n    const data = await operation();\n    return { data, error: null };\n  } catch (err) {\n    if (err instanceof VercelError) {\n      console.error({\n        code: err.code,\n        message: err.message,\n      });\n    }\n    return { data: null, error: err as Error };\n  }\n}\n```\n\n### Step 3: Implement Retry Logic\n```typescript\nasync function withRetry<T>(\n  operation: () => Promise<T>,\n  maxRetries = 3,\n  backoffMs = 1000\n): Promise<T> {\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      return await operation();\n    } catch (err) {\n      if (attempt === maxRetries) throw err;\n      const delay = backoffMs * Math.pow(2, attempt - 1);\n      await new Promise(r => setTimeout(r, delay));\n    }\n  }\n  throw new Error('Unreachable');\n}\n```\n\n## Output\n- Type-safe client singleton\n- Robust error handling with structured logging\n- Automatic retry with exponential backoff\n- Runtime validation for API responses\n\n## Error Handling\n| Pattern | Use Case | Benefit |\n|---------|----------|---------|\n| Safe wrapper | All API calls | Prevents uncaught exceptions |\n| Retry logic | Transient failures | Improves reliability |\n| Type guards | Response validation | Catches API changes |\n| Logging | All operations | Debugging and monitoring |\n\n## Examples\n\n### Factory Pattern (Multi-tenant)\n```typescript\nconst clients = new Map<string, VercelClient>();\n\nexport function getClientForTenant(tenantId: string): VercelClient {\n  if (!clients.has(tenantId)) {\n    const apiKey = getTenantApiKey(tenantId);\n    clients.set(tenantId, new VercelClient({ apiKey }));\n  }\n  return clients.get(tenantId)!;\n}\n```\n\n### Python Context Manager\n```python\nfrom contextlib import asynccontextmanager\nfrom None import VercelClient\n\n@asynccontextmanager\nasync def get_vercel_client():\n    client = VercelClient()\n    try:\n        yield client\n    finally:\n        await client.close()\n```\n\n### Zod Validation\n```typescript\nimport { z } from 'zod';\n\nconst vercelResponseSchema = z.object({\n  id: z.string(),\n  status: z.enum(['active', 'inactive']),\n  createdAt: z.string().datetime(),\n});\n```\n\n## Resources\n- [Vercel SDK Reference](https://vercel.com/docs/sdk)\n- [Vercel API Types](https://vercel.com/docs/types)\n- [Zod Documentation](https://zod.dev/)\n\n## Next Steps\nApply patterns in `vercel-core-workflow-a` for real-world usage.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-sdk-patterns/SKILL.md"
    },
    {
      "slug": "vercel-security-basics",
      "name": "vercel-security-basics",
      "description": "Apply Vercel security best practices for secrets and access control. Use when securing API keys, implementing least privilege access, or auditing Vercel security configuration. Trigger with phrases like \"vercel security\", \"vercel secrets\", \"secure vercel\", \"vercel API key security\". allowed-tools: Read, Write, Grep version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Security Basics\n\n## Overview\nSecurity best practices for Vercel API keys, tokens, and access control.\n\n## Prerequisites\n- Vercel SDK installed\n- Understanding of environment variables\n- Access to Vercel dashboard\n\n## Instructions\n\n### Step 1: Configure Environment Variables\n```bash\n# .env (NEVER commit to git)\nVERCEL_API_KEY=sk_live_***\nVERCEL_SECRET=***\n\n# .gitignore\n.env\n.env.local\n.env.*.local\n```\n\n### Step 2: Implement Secret Rotation\n```bash\n# 1. Generate new key in Vercel dashboard\n# 2. Update environment variable\nexport VERCEL_API_KEY=\"new_key_here\"\n\n# 3. Verify new key works\ncurl -H \"Authorization: Bearer ${VERCEL_API_KEY}\" \\\n  https://api.vercel.com/health\n\n# 4. Revoke old key in dashboard\n```\n\n### Step 3: Apply Least Privilege\n| Environment | Recommended Scopes |\n|-------------|-------------------|\n| Development | `read, deploy` |\n| Staging | `read, write, deploy` |\n| Production | `read, write, deploy, domains` |\n\n## Output\n- Secure API key storage\n- Environment-specific access controls\n- Audit logging enabled\n\n## Error Handling\n| Security Issue | Detection | Mitigation |\n|----------------|-----------|------------|\n| Exposed API key | Git scanning | Rotate immediately |\n| Excessive scopes | Audit logs | Reduce permissions |\n| Missing rotation | Key age check | Schedule rotation |\n\n## Examples\n\n### Service Account Pattern\n```typescript\nconst clients = {\n  reader: new VercelClient({\n    apiKey: process.env.VERCEL_READ_KEY,\n  }),\n  writer: new VercelClient({\n    apiKey: process.env.VERCEL_WRITE_KEY,\n  }),\n};\n```\n\n### Webhook Signature Verification\n```typescript\nimport crypto from 'crypto';\n\nfunction verifyWebhookSignature(\n  payload: string, signature: string, secret: string\n): boolean {\n  const expected = crypto.createHmac('sha256', secret).update(payload).digest('hex');\n  return crypto.timingSafeEqual(Buffer.from(signature), Buffer.from(expected));\n}\n```\n\n### Security Checklist\n- [ ] API keys in environment variables\n- [ ] `.env` files in `.gitignore`\n- [ ] Different keys for dev/staging/prod\n- [ ] Minimal scopes per environment\n- [ ] Webhook signatures validated\n- [ ] Audit logging enabled\n\n### Audit Logging\n```typescript\ninterface AuditEntry {\n  timestamp: Date;\n  action: string;\n  userId: string;\n  resource: string;\n  result: 'success' | 'failure';\n  metadata?: Record<string, any>;\n}\n\nasync function auditLog(entry: Omit<AuditEntry, 'timestamp'>): Promise<void> {\n  const log: AuditEntry = { ...entry, timestamp: new Date() };\n\n  // Log to Vercel analytics\n  await vercelClient.track('audit', log);\n\n  // Also log locally for compliance\n  console.log('[AUDIT]', JSON.stringify(log));\n}\n\n// Usage\nawait auditLog({\n  action: 'vercel.api.call',\n  userId: currentUser.id,\n  resource: '/v1/resource',\n  result: 'success',\n});\n```\n\n## Resources\n- [Vercel Security Guide](https://vercel.com/docs/security)\n- [Vercel API Scopes](https://vercel.com/docs/scopes)\n\n## Next Steps\nFor production deployment, see `vercel-prod-checklist`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-security-basics/SKILL.md"
    },
    {
      "slug": "vercel-upgrade-migration",
      "name": "vercel-upgrade-migration",
      "description": "Analyze, plan, and execute Vercel SDK upgrades with breaking change detection. Use when upgrading Vercel SDK versions, detecting deprecations, or migrating to new API versions. Trigger with phrases like \"upgrade vercel\", \"vercel migration\", \"vercel breaking changes\", \"update vercel SDK\", \"analyze vercel version\". allowed-tools: Read, Write, Edit, Bash(npm:*), Bash(git:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Upgrade & Migration\n\n## Overview\nGuide for upgrading Vercel SDK versions and handling breaking changes.\n\n## Prerequisites\n- Current Vercel SDK installed\n- Git for version control\n- Test suite available\n- Staging environment\n\n## Instructions\n\n### Step 1: Check Current Version\n```bash\nnpm list vercel\nnpm view vercel version\n```\n\n### Step 2: Review Changelog\n```bash\nopen https://github.com/vercel/vercel/releases\n```\n\n### Step 3: Create Upgrade Branch\n```bash\ngit checkout -b upgrade/vercel-sdk-vX.Y.Z\nnpm install vercel@latest\nnpm test\n```\n\n### Step 4: Handle Breaking Changes\nUpdate import statements, configuration, and method signatures as needed.\n\n## Output\n- Updated SDK version\n- Fixed breaking changes\n- Passing test suite\n- Documented rollback procedure\n\n## Error Handling\n| SDK Version | API Version | Node.js | Breaking Changes |\n|-------------|-------------|---------|------------------|\n| 3.x | 2024-01 | 18+ | Major refactor |\n| 2.x | 2023-06 | 16+ | Auth changes |\n| 1.x | 2022-01 | 14+ | Initial release |\n\n## Examples\n\n### Import Changes\n```typescript\n// Before (v1.x)\nimport { Client } from 'vercel';\n\n// After (v2.x)\nimport { VercelClient } from 'vercel';\n```\n\n### Configuration Changes\n```typescript\n// Before (v1.x)\nconst client = new Client({ key: 'xxx' });\n\n// After (v2.x)\nconst client = new VercelClient({\n  apiKey: 'xxx',\n});\n```\n\n### Rollback Procedure\n```bash\nnpm install vercel@1.x.x --save-exact\n```\n\n### Deprecation Handling\n```typescript\n// Monitor for deprecation warnings in development\nif (process.env.NODE_ENV === 'development') {\n  process.on('warning', (warning) => {\n    if (warning.name === 'DeprecationWarning') {\n      console.warn('[Vercel]', warning.message);\n      // Log to tracking system for proactive updates\n    }\n  });\n}\n\n// Common deprecation patterns to watch for:\n// - Renamed methods: client.oldMethod() -> client.newMethod()\n// - Changed parameters: { key: 'x' } -> { apiKey: 'x' }\n// - Removed features: Check release notes before upgrading\n```\n\n## Resources\n- [Vercel Changelog](https://github.com/vercel/vercel/releases)\n- [Vercel Migration Guide](https://vercel.com/docs/migration)\n\n## Next Steps\nFor CI integration during upgrades, see `vercel-ci-integration`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-upgrade-migration/SKILL.md"
    },
    {
      "slug": "vercel-webhooks-events",
      "name": "vercel-webhooks-events",
      "description": "Implement Vercel webhook signature validation and event handling. Use when setting up webhook endpoints, implementing signature verification, or handling Vercel event notifications securely. Trigger with phrases like \"vercel webhook\", \"vercel events\", \"vercel webhook signature\", \"handle vercel events\", \"vercel notifications\". allowed-tools: Read, Write, Edit, Bash(curl:*) version: 1.0.0 license: MIT author: Jeremy Longshore <jeremy@intentsolutions.io>",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vercel Webhooks & Events\n\n## Overview\nSecurely handle Vercel webhooks with signature validation and replay protection.\n\n## Prerequisites\n- Vercel webhook secret configured\n- HTTPS endpoint accessible from internet\n- Understanding of cryptographic signatures\n- Redis or database for idempotency (optional)\n\n## Webhook Endpoint Setup\n\n### Express.js\n```typescript\nimport express from 'express';\nimport crypto from 'crypto';\n\nconst app = express();\n\n// IMPORTANT: Raw body needed for signature verification\napp.post('/webhooks/vercel',\n  express.raw({ type: 'application/json' }),\n  async (req, res) => {\n    const signature = req.headers['x-vercel-signature'] as string;\n    const timestamp = req.headers['x-vercel-timestamp'] as string;\n\n    if (!verifyVercelSignature(req.body, signature, timestamp)) {\n      return res.status(401).json({ error: 'Invalid signature' });\n    }\n\n    const event = JSON.parse(req.body.toString());\n    await handleVercelEvent(event);\n\n    res.status(200).json({ received: true });\n  }\n);\n```\n\n## Signature Verification\n\n```typescript\nfunction verifyVercelSignature(\n  payload: Buffer,\n  signature: string,\n  timestamp: string\n): boolean {\n  const secret = process.env.VERCEL_WEBHOOK_SECRET!;\n\n  // Reject old timestamps (replay attack protection)\n  const timestampAge = Date.now() - parseInt(timestamp) * 1000;\n  if (timestampAge > 300000) { // 5 minutes\n    console.error('Webhook timestamp too old');\n    return false;\n  }\n\n  // Compute expected signature\n  const signedPayload = `${timestamp}.${payload.toString()}`;\n  const expectedSignature = crypto\n    .createHmac('sha256', secret)\n    .update(signedPayload)\n    .digest('hex');\n\n  // Timing-safe comparison\n  return crypto.timingSafeEqual(\n    Buffer.from(signature),\n    Buffer.from(expectedSignature)\n  );\n}\n```\n\n## Event Handler Pattern\n\n```typescript\ntype VercelEventType = 'resource.created' | 'resource.updated' | 'resource.deleted';\n\ninterface VercelEvent {\n  id: string;\n  type: VercelEventType;\n  data: Record<string, any>;\n  created: string;\n}\n\nconst eventHandlers: Record<VercelEventType, (data: any) => Promise<void>> = {\n  'resource.created': async (data) => { /* handle */ },\n  'resource.updated': async (data) => { /* handle */ },\n  'resource.deleted': async (data) => { /* handle */ }\n};\n\nasync function handleVercelEvent(event: VercelEvent): Promise<void> {\n  const handler = eventHandlers[event.type];\n\n  if (!handler) {\n    console.log(`Unhandled event type: ${event.type}`);\n    return;\n  }\n\n  try {\n    await handler(event.data);\n    console.log(`Processed ${event.type}: ${event.id}`);\n  } catch (error) {\n    console.error(`Failed to process ${event.type}: ${event.id}`, error);\n    throw error; // Rethrow to trigger retry\n  }\n}\n```\n\n## Idempotency Handling\n\n```typescript\nimport { Redis } from 'ioredis';\n\nconst redis = new Redis(process.env.REDIS_URL);\n\nasync function isEventProcessed(eventId: string): Promise<boolean> {\n  const key = `vercel:event:${eventId}`;\n  const exists = await redis.exists(key);\n  return exists === 1;\n}\n\nasync function markEventProcessed(eventId: string): Promise<void> {\n  const key = `vercel:event:${eventId}`;\n  await redis.set(key, '1', 'EX', 86400 * 7); // 7 days TTL\n}\n```\n\n## Webhook Testing\n\n```bash\n# Use Vercel CLI to send test events\nvercel dev\n\n# Or use webhook.site for debugging\ncurl -X POST https://webhook.site/your-uuid \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"type\": \"resource.created\", \"data\": {}}'\n```\n\n## Instructions\n\n### Step 1: Register Webhook Endpoint\nConfigure your webhook URL in the Vercel dashboard.\n\n### Step 2: Implement Signature Verification\nUse the signature verification code to validate incoming webhooks.\n\n### Step 3: Handle Events\nImplement handlers for each event type your application needs.\n\n### Step 4: Add Idempotency\nPrevent duplicate processing with event ID tracking.\n\n## Output\n- Secure webhook endpoint\n- Signature validation enabled\n- Event handlers implemented\n- Replay attack protection active\n\n## Error Handling\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Invalid signature | Wrong secret | Verify webhook secret |\n| Timestamp rejected | Clock drift | Check server time sync |\n| Duplicate events | Missing idempotency | Implement event ID tracking |\n| Handler timeout | Slow processing | Use async queue |\n\n## Examples\n\n### Testing Webhooks Locally\n```bash\n# Use ngrok to expose local server\nngrok http 3000\n\n# Send test webhook\ncurl -X POST https://your-ngrok-url/webhooks/vercel \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"type\": \"test\", \"data\": {}}'\n```\n\n## Resources\n- [Vercel Webhooks Guide](https://vercel.com/docs/webhooks)\n- [Webhook Security Best Practices](https://vercel.com/docs/webhooks/security)\n\n## Next Steps\nFor performance optimization, see `vercel-performance-tuning`.",
      "parentPlugin": {
        "name": "vercel-pack",
        "category": "saas-packs",
        "path": "plugins/saas-packs/vercel-pack",
        "version": "1.0.0",
        "description": "Claude Code skill pack for Vercel (30 skills)"
      },
      "filePath": "plugins/saas-packs/vercel-pack/skills/vercel-webhooks-events/SKILL.md"
    },
    {
      "slug": "version-bumper",
      "name": "version-bumper",
      "description": "Automatically handles semantic version updates across plugin.json and marketplace catalog when user mentions version bump, update version, or release. ensures version consistency in AI assistant-code-plugins repository. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Claude Code Plugins <plugins@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Version Bumper\n\n\n\nThis skill provides automated assistance for version bumper tasks.\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe output is a concrete, repo-ready version bump plan and execution summary, including the computed `old_version ‚Üí new_version`, the exact files updated (plugin `.claude-plugin/plugin.json`, `.claude-plugin/marketplace.extended.json`, regenerated `.claude-plugin/marketplace.json` when applicable), and the next validation commands to run.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Resources\n\n- Project documentation\n- Related skills and commands\n## Purpose\nAutomatically manages semantic version updates for Claude Code plugins, ensuring consistency across plugin.json, marketplace catalog, and git tags - optimized for claude-code-plugins repository workflow.\n\n## Trigger Keywords\n- \"bump version\" or \"update version\"\n- \"release\" or \"new release\"\n- \"major version\" or \"minor version\" or \"patch version\"\n- \"increment version\"\n- \"version update\"\n\n## Semantic Versioning\n\n**Format:** MAJOR.MINOR.PATCH (e.g., 2.1.3)\n\n**Rules:**\n- **MAJOR (2.x.x)** - Breaking changes, incompatible API changes\n- **MINOR (x.1.x)** - New features, backward compatible\n- **PATCH (x.x.3)** - Bug fixes, backward compatible\n\n**Examples:**\n- `1.0.0` ‚Üí `1.0.1` (bug fix)\n- `1.0.0` ‚Üí `1.1.0` (new feature)\n- `1.0.0` ‚Üí `2.0.0` (breaking change)\n\n## Version Bump Process\n\nWhen activated, I will:\n\n1. **Identify Current Version**\n   ```bash\n   # Read plugin version\n   current=$(jq -r '.version' .claude-plugin/plugin.json)\n   echo \"Current version: $current\"\n   ```\n\n2. **Determine Bump Type**\n   - From user request (major/minor/patch)\n   - Or suggest based on changes\n   - Or ask user which type\n\n3. **Calculate New Version**\n   ```bash\n   # Example for patch bump: 1.2.3 ‚Üí 1.2.4\n   IFS='.' read -r major minor patch <<< \"$current\"\n   new_version=\"$major.$minor.$((patch + 1))\"\n   ```\n\n4. **Update Files**\n   - Update `.claude-plugin/plugin.json`\n   - Update `.claude-plugin/marketplace.extended.json`\n   - Sync to `marketplace.json`\n\n5. **Validate Consistency**\n   - Verify all files have same version\n   - Check no other plugins use this version\n   - Validate semver format\n\n6. **Create Git Tag (Optional)**\n   ```bash\n   git tag -a \"v$new_version\" -m \"Release v$new_version\"\n   ```\n\n## Update Locations\n\n### 1. Plugin JSON\n```json\n// .claude-plugin/plugin.json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"1.2.4\",  // ‚Üê Update here\n  ...\n}\n```\n\n### 2. Marketplace Extended\n```json\n// .claude-plugin/marketplace.extended.json\n{\n  \"plugins\": [\n    {\n      \"name\": \"plugin-name\",\n      \"version\": \"1.2.4\",  // ‚Üê Update here\n      ...\n    }\n  ]\n}\n```\n\n### 3. Sync CLI Catalog\n```bash\nnpm run sync-marketplace\n# Regenerates marketplace.json with new version\n```\n\n## Bump Types\n\n### Patch Bump (Bug Fix)\n**When to use:**\n- Bug fixes\n- Documentation updates\n- Minor improvements\n- No new features\n\n**Example:** 1.2.3 ‚Üí 1.2.4\n\n### Minor Bump (New Feature)\n**When to use:**\n- New features\n- New commands/agents/skills\n- Backward compatible changes\n- Enhanced functionality\n\n**Example:** 1.2.3 ‚Üí 1.3.0\n\n### Major Bump (Breaking Change)\n**When to use:**\n- Breaking API changes\n- Incompatible updates\n- Major refactor\n- Removed features\n\n**Example:** 1.2.3 ‚Üí 2.0.0\n\n## Validation Checks\n\nBefore bumping:\n- ‚úÖ Current version is valid semver\n- ‚úÖ New version is higher than current\n- ‚úÖ No other plugin uses new version\n- ‚úÖ All files have same current version\n- ‚úÖ Git working directory is clean (optional)\n\nAfter bumping:\n- ‚úÖ plugin.json updated\n- ‚úÖ marketplace.extended.json updated\n- ‚úÖ marketplace.json synced\n- ‚úÖ All versions consistent\n- ‚úÖ CHANGELOG.md updated (if exists)\n\n## Changelog Management\n\nIf CHANGELOG.md exists, I update it:\n\n```markdown\n# Changelog\n\n## [1.2.4] - 2025-10-16\n\n### Fixed\n- Bug fix description\n- Another fix\n\n## [1.2.3] - 2025-10-15\n...\n```\n\n## Git Integration\n\n### Option 1: Version Commit\n```bash\n# Update version files\ngit add .claude-plugin/plugin.json\ngit add .claude-plugin/marketplace.extended.json\ngit add .claude-plugin/marketplace.json\ngit add CHANGELOG.md  # if exists\n\n# Commit version bump\ngit commit -m \"chore: Bump plugin-name to v1.2.4\"\n```\n\n### Option 2: Version Tag\n```bash\n# Create annotated tag\ngit tag -a \"plugin-name-v1.2.4\" -m \"Release plugin-name v1.2.4\"\n\n# Or for monorepo\ngit tag -a \"v1.2.4\" -m \"Release v1.2.4\"\n\n# Push tag\ngit push origin plugin-name-v1.2.4\n```\n\n## Multi-Plugin Updates\n\nFor repository-wide version bump:\n\n```bash\n# Bump marketplace version\njq '.metadata.version = \"1.0.40\"' .claude-plugin/marketplace.extended.json\n\n# Update all plugins (if needed)\nfor plugin in plugins/*/; do\n  # Update plugin.json\n  # Update marketplace entry\ndone\n```\n\n## Version Consistency Check\n\nI verify:\n```bash\n# Plugin version\nplugin_v=$(jq -r '.version' plugins/category/plugin-name/.claude-plugin/plugin.json)\n\n# Marketplace version\nmarket_v=$(jq -r '.plugins[] | select(.name == \"plugin-name\") | .version' .claude-plugin/marketplace.extended.json)\n\n# Should match\nif [ \"$plugin_v\" != \"$market_v\" ]; then\n  echo \"‚ùå Version mismatch!\"\n  echo \"Plugin: $plugin_v\"\n  echo \"Marketplace: $market_v\"\nfi\n```\n\n## Release Workflow\n\nComplete release process:\n\n1. **Determine Bump Type**\n   - Review changes since last version\n   - Decide: patch/minor/major\n\n2. **Update Version**\n   - Bump plugin.json\n   - Update marketplace catalog\n   - Sync marketplace.json\n\n3. **Update Changelog**\n   - Add release notes\n   - List changes\n   - Include date\n\n4. **Commit Changes**\n   ```bash\n   git add .\n   git commit -m \"chore: Release v1.2.4\"\n   ```\n\n5. **Create Tag**\n   ```bash\n   git tag -a \"v1.2.4\" -m \"Release v1.2.4\"\n   ```\n\n6. **Push**\n   ```bash\n   git push origin main\n   git push origin v1.2.4\n   ```\n\n7. **Validate**\n   - Check GitHub release created\n   - Verify marketplace updated\n   - Test plugin installation\n\n## Output Format\n\n```\nüî¢ VERSION BUMP REPORT\n\nPlugin: plugin-name\nOld Version: 1.2.3\nNew Version: 1.2.4\nBump Type: PATCH\n\n‚úÖ UPDATES COMPLETED:\n1. Updated .claude-plugin/plugin.json ‚Üí v1.2.4\n2. Updated marketplace.extended.json ‚Üí v1.2.4\n3. Synced marketplace.json ‚Üí v1.2.4\n4. Updated CHANGELOG.md\n\nüìä CONSISTENCY CHECK:\n‚úÖ All files have version 1.2.4\n‚úÖ No version conflicts\n‚úÖ Semantic versioning valid\n\nüìù CHANGELOG ENTRY:\n## [1.2.4] - 2025-10-16\n### Fixed\n- Bug fix description\n\nüéØ NEXT STEPS:\n1. Review changes: git diff\n2. Commit: git add . && git commit -m \"chore: Bump to v1.2.4\"\n3. Tag: git tag -a \"v1.2.4\" -m \"Release v1.2.4\"\n4. Push: git push origin main && git push origin v1.2.4\n\n‚ú® Ready to release!\n```\n\n## Repository-Specific Features\n\n**For claude-code-plugins repo:**\n- Handles both plugin and marketplace versions\n- Updates marketplace metadata version\n- Manages plugin count in README\n- Syncs both catalog files\n- Creates proper release tags\n\n## Examples\n\n**User says:** \"Bump the security-scanner plugin to patch version\"\n\n**I automatically:**\n1. Read current version: 1.2.3\n2. Calculate patch bump: 1.2.4\n3. Update plugin.json\n4. Update marketplace.extended.json\n5. Sync marketplace.json\n6. Validate consistency\n7. Report success\n\n**User says:** \"Release version 2.0.0 of plugin-name\"\n\n**I automatically:**\n1. Recognize major version (breaking change)\n2. Update all version files\n3. Update CHANGELOG.md with major release notes\n4. Create git commit\n5. Create git tag v2.0.0\n6. Provide push commands\n\n**User says:** \"Increment version for new feature\"\n\n**I automatically:**\n1. Detect this is a minor bump\n2. Calculate new version (1.2.3 ‚Üí 1.3.0)\n3. Update all files\n4. Add changelog entry\n5. Report completion",
      "parentPlugin": {
        "name": "jeremy-plugin-tool",
        "category": "examples",
        "path": "plugins/examples/jeremy-plugin-tool",
        "version": "2.0.0",
        "description": "Ultimate plugin management toolkit with 4 auto-invoked Skills for creating, validating, auditing, and versioning plugins in the claude-code-plugins marketplace"
      },
      "filePath": "plugins/examples/jeremy-plugin-tool/skills/version-bumper/SKILL.md"
    },
    {
      "slug": "versioning-apis",
      "name": "versioning-apis",
      "description": "Implement API versioning with backward compatibility, deprecation notices, and migration paths. Use when managing API versions and backward compatibility. Trigger with phrases like \"version the API\", \"manage API versions\", or \"handle API versioning\". allowed-tools: Read, Write, Edit, Grep, Glob, Bash(api:version-*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <[email protected]>",
      "license": "MIT",
      "content": "# Api Versioning Manager\n\nThis skill provides automated assistance for api versioning manager tasks.\n\n## Prerequisites\n\nBefore using this skill, ensure you have:\n- API design specifications or requirements documented\n- Development environment with necessary frameworks installed\n- Database or backend services accessible for integration\n- Authentication and authorization strategies defined\n- Testing tools and environments configured\n\n## Instructions\n\n### Step 1: Design API Structure\nPlan the API architecture and endpoints:\n1. Use Read tool to examine existing API specifications from {baseDir}/api-specs/\n2. Define resource models, endpoints, and HTTP methods\n3. Document request/response schemas and data types\n4. Identify authentication and authorization requirements\n5. Plan error handling and validation strategies\n\n### Step 2: Implement API Components\nBuild the API implementation:\n1. Generate boilerplate code using Bash(api:version-*) with framework scaffolding\n2. Implement endpoint handlers with business logic\n3. Add input validation and schema enforcement\n4. Integrate authentication and authorization middleware\n5. Configure database connections and ORM models\n\n### Step 3: Add API Features\nEnhance with production-ready capabilities:\n- Implement rate limiting and throttling policies\n- Add request/response logging with correlation IDs\n- Configure error handling with standardized responses\n- Set up health check and monitoring endpoints\n- Enable CORS and security headers\n\n### Step 4: Test and Document\nValidate API functionality:\n1. Write integration tests covering all endpoints\n2. Generate OpenAPI/Swagger documentation automatically\n3. Create usage examples and authentication guides\n4. Test with various HTTP clients (curl, Postman, REST Client)\n5. Perform load testing to validate performance targets\n\n## Output\n\nThe skill generates production-ready API artifacts:\n\n### API Implementation\nGenerated code structure:\n- `{baseDir}/src/routes/` - Endpoint route definitions\n- `{baseDir}/src/controllers/` - Business logic handlers\n- `{baseDir}/src/models/` - Data models and schemas\n- `{baseDir}/src/middleware/` - Authentication, validation, logging\n- `{baseDir}/src/config/` - Configuration and environment variables\n\n### API Documentation\nComprehensive API docs including:\n- OpenAPI 3.0 specification with complete endpoint definitions\n- Authentication and authorization flow diagrams\n- Request/response examples for all endpoints\n- Error code reference with troubleshooting guidance\n- SDK generation instructions for multiple languages\n\n### Testing Artifacts\nComplete test suite:\n- Unit tests for individual controller functions\n- Integration tests for end-to-end API workflows\n- Load test scripts for performance validation\n- Mock data generators for realistic testing\n- Postman/Insomnia collection for manual testing\n\n### Configuration Files\nProduction-ready configs:\n- Environment variable templates (.env.example)\n- Database migration scripts\n- Docker Compose for local development\n- CI/CD pipeline configuration\n- Monitoring and alerting setup\n\n## Error Handling\n\nCommon issues and solutions:\n\n**Schema Validation Failures**\n- Error: Request body does not match expected schema\n- Solution: Add detailed validation error messages; provide schema documentation; implement request sanitization\n\n**Authentication Errors**\n- Error: Invalid or expired authentication tokens\n- Solution: Implement proper token refresh flows; add clear error messages indicating auth failure reason; document token lifecycle\n\n**Rate Limit Exceeded**\n- Error: API consumer exceeded allowed request rate\n- Solution: Return 429 status with Retry-After header; implement exponential backoff guidance; provide rate limit info in response headers\n\n**Database Connection Issues**\n- Error: Cannot connect to database or query timeout\n- Solution: Implement connection pooling; add health checks; configure proper timeouts; implement circuit breaker pattern for resilience\n\n## Resources\n\n### API Development Frameworks\n- Express.js and Fastify for Node.js APIs\n- Flask and FastAPI for Python APIs\n- Spring Boot for Java APIs\n- Gin and Echo for Go APIs\n\n### API Standards and Best Practices\n- OpenAPI Specification 3.0+ for API documentation\n- JSON:API specification for RESTful API conventions\n- OAuth 2.0 and OpenID Connect for authentication\n- HTTP/2 and HTTP/3 for performance optimization\n\n### Testing and Monitoring Tools\n- Postman and Insomnia for API testing\n- Swagger UI for interactive API documentation\n- Artillery and k6 for load testing\n- Prometheus and Grafana for monitoring\n\n### Security Best Practices\n- OWASP API Security Top 10 guidelines\n- JWT best practices for token-based auth\n- Rate limiting strategies to prevent abuse\n- Input validation and sanitization techniques\n\n## Overview\n\n\nThis skill provides automated assistance for api versioning manager tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.",
      "parentPlugin": {
        "name": "api-versioning-manager",
        "category": "api-development",
        "path": "plugins/api-development/api-versioning-manager",
        "version": "1.0.0",
        "description": "Manage API versions with migration strategies and backward compatibility"
      },
      "filePath": "plugins/api-development/api-versioning-manager/skills/versioning-apis/SKILL.md"
    },
    {
      "slug": "vertex-agent-builder",
      "name": "vertex-agent-builder",
      "description": "Build and deploy production-ready generative AI agents using Vertex AI, Gemini models, and Google Cloud infrastructure with RAG, function calling, and multi-modal capabilities. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@claudecodeplugins.io>",
      "license": "MIT",
      "content": "# Vertex AI Agent Builder\n\nBuild and deploy production-ready agents on Vertex AI with Gemini models, retrieval (RAG), function calling, and operational guardrails (validation, monitoring, cost controls).\n\n## Overview\n\n- Produces an agent scaffold aligned with Vertex AI Agent Engine deployment patterns.\n- Helps choose models/regions, design tool/function interfaces, and wire up retrieval.\n- Includes an evaluation + smoke-test checklist so deployments don‚Äôt regress.\n\n## Prerequisites\n\n- Google Cloud project with Vertex AI API enabled\n- Permissions to deploy/operate Agent Engine runtimes (or a local-only build target)\n- If using RAG: a document source (GCS/BigQuery/Firestore/etc) and an embeddings/index strategy\n- Secrets handled via env vars or Secret Manager (never committed)\n\n## Instructions\n\n1. Clarify the agent‚Äôs job (user intents, inputs/outputs, latency and cost constraints).\n2. Choose model + region and define tool/function interfaces (schemas, error contracts).\n3. Implement retrieval (if needed): chunking, embeddings, index, and a ‚Äúcitation-first‚Äù response format.\n4. Add evaluation: golden prompts, offline checks, and a minimal online smoke test.\n5. Deploy (optional): provide the exact deployment command/config and verify endpoints + permissions.\n6. Add ops: logs/metrics, alerting, quota/cost guardrails, and rollback steps.\n\n## Output\n\n- A Vertex AI agent scaffold (code/config) with clear extension points\n- A retrieval plan (when applicable) and a validation/evaluation checklist\n- Optional: deployment commands and post-deploy health checks\n\n## Error Handling\n\n- Quota/region issues: detect the failing service/quota and propose a scoped fix.\n- Auth failures: identify the principal and missing role; prefer least-privilege remediation.\n- Retrieval failures: validate indexing/embedding dimensions and add fallback behavior.\n- Tool/function errors: enforce structured error responses and add regression tests.\n\n## Examples\n\n**Example: RAG support agent**\n- Request: ‚ÄúDeploy a support bot that answers from our docs with citations.‚Äù\n- Result: ingestion plan, retrieval wiring, evaluation prompts, and a smoke test that verifies citations.\n\n**Example: Multimodal intake agent**\n- Request: ‚ÄúBuild an agent that extracts structured fields from PDFs/images and routes tasks.‚Äù\n- Result: schema-first extraction prompts, tool interface contracts, and validation examples.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- Repo standards (source of truth):\n  - `000-docs/6767-a-SPEC-DR-STND-claude-code-plugins-standard.md`\n  - `000-docs/6767-b-SPEC-DR-STND-claude-skills-standard.md`\n- Vertex AI docs: https://cloud.google.com/vertex-ai/docs\n- Agent Engine docs: https://cloud.google.com/vertex-ai/docs/agent-engine",
      "parentPlugin": {
        "name": "jeremy-vertex-ai",
        "category": "jeremy-vertex-ai",
        "path": "plugins/jeremy-vertex-ai",
        "version": "1.0.0",
        "description": "Comprehensive Vertex AI integration plugin for building generative AI agents with Gemini, Vertex AI Studio, and production deployment on Google Cloud"
      },
      "filePath": "plugins/jeremy-vertex-ai/skills/vertex-agent-builder/SKILL.md"
    },
    {
      "slug": "vertex-ai-media-master",
      "name": "vertex-ai-media-master",
      "description": "Automatic activation for all google vertex ai multimodal operations - Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(general:*), Bash(util:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vertex AI Media Master - Comprehensive Multimodal AI Operations\n\nThis Agent Skill provides comprehensive mastery of Google Vertex AI multimodal capabilities for video, audio, image, and text processing with focus on marketing applications.\n\n\n\n## Overview\n\nThis skill provides automated assistance for the described functionality.\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n## Core Capabilities\n\n### üé• Video Processing (Gemini 2.0/2.5)\n- **Video Understanding**: Process videos up to 6 hours at low resolution or 2 hours at default resolution\n- **2M Context Window**: Gemini 2.5 Pro handles massive video content\n- **Audio Track Processing**: Automatic audio transcription from video\n- **Multi-video Analysis**: Process multiple videos in single request\n- **Video Summarization**: Extract key moments, scenes, and insights\n- **Marketing Use Cases**:\n  - Analyze competitor video ads\n  - Extract highlights from long-form content\n  - Generate video summaries for social media\n  - Transcribe and caption video content\n  - Identify brand mentions and product placements\n\n### üéµ Audio Generation & Processing\n- **Lyria Model (2025)**: Native audio and music generation\n- **Speech-to-Text**: Transcribe audio with speaker diarization\n- **Text-to-Speech**: Generate natural voiceovers\n- **Music Composition**: Background music for campaigns\n- **Audio Enhancement**: Noise reduction and quality improvement\n- **Marketing Use Cases**:\n  - Generate podcast scripts and voiceovers\n  - Create audio ads and radio spots\n  - Produce background music for video campaigns\n  - Transcribe customer interviews\n  - Generate multilingual voiceovers\n\n### üñºÔ∏è Image Generation (Imagen 4 & Gemini 2.5 Flash Image)\n- **Imagen 4**: Highest quality text-to-image generation\n- **Gemini 2.5 Flash Image**: Interleaved image generation with text\n- **Style Transfer**: Apply brand styles to generated images\n- **Product Visualization**: Generate product mockups\n- **Campaign Assets**: Create ad creatives and social media graphics\n- **Marketing Use Cases**:\n  - Generate personalized ad images (Adios solution)\n  - Create social media graphics at scale\n  - Produce product lifestyle images\n  - Generate A/B test variations\n  - Create branded campaign visuals\n\n### üì¢ Marketing Campaign Automation\n- **ViGenAiR**: Convert long-form video ads to short formats automatically\n- **Adios**: Generate personalized ad images tailored to audience context\n- **Campaign Asset Generation**: Photos, soundtracks, voiceovers from prompts\n- **Content Pipeline**: Email copy, blog posts, social media, PMax assets\n- **Catalog Enrichment**: Multi-agent workflow for product onboarding\n- **Marketing Use Cases**:\n  - Automated campaign asset production\n  - Personalized content at scale\n  - Multi-channel content distribution\n  - Product catalog enhancement\n  - Visual merchandising automation\n\n### üîß Technical Implementation\n\n**API Integration:**\n```python\nfrom google.cloud import aiplatform\nfrom vertexai.preview.generative_models import GenerativeModel\n\n# Initialize Vertex AI\naiplatform.init(project=\"your-project\", location=\"us-central1\")\n\n# Gemini 2.5 Pro for video\nmodel = GenerativeModel(\"gemini-2.5-pro\")\n\n# Process video with audio\nresponse = model.generate_content([\n    \"Analyze this video and extract key marketing insights\",\n    video_file,  # Up to 6 hours\n])\n\n# Imagen 4 for image generation\nfrom vertexai.preview.vision_models import ImageGenerationModel\nimagen = ImageGenerationModel.from_pretrained(\"imagen-4\")\nimages = imagen.generate_images(\n    prompt=\"Professional product photo, studio lighting, white background\",\n    number_of_images=4\n)\n```\n\n**Gemini 2.5 Flash Image (Interleaved Generation):**\n```python\n# Generate images within text responses\nmodel = GenerativeModel(\"gemini-2.5-flash-image\")\nresponse = model.generate_content([\n    \"Create a 5-step recipe with images for each step\"\n])\n# Returns text + images interleaved\n```\n\n**Audio Generation (Lyria):**\n```python\nfrom vertexai.preview.audio_models import AudioGenerationModel\nlyria = AudioGenerationModel.from_pretrained(\"lyria\")\naudio = lyria.generate_audio(\n    prompt=\"Upbeat background music for product launch video, 30 seconds\",\n    duration=30\n)\n```\n\n### üìä Marketing Workflow Automation\n\n**1. Multi-Channel Campaign Creation:**\n```python\n# Single prompt generates all assets\ncampaign = model.generate_content([\n    \"\"\"Create a product launch campaign for [product]:\n    - Hero image (1920x1080)\n    - 3 social media graphics (1080x1080)\n    - 30-second video script\n    - Background music description\n    - Email marketing copy\n    - Instagram caption\"\"\"\n])\n```\n\n**2. Video Repurposing Pipeline:**\n```python\n# Long-form to short-form conversion (ViGenAiR approach)\nlong_video = \"gs://bucket/original-ad-60s.mp4\"\nresponse = model.generate_content([\n    f\"Extract 3 engaging 15-second clips from this video for TikTok/Reels\",\n    long_video\n])\n# Auto-generates format-specific versions\n```\n\n**3. Personalized Ad Generation:**\n```python\n# Context-aware image generation (Adios approach)\nfor audience in audiences:\n    ad_image = imagen.generate_images(\n        prompt=f\"Product ad for {product}, targeting {audience.demographics}, {audience.style_preference}\",\n        aspect_ratio=\"16:9\"\n    )\n```\n\n### üéØ Best Practices for Jeremy\n\n**1. Project Setup:**\n```bash\n# Set environment variables\nexport GOOGLE_CLOUD_PROJECT=\"your-project-id\"\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/service-account.json\"\n\n# Install SDK\npip install google-cloud-aiplatform[vision,audio] google-generativeai\n```\n\n**2. Rate Limits & Quotas:**\n- Gemini 2.5 Pro: 2M tokens/min (video processing)\n- Imagen 4: 100 images/min\n- Monitor usage in Cloud Console\n\n**3. Cost Optimization:**\n- Use Gemini 2.5 Flash for faster, cheaper operations\n- Batch image generation requests\n- Cache video embeddings for repeated analysis\n- Use low-resolution video setting when appropriate\n\n**4. Security & Compliance:**\n- Keep API keys in Secret Manager, never in code\n- Use service accounts with minimal permissions\n- Enable VPC Service Controls for data residency\n- Log all API calls for audit trails\n\n### üöÄ Advanced Marketing Use Cases\n\n**1. Campaign Performance Analysis:**\n```python\n# Analyze competitor campaigns\ncompetitor_videos = [\"gs://bucket/competitor1.mp4\", \"gs://bucket/competitor2.mp4\"]\nanalysis = model.generate_content([\n    \"Compare these competitor videos: themes, messaging, CTAs, production quality\",\n    *competitor_videos\n])\n```\n\n**2. Content Localization:**\n```python\n# Generate multilingual campaigns\nfor lang in [\"en\", \"es\", \"fr\", \"de\", \"ja\"]:\n    localized_content = model.generate_content([\n        f\"Translate and culturally adapt this campaign for {lang} market:\",\n        campaign_brief,\n        hero_image\n    ])\n```\n\n**3. A/B Test Generation:**\n```python\n# Generate variations automatically\nvariations = []\nfor style in [\"minimalist\", \"bold\", \"luxury\", \"playful\"]:\n    variation = imagen.generate_images(\n        prompt=f\"Product ad, {style} style, {brand_guidelines}\",\n        number_of_images=1\n    )\n    variations.append(variation)\n```\n\n### üìö Reference Documentation\n\n**Official Documentation:**\n- Vertex AI Multimodal: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview\n- Gemini 2.5 Pro: https://cloud.google.com/vertex-ai/generative-ai/docs/models\n- Imagen 4: https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview\n- Video Understanding: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding\n\n**Marketing Solutions:**\n- GenAI for Marketing: https://github.com/GoogleCloudPlatform/genai-for-marketing\n- ViGenAiR (video repurposing)\n- Adios (personalized ad images)\n\n**Pricing:**\n- Gemini 2.5 Pro: $3.50/1M input tokens, $10.50/1M output tokens\n- Imagen 4: $0.04/image\n- Video processing: Included in Gemini token pricing\n\n## When This Skill Activates\n\nThis skill automatically activates when you mention:\n- Video processing, analysis, or understanding\n- Audio generation, music composition, or voiceovers\n- Image generation, ad creatives, or visual content\n- Marketing campaigns, content automation, or asset production\n- Gemini multimodal capabilities\n- Vertex AI media operations\n- Social media content, email marketing, or PMax campaigns\n\n## Integration with Other Tools\n\n**Google Cloud Services:**\n- Cloud Storage for media asset management\n- BigQuery for campaign analytics\n- Cloud Functions for automation triggers\n- Vertex AI Pipelines for content workflows\n\n**Third-Party Integrations:**\n- Social media APIs (LinkedIn, Twitter, Instagram)\n- Marketing automation platforms (HubSpot, Marketo)\n- CMS integrations (WordPress, Contentful)\n- DAM systems (Bynder, Cloudinary)\n\n## Success Metrics\n\n**Track These KPIs:**\n- Asset generation speed (baseline: 5 images/min)\n- Content approval rate (target: >80%)\n- Campaign personalization scale (target: 1000+ variants)\n- Cost per asset (target: <$0.10/image)\n- Time saved vs manual production (target: 90% reduction)\n\n---\n\n**This skill makes Jeremy a Vertex AI multimodal expert with instant access to video processing, audio generation, image creation, and marketing automation capabilities.**\n\n## Prerequisites\n\n- Access to project files in {baseDir}/\n- Required tools and dependencies installed\n- Understanding of skill functionality\n- Permissions for file operations\n\n## Instructions\n\n1. Identify skill activation trigger and context\n2. Gather required inputs and parameters\n3. Execute skill workflow systematically\n4. Validate outputs meet requirements\n5. Handle errors and edge cases appropriately\n6. Provide clear results and next steps\n\n## Output\n\n- Primary deliverables based on skill purpose\n- Status indicators and success metrics\n- Generated files or configurations\n- Reports and summaries as applicable\n- Recommendations for follow-up actions\n\n## Error Handling\n\nIf execution fails:\n- Verify prerequisites are met\n- Check input parameters and formats\n- Validate file paths and permissions\n- Review error messages for root cause\n- Consult documentation for troubleshooting\n\n## Resources\n\n- Official documentation for related tools\n- Best practices guides\n- Example use cases and templates\n- Community forums and support channels",
      "parentPlugin": {
        "name": "003-jeremy-vertex-ai-media-master",
        "category": "productivity",
        "path": "plugins/productivity/003-jeremy-vertex-ai-media-master",
        "version": "1.0.0",
        "description": "Comprehensive Google Vertex AI multimodal mastery for Jeremy - video processing (6+ hours), audio generation, image creation with Gemini 2.0/2.5 and Imagen 4. Marketing campaign automation, content generation, and media asset production."
      },
      "filePath": "plugins/productivity/003-jeremy-vertex-ai-media-master/skills/vertex-ai-media-master/SKILL.md"
    },
    {
      "slug": "vertex-engine-inspector",
      "name": "vertex-engine-inspector",
      "description": "Inspect and validate Vertex AI Agent Engine deployments including Code Execution Sandbox, Memory Bank, A2A protocol compliance, and security posture. Generates production readiness scores. Use when asked to \"inspect agent engine\" or \"validate depl... Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Grep, Glob, Bash(cmd:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vertex Engine Inspector\n\nThis skill provides automated assistance for vertex engine inspector tasks.\n\n## What This Skill Does\n\nExpert inspector for the Vertex AI Agent Engine managed runtime. Performs comprehensive validation of deployed agents including runtime configuration, security posture, performance settings, A2A protocol compliance, and production readiness scoring.\n\n## When This Skill Activates\n\n### Trigger Phrases\n- \"Inspect Vertex AI Engine agent\"\n- \"Validate Agent Engine deployment\"\n- \"Check Code Execution Sandbox configuration\"\n- \"Verify Memory Bank settings\"\n- \"Monitor agent health\"\n- \"Agent Engine production readiness\"\n- \"A2A protocol compliance check\"\n- \"Agent Engine security audit\"\n\n### Use Cases\n- Pre-production deployment validation\n- Post-deployment health monitoring\n- Security compliance audits\n- Performance optimization reviews\n- Troubleshooting agent issues\n- Configuration drift detection\n\n## Inspection Categories\n\n### 1. Runtime Configuration ‚úÖ\n- Model selection (Gemini 2.5 Pro/Flash)\n- Tools enabled (Code Execution, Memory Bank, custom)\n- VPC configuration\n- Resource allocation\n- Scaling policies\n\n### 2. Code Execution Sandbox üîí\n- **Security**: Isolated environment, no external network access\n- **State Persistence**: TTL validation (1-14 days)\n- **IAM**: Least privilege permissions\n- **Performance**: Timeout and resource limits\n- **Concurrent Executions**: Max concurrent code runs\n\n**Critical Checks**:\n```\n‚úÖ State TTL between 7-14 days (optimal for production)\n‚úÖ Sandbox type is SECURE_ISOLATED\n‚úÖ IAM permissions limited to required GCP services only\n‚úÖ Timeout configured appropriately\n‚ö†Ô∏è State TTL < 7 days may cause premature session loss\n‚ùå State TTL > 14 days not allowed by Agent Engine\n```\n\n### 3. Memory Bank Configuration üß†\n- **Enabled Status**: Persistent memory active\n- **Retention Policy**: Max memories, retention days\n- **Storage Backend**: Firestore encryption & region\n- **Query Performance**: Indexing, caching, latency\n- **Auto-Cleanup**: Quota management\n\n**Critical Checks**:\n```\n‚úÖ Max memories >= 100 (prevents conversation truncation)\n‚úÖ Indexing enabled (fast query performance)\n‚úÖ Auto-cleanup enabled (prevents quota exhaustion)\n‚úÖ Encrypted at rest (Firestore default)\n‚ö†Ô∏è Low memory limit may truncate long conversations\n```\n\n### 4. A2A Protocol Compliance üîó\n- **AgentCard**: Available at `/.well-known/agent-card`\n- **Task API**: `POST /v1/tasks:send` responds correctly\n- **Status API**: `GET /v1/tasks/{task_id}` accessible\n- **Protocol Version**: 1.0 compliance\n- **Required Fields**: name, description, tools, version\n\n**Compliance Report**:\n```\n‚úÖ AgentCard accessible and valid\n‚úÖ Task submission API functional\n‚úÖ Status polling API functional\n‚úÖ Protocol version 1.0\n‚ùå Missing AgentCard fields: [...]\n‚ùå Task API not responding (check IAM/networking)\n```\n\n### 5. Security Posture üõ°Ô∏è\n- **IAM Roles**: Least privilege validation\n- **VPC Service Controls**: Perimeter protection\n- **Model Armor**: Prompt injection protection\n- **Encryption**: At-rest and in-transit\n- **Service Account**: Proper configuration\n- **Secret Management**: No hardcoded credentials\n\n**Security Score**:\n```\nüü¢ SECURE (90-100%): Production ready\nüü° NEEDS ATTENTION (70-89%): Address issues before prod\nüî¥ INSECURE (<70%): Do not deploy to production\n```\n\n### 6. Performance Metrics üìä\n- **Auto-Scaling**: Min/max instances configured\n- **Resource Limits**: CPU, memory appropriate\n- **Latency**: P50, P95, P99 within SLOs\n- **Throughput**: Requests per second\n- **Token Usage**: Cost tracking\n- **Error Rate**: < 5% target\n\n**Health Status**:\n```\nüü¢ HEALTHY: Error rate < 5%, latency < 3s (p95)\nüü° DEGRADED: Error rate 5-10% or latency 3-5s\nüî¥ UNHEALTHY: Error rate > 10% or latency > 5s\n```\n\n### 7. Monitoring & Observability üìà\n- **Cloud Monitoring**: Dashboards configured\n- **Alerting**: Policies for errors, latency, costs\n- **Logging**: Structured logs aggregated\n- **Tracing**: OpenTelemetry enabled\n- **Error Tracking**: Cloud Error Reporting\n\n**Observability Score**:\n```\n‚úÖ All 5 pillars configured: Metrics, Logs, Traces, Alerts, Dashboards\n‚ö†Ô∏è Missing alerts for critical scenarios\n‚ùå No monitoring configured (production blocker)\n```\n\n## Production Readiness Scoring\n\n### Scoring Matrix\n\n| Category | Weight | Checks |\n|----------|--------|--------|\n| Security | 30% | 6 checks (IAM, VPC-SC, encryption, etc.) |\n| Performance | 25% | 6 checks (scaling, limits, SLOs, etc.) |\n| Monitoring | 20% | 6 checks (dashboards, alerts, logs, etc.) |\n| Compliance | 15% | 5 checks (audit logs, DR, privacy, etc.) |\n| Reliability | 10% | 5 checks (multi-region, failover, etc.) |\n\n### Overall Readiness Status\n\n```\nüü¢ PRODUCTION READY (85-100%)\n   - All critical checks passed\n   - Minor optimizations recommended\n   - Safe to deploy\n\nüü° NEEDS IMPROVEMENT (70-84%)\n   - Some important checks failed\n   - Address issues before production\n   - Staging deployment acceptable\n\nüî¥ NOT READY (<70%)\n   - Critical failures present\n   - Do not deploy to production\n   - Fix blocking issues first\n```\n\n## Inspection Workflow\n\n### Phase 1: Configuration Analysis\n```\n1. Connect to Agent Engine\n2. Retrieve agent metadata\n3. Parse runtime configuration\n4. Extract Code Execution settings\n5. Extract Memory Bank settings\n6. Document VPC configuration\n```\n\n### Phase 2: Protocol Validation\n```\n1. Test AgentCard endpoint\n2. Validate AgentCard structure\n3. Test Task API (POST /v1/tasks:send)\n4. Test Status API (GET /v1/tasks/{id})\n5. Verify A2A protocol version\n```\n\n### Phase 3: Security Audit\n```\n1. Review IAM roles and permissions\n2. Check VPC Service Controls\n3. Validate encryption settings\n4. Scan for hardcoded secrets\n5. Verify Model Armor enabled\n6. Assess service account security\n```\n\n### Phase 4: Performance Analysis\n```\n1. Query Cloud Monitoring metrics\n2. Calculate error rate (last 24h)\n3. Analyze latency percentiles\n4. Review token usage and costs\n5. Check auto-scaling behavior\n6. Validate resource limits\n```\n\n### Phase 5: Production Readiness\n```\n1. Run all checklist items (28 checks)\n2. Calculate category scores\n3. Calculate overall score\n4. Determine readiness status\n5. Generate recommendations\n6. Create action plan\n```\n\n## Tool Permissions\n\n**Read-only inspection** - Cannot modify configurations:\n- **Read**: Analyze agent configuration files\n- **Grep**: Search for security issues\n- **Glob**: Find related configuration\n- **Bash**: Query GCP APIs (read-only)\n\n## Example Inspection Report\n\n```yaml\nAgent ID: gcp-deployer-agent\nDeployment Status: RUNNING\nInspection Date: 2025-12-09\n\nRuntime Configuration:\n  Model: gemini-2.5-flash\n  Code Execution: ‚úÖ Enabled (TTL: 14 days)\n  Memory Bank: ‚úÖ Enabled (retention: 90 days)\n  VPC: ‚úÖ Configured (private-vpc-prod)\n\nA2A Protocol Compliance:\n  AgentCard: ‚úÖ Valid\n  Task API: ‚úÖ Functional\n  Status API: ‚úÖ Functional\n  Protocol Version: 1.0\n\nSecurity Posture:\n  IAM: ‚úÖ Least privilege (score: 95%)\n  VPC-SC: ‚úÖ Enabled\n  Model Armor: ‚úÖ Enabled\n  Encryption: ‚úÖ At-rest & in-transit\n  Overall: üü¢ SECURE (92%)\n\nPerformance Metrics (24h):\n  Request Count: 12,450\n  Error Rate: 2.3% üü¢\n  Latency (p95): 1,850ms üü¢\n  Token Usage: 450K tokens\n  Cost Estimate: $12.50/day\n\nProduction Readiness:\n  Security: 92% (28/30 points)\n  Performance: 88% (22/25 points)\n  Monitoring: 95% (19/20 points)\n  Compliance: 80% (12/15 points)\n  Reliability: 70% (7/10 points)\n\n  Overall Score: 87% üü¢ PRODUCTION READY\n\nRecommendations:\n  1. Enable multi-region deployment (reliability +10%)\n  2. Configure automated backups (compliance +5%)\n  3. Add circuit breaker pattern (reliability +5%)\n  4. Optimize memory bank indexing (performance +3%)\n```\n\n## Integration with Other Plugins\n\n### Works with jeremy-adk-orchestrator\n- Orchestrator deploys agents\n- Inspector validates deployments\n- Feedback loop for optimization\n\n### Works with jeremy-vertex-validator\n- Validator checks code before deployment\n- Inspector validates runtime after deployment\n- Complementary pre/post checks\n\n### Works with jeremy-adk-terraform\n- Terraform provisions infrastructure\n- Inspector validates provisioned agents\n- Ensures IaC matches runtime\n\n## Troubleshooting Guide\n\n### Issue: Agent not responding\n**Inspector checks**:\n- VPC configuration allows traffic\n- IAM permissions correct\n- Agent Engine status is RUNNING\n- No quota limits exceeded\n\n### Issue: High error rate\n**Inspector checks**:\n- Model configuration appropriate\n- Resource limits not exceeded\n- Code Execution sandbox not timing out\n- Memory Bank not quota-exhausted\n\n### Issue: Slow response times\n**Inspector checks**:\n- Auto-scaling configured\n- Code Execution TTL appropriate\n- Memory Bank indexing enabled\n- Caching strategy implemented\n\n## Version History\n\n- **1.0.0** (2025): Initial release with Agent Engine GA support, Code Execution Sandbox, Memory Bank, A2A protocol validation\n\n## References\n\n- Agent Engine: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\n- Code Execution: https://cloud.google.com/agent-builder/agent-engine/code-execution/overview\n- Memory Bank: https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/overview\n- A2A Protocol: https://google.github.io/adk-docs/a2a/\n\n## Overview\n\n\nThis skill provides automated assistance for vertex engine inspector tasks.\nThis skill provides automated assistance for the described functionality.\n\n## Prerequisites\n\n- Appropriate file access permissions\n- Required dependencies installed\n\n## Instructions\n\n1. Invoke this skill when the trigger conditions are met\n2. Provide necessary context and parameters\n3. Review the generated output\n4. Apply modifications as needed\n\n## Output\n\nThe skill produces structured output relevant to the task.\n\n## Error Handling\n\n- Invalid input: Prompts for correction\n- Missing dependencies: Lists required components\n- Permission errors: Suggests remediation steps\n\n## Examples\n\nExample usage patterns will be demonstrated in context.\n\n## Resources\n\n- Project documentation\n- Related skills and commands",
      "parentPlugin": {
        "name": "jeremy-vertex-engine",
        "category": "ai-ml",
        "path": "plugins/ai-ml/jeremy-vertex-engine",
        "version": "1.0.0",
        "description": "Vertex AI Agent Engine deployment inspector and runtime validator"
      },
      "filePath": "plugins/ai-ml/jeremy-vertex-engine/skills/vertex-engine-inspector/SKILL.md"
    },
    {
      "slug": "vertex-infra-expert",
      "name": "vertex-infra-expert",
      "description": "Use when provisioning Vertex AI infrastructure with Terraform. Trigger with phrases like \"vertex ai terraform\", \"deploy gemini terraform\", \"model garden infrastructure\", \"vertex ai endpoints terraform\", or \"vector search terraform\". Provisions Model Garden models, Gemini endpoints, vector search indices, ML pipelines, and production AI services with encryption and auto-scaling. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(terraform:*), Bash(gcloud:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# Vertex Infra Expert\n\nThis skill provides automated assistance for vertex infra expert tasks.\n\n## Overview\n\nProvision Vertex AI infrastructure with Terraform (endpoints, deployed models, vector search indices, pipelines) with production guardrails: encryption, autoscaling, IAM least privilege, and operational validation steps. Use this skill to generate a minimal working Terraform baseline and iterate toward enterprise-ready deployments.\n\n## Prerequisites\n\nBefore using this skill, ensure:\n- Google Cloud project with Vertex AI API enabled\n- Terraform 1.0+ installed\n- gcloud CLI authenticated with appropriate permissions\n- Understanding of Vertex AI services and ML models\n- KMS keys created for encryption (if required)\n- GCS buckets for model artifacts and embeddings\n\n## Instructions\n\n1. **Define AI Services**: Identify required Vertex AI components (endpoints, vector search, pipelines)\n2. **Configure Terraform**: Set up backend and define project variables\n3. **Provision Endpoints**: Deploy Gemini or custom model endpoints with auto-scaling\n4. **Set Up Vector Search**: Create indices for embeddings with appropriate dimensions\n5. **Configure Encryption**: Apply KMS encryption to endpoints and data\n6. **Implement Monitoring**: Set up Cloud Monitoring for model performance\n7. **Apply IAM Policies**: Grant least privilege access to AI services\n8. **Validate Deployment**: Test endpoints and verify model availability\n\n## Examples\n\n**Example: Deploy a Gemini endpoint + vector search index**\n- Inputs: `project_id`, `region`, KMS key (optional), embedding dimensions, and autoscaling bounds.\n- Outputs: Terraform for endpoint + deployed model + index, and a smoke test that queries the endpoint and verifies index build status.\n\n## Output\n\n**Gemini Model Endpoint:**\n```hcl\n# {baseDir}/terraform/vertex-endpoints.tf\nresource \"google_vertex_ai_endpoint\" \"gemini_endpoint\" {\n  name         = \"gemini-25-flash-endpoint\"\n  display_name = \"Gemini 2.5 Flash Production\"\n  location     = var.region\n\n  encryption_spec {\n    kms_key_name = google_kms_crypto_key.vertex_key.id\n  }\n}\n\nresource \"google_vertex_ai_deployed_model\" \"gemini_deployment\" {\n  endpoint = google_vertex_ai_endpoint.gemini_endpoint.id\n  model    = \"publishers/google/models/gemini-2.5-flash\"\n\n  automatic_resources {\n    min_replica_count = 1\n    max_replica_count = 5\n  }\n}\n```\n\n**Vector Search Index:**\n```hcl\nresource \"google_vertex_ai_index\" \"embeddings_index\" {\n  display_name = \"production-embeddings\"\n  location     = var.region\n\n  metadata {\n    contents_delta_uri = \"gs://${google_storage_bucket.embeddings.name}/index\"\n    config {\n      dimensions = 768\n      approximate_neighbors_count = 150\n      distance_measure_type = \"DOT_PRODUCT_DISTANCE\"\n\n      algorithm_config {\n        tree_ah_config {\n          leaf_node_embedding_count = 1000\n          leaf_nodes_to_search_percent = 10\n        }\n      }\n    }\n  }\n}\n```\n\n## Error Handling\n\n**API Not Enabled**\n- Error: \"Vertex AI API has not been used in project\"\n- Solution: Enable with `gcloud services enable aiplatform.googleapis.com`\n\n**Model Not Found**\n- Error: \"Model publishers/google/models/... not found\"\n- Solution: Verify model ID and region availability\n\n**Quota Exceeded**\n- Error: \"Quota exceeded for resource\"\n- Solution: Request quota increase or reduce replica count\n\n**KMS Key Access Denied**\n- Error: \"Permission denied on KMS key\"\n- Solution: Grant cloudkms.cryptoKeyEncrypterDecrypter role to Vertex AI service account\n\n**Vector Search Build Failed**\n- Error: \"Index build failed\"\n- Solution: Check GCS bucket permissions and embedding format\n\n## Resources\n\n- Vertex AI Terraform: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/vertex_ai_endpoint\n- Vertex AI documentation: https://cloud.google.com/vertex-ai/docs\n- Model Garden: https://cloud.google.com/model-garden\n- Vector Search guide: https://cloud.google.com/vertex-ai/docs/vector-search\n- Terraform examples in {baseDir}/vertex-examples/",
      "parentPlugin": {
        "name": "jeremy-vertex-terraform",
        "category": "devops",
        "path": "plugins/devops/jeremy-vertex-terraform",
        "version": "1.0.0",
        "description": "Terraform configurations for Vertex AI platform and Agent Engine"
      },
      "filePath": "plugins/devops/jeremy-vertex-terraform/skills/vertex-infra-expert/SKILL.md"
    },
    {
      "slug": "yaml-master",
      "name": "yaml-master",
      "description": "Proactive YAML intelligence: automatically activates when working with YAML files. Use when appropriate context detected. Trigger with relevant phrases based on skill purpose. allowed-tools: Read, Write, Edit, Grep, Glob, Bash(general:*), Bash(util:*) version: 1.0.0 author: Jeremy Longshore <jeremy@intentsolutions.io> license: MIT",
      "allowedTools": [],
      "version": "1.0.0",
      "author": "Jeremy Longshore <jeremy@intentsolutions.io>",
      "license": "MIT",
      "content": "# YAML Master\n\nProactive YAML intelligence: validate syntax, enforce consistent formatting, and keep configuration files schema-correct (Kubernetes, GitHub Actions, Docker Compose, and similar).\n\n## Overview\n\nThis skill activates when working with `.yml`/`.yaml` files to detect structural issues early (indentation, anchors, type mismatches), and to produce safe, minimal edits that keep CI/config tooling happy.\n\n## Prerequisites\n\n- The YAML file(s) to inspect and their intended target (e.g., Kubernetes, GitHub Actions, Compose)\n- Any relevant schema or constraints (when available)\n- Permission to edit the file(s) (or to propose a patch)\n\n## Instructions\n\n1. Parse and validate YAML syntax (identify the first breaking error and its location).\n2. Normalize formatting (indentation, quoting) without changing semantics.\n3. Validate structure against the target system‚Äôs expectations (keys, types, required fields).\n4. Identify risky patterns (duplicate keys, ambiguous scalars, anchors used incorrectly).\n5. Output a minimal patch plus a short validation checklist (what to run next).\n\n## Output\n\n- Corrected YAML with minimal diffs\n- A concise list of issues found (syntax vs schema vs best practice)\n- Follow-up validation commands appropriate for the target (e.g., `kubectl apply --dry-run=client`, CI lint)\n\n## Error Handling\n\n- If the schema/target is unknown, ask for the target system and apply syntax-only fixes first.\n- If the YAML is valid but tooling still fails, surface the exact downstream error and reconcile expectations.\n\n## Examples\n\n**Example: Fix an indentation/syntax error**\n- Input: a workflow with a mis-indented `steps:` block.\n- Output: corrected indentation and a note on which job/step was affected.\n\n**Example: Convert JSON to YAML safely**\n- Input: a JSON config blob.\n- Output: YAML with explicit quoting where necessary to avoid type surprises.\n\n## Resources\n\n- Full detailed guide (kept for reference): `{baseDir}/references/SKILL.full.md`\n- YAML spec: https://yaml.org/spec/\n- GitHub Actions workflow syntax: https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions",
      "parentPlugin": {
        "name": "002-jeremy-yaml-master-agent",
        "category": "productivity",
        "path": "plugins/productivity/002-jeremy-yaml-master-agent",
        "version": "1.0.0",
        "description": "Intelligent YAML validation, generation, and transformation agent with schema inference, linting, and format conversion capabilities"
      },
      "filePath": "plugins/productivity/002-jeremy-yaml-master-agent/skills/yaml-master/SKILL.md"
    }
  ],
  "count": 305,
  "generatedAt": "2025-12-29T20:56:34.121Z",
  "categories": [
    "ai-ml",
    "api-development",
    "automation",
    "business-tools",
    "community",
    "crypto",
    "database",
    "devops",
    "examples",
    "jeremy-google-adk",
    "jeremy-vertex-ai",
    "packages",
    "performance",
    "productivity",
    "saas-packs",
    "security",
    "skill-enhancers",
    "testing"
  ],
  "allowedToolsUsed": [
    "\"Read",
    "Bash(analysis:*)\"",
    "Bash(artillery:*)",
    "Bash(audit:*)\"",
    "Bash(awk:*)",
    "Bash(ci:*)",
    "Bash(curl:*)",
    "Bash(date:*)\"",
    "Bash(general:*)",
    "Bash(gh:*)",
    "Bash(git:*)",
    "Bash(grep:*)",
    "Bash(iostat:*)",
    "Bash(jmeter:*)",
    "Bash(k6:*)",
    "Bash(lighthouse:*)",
    "Bash(logs:*)",
    "Bash(memory:*)\"",
    "Bash(metrics:*)",
    "Bash(metrics:*)\"",
    "Bash(monitoring:*)",
    "Bash(monitoring:*)\"",
    "Bash(npm:*)",
    "Bash(performance:*)",
    "Bash(performance:*)\"",
    "Bash(ping:*)",
    "Bash(profiling:*)",
    "Bash(prometheus:*)",
    "Bash(ps:*)",
    "Bash(python:*)",
    "Bash(rum:*)\"",
    "Bash(scan:*)",
    "Bash(security:*)",
    "Bash(system:*)\"",
    "Bash(testing:*)\"",
    "Bash(top:*)",
    "Bash(traceroute:*)",
    "Bash(util:*)\"",
    "Bash(vmstat:*)",
    "Bash(webpack:*)",
    "Edit",
    "Glob",
    "Glob\"",
    "Grep",
    "Grep\"",
    "WebFetch",
    "WebSearch",
    "Write"
  ]
}