---
name: {{ company }}-incident-runbook
description: |
  Detect and respond to {{ display_name }} incidents with triage, mitigation, and postmortem.
  Use when responding to {{ display_name }} outages or service degradation.
  Trigger with phrases like "{{ company }} incident", "{{ company }} outage",
  "{{ company }} down", "{{ company }} on-call".
allowed-tools: Read, Bash({{ company }}:*), Grep
version: 1.0.0
license: MIT
author: Jeremy Longshore <jeremy@intentsolutions.io>
---

# {{ display_name }} Incident Runbook

## Overview
Rapid incident response procedures for {{ display_name }}-related outages.

## Prerequisites
- {{ company }}-install-auth completed
- Access to Kubernetes/infrastructure
- Monitoring dashboards available
- On-call escalation contacts

## Instructions

### Step 1: Assess Severity Levels

| Level | Definition | Response Time | Examples |
|-------|------------|---------------|----------|
| P1 | Complete outage | < 15 min | {{ display_name }} API unreachable |
| P2 | Degraded service | < 1 hour | High latency, partial failures |
| P3 | Minor impact | < 4 hours | Webhook delays, non-critical errors |
| P4 | No user impact | Next business day | Monitoring gaps |

## Quick Triage

```bash
# 1. Check {{ display_name }} status
curl -s {{ status_url | default('https://status.' + company + '.com/api/v2/status.json') }} | jq

# 2. Check our integration health
curl -s https://api.yourapp.com/health | jq '.services.{{ company }}'

# 3. Check error rate (last 5 min)
curl -s localhost:9090/api/v1/query?query=rate({{ company }}_errors_total[5m])

# 4. Recent error logs
kubectl logs -l app={{ company }}-integration --since=5m | grep -i error | tail -20
```

## Decision Tree

```
{{ display_name }} API returning errors?
â”œâ”€ YES: Is status.{{ company }}.com showing incident?
â”‚   â”œâ”€ YES â†’ Wait for {{ display_name }} to resolve. Enable fallback.
â”‚   â””â”€ NO â†’ Our integration issue. Check credentials, config.
â””â”€ NO: Is our service healthy?
    â”œâ”€ YES â†’ Likely resolved or intermittent. Monitor.
    â””â”€ NO â†’ Our infrastructure issue. Check pods, memory, network.
```

## Immediate Actions by Error Type

### 401/403 - Authentication
```bash
# Verify API key is set
kubectl get secret {{ company }}-secrets -o jsonpath='{.data.api-key}' | base64 -d

# Check if key was rotated
# â†’ Verify in {{ display_name }} dashboard

# Remediation: Update secret and restart pods
kubectl create secret generic {{ company }}-secrets --from-literal=api-key=NEW_KEY --dry-run=client -o yaml | kubectl apply -f -
kubectl rollout restart deployment/{{ company }}-integration
```

### 429 - Rate Limited
```bash
# Check rate limit headers
curl -v {{ api_url | default('https://api.' + company + '.com') }} 2>&1 | grep -i rate

# Enable request queuing
kubectl set env deployment/{{ company }}-integration RATE_LIMIT_MODE=queue

# Long-term: Contact {{ display_name }} for limit increase
```

### 500/503 - {{ display_name }} Errors
```bash
# Enable graceful degradation
kubectl set env deployment/{{ company }}-integration {{ company | upper }}_FALLBACK=true

# Notify users of degraded service
# Update status page

# Monitor {{ display_name }} status for resolution
```

## Communication Templates

### Internal (Slack)
```
ðŸ”´ P1 INCIDENT: {{ display_name }} Integration
Status: INVESTIGATING
Impact: [Describe user impact]
Current action: [What you're doing]
Next update: [Time]
Incident commander: @[name]
```

### External (Status Page)
```
{{ display_name }} Integration Issue

We're experiencing issues with our {{ display_name }} integration.
Some users may experience [specific impact].

We're actively investigating and will provide updates.

Last updated: [timestamp]
```

## Post-Incident

### Evidence Collection
```bash
# Generate debug bundle
./scripts/{{ company }}-debug-bundle.sh

# Export relevant logs
kubectl logs -l app={{ company }}-integration --since=1h > incident-logs.txt

# Capture metrics
curl "localhost:9090/api/v1/query_range?query={{ company }}_errors_total&start=2h" > metrics.json
```

### Postmortem Template
```markdown
## Incident: {{ display_name }} [Error Type]
**Date:** YYYY-MM-DD
**Duration:** X hours Y minutes
**Severity:** P[1-4]

### Summary
[1-2 sentence description]

### Timeline
- HH:MM - [Event]
- HH:MM - [Event]

### Root Cause
[Technical explanation]

### Impact
- Users affected: N
- Revenue impact: $X

### Action Items
- [ ] [Preventive measure] - Owner - Due date
```

## Output
- Incident triaged and severity assigned
- Root cause identified or escalated
- Mitigation applied and verified
- Postmortem documented with action items

## Error Handling

| Error | Cause | Solution |
|-------|-------|----------|
| Can't access logs | RBAC permissions | Use break-glass access or escalate |
| Status page disagrees | Stale status or different issue | Check multiple sources, trust metrics |
| Rollback failed | Bad previous version | Deploy known-good version manually |
| Unable to reach {{ display_name }} | Network/DNS issue | Try alternative endpoints, check DNS |

## Examples

### PagerDuty Automation

```yaml
# Automatic {{ display_name }} incident creation
on_call:
  service: {{ company }}-integration
  escalation_policy: engineering-on-call
  severity_mapping:
    P1: critical
    P2: high
    P3: warning
    P4: info
```

## Resources
- [{{ display_name }} Status Page]({{ status_url | default('https://status.' + company + '.com') }})
- [Incident Response Best Practices]({{ docs_url | default('https://docs.' + company + '.com') }}/incident-response)
- [Support Contact]({{ docs_url | default('https://docs.' + company + '.com') }}/support)

## Next Steps
For data handling, see `{{ company }}-data-handling`.
